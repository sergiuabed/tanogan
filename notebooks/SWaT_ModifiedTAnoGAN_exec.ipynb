{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piEBuS7wOuFz"
      },
      "source": [
        "Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-THl5TZeOkOM",
        "outputId": "4c09f740-3c74-452d-ddb3-fbcbb4cc3472"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O1zwxgHO04o"
      },
      "source": [
        "# Import GitHub repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4Ox58ejO4y4",
        "outputId": "74dad01a-c008-458f-8ae3-b0a5461e8a17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tanogan'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 41 (delta 18), reused 34 (delta 11), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (41/41), 8.43 KiB | 4.21 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir('tanogan'):\n",
        "  !git clone https://github.com/sergiuabed/tanogan\n",
        "else:\n",
        "  %cd tanogan/\n",
        "  !git pull\n",
        "  %cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1bV4WRPZD8S"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "R9zT1LVaYpj-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tanogan.data_utils.data_utils import get_df_action, get_standardizer, standardization\n",
        "from tanogan.architecture import init_generator, init_discriminator, init_encoder\n",
        "from tanogan.training import adversarial_training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlWUp5aRPmHN"
      },
      "source": [
        "# Load Dataset\n",
        "Load dataset locally from Google Drive\n",
        "\n",
        "The dataset comes from a SWaT testbed, specifically on the physical properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zCAar3jmPp47"
      },
      "outputs": [],
      "source": [
        "if not os.path.isdir('SWaT_physical_2015'):\n",
        "  !cp -R /content/drive/MyDrive/ml-applications/TimeSeriesAnomalyDetection_project/SWaT_physical_2015/ .\n",
        "\n",
        "path = './SWaT_physical_2015'\n",
        "\n",
        "if os.path.isfile(path + '/normal_and_attacked_dfs.h5'):\n",
        "  store = pd.HDFStore(path + '/normal_and_attacked_dfs.h5')\n",
        "\n",
        "  df_normal = store['df_normal']\n",
        "  normal_labels = store['normal_labels']\n",
        "  df_attack = store['df_attack']\n",
        "  attack_labels = store['attack_labels']\n",
        "else:\n",
        "  # Load normal (non-anomalous) data\n",
        "  filepath_xlsx_normal = path + '/SWaT_Dataset_Normal_v1.xlsx'\n",
        "  df_normal = pd.read_excel(filepath_xlsx_normal)\n",
        "\n",
        "  column_names = df_normal.iloc[0].copy() #first row contains the names of the columns\n",
        "  df_normal = df_normal.iloc[1:]  #drop first row\n",
        "\n",
        "  df_normal.columns = column_names  #assign column names\n",
        "  df_normal.index = df_normal[' Timestamp'].copy()\n",
        "\n",
        "  df_normal.drop(\" Timestamp\", axis=1, inplace=True)\n",
        "\n",
        "  # Load anomalous data\n",
        "  filepath_xlsx_attack = path + '/SWaT_Dataset_Attack_v0.xlsx'\n",
        "  df_attack = pd.read_excel(filepath_xlsx_attack)\n",
        "\n",
        "  column_names = df_attack.iloc[0].copy() #first row contains the names of the columns\n",
        "  df_attack = df_attack.iloc[1:]  #drop first row\n",
        "\n",
        "  df_attack.columns = column_names  #assign column names\n",
        "  df_attack.index = df_attack[' Timestamp'].copy()\n",
        "\n",
        "  df_attack.drop(\" Timestamp\", axis=1, inplace=True)\n",
        "\n",
        "  # Split labels from data\n",
        "  normal_labels = df_normal['Normal/Attack']\n",
        "  normal_labels = pd.Series([0 if l == 'Normal' else 1 for l in normal_labels])\n",
        "\n",
        "  attack_labels = df_attack['Normal/Attack']\n",
        "  attack_labels = pd.Series([0 if l == 'Normal' else 1 for l in attack_labels])\n",
        "\n",
        "  df_normal.drop('Normal/Attack', axis=1, inplace=True)\n",
        "  df_attack.drop('Normal/Attack', axis=1, inplace=True)\n",
        "\n",
        "  # Cast columns datatype to float\n",
        "  df_normal = df_normal.apply(pd.to_numeric)\n",
        "  df_attack = df_attack.apply(pd.to_numeric)\n",
        "\n",
        "  # Remove blank characters from column names\n",
        "  def remove_blank(string):\n",
        "    return \"\".join(string.split())\n",
        "\n",
        "  normal_cols = df_normal.columns.copy()\n",
        "  attack_cols = df_attack.columns.copy()\n",
        "\n",
        "  normal_cols2 = [remove_blank(c) for c in normal_cols]\n",
        "  attack_cols2 = [remove_blank(c) for c in attack_cols]\n",
        "\n",
        "  map_n = {i: j for (i, j) in zip(normal_cols, normal_cols2)}\n",
        "  map_a = {i: j for (i, j) in zip(attack_cols, attack_cols2)}\n",
        "\n",
        "  df_normal.rename(columns=map_n, inplace=True)\n",
        "  df_attack.rename(columns=map_a, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "bQOf3wZyqY0Z",
        "outputId": "ade3a5f5-602e-4b40-d8c2-0eed8dcb1d16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                       FIT101    LIT101  MV101  P101  P102    AIT201  \\\n",
              " Timestamp                                                              \n",
              " 22/12/2015 4:30:00 PM     0.0  124.3135      1     1     1  251.9226   \n",
              " 22/12/2015 4:30:01 PM     0.0  124.3920      1     1     1  251.9226   \n",
              " 22/12/2015 4:30:02 PM     0.0  124.4705      1     1     1  251.9226   \n",
              " 22/12/2015 4:30:03 PM     0.0  124.6668      1     1     1  251.9226   \n",
              " 22/12/2015 4:30:04 PM     0.0  124.5098      1     1     1  251.9226   \n",
              " 22/12/2015 4:30:05 PM     0.0  123.9210      1     1     1  251.9226   \n",
              " 22/12/2015 4:30:06 PM     0.0  123.5284      1     1     1  251.9226   \n",
              " 22/12/2015 4:30:07 PM     0.0  123.4107      1     1     1  251.9226   \n",
              " 22/12/2015 4:30:08 PM     0.0  123.2144      1     1     1  251.9226   \n",
              " 22/12/2015 4:30:09 PM     0.0  123.3322      1     1     1  251.9226   \n",
              "\n",
              "0                         AIT202    AIT203  FIT201  MV201  ...  FIT504  P501  \\\n",
              " Timestamp                                                 ...                 \n",
              " 22/12/2015 4:30:00 PM  8.313446  312.7916     0.0      1  ...     0.0     1   \n",
              " 22/12/2015 4:30:01 PM  8.313446  312.7916     0.0      1  ...     0.0     1   \n",
              " 22/12/2015 4:30:02 PM  8.313446  312.7916     0.0      1  ...     0.0     1   \n",
              " 22/12/2015 4:30:03 PM  8.313446  312.7916     0.0      1  ...     0.0     1   \n",
              " 22/12/2015 4:30:04 PM  8.313446  312.7916     0.0      1  ...     0.0     1   \n",
              " 22/12/2015 4:30:05 PM  8.313446  312.7916     0.0      1  ...     0.0     1   \n",
              " 22/12/2015 4:30:06 PM  8.313446  312.7916     0.0      1  ...     0.0     1   \n",
              " 22/12/2015 4:30:07 PM  8.313446  312.7916     0.0      1  ...     0.0     1   \n",
              " 22/12/2015 4:30:08 PM  8.312805  312.7916     0.0      1  ...     0.0     1   \n",
              " 22/12/2015 4:30:09 PM  8.310242  312.7916     0.0      1  ...     0.0     1   \n",
              "\n",
              "0                       P502    PIT501  PIT502  PIT503    FIT601  P601  P602  \\\n",
              " Timestamp                                                                     \n",
              " 22/12/2015 4:30:00 PM     1  9.100231     0.0  3.3485  0.000256     1     1   \n",
              " 22/12/2015 4:30:01 PM     1  9.100231     0.0  3.3485  0.000256     1     1   \n",
              " 22/12/2015 4:30:02 PM     1  9.100231     0.0  3.3485  0.000256     1     1   \n",
              " 22/12/2015 4:30:03 PM     1  9.100231     0.0  3.3485  0.000256     1     1   \n",
              " 22/12/2015 4:30:04 PM     1  9.100231     0.0  3.3485  0.000256     1     1   \n",
              " 22/12/2015 4:30:05 PM     1  9.100231     0.0  3.3485  0.000256     1     1   \n",
              " 22/12/2015 4:30:06 PM     1  9.100231     0.0  3.3485  0.000256     1     1   \n",
              " 22/12/2015 4:30:07 PM     1  9.100231     0.0  3.3485  0.000256     1     1   \n",
              " 22/12/2015 4:30:08 PM     1  9.100231     0.0  3.3485  0.000256     1     1   \n",
              " 22/12/2015 4:30:09 PM     1  9.100231     0.0  3.3485  0.000256     1     1   \n",
              "\n",
              "0                       P603  \n",
              " Timestamp                    \n",
              " 22/12/2015 4:30:00 PM     1  \n",
              " 22/12/2015 4:30:01 PM     1  \n",
              " 22/12/2015 4:30:02 PM     1  \n",
              " 22/12/2015 4:30:03 PM     1  \n",
              " 22/12/2015 4:30:04 PM     1  \n",
              " 22/12/2015 4:30:05 PM     1  \n",
              " 22/12/2015 4:30:06 PM     1  \n",
              " 22/12/2015 4:30:07 PM     1  \n",
              " 22/12/2015 4:30:08 PM     1  \n",
              " 22/12/2015 4:30:09 PM     1  \n",
              "\n",
              "[10 rows x 51 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-99bdfb6d-92ea-457f-be50-db75622836fa\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FIT101</th>\n",
              "      <th>LIT101</th>\n",
              "      <th>MV101</th>\n",
              "      <th>P101</th>\n",
              "      <th>P102</th>\n",
              "      <th>AIT201</th>\n",
              "      <th>AIT202</th>\n",
              "      <th>AIT203</th>\n",
              "      <th>FIT201</th>\n",
              "      <th>MV201</th>\n",
              "      <th>...</th>\n",
              "      <th>FIT504</th>\n",
              "      <th>P501</th>\n",
              "      <th>P502</th>\n",
              "      <th>PIT501</th>\n",
              "      <th>PIT502</th>\n",
              "      <th>PIT503</th>\n",
              "      <th>FIT601</th>\n",
              "      <th>P601</th>\n",
              "      <th>P602</th>\n",
              "      <th>P603</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>22/12/2015 4:30:00 PM</th>\n",
              "      <td>0.0</td>\n",
              "      <td>124.3135</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>251.9226</td>\n",
              "      <td>8.313446</td>\n",
              "      <td>312.7916</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9.100231</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.3485</td>\n",
              "      <td>0.000256</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22/12/2015 4:30:01 PM</th>\n",
              "      <td>0.0</td>\n",
              "      <td>124.3920</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>251.9226</td>\n",
              "      <td>8.313446</td>\n",
              "      <td>312.7916</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9.100231</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.3485</td>\n",
              "      <td>0.000256</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22/12/2015 4:30:02 PM</th>\n",
              "      <td>0.0</td>\n",
              "      <td>124.4705</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>251.9226</td>\n",
              "      <td>8.313446</td>\n",
              "      <td>312.7916</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9.100231</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.3485</td>\n",
              "      <td>0.000256</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22/12/2015 4:30:03 PM</th>\n",
              "      <td>0.0</td>\n",
              "      <td>124.6668</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>251.9226</td>\n",
              "      <td>8.313446</td>\n",
              "      <td>312.7916</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9.100231</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.3485</td>\n",
              "      <td>0.000256</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22/12/2015 4:30:04 PM</th>\n",
              "      <td>0.0</td>\n",
              "      <td>124.5098</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>251.9226</td>\n",
              "      <td>8.313446</td>\n",
              "      <td>312.7916</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9.100231</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.3485</td>\n",
              "      <td>0.000256</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22/12/2015 4:30:05 PM</th>\n",
              "      <td>0.0</td>\n",
              "      <td>123.9210</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>251.9226</td>\n",
              "      <td>8.313446</td>\n",
              "      <td>312.7916</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9.100231</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.3485</td>\n",
              "      <td>0.000256</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22/12/2015 4:30:06 PM</th>\n",
              "      <td>0.0</td>\n",
              "      <td>123.5284</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>251.9226</td>\n",
              "      <td>8.313446</td>\n",
              "      <td>312.7916</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9.100231</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.3485</td>\n",
              "      <td>0.000256</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22/12/2015 4:30:07 PM</th>\n",
              "      <td>0.0</td>\n",
              "      <td>123.4107</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>251.9226</td>\n",
              "      <td>8.313446</td>\n",
              "      <td>312.7916</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9.100231</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.3485</td>\n",
              "      <td>0.000256</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22/12/2015 4:30:08 PM</th>\n",
              "      <td>0.0</td>\n",
              "      <td>123.2144</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>251.9226</td>\n",
              "      <td>8.312805</td>\n",
              "      <td>312.7916</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9.100231</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.3485</td>\n",
              "      <td>0.000256</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22/12/2015 4:30:09 PM</th>\n",
              "      <td>0.0</td>\n",
              "      <td>123.3322</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>251.9226</td>\n",
              "      <td>8.310242</td>\n",
              "      <td>312.7916</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9.100231</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.3485</td>\n",
              "      <td>0.000256</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 51 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-99bdfb6d-92ea-457f-be50-db75622836fa')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-99bdfb6d-92ea-457f-be50-db75622836fa button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-99bdfb6d-92ea-457f-be50-db75622836fa');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-26e2cbb3-6e70-4ae6-bc62-a04347422362\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-26e2cbb3-6e70-4ae6-bc62-a04347422362')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-26e2cbb3-6e70-4ae6-bc62-a04347422362 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df_normal.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFihURMYO_6h"
      },
      "source": [
        "## Suggestion:\n",
        "Loading data from excel files in dataframes is very slow. To avoid doing this everytime you need to execute this notebook, save the dataframes in secondary memory using HDF5.\n",
        "\n",
        "To save the dataframes, run the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wVfs-dCiJDyb"
      },
      "outputs": [],
      "source": [
        "if not os.path.isfile(path + '/normal_and_attacked_dfs.h5'):\n",
        "  #store = pd.HDFStore('normal_and_attacked_dfs.h5')\n",
        "  #store['df_normal'] = df_normal\n",
        "  #store['normal_labels'] = normal_labels\n",
        "  #store['df_attack'] = df_attack\n",
        "  #store['attack_labels'] = attack_labels\n",
        "\n",
        "  df_normal.to_hdf('normal_and_attacked_dfs.h5', key='df_normal')\n",
        "  normal_labels.to_hdf('normal_and_attacked_dfs.h5', key='normal_labels')\n",
        "  df_attack.to_hdf('normal_and_attacked_dfs.h5', key='df_attack')\n",
        "  attack_labels.to_hdf('normal_and_attacked_dfs.h5', key='attack_labels')\n",
        "\n",
        "  # copy file 'normal_and_attacked_dfs.h5' to Drive\n",
        "  !cp ./normal_and_attacked_dfs.h5 /content/drive/MyDrive/ml-applications/TimeSeriesAnomalyDetection_project/SWaT_physical_2015/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oiJI1snIKTg"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aaXuEH3qk_y"
      },
      "source": [
        "Standardize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "C1VGD-AEqlg7"
      },
      "outputs": [],
      "source": [
        "# standardize data\n",
        "standardizer = get_standardizer(df_normal)\n",
        "\n",
        "df_normal = standardization(df_normal, standardizer)\n",
        "df_attack = standardization(df_attack, standardizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "-xbgScquyPo8",
        "outputId": "895a2e51-964b-481d-d165-8f1a123b63e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                          FIT101    LIT101     MV101     P101  P102  \\\n",
              " Timestamp                                                             \n",
              " 28/12/2015 10:00:00 AM  0.509078 -0.531668  0.610641  0.57866   0.0   \n",
              " 28/12/2015 10:00:01 AM  0.526046 -0.531345  0.610641  0.57866   0.0   \n",
              " 28/12/2015 10:00:02 AM  0.563942 -0.531668  0.610641  0.57866   0.0   \n",
              " 28/12/2015 10:00:03 AM  0.603816 -0.530699  0.610641  0.57866   0.0   \n",
              " 28/12/2015 10:00:04 AM  0.634641 -0.526505  0.610641  0.57866   0.0   \n",
              " 28/12/2015 10:00:05 AM  0.669991 -0.523279  0.610641  0.57866   0.0   \n",
              " 28/12/2015 10:00:06 AM  0.694595 -0.521344  0.610641  0.57866   0.0   \n",
              " 28/12/2015 10:00:07 AM  0.707886 -0.520375  0.610641  0.57866   0.0   \n",
              " 28/12/2015 10:00:08 AM  0.710998 -0.518117  0.610641  0.57866   0.0   \n",
              " 28/12/2015 10:00:09 AM  0.705907 -0.521666  0.610641  0.57866   0.0   \n",
              "\n",
              "0                          AIT201    AIT202    AIT203    FIT201     MV201  \\\n",
              " Timestamp                                                                  \n",
              " 28/12/2015 10:00:00 AM -0.369209  0.091710 -0.399305  0.577082  0.571882   \n",
              " 28/12/2015 10:00:01 AM -0.369209  0.091710 -0.399305  0.577082  0.571882   \n",
              " 28/12/2015 10:00:02 AM -0.369209  0.070398 -0.399305  0.574180  0.571882   \n",
              " 28/12/2015 10:00:03 AM -0.369209  0.070398 -0.399305  0.574180  0.571882   \n",
              " 28/12/2015 10:00:04 AM -0.369209  0.070398 -0.399305  0.574906  0.571882   \n",
              " 28/12/2015 10:00:05 AM -0.369209  0.070398 -0.399305  0.575873  0.571882   \n",
              " 28/12/2015 10:00:06 AM -0.369209  0.070398 -0.399305  0.575873  0.571882   \n",
              " 28/12/2015 10:00:07 AM -0.369209  0.070398 -0.399305  0.573695  0.571882   \n",
              " 28/12/2015 10:00:08 AM -0.369209  0.070398 -0.399305  0.573695  0.571882   \n",
              " 28/12/2015 10:00:09 AM -0.369209  0.070398 -0.399305  0.573695  0.571882   \n",
              "\n",
              "0                        ...    FIT504      P501  P502    PIT501    PIT502  \\\n",
              " Timestamp               ...                                                 \n",
              " 28/12/2015 10:00:00 AM  ...  0.030545  0.055828   0.0 -0.067305  2.260671   \n",
              " 28/12/2015 10:00:01 AM  ...  0.030545  0.055828   0.0 -0.067305  2.260671   \n",
              " 28/12/2015 10:00:02 AM  ...  0.078364  0.055828   0.0 -0.066129  2.260671   \n",
              " 28/12/2015 10:00:03 AM  ...  0.078364  0.055828   0.0 -0.066129  2.260671   \n",
              " 28/12/2015 10:00:04 AM  ...  0.078364  0.055828   0.0 -0.066129  2.260671   \n",
              " 28/12/2015 10:00:05 AM  ...  0.078364  0.055828   0.0 -0.075555  2.260671   \n",
              " 28/12/2015 10:00:06 AM  ...  0.078364  0.055828   0.0 -0.087333  2.260671   \n",
              " 28/12/2015 10:00:07 AM  ...  0.078364  0.055828   0.0 -0.087333  2.260671   \n",
              " 28/12/2015 10:00:08 AM  ...  0.078364  0.055828   0.0 -0.063776  2.260671   \n",
              " 28/12/2015 10:00:09 AM  ...  0.071008  0.055828   0.0 -0.063776  2.260671   \n",
              "\n",
              "0                          PIT503    FIT601  P601      P602  P603  \n",
              " Timestamp                                                         \n",
              " 28/12/2015 10:00:00 AM -0.125269 -0.096008   0.0 -0.089482   0.0  \n",
              " 28/12/2015 10:00:01 AM -0.117722 -0.096008   0.0 -0.089482   0.0  \n",
              " 28/12/2015 10:00:02 AM -0.117722 -0.096008   0.0 -0.089482   0.0  \n",
              " 28/12/2015 10:00:03 AM -0.123761 -0.096008   0.0 -0.089482   0.0  \n",
              " 28/12/2015 10:00:04 AM -0.134322 -0.096008   0.0 -0.089482   0.0  \n",
              " 28/12/2015 10:00:05 AM -0.134322 -0.096008   0.0 -0.089482   0.0  \n",
              " 28/12/2015 10:00:06 AM -0.134322 -0.096008   0.0 -0.089482   0.0  \n",
              " 28/12/2015 10:00:07 AM -0.134322 -0.096008   0.0 -0.089482   0.0  \n",
              " 28/12/2015 10:00:08 AM -0.134322 -0.096008   0.0 -0.089482   0.0  \n",
              " 28/12/2015 10:00:09 AM -0.128283 -0.096008   0.0 -0.089482   0.0  \n",
              "\n",
              "[10 rows x 51 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9f1fa03a-4030-4723-8d08-7dea3037b254\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FIT101</th>\n",
              "      <th>LIT101</th>\n",
              "      <th>MV101</th>\n",
              "      <th>P101</th>\n",
              "      <th>P102</th>\n",
              "      <th>AIT201</th>\n",
              "      <th>AIT202</th>\n",
              "      <th>AIT203</th>\n",
              "      <th>FIT201</th>\n",
              "      <th>MV201</th>\n",
              "      <th>...</th>\n",
              "      <th>FIT504</th>\n",
              "      <th>P501</th>\n",
              "      <th>P502</th>\n",
              "      <th>PIT501</th>\n",
              "      <th>PIT502</th>\n",
              "      <th>PIT503</th>\n",
              "      <th>FIT601</th>\n",
              "      <th>P601</th>\n",
              "      <th>P602</th>\n",
              "      <th>P603</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>28/12/2015 10:00:00 AM</th>\n",
              "      <td>0.509078</td>\n",
              "      <td>-0.531668</td>\n",
              "      <td>0.610641</td>\n",
              "      <td>0.57866</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.369209</td>\n",
              "      <td>0.091710</td>\n",
              "      <td>-0.399305</td>\n",
              "      <td>0.577082</td>\n",
              "      <td>0.571882</td>\n",
              "      <td>...</td>\n",
              "      <td>0.030545</td>\n",
              "      <td>0.055828</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.067305</td>\n",
              "      <td>2.260671</td>\n",
              "      <td>-0.125269</td>\n",
              "      <td>-0.096008</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.089482</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28/12/2015 10:00:01 AM</th>\n",
              "      <td>0.526046</td>\n",
              "      <td>-0.531345</td>\n",
              "      <td>0.610641</td>\n",
              "      <td>0.57866</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.369209</td>\n",
              "      <td>0.091710</td>\n",
              "      <td>-0.399305</td>\n",
              "      <td>0.577082</td>\n",
              "      <td>0.571882</td>\n",
              "      <td>...</td>\n",
              "      <td>0.030545</td>\n",
              "      <td>0.055828</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.067305</td>\n",
              "      <td>2.260671</td>\n",
              "      <td>-0.117722</td>\n",
              "      <td>-0.096008</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.089482</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28/12/2015 10:00:02 AM</th>\n",
              "      <td>0.563942</td>\n",
              "      <td>-0.531668</td>\n",
              "      <td>0.610641</td>\n",
              "      <td>0.57866</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.369209</td>\n",
              "      <td>0.070398</td>\n",
              "      <td>-0.399305</td>\n",
              "      <td>0.574180</td>\n",
              "      <td>0.571882</td>\n",
              "      <td>...</td>\n",
              "      <td>0.078364</td>\n",
              "      <td>0.055828</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.066129</td>\n",
              "      <td>2.260671</td>\n",
              "      <td>-0.117722</td>\n",
              "      <td>-0.096008</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.089482</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28/12/2015 10:00:03 AM</th>\n",
              "      <td>0.603816</td>\n",
              "      <td>-0.530699</td>\n",
              "      <td>0.610641</td>\n",
              "      <td>0.57866</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.369209</td>\n",
              "      <td>0.070398</td>\n",
              "      <td>-0.399305</td>\n",
              "      <td>0.574180</td>\n",
              "      <td>0.571882</td>\n",
              "      <td>...</td>\n",
              "      <td>0.078364</td>\n",
              "      <td>0.055828</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.066129</td>\n",
              "      <td>2.260671</td>\n",
              "      <td>-0.123761</td>\n",
              "      <td>-0.096008</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.089482</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28/12/2015 10:00:04 AM</th>\n",
              "      <td>0.634641</td>\n",
              "      <td>-0.526505</td>\n",
              "      <td>0.610641</td>\n",
              "      <td>0.57866</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.369209</td>\n",
              "      <td>0.070398</td>\n",
              "      <td>-0.399305</td>\n",
              "      <td>0.574906</td>\n",
              "      <td>0.571882</td>\n",
              "      <td>...</td>\n",
              "      <td>0.078364</td>\n",
              "      <td>0.055828</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.066129</td>\n",
              "      <td>2.260671</td>\n",
              "      <td>-0.134322</td>\n",
              "      <td>-0.096008</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.089482</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28/12/2015 10:00:05 AM</th>\n",
              "      <td>0.669991</td>\n",
              "      <td>-0.523279</td>\n",
              "      <td>0.610641</td>\n",
              "      <td>0.57866</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.369209</td>\n",
              "      <td>0.070398</td>\n",
              "      <td>-0.399305</td>\n",
              "      <td>0.575873</td>\n",
              "      <td>0.571882</td>\n",
              "      <td>...</td>\n",
              "      <td>0.078364</td>\n",
              "      <td>0.055828</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.075555</td>\n",
              "      <td>2.260671</td>\n",
              "      <td>-0.134322</td>\n",
              "      <td>-0.096008</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.089482</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28/12/2015 10:00:06 AM</th>\n",
              "      <td>0.694595</td>\n",
              "      <td>-0.521344</td>\n",
              "      <td>0.610641</td>\n",
              "      <td>0.57866</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.369209</td>\n",
              "      <td>0.070398</td>\n",
              "      <td>-0.399305</td>\n",
              "      <td>0.575873</td>\n",
              "      <td>0.571882</td>\n",
              "      <td>...</td>\n",
              "      <td>0.078364</td>\n",
              "      <td>0.055828</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.087333</td>\n",
              "      <td>2.260671</td>\n",
              "      <td>-0.134322</td>\n",
              "      <td>-0.096008</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.089482</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28/12/2015 10:00:07 AM</th>\n",
              "      <td>0.707886</td>\n",
              "      <td>-0.520375</td>\n",
              "      <td>0.610641</td>\n",
              "      <td>0.57866</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.369209</td>\n",
              "      <td>0.070398</td>\n",
              "      <td>-0.399305</td>\n",
              "      <td>0.573695</td>\n",
              "      <td>0.571882</td>\n",
              "      <td>...</td>\n",
              "      <td>0.078364</td>\n",
              "      <td>0.055828</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.087333</td>\n",
              "      <td>2.260671</td>\n",
              "      <td>-0.134322</td>\n",
              "      <td>-0.096008</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.089482</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28/12/2015 10:00:08 AM</th>\n",
              "      <td>0.710998</td>\n",
              "      <td>-0.518117</td>\n",
              "      <td>0.610641</td>\n",
              "      <td>0.57866</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.369209</td>\n",
              "      <td>0.070398</td>\n",
              "      <td>-0.399305</td>\n",
              "      <td>0.573695</td>\n",
              "      <td>0.571882</td>\n",
              "      <td>...</td>\n",
              "      <td>0.078364</td>\n",
              "      <td>0.055828</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.063776</td>\n",
              "      <td>2.260671</td>\n",
              "      <td>-0.134322</td>\n",
              "      <td>-0.096008</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.089482</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28/12/2015 10:00:09 AM</th>\n",
              "      <td>0.705907</td>\n",
              "      <td>-0.521666</td>\n",
              "      <td>0.610641</td>\n",
              "      <td>0.57866</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.369209</td>\n",
              "      <td>0.070398</td>\n",
              "      <td>-0.399305</td>\n",
              "      <td>0.573695</td>\n",
              "      <td>0.571882</td>\n",
              "      <td>...</td>\n",
              "      <td>0.071008</td>\n",
              "      <td>0.055828</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.063776</td>\n",
              "      <td>2.260671</td>\n",
              "      <td>-0.128283</td>\n",
              "      <td>-0.096008</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.089482</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 51 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f1fa03a-4030-4723-8d08-7dea3037b254')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9f1fa03a-4030-4723-8d08-7dea3037b254 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9f1fa03a-4030-4723-8d08-7dea3037b254');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6ec87d91-37a3-49c4-b83c-94f006fac161\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6ec87d91-37a3-49c4-b83c-94f006fac161')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6ec87d91-37a3-49c4-b83c-94f006fac161 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df_attack.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwQFEZv4rT0O"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qRTR4Q8wrXMs"
      },
      "outputs": [],
      "source": [
        "DATASET_FREQUENCY = 10 #Hz\n",
        "WINDOW_TIME = 10#6 #seconds\n",
        "WINDOW_SIZE = 10 # nr of samples in a window\n",
        "SAMPLE_SIZE = 51\n",
        "LATENT_VAR_SIZE = 32 #SAMPLE_SIZE\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "TEST_BATCH_SIZE = 1\n",
        "EPOCHS = 10\n",
        "LAMBDA = 0.1\n",
        "\n",
        "training_loss = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "discr_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "gen_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGVuJTE8nnHi"
      },
      "source": [
        "Set the same index from dataframes to series containing the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nrlAOsRX4BFy"
      },
      "outputs": [],
      "source": [
        "normal_labels.index = df_normal.index\n",
        "attack_labels.index = df_attack.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "si-X9Jsa8A_Q"
      },
      "outputs": [],
      "source": [
        "#df_normall = df_normal.copy()\n",
        "#df_normall.insert(0, 'anomaly', normal_labels)\n",
        "\n",
        "df_normal.insert(0, 'anomaly', normal_labels)\n",
        "df_attack.insert(0, 'anomaly', attack_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyMX8k7dM90-"
      },
      "source": [
        "## Window the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ScNE1HgAJzz2"
      },
      "outputs": [],
      "source": [
        "def window_data(df, window_size, shift=1): # window_size refers to the nr of samples in a window\n",
        "  df.sort_index(inplace=True)\n",
        "  #df.index = [i for i in range(len(df.index))]\n",
        "\n",
        "  windows_list = []\n",
        "  label_list = []\n",
        "\n",
        "  for i in range(0, len(df.index), shift):\n",
        "    if i + window_size <= len(df.index):\n",
        "      window = df.iloc[i : i + window_size].copy()\n",
        "\n",
        "      nr_anomalies = window['anomaly'].sum()\n",
        "\n",
        "      if nr_anomalies > 0:\n",
        "        #print(f\"This happened! nr_anomalies={nr_anomalies}\")\n",
        "        label_list.append(1)\n",
        "      else:\n",
        "        label_list.append(0)\n",
        "\n",
        "      window = window.drop([\"anomaly\"], axis=1).to_numpy()\n",
        "\n",
        "      windows_list.append(window)\n",
        "      #print(len(windows_list))\n",
        "\n",
        "  return np.array(windows_list), np.array(label_list)\n",
        "  #return windows_list, label_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KJdlvHHf-FBF"
      },
      "outputs": [],
      "source": [
        "#windows_normal, labels_normal = window_data(df_normal, WINDOW_SIZE, 1)\n",
        "windows_attack, labels_attack = window_data(df_attack, WINDOW_SIZE, WINDOW_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "p5DUavbD_aST",
        "outputId": "cefdd767-2eb9-4c5b-817b-ed7f8ea89021"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nr samples in df_normal: (495000, 52)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d9b43c4e930b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Nr samples in df_normal: {df_normal.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Nr of windows in windows_normal: {windows_normal.shape}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Nr of anomalous windows in windows_regular: {windows_normal[labels_normal==1].shape[0]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'windows_normal' is not defined"
          ]
        }
      ],
      "source": [
        "print(f\"Nr samples in df_normal: {df_normal.shape}\")\n",
        "print(f\"Nr of windows in windows_normal: {windows_normal.shape}\\n\")\n",
        "\n",
        "print(f\"Nr of anomalous windows in windows_regular: {windows_normal[labels_normal==1].shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLJ_WnpbHEx3"
      },
      "outputs": [],
      "source": [
        "#overlapped_windows_rec1, overlapped_labels_rec1 = window_data(df_rec1_labelled, WINDOW_SIZE, 1)\n",
        "#overlapped_windows_rec5, overlapped_labels_rec5 = window_data(df_rec5_labelled, WINDOW_SIZE, 1)\n",
        "\n",
        "del df_normal, df_attack, normal_labels, attack_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fptm3aOX3rBR"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I718YhNTUFHY"
      },
      "source": [
        "Make TensorFlow Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iP1KcY-XUIBn"
      },
      "outputs": [],
      "source": [
        "#train_dataset = tf.data.Dataset.from_tensor_slices((windows_normal, labels_normal)).map(lambda x, y: (tf.cast(x, tf.float32), y)).batch(TRAIN_BATCH_SIZE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((windows_attack, labels_attack)).map(lambda x, y: (tf.cast(x, tf.float32), y)).batch(TEST_BATCH_SIZE)\n",
        "\n",
        "#validation_dataset = tf.data.Dataset.from_tensor_slices((windows_rec5, labels_rec5)).map(lambda x, y: (tf.cast(x, tf.float32), y)).batch(TEST_BATCH_SIZE)\n",
        "#test_dataset = tf.data.Dataset.from_tensor_slices((windows_rec1, labels_rec1)).map(lambda x, y: (tf.cast(x, tf.float32), y)).batch(TEST_BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGBGeY7i2usT"
      },
      "source": [
        "# Generator and Discriminator instantiation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW6EROxz1aeh"
      },
      "outputs": [],
      "source": [
        "generator = init_generator(in_dim=(WINDOW_SIZE, LATENT_VAR_SIZE), out_dim=(WINDOW_SIZE, SAMPLE_SIZE))\n",
        "discriminator = init_discriminator(in_dim=(WINDOW_SIZE, SAMPLE_SIZE), out_dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTHopsOqyo5P"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCCNeaFthFX9"
      },
      "outputs": [],
      "source": [
        "def adversarial_training(generator, discriminator, data, batch_size, windows_size, latent_size, loss_fn, discr_optimizer, gen_optimizer, checkpoints_backup_path, num_epochs=1):\n",
        "\n",
        "# setup checkpoint saving\n",
        "  directory_path = \"./checkpoints\"\n",
        "\n",
        "  filepath_generator = directory_path + \"/generator/\"\n",
        "  filepath_discriminator = directory_path + \"/discriminator/\"\n",
        "\n",
        "  if not os.path.isdir('checkpoints'):\n",
        "    ! mkdir checkpoints\n",
        "\n",
        "    ! mkdir {filepath_generator}\n",
        "    ! mkdir {filepath_discriminator}\n",
        "\n",
        "  real_label = 0\n",
        "  fake_label = 1\n",
        "\n",
        "  print(\"epoch: 0/%d\")\n",
        "  for epoch in range(num_epochs):\n",
        "    for i, (time_sequences, labels) in enumerate(data):\n",
        "      # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "\n",
        "      #Train with real data\n",
        "      with tf.GradientTape() as tape:\n",
        "        scores = discriminator(time_sequences)\n",
        "        discr_real_loss = loss_fn(labels, scores)\n",
        "\n",
        "      gradients = tape.gradient(discr_real_loss, discriminator.trainable_weights)\n",
        "      discr_optimizer.apply_gradients(zip(gradients, discriminator.trainable_weights))\n",
        "\n",
        "      #Train with fake data\n",
        "      noise = tf.random.normal((time_sequences.shape[0], time_sequences.shape[1], latent_size), mean=0, stddev=1)\n",
        "      fake_sequences = generator(noise)\n",
        "      fake_labels = tf.ones(labels.shape) * fake_label\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "          scores = discriminator(fake_sequences)\n",
        "          discr_fake_loss = loss_fn(fake_labels, scores)\n",
        "\n",
        "      gradients = tape.gradient(discr_fake_loss, discriminator.trainable_weights)\n",
        "      discr_optimizer.apply_gradients(zip(gradients, discriminator.trainable_weights))\n",
        "\n",
        "\n",
        "      # (2) Update G network: maximize log(D(G(z)))\n",
        "\n",
        "      noise = tf.random.normal((time_sequences.shape[0], time_sequences.shape[1], latent_size), mean=0, stddev=1)\n",
        "      with tf.GradientTape() as tape:\n",
        "        fake_sequences = generator(noise)\n",
        "        #real_labels = tf.ones(noise.shape)\n",
        "        discriminator_output = discriminator(fake_sequences)    # discriminator tells which fake sequences it considers to be real and which not\n",
        "        generator_loss = loss_fn(labels, discriminator_output)\n",
        "\n",
        "      gradients = tape.gradient(generator_loss, generator.trainable_weights)\n",
        "      gen_optimizer.apply_gradients(zip(gradients, generator.trainable_weights))\n",
        "\n",
        "      print(\n",
        "          f\"epoch: {epoch}/{num_epochs},    batch: {i}/{len(data)}    Discriminator_loss: {discr_real_loss+discr_fake_loss}  Generator_loss: {generator_loss}\"\n",
        "      )\n",
        "\n",
        "    generator.save_weights(filepath_generator + f\"gen_epoch{epoch}.h5\")\n",
        "    discriminator.save_weights(filepath_discriminator + f\"discr_epoch{epoch}.h5\")\n",
        "\n",
        "    # zip and move a copy of the checkpoints\n",
        "    now = datetime.datetime.now()\n",
        "    zip_name = f\"checkpoints_{str(now.date())+'_'+str(now.time())}.zip\"\n",
        "\n",
        "    !zip -r {zip_name} ./checkpoints\n",
        "    !cp {zip_name} {checkpoints_backup_path}\n",
        "\n",
        "#    gen_symbolic_weights = getattr(gen_optimizer, 'weights')  # save gen_optimizer parameters\n",
        "#    weight_values = K.batch_get_value(gen_symbolic_weights)\n",
        "#    with open(f'checkpoints/generator/gen_optimizer_epoch{epoch}.pkl', 'wb') as f:\n",
        "#      pickle.dump(weight_values, f)\n",
        "#\n",
        "#    discr_symbolic_weights = getattr(discr_optimizer, 'weights')  # save discr_optimizer parameters\n",
        "#    weight_values = K.batch_get_value(discr_symbolic_weights)\n",
        "#    with open(f'checkpoints/discriminator/discr_optimizer_epoch{epoch}.pkl', 'wb') as f:\n",
        "#      pickle.dump(weight_values, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qef-oXZAyn_V",
        "outputId": "9de665f4-aea9-4ecf-8910-b25087a7f86e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0/%d\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7b92190cd7e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function _BaseOptimizer._update_step_xla at 0x7b92119a4040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "epoch: 9/10,    batch: 10492/15469    Discriminator_loss: 0.24244047701358795  Generator_loss: 1.5787270069122314\n",
            "epoch: 9/10,    batch: 10493/15469    Discriminator_loss: 0.23584099113941193  Generator_loss: 1.5746498107910156\n",
            "epoch: 9/10,    batch: 10494/15469    Discriminator_loss: 0.2338649034500122  Generator_loss: 1.5717999935150146\n",
            "epoch: 9/10,    batch: 10495/15469    Discriminator_loss: 0.23487521708011627  Generator_loss: 1.5708661079406738\n",
            "epoch: 9/10,    batch: 10496/15469    Discriminator_loss: 0.23499156534671783  Generator_loss: 1.5680882930755615\n",
            "epoch: 9/10,    batch: 10497/15469    Discriminator_loss: 0.23471951484680176  Generator_loss: 1.5674941539764404\n",
            "epoch: 9/10,    batch: 10498/15469    Discriminator_loss: 0.2345684915781021  Generator_loss: 1.5682694911956787\n",
            "epoch: 9/10,    batch: 10499/15469    Discriminator_loss: 0.2347453385591507  Generator_loss: 1.5691386461257935\n",
            "epoch: 9/10,    batch: 10500/15469    Discriminator_loss: 0.2351047396659851  Generator_loss: 1.570481538772583\n",
            "epoch: 9/10,    batch: 10501/15469    Discriminator_loss: 0.2341974973678589  Generator_loss: 1.570631742477417\n",
            "epoch: 9/10,    batch: 10502/15469    Discriminator_loss: 0.23364178836345673  Generator_loss: 1.5727605819702148\n",
            "epoch: 9/10,    batch: 10503/15469    Discriminator_loss: 0.2329951971769333  Generator_loss: 1.5745660066604614\n",
            "epoch: 9/10,    batch: 10504/15469    Discriminator_loss: 0.23243844509124756  Generator_loss: 1.5768442153930664\n",
            "epoch: 9/10,    batch: 10505/15469    Discriminator_loss: 0.23196513950824738  Generator_loss: 1.579016923904419\n",
            "epoch: 9/10,    batch: 10506/15469    Discriminator_loss: 0.23143048584461212  Generator_loss: 1.5814359188079834\n",
            "epoch: 9/10,    batch: 10507/15469    Discriminator_loss: 0.23047134280204773  Generator_loss: 1.5844722986221313\n",
            "epoch: 9/10,    batch: 10508/15469    Discriminator_loss: 0.2298114001750946  Generator_loss: 1.5869178771972656\n",
            "epoch: 9/10,    batch: 10509/15469    Discriminator_loss: 0.22927193343639374  Generator_loss: 1.5898010730743408\n",
            "epoch: 9/10,    batch: 10510/15469    Discriminator_loss: 0.22829760611057281  Generator_loss: 1.5943206548690796\n",
            "epoch: 9/10,    batch: 10511/15469    Discriminator_loss: 0.22735989093780518  Generator_loss: 1.596581220626831\n",
            "epoch: 9/10,    batch: 10512/15469    Discriminator_loss: 0.22665558755397797  Generator_loss: 1.600156545639038\n",
            "epoch: 9/10,    batch: 10513/15469    Discriminator_loss: 0.22596976161003113  Generator_loss: 1.6021267175674438\n",
            "epoch: 9/10,    batch: 10514/15469    Discriminator_loss: 0.22512286901474  Generator_loss: 1.6075775623321533\n",
            "epoch: 9/10,    batch: 10515/15469    Discriminator_loss: 0.22399753332138062  Generator_loss: 1.6117520332336426\n",
            "epoch: 9/10,    batch: 10516/15469    Discriminator_loss: 0.223172128200531  Generator_loss: 1.6128766536712646\n",
            "epoch: 9/10,    batch: 10517/15469    Discriminator_loss: 0.22257740795612335  Generator_loss: 1.617814064025879\n",
            "epoch: 9/10,    batch: 10518/15469    Discriminator_loss: 0.22126688063144684  Generator_loss: 1.6203467845916748\n",
            "epoch: 9/10,    batch: 10519/15469    Discriminator_loss: 0.2205885648727417  Generator_loss: 1.6235573291778564\n",
            "epoch: 9/10,    batch: 10520/15469    Discriminator_loss: 0.21961085498332977  Generator_loss: 1.6273576021194458\n",
            "epoch: 9/10,    batch: 10521/15469    Discriminator_loss: 0.21888810396194458  Generator_loss: 1.631373643875122\n",
            "epoch: 9/10,    batch: 10522/15469    Discriminator_loss: 0.2179316282272339  Generator_loss: 1.6347349882125854\n",
            "epoch: 9/10,    batch: 10523/15469    Discriminator_loss: 0.21708403527736664  Generator_loss: 1.638968586921692\n",
            "epoch: 9/10,    batch: 10524/15469    Discriminator_loss: 0.21584443747997284  Generator_loss: 1.6425254344940186\n",
            "epoch: 9/10,    batch: 10525/15469    Discriminator_loss: 0.21506784856319427  Generator_loss: 1.6453253030776978\n",
            "epoch: 9/10,    batch: 10526/15469    Discriminator_loss: 0.22984977066516876  Generator_loss: 1.6492691040039062\n",
            "epoch: 9/10,    batch: 10527/15469    Discriminator_loss: 0.21356923878192902  Generator_loss: 1.653778314590454\n",
            "epoch: 9/10,    batch: 10528/15469    Discriminator_loss: 0.21235531568527222  Generator_loss: 1.657973051071167\n",
            "epoch: 9/10,    batch: 10529/15469    Discriminator_loss: 0.21134895086288452  Generator_loss: 1.6607956886291504\n",
            "epoch: 9/10,    batch: 10530/15469    Discriminator_loss: 0.2103515863418579  Generator_loss: 1.6637077331542969\n",
            "epoch: 9/10,    batch: 10531/15469    Discriminator_loss: 0.20977453887462616  Generator_loss: 1.6685172319412231\n",
            "epoch: 9/10,    batch: 10532/15469    Discriminator_loss: 0.20938530564308167  Generator_loss: 1.6736054420471191\n",
            "epoch: 9/10,    batch: 10533/15469    Discriminator_loss: 0.20764470100402832  Generator_loss: 1.6770014762878418\n",
            "epoch: 9/10,    batch: 10534/15469    Discriminator_loss: 0.20739053189754486  Generator_loss: 1.680656909942627\n",
            "epoch: 9/10,    batch: 10535/15469    Discriminator_loss: 0.20630469918251038  Generator_loss: 1.6832339763641357\n",
            "epoch: 9/10,    batch: 10536/15469    Discriminator_loss: 0.2057531476020813  Generator_loss: 1.6860651969909668\n",
            "epoch: 9/10,    batch: 10537/15469    Discriminator_loss: 0.2049015760421753  Generator_loss: 1.6900198459625244\n",
            "epoch: 9/10,    batch: 10538/15469    Discriminator_loss: 0.20381484925746918  Generator_loss: 1.6929802894592285\n",
            "epoch: 9/10,    batch: 10539/15469    Discriminator_loss: 0.20277515053749084  Generator_loss: 1.6975733041763306\n",
            "epoch: 9/10,    batch: 10540/15469    Discriminator_loss: 0.20248189568519592  Generator_loss: 1.6994612216949463\n",
            "epoch: 9/10,    batch: 10541/15469    Discriminator_loss: 0.20173199474811554  Generator_loss: 1.7028388977050781\n",
            "epoch: 9/10,    batch: 10542/15469    Discriminator_loss: 0.20162664353847504  Generator_loss: 1.7094857692718506\n",
            "epoch: 9/10,    batch: 10543/15469    Discriminator_loss: 0.20012922585010529  Generator_loss: 1.7104432582855225\n",
            "epoch: 9/10,    batch: 10544/15469    Discriminator_loss: 0.19987189769744873  Generator_loss: 1.712407112121582\n",
            "epoch: 9/10,    batch: 10545/15469    Discriminator_loss: 0.19920235872268677  Generator_loss: 1.7170603275299072\n",
            "epoch: 9/10,    batch: 10546/15469    Discriminator_loss: 0.19818073511123657  Generator_loss: 1.7199335098266602\n",
            "epoch: 9/10,    batch: 10547/15469    Discriminator_loss: 0.1975192427635193  Generator_loss: 1.71543288230896\n",
            "epoch: 9/10,    batch: 10548/15469    Discriminator_loss: 0.19828224182128906  Generator_loss: 1.7182306051254272\n",
            "epoch: 9/10,    batch: 10549/15469    Discriminator_loss: 0.19839082658290863  Generator_loss: 1.7196422815322876\n",
            "epoch: 9/10,    batch: 10550/15469    Discriminator_loss: 0.19699394702911377  Generator_loss: 1.7242448329925537\n",
            "epoch: 9/10,    batch: 10551/15469    Discriminator_loss: 0.1983349621295929  Generator_loss: 1.7189264297485352\n",
            "epoch: 9/10,    batch: 10552/15469    Discriminator_loss: 0.19885356724262238  Generator_loss: 1.7114553451538086\n",
            "epoch: 9/10,    batch: 10553/15469    Discriminator_loss: 0.20221486687660217  Generator_loss: 1.7025797367095947\n",
            "epoch: 9/10,    batch: 10554/15469    Discriminator_loss: 0.20529389381408691  Generator_loss: 1.6994760036468506\n",
            "epoch: 9/10,    batch: 10555/15469    Discriminator_loss: 0.21449916064739227  Generator_loss: 1.6579030752182007\n",
            "epoch: 9/10,    batch: 10556/15469    Discriminator_loss: 0.2351100891828537  Generator_loss: 1.6069144010543823\n",
            "epoch: 9/10,    batch: 10557/15469    Discriminator_loss: 0.28994178771972656  Generator_loss: 1.366735577583313\n",
            "epoch: 9/10,    batch: 10558/15469    Discriminator_loss: 0.6397481560707092  Generator_loss: 0.9725344181060791\n",
            "epoch: 9/10,    batch: 10559/15469    Discriminator_loss: 1.8791096210479736  Generator_loss: 0.39751434326171875\n",
            "epoch: 9/10,    batch: 10560/15469    Discriminator_loss: 2.0485072135925293  Generator_loss: 0.43414756655693054\n",
            "epoch: 9/10,    batch: 10561/15469    Discriminator_loss: 1.2014826536178589  Generator_loss: 0.8631035089492798\n",
            "epoch: 9/10,    batch: 10562/15469    Discriminator_loss: 0.48591578006744385  Generator_loss: 1.440246820449829\n",
            "epoch: 9/10,    batch: 10563/15469    Discriminator_loss: 0.23914706707000732  Generator_loss: 1.8034138679504395\n",
            "epoch: 9/10,    batch: 10564/15469    Discriminator_loss: 0.16670812666416168  Generator_loss: 2.0344038009643555\n",
            "epoch: 9/10,    batch: 10565/15469    Discriminator_loss: 0.13478581607341766  Generator_loss: 2.1851398944854736\n",
            "epoch: 9/10,    batch: 10566/15469    Discriminator_loss: 0.11742698401212692  Generator_loss: 2.2769861221313477\n",
            "epoch: 9/10,    batch: 10567/15469    Discriminator_loss: 0.1080268993973732  Generator_loss: 2.347565174102783\n",
            "epoch: 9/10,    batch: 10568/15469    Discriminator_loss: 0.10244259983301163  Generator_loss: 2.3861639499664307\n",
            "epoch: 9/10,    batch: 10569/15469    Discriminator_loss: 0.10018693655729294  Generator_loss: 2.3971610069274902\n",
            "epoch: 9/10,    batch: 10570/15469    Discriminator_loss: 0.09925904870033264  Generator_loss: 2.397587299346924\n",
            "epoch: 9/10,    batch: 10571/15469    Discriminator_loss: 0.09956809133291245  Generator_loss: 2.3885741233825684\n",
            "epoch: 9/10,    batch: 10572/15469    Discriminator_loss: 0.1008525863289833  Generator_loss: 2.3735437393188477\n",
            "epoch: 9/10,    batch: 10573/15469    Discriminator_loss: 0.1024835854768753  Generator_loss: 2.358259916305542\n",
            "epoch: 9/10,    batch: 10574/15469    Discriminator_loss: 0.10351641476154327  Generator_loss: 2.3448610305786133\n",
            "epoch: 9/10,    batch: 10575/15469    Discriminator_loss: 0.10409063845872879  Generator_loss: 2.336022138595581\n",
            "epoch: 9/10,    batch: 10576/15469    Discriminator_loss: 0.10444538295269012  Generator_loss: 2.3322973251342773\n",
            "epoch: 9/10,    batch: 10577/15469    Discriminator_loss: 0.10440028458833694  Generator_loss: 2.330885410308838\n",
            "epoch: 9/10,    batch: 10578/15469    Discriminator_loss: 0.10397866368293762  Generator_loss: 2.3328185081481934\n",
            "epoch: 9/10,    batch: 10579/15469    Discriminator_loss: 0.10333511233329773  Generator_loss: 2.3371243476867676\n",
            "epoch: 9/10,    batch: 10580/15469    Discriminator_loss: 0.10268150269985199  Generator_loss: 2.3422653675079346\n",
            "epoch: 9/10,    batch: 10581/15469    Discriminator_loss: 0.1020301878452301  Generator_loss: 2.3483099937438965\n",
            "epoch: 9/10,    batch: 10582/15469    Discriminator_loss: 0.10123410820960999  Generator_loss: 2.3548312187194824\n",
            "epoch: 9/10,    batch: 10583/15469    Discriminator_loss: 0.10040374845266342  Generator_loss: 2.3618874549865723\n",
            "epoch: 9/10,    batch: 10584/15469    Discriminator_loss: 0.09961000829935074  Generator_loss: 2.367821455001831\n",
            "epoch: 9/10,    batch: 10585/15469    Discriminator_loss: 0.09900890290737152  Generator_loss: 2.3739328384399414\n",
            "epoch: 9/10,    batch: 10586/15469    Discriminator_loss: 0.09822236001491547  Generator_loss: 2.3830108642578125\n",
            "epoch: 9/10,    batch: 10587/15469    Discriminator_loss: 0.0975940003991127  Generator_loss: 2.386281967163086\n",
            "epoch: 9/10,    batch: 10588/15469    Discriminator_loss: 0.09694816172122955  Generator_loss: 2.391648292541504\n",
            "epoch: 9/10,    batch: 10589/15469    Discriminator_loss: 0.09627165645360947  Generator_loss: 2.398556709289551\n",
            "epoch: 9/10,    batch: 10590/15469    Discriminator_loss: 0.09564066678285599  Generator_loss: 2.4049930572509766\n",
            "epoch: 9/10,    batch: 10591/15469    Discriminator_loss: 0.09516207873821259  Generator_loss: 2.412032127380371\n",
            "epoch: 9/10,    batch: 10592/15469    Discriminator_loss: 0.09461658447980881  Generator_loss: 2.414738178253174\n",
            "epoch: 9/10,    batch: 10593/15469    Discriminator_loss: 0.09405143558979034  Generator_loss: 2.420213460922241\n",
            "epoch: 9/10,    batch: 10594/15469    Discriminator_loss: 0.09344647824764252  Generator_loss: 2.4258852005004883\n",
            "epoch: 9/10,    batch: 10595/15469    Discriminator_loss: 0.09289105981588364  Generator_loss: 2.4305896759033203\n",
            "epoch: 9/10,    batch: 10596/15469    Discriminator_loss: 0.09247706085443497  Generator_loss: 2.436267375946045\n",
            "epoch: 9/10,    batch: 10597/15469    Discriminator_loss: 0.0920058861374855  Generator_loss: 2.4431114196777344\n",
            "epoch: 9/10,    batch: 10598/15469    Discriminator_loss: 0.09176551550626755  Generator_loss: 2.444295883178711\n",
            "epoch: 9/10,    batch: 10599/15469    Discriminator_loss: 0.09131904691457748  Generator_loss: 2.4486165046691895\n",
            "epoch: 9/10,    batch: 10600/15469    Discriminator_loss: 0.09073451906442642  Generator_loss: 2.4542107582092285\n",
            "epoch: 9/10,    batch: 10601/15469    Discriminator_loss: 0.09040609002113342  Generator_loss: 2.4568405151367188\n",
            "epoch: 9/10,    batch: 10602/15469    Discriminator_loss: 0.09018862992525101  Generator_loss: 2.4590539932250977\n",
            "epoch: 9/10,    batch: 10603/15469    Discriminator_loss: 0.08980455994606018  Generator_loss: 2.4619107246398926\n",
            "epoch: 9/10,    batch: 10604/15469    Discriminator_loss: 0.08950690925121307  Generator_loss: 2.4644827842712402\n",
            "epoch: 9/10,    batch: 10605/15469    Discriminator_loss: 0.08972634375095367  Generator_loss: 2.466073989868164\n",
            "epoch: 9/10,    batch: 10606/15469    Discriminator_loss: 0.08944108337163925  Generator_loss: 2.465097427368164\n",
            "epoch: 9/10,    batch: 10607/15469    Discriminator_loss: 0.08945345878601074  Generator_loss: 2.467148542404175\n",
            "epoch: 9/10,    batch: 10608/15469    Discriminator_loss: 0.08938534557819366  Generator_loss: 2.465261936187744\n",
            "epoch: 9/10,    batch: 10609/15469    Discriminator_loss: 0.08940356969833374  Generator_loss: 2.464812755584717\n",
            "epoch: 9/10,    batch: 10610/15469    Discriminator_loss: 0.0894244834780693  Generator_loss: 2.464585781097412\n",
            "epoch: 9/10,    batch: 10611/15469    Discriminator_loss: 0.08932363986968994  Generator_loss: 2.463501453399658\n",
            "epoch: 9/10,    batch: 10612/15469    Discriminator_loss: 0.08954563736915588  Generator_loss: 2.4615654945373535\n",
            "epoch: 9/10,    batch: 10613/15469    Discriminator_loss: 0.08980603516101837  Generator_loss: 2.460692882537842\n",
            "epoch: 9/10,    batch: 10614/15469    Discriminator_loss: 0.08990785479545593  Generator_loss: 2.4563896656036377\n",
            "epoch: 9/10,    batch: 10615/15469    Discriminator_loss: 0.09006775915622711  Generator_loss: 2.4575138092041016\n",
            "epoch: 9/10,    batch: 10616/15469    Discriminator_loss: 0.0901302769780159  Generator_loss: 2.4558298587799072\n",
            "epoch: 9/10,    batch: 10617/15469    Discriminator_loss: 0.09027567505836487  Generator_loss: 2.4547855854034424\n",
            "epoch: 9/10,    batch: 10618/15469    Discriminator_loss: 0.09032636880874634  Generator_loss: 2.4519193172454834\n",
            "epoch: 9/10,    batch: 10619/15469    Discriminator_loss: 0.09047438204288483  Generator_loss: 2.4524285793304443\n",
            "epoch: 9/10,    batch: 10620/15469    Discriminator_loss: 0.09062981605529785  Generator_loss: 2.4507341384887695\n",
            "epoch: 9/10,    batch: 10621/15469    Discriminator_loss: 0.09046684950590134  Generator_loss: 2.450075626373291\n",
            "epoch: 9/10,    batch: 10622/15469    Discriminator_loss: 0.09063852578401566  Generator_loss: 2.4499220848083496\n",
            "epoch: 9/10,    batch: 10623/15469    Discriminator_loss: 0.09060736000537872  Generator_loss: 2.4490208625793457\n",
            "epoch: 9/10,    batch: 10624/15469    Discriminator_loss: 0.09062488377094269  Generator_loss: 2.449380874633789\n",
            "epoch: 9/10,    batch: 10625/15469    Discriminator_loss: 0.09062687307596207  Generator_loss: 2.4517147541046143\n",
            "epoch: 9/10,    batch: 10626/15469    Discriminator_loss: 0.09052401036024094  Generator_loss: 2.448793411254883\n",
            "epoch: 9/10,    batch: 10627/15469    Discriminator_loss: 0.09061362594366074  Generator_loss: 2.4481985569000244\n",
            "epoch: 9/10,    batch: 10628/15469    Discriminator_loss: 0.09055722504854202  Generator_loss: 2.4501304626464844\n",
            "epoch: 9/10,    batch: 10629/15469    Discriminator_loss: 0.09058920294046402  Generator_loss: 2.450105667114258\n",
            "epoch: 9/10,    batch: 10630/15469    Discriminator_loss: 0.09050333499908447  Generator_loss: 2.449923515319824\n",
            "epoch: 9/10,    batch: 10631/15469    Discriminator_loss: 0.09043050557374954  Generator_loss: 2.451669931411743\n",
            "epoch: 9/10,    batch: 10632/15469    Discriminator_loss: 0.09036898612976074  Generator_loss: 2.452314853668213\n",
            "epoch: 9/10,    batch: 10633/15469    Discriminator_loss: 0.09030146151781082  Generator_loss: 2.4545698165893555\n",
            "epoch: 9/10,    batch: 10634/15469    Discriminator_loss: 0.09017923474311829  Generator_loss: 2.454810619354248\n",
            "epoch: 9/10,    batch: 10635/15469    Discriminator_loss: 0.09012354165315628  Generator_loss: 2.455648422241211\n",
            "epoch: 9/10,    batch: 10636/15469    Discriminator_loss: 0.09001728892326355  Generator_loss: 2.4552059173583984\n",
            "epoch: 9/10,    batch: 10637/15469    Discriminator_loss: 0.08993931859731674  Generator_loss: 2.4569034576416016\n",
            "epoch: 9/10,    batch: 10638/15469    Discriminator_loss: 0.08982611447572708  Generator_loss: 2.4585866928100586\n",
            "epoch: 9/10,    batch: 10639/15469    Discriminator_loss: 0.08956083655357361  Generator_loss: 2.4591240882873535\n",
            "epoch: 9/10,    batch: 10640/15469    Discriminator_loss: 0.08956272900104523  Generator_loss: 2.4615368843078613\n",
            "epoch: 9/10,    batch: 10641/15469    Discriminator_loss: 0.08941429108381271  Generator_loss: 2.4627022743225098\n",
            "epoch: 9/10,    batch: 10642/15469    Discriminator_loss: 0.08924999833106995  Generator_loss: 2.463414192199707\n",
            "epoch: 9/10,    batch: 10643/15469    Discriminator_loss: 0.08910619467496872  Generator_loss: 2.4650211334228516\n",
            "epoch: 9/10,    batch: 10644/15469    Discriminator_loss: 0.08896465599536896  Generator_loss: 2.466446876525879\n",
            "epoch: 9/10,    batch: 10645/15469    Discriminator_loss: 0.08884195238351822  Generator_loss: 2.468881607055664\n",
            "epoch: 9/10,    batch: 10646/15469    Discriminator_loss: 0.08867727965116501  Generator_loss: 2.4700469970703125\n",
            "epoch: 9/10,    batch: 10647/15469    Discriminator_loss: 0.08851800113916397  Generator_loss: 2.4721670150756836\n",
            "epoch: 9/10,    batch: 10648/15469    Discriminator_loss: 0.08827144652605057  Generator_loss: 2.4748473167419434\n",
            "epoch: 9/10,    batch: 10649/15469    Discriminator_loss: 0.08817998319864273  Generator_loss: 2.476077079772949\n",
            "epoch: 9/10,    batch: 10650/15469    Discriminator_loss: 0.0879780501127243  Generator_loss: 2.4781506061553955\n",
            "epoch: 9/10,    batch: 10651/15469    Discriminator_loss: 0.0878014862537384  Generator_loss: 2.4803578853607178\n",
            "epoch: 9/10,    batch: 10652/15469    Discriminator_loss: 0.08757198601961136  Generator_loss: 2.4817304611206055\n",
            "epoch: 9/10,    batch: 10653/15469    Discriminator_loss: 0.08739078044891357  Generator_loss: 2.484372615814209\n",
            "epoch: 9/10,    batch: 10654/15469    Discriminator_loss: 0.0872112363576889  Generator_loss: 2.486846446990967\n",
            "epoch: 9/10,    batch: 10655/15469    Discriminator_loss: 0.08696263283491135  Generator_loss: 2.4884262084960938\n",
            "epoch: 9/10,    batch: 10656/15469    Discriminator_loss: 0.08672778308391571  Generator_loss: 2.4909873008728027\n",
            "epoch: 9/10,    batch: 10657/15469    Discriminator_loss: 0.086549311876297  Generator_loss: 2.4932103157043457\n",
            "epoch: 9/10,    batch: 10658/15469    Discriminator_loss: 0.0863664522767067  Generator_loss: 2.495388984680176\n",
            "epoch: 9/10,    batch: 10659/15469    Discriminator_loss: 0.0861152708530426  Generator_loss: 2.4980671405792236\n",
            "epoch: 9/10,    batch: 10660/15469    Discriminator_loss: 0.0859062597155571  Generator_loss: 2.500342845916748\n",
            "epoch: 9/10,    batch: 10661/15469    Discriminator_loss: 0.08570826798677444  Generator_loss: 2.5025582313537598\n",
            "epoch: 9/10,    batch: 10662/15469    Discriminator_loss: 0.08548105508089066  Generator_loss: 2.5055439472198486\n",
            "epoch: 9/10,    batch: 10663/15469    Discriminator_loss: 0.08527495712041855  Generator_loss: 2.5080950260162354\n",
            "epoch: 9/10,    batch: 10664/15469    Discriminator_loss: 0.08502945303916931  Generator_loss: 2.510352849960327\n",
            "epoch: 9/10,    batch: 10665/15469    Discriminator_loss: 0.08480284363031387  Generator_loss: 2.5134735107421875\n",
            "epoch: 9/10,    batch: 10666/15469    Discriminator_loss: 0.08467395603656769  Generator_loss: 2.515692710876465\n",
            "epoch: 9/10,    batch: 10667/15469    Discriminator_loss: 0.08467511832714081  Generator_loss: 2.5184364318847656\n",
            "epoch: 9/10,    batch: 10668/15469    Discriminator_loss: 0.08460383862257004  Generator_loss: 2.520953416824341\n",
            "epoch: 9/10,    batch: 10669/15469    Discriminator_loss: 0.0849776566028595  Generator_loss: 2.5239975452423096\n",
            "epoch: 9/10,    batch: 10670/15469    Discriminator_loss: 0.0880138948559761  Generator_loss: 2.527226448059082\n",
            "epoch: 9/10,    batch: 10671/15469    Discriminator_loss: 0.0872969925403595  Generator_loss: 2.5295634269714355\n",
            "epoch: 9/10,    batch: 10672/15469    Discriminator_loss: 0.08745215088129044  Generator_loss: 2.532191753387451\n",
            "epoch: 9/10,    batch: 10673/15469    Discriminator_loss: 0.09153624624013901  Generator_loss: 2.5345041751861572\n",
            "epoch: 9/10,    batch: 10674/15469    Discriminator_loss: 0.09252747893333435  Generator_loss: 2.5376951694488525\n",
            "epoch: 9/10,    batch: 10675/15469    Discriminator_loss: 0.08237719535827637  Generator_loss: 2.540581226348877\n",
            "epoch: 9/10,    batch: 10676/15469    Discriminator_loss: 0.08213231712579727  Generator_loss: 2.5433788299560547\n",
            "epoch: 9/10,    batch: 10677/15469    Discriminator_loss: 0.0819162055850029  Generator_loss: 2.546229362487793\n",
            "epoch: 9/10,    batch: 10678/15469    Discriminator_loss: 0.08162309229373932  Generator_loss: 2.549009323120117\n",
            "epoch: 9/10,    batch: 10679/15469    Discriminator_loss: 0.08138158172369003  Generator_loss: 2.552090644836426\n",
            "epoch: 9/10,    batch: 10680/15469    Discriminator_loss: 0.08145532011985779  Generator_loss: 2.5550103187561035\n",
            "epoch: 9/10,    batch: 10681/15469    Discriminator_loss: 0.0809604674577713  Generator_loss: 2.5577664375305176\n",
            "epoch: 9/10,    batch: 10682/15469    Discriminator_loss: 0.08105769008398056  Generator_loss: 2.5608971118927\n",
            "epoch: 9/10,    batch: 10683/15469    Discriminator_loss: 0.08037615567445755  Generator_loss: 2.563938617706299\n",
            "epoch: 9/10,    batch: 10684/15469    Discriminator_loss: 0.08011890202760696  Generator_loss: 2.567173480987549\n",
            "epoch: 9/10,    batch: 10685/15469    Discriminator_loss: 0.07985486835241318  Generator_loss: 2.5704269409179688\n",
            "epoch: 9/10,    batch: 10686/15469    Discriminator_loss: 0.07959539443254471  Generator_loss: 2.5736632347106934\n",
            "epoch: 9/10,    batch: 10687/15469    Discriminator_loss: 0.07934388518333435  Generator_loss: 2.5763206481933594\n",
            "epoch: 9/10,    batch: 10688/15469    Discriminator_loss: 0.07907700538635254  Generator_loss: 2.580019474029541\n",
            "epoch: 9/10,    batch: 10689/15469    Discriminator_loss: 0.07879535853862762  Generator_loss: 2.5829217433929443\n",
            "epoch: 9/10,    batch: 10690/15469    Discriminator_loss: 0.07852661609649658  Generator_loss: 2.5864503383636475\n",
            "epoch: 9/10,    batch: 10691/15469    Discriminator_loss: 0.07825803011655807  Generator_loss: 2.589453935623169\n",
            "epoch: 9/10,    batch: 10692/15469    Discriminator_loss: 0.07797075062990189  Generator_loss: 2.5926666259765625\n",
            "epoch: 9/10,    batch: 10693/15469    Discriminator_loss: 0.07775813341140747  Generator_loss: 2.596099376678467\n",
            "epoch: 9/10,    batch: 10694/15469    Discriminator_loss: 0.07746592164039612  Generator_loss: 2.59952449798584\n",
            "epoch: 9/10,    batch: 10695/15469    Discriminator_loss: 0.07720919698476791  Generator_loss: 2.6027297973632812\n",
            "epoch: 9/10,    batch: 10696/15469    Discriminator_loss: 0.07695604115724564  Generator_loss: 2.6060619354248047\n",
            "epoch: 9/10,    batch: 10697/15469    Discriminator_loss: 0.07666970789432526  Generator_loss: 2.6094601154327393\n",
            "epoch: 9/10,    batch: 10698/15469    Discriminator_loss: 0.0764065533876419  Generator_loss: 2.612795829772949\n",
            "epoch: 9/10,    batch: 10699/15469    Discriminator_loss: 0.07615652680397034  Generator_loss: 2.616431713104248\n",
            "epoch: 9/10,    batch: 10700/15469    Discriminator_loss: 0.0758829414844513  Generator_loss: 2.6197409629821777\n",
            "epoch: 9/10,    batch: 10701/15469    Discriminator_loss: 0.0755963996052742  Generator_loss: 2.6229450702667236\n",
            "epoch: 9/10,    batch: 10702/15469    Discriminator_loss: 0.07531075924634933  Generator_loss: 2.6265108585357666\n",
            "epoch: 9/10,    batch: 10703/15469    Discriminator_loss: 0.07505455613136292  Generator_loss: 2.630125045776367\n",
            "epoch: 9/10,    batch: 10704/15469    Discriminator_loss: 0.07478220760822296  Generator_loss: 2.6336302757263184\n",
            "epoch: 9/10,    batch: 10705/15469    Discriminator_loss: 0.07451052963733673  Generator_loss: 2.636934757232666\n",
            "epoch: 9/10,    batch: 10706/15469    Discriminator_loss: 0.07425404340028763  Generator_loss: 2.6404547691345215\n",
            "epoch: 9/10,    batch: 10707/15469    Discriminator_loss: 0.07396972179412842  Generator_loss: 2.6442134380340576\n",
            "epoch: 9/10,    batch: 10708/15469    Discriminator_loss: 0.07367437332868576  Generator_loss: 2.6479249000549316\n",
            "epoch: 9/10,    batch: 10709/15469    Discriminator_loss: 0.0734144002199173  Generator_loss: 2.651648759841919\n",
            "epoch: 9/10,    batch: 10710/15469    Discriminator_loss: 0.07316644489765167  Generator_loss: 2.6551930904388428\n",
            "epoch: 9/10,    batch: 10711/15469    Discriminator_loss: 0.07288117706775665  Generator_loss: 2.658700942993164\n",
            "epoch: 9/10,    batch: 10712/15469    Discriminator_loss: 0.07260746508836746  Generator_loss: 2.662061929702759\n",
            "epoch: 9/10,    batch: 10713/15469    Discriminator_loss: 0.07232648134231567  Generator_loss: 2.665966033935547\n",
            "epoch: 9/10,    batch: 10714/15469    Discriminator_loss: 0.07204806059598923  Generator_loss: 2.6694424152374268\n",
            "epoch: 9/10,    batch: 10715/15469    Discriminator_loss: 0.0717863216996193  Generator_loss: 2.6730566024780273\n",
            "epoch: 9/10,    batch: 10716/15469    Discriminator_loss: 0.07151785492897034  Generator_loss: 2.6769473552703857\n",
            "epoch: 9/10,    batch: 10717/15469    Discriminator_loss: 0.07124505937099457  Generator_loss: 2.680664539337158\n",
            "epoch: 9/10,    batch: 10718/15469    Discriminator_loss: 0.07093778997659683  Generator_loss: 2.684514284133911\n",
            "epoch: 9/10,    batch: 10719/15469    Discriminator_loss: 0.07067861407995224  Generator_loss: 2.6884419918060303\n",
            "epoch: 9/10,    batch: 10720/15469    Discriminator_loss: 0.07039902359247208  Generator_loss: 2.6919288635253906\n",
            "epoch: 9/10,    batch: 10721/15469    Discriminator_loss: 2.7005529403686523  Generator_loss: 2.6809885501861572\n",
            "epoch: 9/10,    batch: 10722/15469    Discriminator_loss: 2.887800455093384  Generator_loss: 2.6584315299987793\n",
            "epoch: 9/10,    batch: 10723/15469    Discriminator_loss: 2.8439433574676514  Generator_loss: 2.6273088455200195\n",
            "epoch: 9/10,    batch: 10724/15469    Discriminator_loss: 2.8140246868133545  Generator_loss: 2.5902700424194336\n",
            "epoch: 9/10,    batch: 10725/15469    Discriminator_loss: 2.7837796211242676  Generator_loss: 2.5496606826782227\n",
            "epoch: 9/10,    batch: 10726/15469    Discriminator_loss: 2.7353219985961914  Generator_loss: 2.5061593055725098\n",
            "epoch: 9/10,    batch: 10727/15469    Discriminator_loss: 2.6824886798858643  Generator_loss: 2.462510824203491\n",
            "epoch: 9/10,    batch: 10728/15469    Discriminator_loss: 2.6596670150756836  Generator_loss: 2.4180727005004883\n",
            "epoch: 9/10,    batch: 10729/15469    Discriminator_loss: 2.594345808029175  Generator_loss: 2.374284505844116\n",
            "epoch: 9/10,    batch: 10730/15469    Discriminator_loss: 2.560260534286499  Generator_loss: 2.331167697906494\n",
            "epoch: 9/10,    batch: 10731/15469    Discriminator_loss: 2.5132346153259277  Generator_loss: 2.2893776893615723\n",
            "epoch: 9/10,    batch: 10732/15469    Discriminator_loss: 2.474174976348877  Generator_loss: 2.248354911804199\n",
            "epoch: 9/10,    batch: 10733/15469    Discriminator_loss: 2.4293482303619385  Generator_loss: 2.2089853286743164\n",
            "epoch: 9/10,    batch: 10734/15469    Discriminator_loss: 2.4057257175445557  Generator_loss: 2.1709370613098145\n",
            "epoch: 9/10,    batch: 10735/15469    Discriminator_loss: 2.359884738922119  Generator_loss: 2.1338963508605957\n",
            "epoch: 9/10,    batch: 10736/15469    Discriminator_loss: 2.3294789791107178  Generator_loss: 2.0983500480651855\n",
            "epoch: 9/10,    batch: 10737/15469    Discriminator_loss: 2.2984604835510254  Generator_loss: 2.064016103744507\n",
            "epoch: 9/10,    batch: 10738/15469    Discriminator_loss: 2.267615795135498  Generator_loss: 2.0308051109313965\n",
            "epoch: 9/10,    batch: 10739/15469    Discriminator_loss: 2.23187255859375  Generator_loss: 1.9986779689788818\n",
            "epoch: 9/10,    batch: 10740/15469    Discriminator_loss: 2.2147536277770996  Generator_loss: 1.9674928188323975\n",
            "epoch: 9/10,    batch: 10741/15469    Discriminator_loss: 2.1876204013824463  Generator_loss: 1.937390923500061\n",
            "epoch: 9/10,    batch: 10742/15469    Discriminator_loss: 2.157409429550171  Generator_loss: 1.9081825017929077\n",
            "epoch: 9/10,    batch: 10743/15469    Discriminator_loss: 2.133458375930786  Generator_loss: 1.8803751468658447\n",
            "epoch: 9/10,    batch: 10744/15469    Discriminator_loss: 2.1061336994171143  Generator_loss: 1.8527840375900269\n",
            "epoch: 9/10,    batch: 10745/15469    Discriminator_loss: 2.084613800048828  Generator_loss: 1.8261940479278564\n",
            "epoch: 9/10,    batch: 10746/15469    Discriminator_loss: 2.0623302459716797  Generator_loss: 1.8004279136657715\n",
            "epoch: 9/10,    batch: 10747/15469    Discriminator_loss: 2.047961711883545  Generator_loss: 1.7752655744552612\n",
            "epoch: 9/10,    batch: 10748/15469    Discriminator_loss: 2.0290427207946777  Generator_loss: 1.7507951259613037\n",
            "epoch: 9/10,    batch: 10749/15469    Discriminator_loss: 2.0116705894470215  Generator_loss: 1.7269864082336426\n",
            "epoch: 9/10,    batch: 10750/15469    Discriminator_loss: 1.9984931945800781  Generator_loss: 1.7043800354003906\n",
            "epoch: 9/10,    batch: 10751/15469    Discriminator_loss: 1.9824823141098022  Generator_loss: 1.6816600561141968\n",
            "epoch: 9/10,    batch: 10752/15469    Discriminator_loss: 1.957494854927063  Generator_loss: 1.6598269939422607\n",
            "epoch: 9/10,    batch: 10753/15469    Discriminator_loss: 1.941900610923767  Generator_loss: 1.6384193897247314\n",
            "epoch: 9/10,    batch: 10754/15469    Discriminator_loss: 1.9124852418899536  Generator_loss: 1.6173652410507202\n",
            "epoch: 9/10,    batch: 10755/15469    Discriminator_loss: 1.8947014808654785  Generator_loss: 1.5966302156448364\n",
            "epoch: 9/10,    batch: 10756/15469    Discriminator_loss: 1.8852124214172363  Generator_loss: 1.576875925064087\n",
            "epoch: 9/10,    batch: 10757/15469    Discriminator_loss: 1.8866978883743286  Generator_loss: 1.5573196411132812\n",
            "epoch: 9/10,    batch: 10758/15469    Discriminator_loss: 1.854718804359436  Generator_loss: 1.5379939079284668\n",
            "epoch: 9/10,    batch: 10759/15469    Discriminator_loss: 1.842036247253418  Generator_loss: 1.519425868988037\n",
            "epoch: 9/10,    batch: 10760/15469    Discriminator_loss: 1.8226524591445923  Generator_loss: 1.5012478828430176\n",
            "epoch: 9/10,    batch: 10761/15469    Discriminator_loss: 1.8143819570541382  Generator_loss: 1.4831783771514893\n",
            "epoch: 9/10,    batch: 10762/15469    Discriminator_loss: 0.7968252897262573  Generator_loss: 1.46840238571167\n",
            "epoch: 9/10,    batch: 10763/15469    Discriminator_loss: 1.0393688678741455  Generator_loss: 1.4559433460235596\n",
            "epoch: 9/10,    batch: 10764/15469    Discriminator_loss: 0.2673482894897461  Generator_loss: 1.4468026161193848\n",
            "epoch: 9/10,    batch: 10765/15469    Discriminator_loss: 0.26993101835250854  Generator_loss: 1.4407806396484375\n",
            "epoch: 9/10,    batch: 10766/15469    Discriminator_loss: 0.2711043953895569  Generator_loss: 1.4371724128723145\n",
            "epoch: 9/10,    batch: 10767/15469    Discriminator_loss: 0.2718816101551056  Generator_loss: 1.4356528520584106\n",
            "epoch: 9/10,    batch: 10768/15469    Discriminator_loss: 0.27217110991477966  Generator_loss: 1.4355840682983398\n",
            "epoch: 9/10,    batch: 10769/15469    Discriminator_loss: 0.27293387055397034  Generator_loss: 1.4366824626922607\n",
            "epoch: 9/10,    batch: 10770/15469    Discriminator_loss: 0.2714320719242096  Generator_loss: 1.4391435384750366\n",
            "epoch: 9/10,    batch: 10771/15469    Discriminator_loss: 0.31483620405197144  Generator_loss: 1.442052960395813\n",
            "epoch: 9/10,    batch: 10772/15469    Discriminator_loss: 1.412782907485962  Generator_loss: 1.442721962928772\n",
            "epoch: 9/10,    batch: 10773/15469    Discriminator_loss: 1.815151572227478  Generator_loss: 1.4400339126586914\n",
            "epoch: 9/10,    batch: 10774/15469    Discriminator_loss: 1.797797441482544  Generator_loss: 1.4347875118255615\n",
            "epoch: 9/10,    batch: 10775/15469    Discriminator_loss: 1.8082904815673828  Generator_loss: 1.4272892475128174\n",
            "epoch: 9/10,    batch: 10776/15469    Discriminator_loss: 1.8226096630096436  Generator_loss: 1.418257474899292\n",
            "epoch: 9/10,    batch: 10777/15469    Discriminator_loss: 1.801607370376587  Generator_loss: 1.407965898513794\n",
            "epoch: 9/10,    batch: 10778/15469    Discriminator_loss: 1.8133182525634766  Generator_loss: 1.3966689109802246\n",
            "epoch: 9/10,    batch: 10779/15469    Discriminator_loss: 1.776702880859375  Generator_loss: 1.3842849731445312\n",
            "epoch: 9/10,    batch: 10780/15469    Discriminator_loss: 1.791471242904663  Generator_loss: 1.3713217973709106\n",
            "epoch: 9/10,    batch: 10781/15469    Discriminator_loss: 1.814107894897461  Generator_loss: 1.3574858903884888\n",
            "epoch: 9/10,    batch: 10782/15469    Discriminator_loss: 1.7660521268844604  Generator_loss: 1.3434607982635498\n",
            "epoch: 9/10,    batch: 10783/15469    Discriminator_loss: 1.7331454753875732  Generator_loss: 1.3290677070617676\n",
            "epoch: 9/10,    batch: 10784/15469    Discriminator_loss: 1.7068485021591187  Generator_loss: 1.3146576881408691\n",
            "epoch: 9/10,    batch: 10785/15469    Discriminator_loss: 1.7141504287719727  Generator_loss: 1.3002393245697021\n",
            "epoch: 9/10,    batch: 10786/15469    Discriminator_loss: 1.7221589088439941  Generator_loss: 1.2857322692871094\n",
            "epoch: 9/10,    batch: 10787/15469    Discriminator_loss: 1.6741325855255127  Generator_loss: 1.2713122367858887\n",
            "epoch: 9/10,    batch: 10788/15469    Discriminator_loss: 1.5272650718688965  Generator_loss: 1.2575044631958008\n",
            "epoch: 9/10,    batch: 10789/15469    Discriminator_loss: 0.3371259272098541  Generator_loss: 1.2477798461914062\n",
            "epoch: 9/10,    batch: 10790/15469    Discriminator_loss: 0.3403744399547577  Generator_loss: 1.2414329051971436\n",
            "epoch: 9/10,    batch: 10791/15469    Discriminator_loss: 0.3423892557621002  Generator_loss: 1.2376391887664795\n",
            "epoch: 9/10,    batch: 10792/15469    Discriminator_loss: 0.3434202969074249  Generator_loss: 1.2361726760864258\n",
            "epoch: 9/10,    batch: 10793/15469    Discriminator_loss: 0.34366077184677124  Generator_loss: 1.2364355325698853\n",
            "epoch: 9/10,    batch: 10794/15469    Discriminator_loss: 0.34320539236068726  Generator_loss: 1.2381497621536255\n",
            "epoch: 9/10,    batch: 10795/15469    Discriminator_loss: 0.3422558307647705  Generator_loss: 1.2410342693328857\n",
            "epoch: 9/10,    batch: 10796/15469    Discriminator_loss: 0.34084850549697876  Generator_loss: 1.2450683116912842\n",
            "epoch: 9/10,    batch: 10797/15469    Discriminator_loss: 0.3390544652938843  Generator_loss: 1.2495814561843872\n",
            "epoch: 9/10,    batch: 10798/15469    Discriminator_loss: 0.33703747391700745  Generator_loss: 1.2551023960113525\n",
            "epoch: 9/10,    batch: 10799/15469    Discriminator_loss: 0.33477240800857544  Generator_loss: 1.2609530687332153\n",
            "epoch: 9/10,    batch: 10800/15469    Discriminator_loss: 0.3323493003845215  Generator_loss: 1.2673614025115967\n",
            "epoch: 9/10,    batch: 10801/15469    Discriminator_loss: 0.3297985792160034  Generator_loss: 1.2739578485488892\n",
            "epoch: 9/10,    batch: 10802/15469    Discriminator_loss: 0.3271770775318146  Generator_loss: 1.2808674573898315\n",
            "epoch: 9/10,    batch: 10803/15469    Discriminator_loss: 0.3244362771511078  Generator_loss: 1.2879745960235596\n",
            "epoch: 9/10,    batch: 10804/15469    Discriminator_loss: 0.32167577743530273  Generator_loss: 1.2952678203582764\n",
            "epoch: 9/10,    batch: 10805/15469    Discriminator_loss: 0.3188905715942383  Generator_loss: 1.3027067184448242\n",
            "epoch: 9/10,    batch: 10806/15469    Discriminator_loss: 0.31609585881233215  Generator_loss: 1.310321569442749\n",
            "epoch: 9/10,    batch: 10807/15469    Discriminator_loss: 0.31329941749572754  Generator_loss: 1.317832112312317\n",
            "epoch: 9/10,    batch: 10808/15469    Discriminator_loss: 0.3105122148990631  Generator_loss: 1.3255339860916138\n",
            "epoch: 9/10,    batch: 10809/15469    Discriminator_loss: 0.30773839354515076  Generator_loss: 1.333122730255127\n",
            "epoch: 9/10,    batch: 10810/15469    Discriminator_loss: 0.30497828125953674  Generator_loss: 1.340768575668335\n",
            "epoch: 9/10,    batch: 10811/15469    Discriminator_loss: 0.3022310137748718  Generator_loss: 1.348481297492981\n",
            "epoch: 9/10,    batch: 10812/15469    Discriminator_loss: 0.29959243535995483  Generator_loss: 1.356021523475647\n",
            "epoch: 9/10,    batch: 10813/15469    Discriminator_loss: 0.29691243171691895  Generator_loss: 1.3637044429779053\n",
            "epoch: 9/10,    batch: 10814/15469    Discriminator_loss: 0.2943136394023895  Generator_loss: 1.3713297843933105\n",
            "epoch: 9/10,    batch: 10815/15469    Discriminator_loss: 0.29171136021614075  Generator_loss: 1.37885582447052\n",
            "epoch: 9/10,    batch: 10816/15469    Discriminator_loss: 0.28918054699897766  Generator_loss: 1.386319637298584\n",
            "epoch: 9/10,    batch: 10817/15469    Discriminator_loss: 0.28669533133506775  Generator_loss: 1.3938586711883545\n",
            "epoch: 9/10,    batch: 10818/15469    Discriminator_loss: 0.28422680497169495  Generator_loss: 1.4012775421142578\n",
            "epoch: 9/10,    batch: 10819/15469    Discriminator_loss: 0.28180748224258423  Generator_loss: 1.408536672592163\n",
            "epoch: 9/10,    batch: 10820/15469    Discriminator_loss: 0.27943751215934753  Generator_loss: 1.416062831878662\n",
            "epoch: 9/10,    batch: 10821/15469    Discriminator_loss: 0.27709338068962097  Generator_loss: 1.4231243133544922\n",
            "epoch: 9/10,    batch: 10822/15469    Discriminator_loss: 0.2747702896595001  Generator_loss: 1.4303752183914185\n",
            "epoch: 9/10,    batch: 10823/15469    Discriminator_loss: 0.2725239396095276  Generator_loss: 1.4375461339950562\n",
            "epoch: 9/10,    batch: 10824/15469    Discriminator_loss: 0.270265132188797  Generator_loss: 1.4446892738342285\n",
            "epoch: 9/10,    batch: 10825/15469    Discriminator_loss: 0.26809659600257874  Generator_loss: 1.4517822265625\n",
            "epoch: 9/10,    batch: 10826/15469    Discriminator_loss: 0.2659339904785156  Generator_loss: 1.4588279724121094\n",
            "epoch: 9/10,    batch: 10827/15469    Discriminator_loss: 0.2637973129749298  Generator_loss: 1.4657917022705078\n",
            "epoch: 9/10,    batch: 10828/15469    Discriminator_loss: 0.26170995831489563  Generator_loss: 1.472722053527832\n",
            "epoch: 9/10,    batch: 10829/15469    Discriminator_loss: 0.2596352994441986  Generator_loss: 1.4796051979064941\n",
            "epoch: 9/10,    batch: 10830/15469    Discriminator_loss: 0.2576194405555725  Generator_loss: 1.4865556955337524\n",
            "epoch: 9/10,    batch: 10831/15469    Discriminator_loss: 0.2556118667125702  Generator_loss: 1.49329674243927\n",
            "epoch: 9/10,    batch: 10832/15469    Discriminator_loss: 0.2536377012729645  Generator_loss: 1.5000969171524048\n",
            "epoch: 9/10,    batch: 10833/15469    Discriminator_loss: 0.2517032027244568  Generator_loss: 1.506835699081421\n",
            "epoch: 9/10,    batch: 10834/15469    Discriminator_loss: 0.2497837096452713  Generator_loss: 1.5135130882263184\n",
            "epoch: 9/10,    batch: 10835/15469    Discriminator_loss: 0.24787822365760803  Generator_loss: 1.5201698541641235\n",
            "epoch: 9/10,    batch: 10836/15469    Discriminator_loss: 0.24601882696151733  Generator_loss: 1.5268316268920898\n",
            "epoch: 9/10,    batch: 10837/15469    Discriminator_loss: 0.24418491125106812  Generator_loss: 1.5333948135375977\n",
            "epoch: 9/10,    batch: 10838/15469    Discriminator_loss: 0.242374986410141  Generator_loss: 1.5399982929229736\n",
            "epoch: 9/10,    batch: 10839/15469    Discriminator_loss: 0.24058133363723755  Generator_loss: 1.5464937686920166\n",
            "epoch: 9/10,    batch: 10840/15469    Discriminator_loss: 0.23880982398986816  Generator_loss: 1.5530215501785278\n",
            "epoch: 9/10,    batch: 10841/15469    Discriminator_loss: 0.23707012832164764  Generator_loss: 1.5595216751098633\n",
            "epoch: 9/10,    batch: 10842/15469    Discriminator_loss: 0.23534969985485077  Generator_loss: 1.5658841133117676\n",
            "epoch: 9/10,    batch: 10843/15469    Discriminator_loss: 0.23365649580955505  Generator_loss: 1.5723347663879395\n",
            "epoch: 9/10,    batch: 10844/15469    Discriminator_loss: 0.23197263479232788  Generator_loss: 1.5786701440811157\n",
            "epoch: 9/10,    batch: 10845/15469    Discriminator_loss: 0.2303212583065033  Generator_loss: 1.5850036144256592\n",
            "epoch: 9/10,    batch: 10846/15469    Discriminator_loss: 0.2286859154701233  Generator_loss: 1.5913655757904053\n",
            "epoch: 9/10,    batch: 10847/15469    Discriminator_loss: 0.22706690430641174  Generator_loss: 1.5976340770721436\n",
            "epoch: 9/10,    batch: 10848/15469    Discriminator_loss: 0.22547689080238342  Generator_loss: 1.603868842124939\n",
            "epoch: 9/10,    batch: 10849/15469    Discriminator_loss: 0.22390387952327728  Generator_loss: 1.610137939453125\n",
            "epoch: 9/10,    batch: 10850/15469    Discriminator_loss: 0.2223501056432724  Generator_loss: 1.6163935661315918\n",
            "epoch: 9/10,    batch: 10851/15469    Discriminator_loss: 0.22080816328525543  Generator_loss: 1.6225436925888062\n",
            "epoch: 9/10,    batch: 10852/15469    Discriminator_loss: 0.2192905992269516  Generator_loss: 1.6287040710449219\n",
            "epoch: 9/10,    batch: 10853/15469    Discriminator_loss: 0.21778538823127747  Generator_loss: 1.6348212957382202\n",
            "epoch: 9/10,    batch: 10854/15469    Discriminator_loss: 0.21629469096660614  Generator_loss: 1.6409173011779785\n",
            "epoch: 9/10,    batch: 10855/15469    Discriminator_loss: 0.21483489871025085  Generator_loss: 1.6470777988433838\n",
            "epoch: 9/10,    batch: 10856/15469    Discriminator_loss: 0.2133886218070984  Generator_loss: 1.653085470199585\n",
            "epoch: 9/10,    batch: 10857/15469    Discriminator_loss: 0.21195325255393982  Generator_loss: 1.6591103076934814\n",
            "epoch: 9/10,    batch: 10858/15469    Discriminator_loss: 0.21052739024162292  Generator_loss: 1.6651413440704346\n",
            "epoch: 9/10,    batch: 10859/15469    Discriminator_loss: 0.20912377536296844  Generator_loss: 1.6711320877075195\n",
            "epoch: 9/10,    batch: 10860/15469    Discriminator_loss: 0.2077433317899704  Generator_loss: 1.6771515607833862\n",
            "epoch: 9/10,    batch: 10861/15469    Discriminator_loss: 0.20637111365795135  Generator_loss: 1.6830368041992188\n",
            "epoch: 9/10,    batch: 10862/15469    Discriminator_loss: 0.20501510798931122  Generator_loss: 1.6889811754226685\n",
            "epoch: 9/10,    batch: 10863/15469    Discriminator_loss: 0.20366334915161133  Generator_loss: 1.6949186325073242\n",
            "epoch: 9/10,    batch: 10864/15469    Discriminator_loss: 0.2023363560438156  Generator_loss: 1.7008310556411743\n",
            "epoch: 9/10,    batch: 10865/15469    Discriminator_loss: 0.20102211833000183  Generator_loss: 1.706627607345581\n",
            "epoch: 9/10,    batch: 10866/15469    Discriminator_loss: 0.19973595440387726  Generator_loss: 1.7124836444854736\n",
            "epoch: 9/10,    batch: 10867/15469    Discriminator_loss: 0.19844232499599457  Generator_loss: 1.7183456420898438\n",
            "epoch: 9/10,    batch: 10868/15469    Discriminator_loss: 0.19717681407928467  Generator_loss: 1.7241523265838623\n",
            "epoch: 9/10,    batch: 10869/15469    Discriminator_loss: 0.1958995759487152  Generator_loss: 1.7299768924713135\n",
            "epoch: 9/10,    batch: 10870/15469    Discriminator_loss: 0.194663867354393  Generator_loss: 1.735689401626587\n",
            "epoch: 9/10,    batch: 10871/15469    Discriminator_loss: 0.1934371441602707  Generator_loss: 1.7414498329162598\n",
            "epoch: 9/10,    batch: 10872/15469    Discriminator_loss: 0.19221153855323792  Generator_loss: 1.747182846069336\n",
            "epoch: 9/10,    batch: 10873/15469    Discriminator_loss: 0.1909952014684677  Generator_loss: 1.7529654502868652\n",
            "epoch: 9/10,    batch: 10874/15469    Discriminator_loss: 0.18981313705444336  Generator_loss: 1.7585910558700562\n",
            "epoch: 9/10,    batch: 10875/15469    Discriminator_loss: 0.18861402571201324  Generator_loss: 1.7642993927001953\n",
            "epoch: 9/10,    batch: 10876/15469    Discriminator_loss: 0.1874493807554245  Generator_loss: 1.7699275016784668\n",
            "epoch: 9/10,    batch: 10877/15469    Discriminator_loss: 0.18628710508346558  Generator_loss: 1.7755851745605469\n",
            "epoch: 9/10,    batch: 10878/15469    Discriminator_loss: 0.18512676656246185  Generator_loss: 1.781275987625122\n",
            "epoch: 9/10,    batch: 10879/15469    Discriminator_loss: 0.18398483097553253  Generator_loss: 1.7868292331695557\n",
            "epoch: 9/10,    batch: 10880/15469    Discriminator_loss: 0.1828586757183075  Generator_loss: 1.7924926280975342\n",
            "epoch: 9/10,    batch: 10881/15469    Discriminator_loss: 0.1817380040884018  Generator_loss: 1.798051118850708\n",
            "epoch: 9/10,    batch: 10882/15469    Discriminator_loss: 0.18063661456108093  Generator_loss: 1.803619146347046\n",
            "epoch: 9/10,    batch: 10883/15469    Discriminator_loss: 0.17966094613075256  Generator_loss: 1.809205174446106\n",
            "epoch: 9/10,    batch: 10884/15469    Discriminator_loss: 0.1787327378988266  Generator_loss: 1.8147019147872925\n",
            "epoch: 9/10,    batch: 10885/15469    Discriminator_loss: 0.1782422810792923  Generator_loss: 1.8202346563339233\n",
            "epoch: 9/10,    batch: 10886/15469    Discriminator_loss: 0.17628949880599976  Generator_loss: 1.825749158859253\n",
            "epoch: 9/10,    batch: 10887/15469    Discriminator_loss: 0.17524057626724243  Generator_loss: 1.831242322921753\n",
            "epoch: 9/10,    batch: 10888/15469    Discriminator_loss: 0.17418500781059265  Generator_loss: 1.8367362022399902\n",
            "epoch: 9/10,    batch: 10889/15469    Discriminator_loss: 0.1731591522693634  Generator_loss: 1.8422917127609253\n",
            "epoch: 9/10,    batch: 10890/15469    Discriminator_loss: 0.1721363216638565  Generator_loss: 1.8476157188415527\n",
            "epoch: 9/10,    batch: 10891/15469    Discriminator_loss: 0.17110490798950195  Generator_loss: 1.853070616722107\n",
            "epoch: 9/10,    batch: 10892/15469    Discriminator_loss: 0.1700863391160965  Generator_loss: 1.8584959506988525\n",
            "epoch: 9/10,    batch: 10893/15469    Discriminator_loss: 0.16909641027450562  Generator_loss: 1.8639248609542847\n",
            "epoch: 9/10,    batch: 10894/15469    Discriminator_loss: 0.16810032725334167  Generator_loss: 1.8693156242370605\n",
            "epoch: 9/10,    batch: 10895/15469    Discriminator_loss: 0.16711565852165222  Generator_loss: 1.8747267723083496\n",
            "epoch: 9/10,    batch: 10896/15469    Discriminator_loss: 0.16614913940429688  Generator_loss: 1.8801302909851074\n",
            "epoch: 9/10,    batch: 10897/15469    Discriminator_loss: 0.16516971588134766  Generator_loss: 1.8854997158050537\n",
            "epoch: 9/10,    batch: 10898/15469    Discriminator_loss: 0.16421698033809662  Generator_loss: 1.8908696174621582\n",
            "epoch: 9/10,    batch: 10899/15469    Discriminator_loss: 0.16326278448104858  Generator_loss: 1.8962092399597168\n",
            "epoch: 9/10,    batch: 10900/15469    Discriminator_loss: 0.16232626140117645  Generator_loss: 1.901550054550171\n",
            "epoch: 9/10,    batch: 10901/15469    Discriminator_loss: 0.1613844782114029  Generator_loss: 1.9068641662597656\n",
            "epoch: 9/10,    batch: 10902/15469    Discriminator_loss: 0.16045445203781128  Generator_loss: 1.9121954441070557\n",
            "epoch: 9/10,    batch: 10903/15469    Discriminator_loss: 0.1595449298620224  Generator_loss: 1.9175159931182861\n",
            "epoch: 9/10,    batch: 10904/15469    Discriminator_loss: 0.1586180478334427  Generator_loss: 1.9227323532104492\n",
            "epoch: 9/10,    batch: 10905/15469    Discriminator_loss: 0.15772423148155212  Generator_loss: 1.9280152320861816\n",
            "epoch: 9/10,    batch: 10906/15469    Discriminator_loss: 0.15685111284255981  Generator_loss: 1.9333491325378418\n",
            "epoch: 9/10,    batch: 10907/15469    Discriminator_loss: 0.15597721934318542  Generator_loss: 1.9386138916015625\n",
            "epoch: 9/10,    batch: 10908/15469    Discriminator_loss: 0.15513041615486145  Generator_loss: 1.9438443183898926\n",
            "epoch: 9/10,    batch: 10909/15469    Discriminator_loss: 0.15427425503730774  Generator_loss: 1.9490463733673096\n",
            "epoch: 9/10,    batch: 10910/15469    Discriminator_loss: 0.15353183448314667  Generator_loss: 1.9543030261993408\n",
            "epoch: 9/10,    batch: 10911/15469    Discriminator_loss: 0.15289025008678436  Generator_loss: 1.959529995918274\n",
            "epoch: 9/10,    batch: 10912/15469    Discriminator_loss: 0.152274027466774  Generator_loss: 1.96477210521698\n",
            "epoch: 9/10,    batch: 10913/15469    Discriminator_loss: 0.15281638503074646  Generator_loss: 1.9699962139129639\n",
            "epoch: 9/10,    batch: 10914/15469    Discriminator_loss: 0.15949204564094543  Generator_loss: 1.9750151634216309\n",
            "epoch: 9/10,    batch: 10915/15469    Discriminator_loss: 0.18028518557548523  Generator_loss: 1.979931116104126\n",
            "epoch: 9/10,    batch: 10916/15469    Discriminator_loss: 0.1630963683128357  Generator_loss: 1.984664797782898\n",
            "epoch: 9/10,    batch: 10917/15469    Discriminator_loss: 0.18376320600509644  Generator_loss: 1.9892765283584595\n",
            "epoch: 9/10,    batch: 10918/15469    Discriminator_loss: 0.17863446474075317  Generator_loss: 1.9938318729400635\n",
            "epoch: 9/10,    batch: 10919/15469    Discriminator_loss: 0.1461215764284134  Generator_loss: 1.998473882675171\n",
            "epoch: 9/10,    batch: 10920/15469    Discriminator_loss: 0.14540381729602814  Generator_loss: 2.003248691558838\n",
            "epoch: 9/10,    batch: 10921/15469    Discriminator_loss: 0.14467358589172363  Generator_loss: 2.007936477661133\n",
            "epoch: 9/10,    batch: 10922/15469    Discriminator_loss: 0.14392848312854767  Generator_loss: 2.0128493309020996\n",
            "epoch: 9/10,    batch: 10923/15469    Discriminator_loss: 0.14318038523197174  Generator_loss: 2.0176708698272705\n",
            "epoch: 9/10,    batch: 10924/15469    Discriminator_loss: 0.14242427051067352  Generator_loss: 2.022778272628784\n",
            "epoch: 9/10,    batch: 10925/15469    Discriminator_loss: 0.14167259633541107  Generator_loss: 2.027635097503662\n",
            "epoch: 9/10,    batch: 10926/15469    Discriminator_loss: 0.1409299522638321  Generator_loss: 2.0326409339904785\n",
            "epoch: 9/10,    batch: 10927/15469    Discriminator_loss: 0.14017416536808014  Generator_loss: 2.0376009941101074\n",
            "epoch: 9/10,    batch: 10928/15469    Discriminator_loss: 0.13943403959274292  Generator_loss: 2.04262113571167\n",
            "epoch: 9/10,    batch: 10929/15469    Discriminator_loss: 0.1387074887752533  Generator_loss: 2.047656297683716\n",
            "epoch: 9/10,    batch: 10930/15469    Discriminator_loss: 0.13798712193965912  Generator_loss: 2.0526862144470215\n",
            "epoch: 9/10,    batch: 10931/15469    Discriminator_loss: 0.13728003203868866  Generator_loss: 2.0576581954956055\n",
            "epoch: 9/10,    batch: 10932/15469    Discriminator_loss: 0.13653001189231873  Generator_loss: 2.062774181365967\n",
            "epoch: 9/10,    batch: 10933/15469    Discriminator_loss: 0.13581576943397522  Generator_loss: 2.067802906036377\n",
            "epoch: 9/10,    batch: 10934/15469    Discriminator_loss: 0.13511903584003448  Generator_loss: 2.072821617126465\n",
            "epoch: 9/10,    batch: 10935/15469    Discriminator_loss: 0.13436126708984375  Generator_loss: 2.0778684616088867\n",
            "epoch: 9/10,    batch: 10936/15469    Discriminator_loss: 0.13370929658412933  Generator_loss: 2.082845449447632\n",
            "epoch: 9/10,    batch: 10937/15469    Discriminator_loss: 0.13305449485778809  Generator_loss: 2.0878732204437256\n",
            "epoch: 9/10,    batch: 10938/15469    Discriminator_loss: 0.1323375403881073  Generator_loss: 2.0929436683654785\n",
            "epoch: 9/10,    batch: 10939/15469    Discriminator_loss: 0.13168714940547943  Generator_loss: 2.0980100631713867\n",
            "epoch: 9/10,    batch: 10940/15469    Discriminator_loss: 0.13100090622901917  Generator_loss: 2.102928638458252\n",
            "epoch: 9/10,    batch: 10941/15469    Discriminator_loss: 0.1299506574869156  Generator_loss: 2.1080877780914307\n",
            "epoch: 9/10,    batch: 10942/15469    Discriminator_loss: 0.12925098836421967  Generator_loss: 2.1130619049072266\n",
            "epoch: 9/10,    batch: 10943/15469    Discriminator_loss: 0.12857457995414734  Generator_loss: 2.1180126667022705\n",
            "epoch: 9/10,    batch: 10944/15469    Discriminator_loss: 0.12786997854709625  Generator_loss: 2.1230309009552\n",
            "epoch: 9/10,    batch: 10945/15469    Discriminator_loss: 0.1273200660943985  Generator_loss: 2.1280617713928223\n",
            "epoch: 9/10,    batch: 10946/15469    Discriminator_loss: 0.12651552259922028  Generator_loss: 2.133009910583496\n",
            "epoch: 9/10,    batch: 10947/15469    Discriminator_loss: 0.12584728002548218  Generator_loss: 2.13804030418396\n",
            "epoch: 9/10,    batch: 10948/15469    Discriminator_loss: 0.12517695128917694  Generator_loss: 2.143064022064209\n",
            "epoch: 9/10,    batch: 10949/15469    Discriminator_loss: 0.12451228499412537  Generator_loss: 2.1480276584625244\n",
            "epoch: 9/10,    batch: 10950/15469    Discriminator_loss: 0.12384998798370361  Generator_loss: 2.153172492980957\n",
            "epoch: 9/10,    batch: 10951/15469    Discriminator_loss: 0.12319410592317581  Generator_loss: 2.158017635345459\n",
            "epoch: 9/10,    batch: 10952/15469    Discriminator_loss: 0.12253772467374802  Generator_loss: 2.1630172729492188\n",
            "epoch: 9/10,    batch: 10953/15469    Discriminator_loss: 0.1218976378440857  Generator_loss: 2.1679654121398926\n",
            "epoch: 9/10,    batch: 10954/15469    Discriminator_loss: 0.12124131619930267  Generator_loss: 2.1730189323425293\n",
            "epoch: 9/10,    batch: 10955/15469    Discriminator_loss: 0.12060151994228363  Generator_loss: 2.178081512451172\n",
            "epoch: 9/10,    batch: 10956/15469    Discriminator_loss: 0.11997343599796295  Generator_loss: 2.1829419136047363\n",
            "epoch: 9/10,    batch: 10957/15469    Discriminator_loss: 0.11934280395507812  Generator_loss: 2.187939167022705\n",
            "epoch: 9/10,    batch: 10958/15469    Discriminator_loss: 0.11870647966861725  Generator_loss: 2.1929242610931396\n",
            "epoch: 9/10,    batch: 10959/15469    Discriminator_loss: 0.11808404326438904  Generator_loss: 2.197906017303467\n",
            "epoch: 9/10,    batch: 10960/15469    Discriminator_loss: 0.1174691766500473  Generator_loss: 2.202901840209961\n",
            "epoch: 9/10,    batch: 10961/15469    Discriminator_loss: 0.11684250086545944  Generator_loss: 2.2079546451568604\n",
            "epoch: 9/10,    batch: 10962/15469    Discriminator_loss: 0.11621960252523422  Generator_loss: 2.212855339050293\n",
            "epoch: 9/10,    batch: 10963/15469    Discriminator_loss: 0.1156192198395729  Generator_loss: 2.2177929878234863\n",
            "epoch: 9/10,    batch: 10964/15469    Discriminator_loss: 0.11500842124223709  Generator_loss: 2.222777843475342\n",
            "epoch: 9/10,    batch: 10965/15469    Discriminator_loss: 0.11440357565879822  Generator_loss: 2.227700710296631\n",
            "epoch: 9/10,    batch: 10966/15469    Discriminator_loss: 0.11380621790885925  Generator_loss: 2.2327051162719727\n",
            "epoch: 9/10,    batch: 10967/15469    Discriminator_loss: 0.1132102981209755  Generator_loss: 2.2376630306243896\n",
            "epoch: 9/10,    batch: 10968/15469    Discriminator_loss: 0.11261297762393951  Generator_loss: 2.2426300048828125\n",
            "epoch: 9/10,    batch: 10969/15469    Discriminator_loss: 0.11201675981283188  Generator_loss: 2.24755597114563\n",
            "epoch: 9/10,    batch: 10970/15469    Discriminator_loss: 0.1114407330751419  Generator_loss: 2.252514362335205\n",
            "epoch: 9/10,    batch: 10971/15469    Discriminator_loss: 0.11085664480924606  Generator_loss: 2.2575011253356934\n",
            "epoch: 9/10,    batch: 10972/15469    Discriminator_loss: 0.11026600748300552  Generator_loss: 2.262505531311035\n",
            "epoch: 9/10,    batch: 10973/15469    Discriminator_loss: 0.1096964180469513  Generator_loss: 2.2674875259399414\n",
            "epoch: 9/10,    batch: 10974/15469    Discriminator_loss: 0.10911724716424942  Generator_loss: 2.2723984718322754\n",
            "epoch: 9/10,    batch: 10975/15469    Discriminator_loss: 0.10855328291654587  Generator_loss: 2.277446746826172\n",
            "epoch: 9/10,    batch: 10976/15469    Discriminator_loss: 0.10798662155866623  Generator_loss: 2.2822957038879395\n",
            "epoch: 9/10,    batch: 10977/15469    Discriminator_loss: 0.10742464661598206  Generator_loss: 2.2872509956359863\n",
            "epoch: 9/10,    batch: 10978/15469    Discriminator_loss: 0.10686187446117401  Generator_loss: 2.292255401611328\n",
            "epoch: 9/10,    batch: 10979/15469    Discriminator_loss: 0.10630389302968979  Generator_loss: 2.2971677780151367\n",
            "epoch: 9/10,    batch: 10980/15469    Discriminator_loss: 0.10575350373983383  Generator_loss: 2.302124500274658\n",
            "epoch: 9/10,    batch: 10981/15469    Discriminator_loss: 0.10520545393228531  Generator_loss: 2.307102918624878\n",
            "epoch: 9/10,    batch: 10982/15469    Discriminator_loss: 0.10464821755886078  Generator_loss: 2.3119964599609375\n",
            "epoch: 9/10,    batch: 10983/15469    Discriminator_loss: 0.1041065976023674  Generator_loss: 2.3169708251953125\n",
            "epoch: 9/10,    batch: 10984/15469    Discriminator_loss: 0.10355984419584274  Generator_loss: 2.321904182434082\n",
            "epoch: 9/10,    batch: 10985/15469    Discriminator_loss: 0.10302872955799103  Generator_loss: 2.3268401622772217\n",
            "epoch: 9/10,    batch: 10986/15469    Discriminator_loss: 0.10249491780996323  Generator_loss: 2.331836223602295\n",
            "epoch: 9/10,    batch: 10987/15469    Discriminator_loss: 0.10195817798376083  Generator_loss: 2.3367581367492676\n",
            "epoch: 9/10,    batch: 10988/15469    Discriminator_loss: 0.10142824798822403  Generator_loss: 2.341844081878662\n",
            "epoch: 9/10,    batch: 10989/15469    Discriminator_loss: 0.10090013593435287  Generator_loss: 2.3467366695404053\n",
            "epoch: 9/10,    batch: 10990/15469    Discriminator_loss: 0.1003773957490921  Generator_loss: 2.351637840270996\n",
            "epoch: 9/10,    batch: 10991/15469    Discriminator_loss: 0.09984961152076721  Generator_loss: 2.3566360473632812\n",
            "epoch: 9/10,    batch: 10992/15469    Discriminator_loss: 0.09935023635625839  Generator_loss: 2.361546277999878\n",
            "epoch: 9/10,    batch: 10993/15469    Discriminator_loss: 0.09882115572690964  Generator_loss: 2.3665871620178223\n",
            "epoch: 9/10,    batch: 10994/15469    Discriminator_loss: 0.09830725193023682  Generator_loss: 2.371530055999756\n",
            "epoch: 9/10,    batch: 10995/15469    Discriminator_loss: 0.09780728071928024  Generator_loss: 2.376376152038574\n",
            "epoch: 9/10,    batch: 10996/15469    Discriminator_loss: 0.09729952365159988  Generator_loss: 2.3813486099243164\n",
            "epoch: 9/10,    batch: 10997/15469    Discriminator_loss: 0.09680145233869553  Generator_loss: 2.386204481124878\n",
            "epoch: 9/10,    batch: 10998/15469    Discriminator_loss: 0.09629307687282562  Generator_loss: 2.39119815826416\n",
            "epoch: 9/10,    batch: 10999/15469    Discriminator_loss: 0.0957992747426033  Generator_loss: 2.396174907684326\n",
            "epoch: 9/10,    batch: 11000/15469    Discriminator_loss: 0.09529971331357956  Generator_loss: 2.401066780090332\n",
            "epoch: 9/10,    batch: 11001/15469    Discriminator_loss: 0.0948028638958931  Generator_loss: 2.4060940742492676\n",
            "epoch: 9/10,    batch: 11002/15469    Discriminator_loss: 0.09431128948926926  Generator_loss: 2.410891056060791\n",
            "epoch: 9/10,    batch: 11003/15469    Discriminator_loss: 0.09381961077451706  Generator_loss: 2.4159388542175293\n",
            "epoch: 9/10,    batch: 11004/15469    Discriminator_loss: 0.09335137903690338  Generator_loss: 2.4207935333251953\n",
            "epoch: 9/10,    batch: 11005/15469    Discriminator_loss: 0.09285735338926315  Generator_loss: 2.425785779953003\n",
            "epoch: 9/10,    batch: 11006/15469    Discriminator_loss: 0.09238087385892868  Generator_loss: 2.430673599243164\n",
            "epoch: 9/10,    batch: 11007/15469    Discriminator_loss: 0.09190832078456879  Generator_loss: 2.4356696605682373\n",
            "epoch: 9/10,    batch: 11008/15469    Discriminator_loss: 0.09143849462270737  Generator_loss: 2.440577745437622\n",
            "epoch: 9/10,    batch: 11009/15469    Discriminator_loss: 0.09095852077007294  Generator_loss: 2.4455692768096924\n",
            "epoch: 9/10,    batch: 11010/15469    Discriminator_loss: 0.09048596769571304  Generator_loss: 2.450479507446289\n",
            "epoch: 9/10,    batch: 11011/15469    Discriminator_loss: 0.09002988040447235  Generator_loss: 2.4553260803222656\n",
            "epoch: 9/10,    batch: 11012/15469    Discriminator_loss: 0.08956252038478851  Generator_loss: 2.4604034423828125\n",
            "epoch: 9/10,    batch: 11013/15469    Discriminator_loss: 0.08910755813121796  Generator_loss: 2.4653115272521973\n",
            "epoch: 9/10,    batch: 11014/15469    Discriminator_loss: 0.08863089978694916  Generator_loss: 2.470150947570801\n",
            "epoch: 9/10,    batch: 11015/15469    Discriminator_loss: 0.08818763494491577  Generator_loss: 2.475101947784424\n",
            "epoch: 9/10,    batch: 11016/15469    Discriminator_loss: 0.08774461597204208  Generator_loss: 2.480013608932495\n",
            "epoch: 9/10,    batch: 11017/15469    Discriminator_loss: 0.08728665858507156  Generator_loss: 2.484980583190918\n",
            "epoch: 9/10,    batch: 11018/15469    Discriminator_loss: 0.08684930950403214  Generator_loss: 2.489823818206787\n",
            "epoch: 9/10,    batch: 11019/15469    Discriminator_loss: 0.086395762860775  Generator_loss: 2.4947166442871094\n",
            "epoch: 9/10,    batch: 11020/15469    Discriminator_loss: 0.08594818413257599  Generator_loss: 2.4996588230133057\n",
            "epoch: 9/10,    batch: 11021/15469    Discriminator_loss: 0.08551812171936035  Generator_loss: 2.5046417713165283\n",
            "epoch: 9/10,    batch: 11022/15469    Discriminator_loss: 0.08507988601922989  Generator_loss: 2.509542942047119\n",
            "epoch: 9/10,    batch: 11023/15469    Discriminator_loss: 0.08463883399963379  Generator_loss: 2.5144896507263184\n",
            "epoch: 9/10,    batch: 11024/15469    Discriminator_loss: 0.08421344310045242  Generator_loss: 2.5193819999694824\n",
            "epoch: 9/10,    batch: 11025/15469    Discriminator_loss: 0.08378305286169052  Generator_loss: 2.5242433547973633\n",
            "epoch: 9/10,    batch: 11026/15469    Discriminator_loss: 0.08335475623607635  Generator_loss: 2.5292294025421143\n",
            "epoch: 9/10,    batch: 11027/15469    Discriminator_loss: 0.0829349085688591  Generator_loss: 2.534120798110962\n",
            "epoch: 9/10,    batch: 11028/15469    Discriminator_loss: 0.08250226825475693  Generator_loss: 2.5390801429748535\n",
            "epoch: 9/10,    batch: 11029/15469    Discriminator_loss: 0.08208960294723511  Generator_loss: 2.5438005924224854\n",
            "epoch: 9/10,    batch: 11030/15469    Discriminator_loss: 0.08166153728961945  Generator_loss: 2.548689365386963\n",
            "epoch: 9/10,    batch: 11031/15469    Discriminator_loss: 0.08125417679548264  Generator_loss: 2.553563356399536\n",
            "epoch: 9/10,    batch: 11032/15469    Discriminator_loss: 0.0808483138680458  Generator_loss: 2.558619499206543\n",
            "epoch: 9/10,    batch: 11033/15469    Discriminator_loss: 0.08042711019515991  Generator_loss: 2.5634312629699707\n",
            "epoch: 9/10,    batch: 11034/15469    Discriminator_loss: 0.08002191036939621  Generator_loss: 2.5682220458984375\n",
            "epoch: 9/10,    batch: 11035/15469    Discriminator_loss: 0.07960924506187439  Generator_loss: 2.5730419158935547\n",
            "epoch: 9/10,    batch: 11036/15469    Discriminator_loss: 0.07922366261482239  Generator_loss: 2.577984094619751\n",
            "epoch: 9/10,    batch: 11037/15469    Discriminator_loss: 0.07881314307451248  Generator_loss: 2.5828919410705566\n",
            "epoch: 9/10,    batch: 11038/15469    Discriminator_loss: 0.07842177152633667  Generator_loss: 2.5877065658569336\n",
            "epoch: 9/10,    batch: 11039/15469    Discriminator_loss: 0.0780302956700325  Generator_loss: 2.5926036834716797\n",
            "epoch: 9/10,    batch: 11040/15469    Discriminator_loss: 0.07763319462537766  Generator_loss: 2.5974276065826416\n",
            "epoch: 9/10,    batch: 11041/15469    Discriminator_loss: 0.07724547386169434  Generator_loss: 2.602250814437866\n",
            "epoch: 9/10,    batch: 11042/15469    Discriminator_loss: 0.0768595039844513  Generator_loss: 2.606991767883301\n",
            "epoch: 9/10,    batch: 11043/15469    Discriminator_loss: 0.07647553831338882  Generator_loss: 2.6119232177734375\n",
            "epoch: 9/10,    batch: 11044/15469    Discriminator_loss: 0.07609163224697113  Generator_loss: 2.616766929626465\n",
            "epoch: 9/10,    batch: 11045/15469    Discriminator_loss: 0.0757148414850235  Generator_loss: 2.621575117111206\n",
            "epoch: 9/10,    batch: 11046/15469    Discriminator_loss: 0.07533303648233414  Generator_loss: 2.6264498233795166\n",
            "epoch: 9/10,    batch: 11047/15469    Discriminator_loss: 0.07496418803930283  Generator_loss: 2.631159782409668\n",
            "epoch: 9/10,    batch: 11048/15469    Discriminator_loss: 0.07459061592817307  Generator_loss: 2.6360247135162354\n",
            "epoch: 9/10,    batch: 11049/15469    Discriminator_loss: 0.07421276718378067  Generator_loss: 2.640777111053467\n",
            "epoch: 9/10,    batch: 11050/15469    Discriminator_loss: 0.07385364174842834  Generator_loss: 2.645606517791748\n",
            "epoch: 9/10,    batch: 11051/15469    Discriminator_loss: 0.07349057495594025  Generator_loss: 2.6503400802612305\n",
            "epoch: 9/10,    batch: 11052/15469    Discriminator_loss: 0.0731019675731659  Generator_loss: 2.6550984382629395\n",
            "epoch: 9/10,    batch: 11053/15469    Discriminator_loss: 0.07275364547967911  Generator_loss: 2.6598961353302\n",
            "epoch: 9/10,    batch: 11054/15469    Discriminator_loss: 0.07239067554473877  Generator_loss: 2.664708137512207\n",
            "epoch: 9/10,    batch: 11055/15469    Discriminator_loss: 0.07203688472509384  Generator_loss: 2.6694445610046387\n",
            "epoch: 9/10,    batch: 11056/15469    Discriminator_loss: 0.07168048620223999  Generator_loss: 2.6742427349090576\n",
            "epoch: 9/10,    batch: 11057/15469    Discriminator_loss: 0.0713278278708458  Generator_loss: 2.6789534091949463\n",
            "epoch: 9/10,    batch: 11058/15469    Discriminator_loss: 0.07098042964935303  Generator_loss: 2.683751106262207\n",
            "epoch: 9/10,    batch: 11059/15469    Discriminator_loss: 0.07063617557287216  Generator_loss: 2.688507080078125\n",
            "epoch: 9/10,    batch: 11060/15469    Discriminator_loss: 0.07028766721487045  Generator_loss: 2.6931469440460205\n",
            "epoch: 9/10,    batch: 11061/15469    Discriminator_loss: 0.06994619220495224  Generator_loss: 2.697888135910034\n",
            "epoch: 9/10,    batch: 11062/15469    Discriminator_loss: 0.0696047693490982  Generator_loss: 2.7025794982910156\n",
            "epoch: 9/10,    batch: 11063/15469    Discriminator_loss: 0.06926238536834717  Generator_loss: 2.707261085510254\n",
            "epoch: 9/10,    batch: 11064/15469    Discriminator_loss: 0.06892731785774231  Generator_loss: 2.7120065689086914\n",
            "epoch: 9/10,    batch: 11065/15469    Discriminator_loss: 0.06859547644853592  Generator_loss: 2.7166543006896973\n",
            "epoch: 9/10,    batch: 11066/15469    Discriminator_loss: 0.06826623529195786  Generator_loss: 2.721414089202881\n",
            "epoch: 9/10,    batch: 11067/15469    Discriminator_loss: 0.06793547421693802  Generator_loss: 2.7260260581970215\n",
            "epoch: 9/10,    batch: 11068/15469    Discriminator_loss: 0.06760948896408081  Generator_loss: 2.7306466102600098\n",
            "epoch: 9/10,    batch: 11069/15469    Discriminator_loss: 0.06728329509496689  Generator_loss: 2.735299587249756\n",
            "epoch: 9/10,    batch: 11070/15469    Discriminator_loss: 0.06696346402168274  Generator_loss: 2.7399790287017822\n",
            "epoch: 9/10,    batch: 11071/15469    Discriminator_loss: 0.06664010137319565  Generator_loss: 2.744662284851074\n",
            "epoch: 9/10,    batch: 11072/15469    Discriminator_loss: 0.06632277369499207  Generator_loss: 2.749206781387329\n",
            "epoch: 9/10,    batch: 11073/15469    Discriminator_loss: 0.06600681692361832  Generator_loss: 2.7538998126983643\n",
            "epoch: 9/10,    batch: 11074/15469    Discriminator_loss: 0.06569197028875351  Generator_loss: 2.758497953414917\n",
            "epoch: 9/10,    batch: 11075/15469    Discriminator_loss: 0.06538226455450058  Generator_loss: 2.7631001472473145\n",
            "epoch: 9/10,    batch: 11076/15469    Discriminator_loss: 0.0650685653090477  Generator_loss: 2.7677431106567383\n",
            "epoch: 9/10,    batch: 11077/15469    Discriminator_loss: 0.06476316601037979  Generator_loss: 2.772277355194092\n",
            "epoch: 9/10,    batch: 11078/15469    Discriminator_loss: 0.06445328146219254  Generator_loss: 2.776881694793701\n",
            "epoch: 9/10,    batch: 11079/15469    Discriminator_loss: 0.06414461880922318  Generator_loss: 2.781479597091675\n",
            "epoch: 9/10,    batch: 11080/15469    Discriminator_loss: 0.0638403668999672  Generator_loss: 2.7859764099121094\n",
            "epoch: 9/10,    batch: 11081/15469    Discriminator_loss: 0.06354591250419617  Generator_loss: 2.790677070617676\n",
            "epoch: 9/10,    batch: 11082/15469    Discriminator_loss: 0.06324931979179382  Generator_loss: 2.7951865196228027\n",
            "epoch: 9/10,    batch: 11083/15469    Discriminator_loss: 0.06295333802700043  Generator_loss: 2.7997279167175293\n",
            "epoch: 9/10,    batch: 11084/15469    Discriminator_loss: 0.0626627653837204  Generator_loss: 2.804248332977295\n",
            "epoch: 9/10,    batch: 11085/15469    Discriminator_loss: 0.06236761063337326  Generator_loss: 2.808748722076416\n",
            "epoch: 9/10,    batch: 11086/15469    Discriminator_loss: 0.062074027955532074  Generator_loss: 2.81333065032959\n",
            "epoch: 9/10,    batch: 11087/15469    Discriminator_loss: 0.061782002449035645  Generator_loss: 2.8178348541259766\n",
            "epoch: 9/10,    batch: 11088/15469    Discriminator_loss: 0.061497241258621216  Generator_loss: 2.822321891784668\n",
            "epoch: 9/10,    batch: 11089/15469    Discriminator_loss: 0.06121087074279785  Generator_loss: 2.8268136978149414\n",
            "epoch: 9/10,    batch: 11090/15469    Discriminator_loss: 0.060925524681806564  Generator_loss: 2.8313679695129395\n",
            "epoch: 9/10,    batch: 11091/15469    Discriminator_loss: 0.06064439192414284  Generator_loss: 2.8358538150787354\n",
            "epoch: 9/10,    batch: 11092/15469    Discriminator_loss: 0.06036084145307541  Generator_loss: 2.840376615524292\n",
            "epoch: 9/10,    batch: 11093/15469    Discriminator_loss: 0.06009426712989807  Generator_loss: 2.8448739051818848\n",
            "epoch: 9/10,    batch: 11094/15469    Discriminator_loss: 0.059823647141456604  Generator_loss: 2.8493220806121826\n",
            "epoch: 9/10,    batch: 11095/15469    Discriminator_loss: 0.059546295553445816  Generator_loss: 2.8538317680358887\n",
            "epoch: 9/10,    batch: 11096/15469    Discriminator_loss: 0.0592646561563015  Generator_loss: 2.858267068862915\n",
            "epoch: 9/10,    batch: 11097/15469    Discriminator_loss: 0.05899054929614067  Generator_loss: 2.8627281188964844\n",
            "epoch: 9/10,    batch: 11098/15469    Discriminator_loss: 0.058719899505376816  Generator_loss: 2.8672685623168945\n",
            "epoch: 9/10,    batch: 11099/15469    Discriminator_loss: 0.058446526527404785  Generator_loss: 2.8716561794281006\n",
            "epoch: 9/10,    batch: 11100/15469    Discriminator_loss: 0.05818565934896469  Generator_loss: 2.8761329650878906\n",
            "epoch: 9/10,    batch: 11101/15469    Discriminator_loss: 0.057915352284908295  Generator_loss: 2.8805055618286133\n",
            "epoch: 9/10,    batch: 11102/15469    Discriminator_loss: 0.05765547230839729  Generator_loss: 2.8849315643310547\n",
            "epoch: 9/10,    batch: 11103/15469    Discriminator_loss: 0.057391948997974396  Generator_loss: 2.8894267082214355\n",
            "epoch: 9/10,    batch: 11104/15469    Discriminator_loss: 0.057133086025714874  Generator_loss: 2.8938217163085938\n",
            "epoch: 9/10,    batch: 11105/15469    Discriminator_loss: 0.05687286704778671  Generator_loss: 2.8982701301574707\n",
            "epoch: 9/10,    batch: 11106/15469    Discriminator_loss: 0.05661687254905701  Generator_loss: 2.902616500854492\n",
            "epoch: 9/10,    batch: 11107/15469    Discriminator_loss: 0.05635572597384453  Generator_loss: 2.9070205688476562\n",
            "epoch: 9/10,    batch: 11108/15469    Discriminator_loss: 0.056106194853782654  Generator_loss: 2.9114010334014893\n",
            "epoch: 9/10,    batch: 11109/15469    Discriminator_loss: 0.055851299315690994  Generator_loss: 2.9157824516296387\n",
            "epoch: 9/10,    batch: 11110/15469    Discriminator_loss: 0.05559911951422691  Generator_loss: 2.9201459884643555\n",
            "epoch: 9/10,    batch: 11111/15469    Discriminator_loss: 0.0553511381149292  Generator_loss: 2.924591302871704\n",
            "epoch: 9/10,    batch: 11112/15469    Discriminator_loss: 0.05510324984788895  Generator_loss: 2.9289584159851074\n",
            "epoch: 9/10,    batch: 11113/15469    Discriminator_loss: 0.05485674738883972  Generator_loss: 2.9333038330078125\n",
            "epoch: 9/10,    batch: 11114/15469    Discriminator_loss: 0.054614774882793427  Generator_loss: 2.9376983642578125\n",
            "epoch: 9/10,    batch: 11115/15469    Discriminator_loss: 0.05436835438013077  Generator_loss: 2.9420602321624756\n",
            "epoch: 9/10,    batch: 11116/15469    Discriminator_loss: 0.05412674322724342  Generator_loss: 2.946385145187378\n",
            "epoch: 9/10,    batch: 11117/15469    Discriminator_loss: 0.05388769507408142  Generator_loss: 2.950654983520508\n",
            "epoch: 9/10,    batch: 11118/15469    Discriminator_loss: 0.05364639312028885  Generator_loss: 2.9550483226776123\n",
            "epoch: 9/10,    batch: 11119/15469    Discriminator_loss: 0.05340592935681343  Generator_loss: 2.959331512451172\n",
            "epoch: 9/10,    batch: 11120/15469    Discriminator_loss: 0.053170643746852875  Generator_loss: 2.9636878967285156\n",
            "epoch: 9/10,    batch: 11121/15469    Discriminator_loss: 0.05293399095535278  Generator_loss: 2.9679455757141113\n",
            "epoch: 9/10,    batch: 11122/15469    Discriminator_loss: 0.052701983600854874  Generator_loss: 2.97225022315979\n",
            "epoch: 9/10,    batch: 11123/15469    Discriminator_loss: 0.05247350409626961  Generator_loss: 2.9765729904174805\n",
            "epoch: 9/10,    batch: 11124/15469    Discriminator_loss: 0.05224284529685974  Generator_loss: 2.980849266052246\n",
            "epoch: 9/10,    batch: 11125/15469    Discriminator_loss: 0.05201494321227074  Generator_loss: 2.9851369857788086\n",
            "epoch: 9/10,    batch: 11126/15469    Discriminator_loss: 0.051785193383693695  Generator_loss: 2.98936128616333\n",
            "epoch: 9/10,    batch: 11127/15469    Discriminator_loss: 0.05155964568257332  Generator_loss: 2.993647336959839\n",
            "epoch: 9/10,    batch: 11128/15469    Discriminator_loss: 0.05133804678916931  Generator_loss: 2.9978749752044678\n",
            "epoch: 9/10,    batch: 11129/15469    Discriminator_loss: 0.051116250455379486  Generator_loss: 3.0021414756774902\n",
            "epoch: 9/10,    batch: 11130/15469    Discriminator_loss: 0.05089414864778519  Generator_loss: 3.006427764892578\n",
            "epoch: 9/10,    batch: 11131/15469    Discriminator_loss: 0.050668150186538696  Generator_loss: 3.0105700492858887\n",
            "epoch: 9/10,    batch: 11132/15469    Discriminator_loss: 0.05045357719063759  Generator_loss: 3.0148162841796875\n",
            "epoch: 9/10,    batch: 11133/15469    Discriminator_loss: 0.0502377524971962  Generator_loss: 3.0190517902374268\n",
            "epoch: 9/10,    batch: 11134/15469    Discriminator_loss: 0.050019294023513794  Generator_loss: 3.0232627391815186\n",
            "epoch: 9/10,    batch: 11135/15469    Discriminator_loss: 0.04980672150850296  Generator_loss: 3.0274569988250732\n",
            "epoch: 9/10,    batch: 11136/15469    Discriminator_loss: 0.04959120601415634  Generator_loss: 3.0316030979156494\n",
            "epoch: 9/10,    batch: 11137/15469    Discriminator_loss: 0.049380429089069366  Generator_loss: 3.0357837677001953\n",
            "epoch: 9/10,    batch: 11138/15469    Discriminator_loss: 0.04917262867093086  Generator_loss: 3.039959192276001\n",
            "epoch: 9/10,    batch: 11139/15469    Discriminator_loss: 0.04896426945924759  Generator_loss: 3.044097900390625\n",
            "epoch: 9/10,    batch: 11140/15469    Discriminator_loss: 0.048756204545497894  Generator_loss: 3.0482969284057617\n",
            "epoch: 9/10,    batch: 11141/15469    Discriminator_loss: 0.04854798689484596  Generator_loss: 3.0523972511291504\n",
            "epoch: 9/10,    batch: 11142/15469    Discriminator_loss: 0.048343706876039505  Generator_loss: 3.0564634799957275\n",
            "epoch: 9/10,    batch: 11143/15469    Discriminator_loss: 0.04814290627837181  Generator_loss: 3.0605967044830322\n",
            "epoch: 9/10,    batch: 11144/15469    Discriminator_loss: 0.04793962091207504  Generator_loss: 3.064737319946289\n",
            "epoch: 9/10,    batch: 11145/15469    Discriminator_loss: 0.047741323709487915  Generator_loss: 3.06876802444458\n",
            "epoch: 9/10,    batch: 11146/15469    Discriminator_loss: 0.047540996223688126  Generator_loss: 3.0728580951690674\n",
            "epoch: 9/10,    batch: 11147/15469    Discriminator_loss: 0.04734503850340843  Generator_loss: 3.076925754547119\n",
            "epoch: 9/10,    batch: 11148/15469    Discriminator_loss: 0.04714909568428993  Generator_loss: 3.0809850692749023\n",
            "epoch: 9/10,    batch: 11149/15469    Discriminator_loss: 0.046957261860370636  Generator_loss: 3.0850086212158203\n",
            "epoch: 9/10,    batch: 11150/15469    Discriminator_loss: 0.04675957188010216  Generator_loss: 3.0889925956726074\n",
            "epoch: 9/10,    batch: 11151/15469    Discriminator_loss: 0.046572037041187286  Generator_loss: 3.093048334121704\n",
            "epoch: 9/10,    batch: 11152/15469    Discriminator_loss: 0.04638145491480827  Generator_loss: 3.0970256328582764\n",
            "epoch: 9/10,    batch: 11153/15469    Discriminator_loss: 0.04619251936674118  Generator_loss: 3.101022720336914\n",
            "epoch: 9/10,    batch: 11154/15469    Discriminator_loss: 0.04600684717297554  Generator_loss: 3.1049678325653076\n",
            "epoch: 9/10,    batch: 11155/15469    Discriminator_loss: 0.045821934938430786  Generator_loss: 3.108952760696411\n",
            "epoch: 9/10,    batch: 11156/15469    Discriminator_loss: 0.045636486262083054  Generator_loss: 3.112863063812256\n",
            "epoch: 9/10,    batch: 11157/15469    Discriminator_loss: 0.04545329883694649  Generator_loss: 3.1168012619018555\n",
            "epoch: 9/10,    batch: 11158/15469    Discriminator_loss: 0.0452742725610733  Generator_loss: 3.120689868927002\n",
            "epoch: 9/10,    batch: 11159/15469    Discriminator_loss: 0.04509435221552849  Generator_loss: 3.124602794647217\n",
            "epoch: 9/10,    batch: 11160/15469    Discriminator_loss: 0.044915731996297836  Generator_loss: 3.128523588180542\n",
            "epoch: 9/10,    batch: 11161/15469    Discriminator_loss: 0.04473892226815224  Generator_loss: 3.132359027862549\n",
            "epoch: 9/10,    batch: 11162/15469    Discriminator_loss: 0.04456590116024017  Generator_loss: 3.1362361907958984\n",
            "epoch: 9/10,    batch: 11163/15469    Discriminator_loss: 0.04439465329051018  Generator_loss: 3.1400656700134277\n",
            "epoch: 9/10,    batch: 11164/15469    Discriminator_loss: 0.825931191444397  Generator_loss: 3.1379408836364746\n",
            "epoch: 9/10,    batch: 11165/15469    Discriminator_loss: 3.5528035163879395  Generator_loss: 3.112490653991699\n",
            "epoch: 9/10,    batch: 11166/15469    Discriminator_loss: 3.5147440433502197  Generator_loss: 3.071359872817993\n",
            "epoch: 9/10,    batch: 11167/15469    Discriminator_loss: 3.4561703205108643  Generator_loss: 3.020724296569824\n",
            "epoch: 9/10,    batch: 11168/15469    Discriminator_loss: 3.3771634101867676  Generator_loss: 2.9648642539978027\n",
            "epoch: 9/10,    batch: 11169/15469    Discriminator_loss: 3.3211610317230225  Generator_loss: 2.9066274166107178\n",
            "epoch: 9/10,    batch: 11170/15469    Discriminator_loss: 3.226470470428467  Generator_loss: 2.847917079925537\n",
            "epoch: 9/10,    batch: 11171/15469    Discriminator_loss: 3.1302478313446045  Generator_loss: 2.7899680137634277\n",
            "epoch: 9/10,    batch: 11172/15469    Discriminator_loss: 3.0521790981292725  Generator_loss: 2.7334976196289062\n",
            "epoch: 9/10,    batch: 11173/15469    Discriminator_loss: 2.9865970611572266  Generator_loss: 2.678795576095581\n",
            "epoch: 9/10,    batch: 11174/15469    Discriminator_loss: 2.913198232650757  Generator_loss: 2.6257779598236084\n",
            "epoch: 9/10,    batch: 11175/15469    Discriminator_loss: 2.8525021076202393  Generator_loss: 2.574799060821533\n",
            "epoch: 9/10,    batch: 11176/15469    Discriminator_loss: 2.793401002883911  Generator_loss: 2.5257728099823\n",
            "epoch: 9/10,    batch: 11177/15469    Discriminator_loss: 2.72959566116333  Generator_loss: 2.478654384613037\n",
            "epoch: 9/10,    batch: 11178/15469    Discriminator_loss: 2.675654649734497  Generator_loss: 2.433204174041748\n",
            "epoch: 9/10,    batch: 11179/15469    Discriminator_loss: 2.620394706726074  Generator_loss: 2.3894095420837402\n",
            "epoch: 9/10,    batch: 11180/15469    Discriminator_loss: 2.5760598182678223  Generator_loss: 2.347193956375122\n",
            "epoch: 9/10,    batch: 11181/15469    Discriminator_loss: 2.5302250385284424  Generator_loss: 2.306795120239258\n",
            "epoch: 9/10,    batch: 11182/15469    Discriminator_loss: 2.485412836074829  Generator_loss: 2.2677536010742188\n",
            "epoch: 9/10,    batch: 11183/15469    Discriminator_loss: 2.441122055053711  Generator_loss: 2.23022198677063\n",
            "epoch: 9/10,    batch: 11184/15469    Discriminator_loss: 2.4004781246185303  Generator_loss: 2.1939029693603516\n",
            "epoch: 9/10,    batch: 11185/15469    Discriminator_loss: 2.3682267665863037  Generator_loss: 2.1587958335876465\n",
            "epoch: 9/10,    batch: 11186/15469    Discriminator_loss: 2.3382208347320557  Generator_loss: 2.124898910522461\n",
            "epoch: 9/10,    batch: 11187/15469    Discriminator_loss: 2.2983920574188232  Generator_loss: 2.092106342315674\n",
            "epoch: 9/10,    batch: 11188/15469    Discriminator_loss: 2.268709897994995  Generator_loss: 2.06038498878479\n",
            "epoch: 9/10,    batch: 11189/15469    Discriminator_loss: 2.237596035003662  Generator_loss: 2.0296454429626465\n",
            "epoch: 9/10,    batch: 11190/15469    Discriminator_loss: 2.2076659202575684  Generator_loss: 1.999922513961792\n",
            "epoch: 9/10,    batch: 11191/15469    Discriminator_loss: 2.181385040283203  Generator_loss: 1.9710743427276611\n",
            "epoch: 9/10,    batch: 11192/15469    Discriminator_loss: 2.1560943126678467  Generator_loss: 1.9431781768798828\n",
            "epoch: 9/10,    batch: 11193/15469    Discriminator_loss: 2.1278765201568604  Generator_loss: 1.916052222251892\n",
            "epoch: 9/10,    batch: 11194/15469    Discriminator_loss: 2.102922201156616  Generator_loss: 1.8897008895874023\n",
            "epoch: 9/10,    batch: 11195/15469    Discriminator_loss: 2.077824592590332  Generator_loss: 1.8641257286071777\n",
            "epoch: 9/10,    batch: 11196/15469    Discriminator_loss: 2.059854030609131  Generator_loss: 1.8393566608428955\n",
            "epoch: 9/10,    batch: 11197/15469    Discriminator_loss: 2.03059458732605  Generator_loss: 1.815312385559082\n",
            "epoch: 9/10,    batch: 11198/15469    Discriminator_loss: 2.0079565048217773  Generator_loss: 1.7918896675109863\n",
            "epoch: 9/10,    batch: 11199/15469    Discriminator_loss: 1.9912972450256348  Generator_loss: 1.7691024541854858\n",
            "epoch: 9/10,    batch: 11200/15469    Discriminator_loss: 1.9659130573272705  Generator_loss: 1.7469336986541748\n",
            "epoch: 9/10,    batch: 11201/15469    Discriminator_loss: 1.946810007095337  Generator_loss: 1.7253835201263428\n",
            "epoch: 9/10,    batch: 11202/15469    Discriminator_loss: 1.9272027015686035  Generator_loss: 1.7043125629425049\n",
            "epoch: 9/10,    batch: 11203/15469    Discriminator_loss: 1.9088040590286255  Generator_loss: 1.6838141679763794\n",
            "epoch: 9/10,    batch: 11204/15469    Discriminator_loss: 1.2589590549468994  Generator_loss: 1.6653954982757568\n",
            "epoch: 9/10,    batch: 11205/15469    Discriminator_loss: 0.21192969381809235  Generator_loss: 1.651395559310913\n",
            "epoch: 9/10,    batch: 11206/15469    Discriminator_loss: 0.21462853252887726  Generator_loss: 1.6409039497375488\n",
            "epoch: 9/10,    batch: 11207/15469    Discriminator_loss: 0.37516671419143677  Generator_loss: 1.633040189743042\n",
            "epoch: 9/10,    batch: 11208/15469    Discriminator_loss: 0.2183798998594284  Generator_loss: 1.6275689601898193\n",
            "epoch: 9/10,    batch: 11209/15469    Discriminator_loss: 0.21947215497493744  Generator_loss: 1.6240167617797852\n",
            "epoch: 9/10,    batch: 11210/15469    Discriminator_loss: 0.22013239562511444  Generator_loss: 1.6220617294311523\n",
            "epoch: 9/10,    batch: 11211/15469    Discriminator_loss: 0.22104892134666443  Generator_loss: 1.621337890625\n",
            "epoch: 9/10,    batch: 11212/15469    Discriminator_loss: 0.23241685330867767  Generator_loss: 1.6215620040893555\n",
            "epoch: 9/10,    batch: 11213/15469    Discriminator_loss: 0.22028794884681702  Generator_loss: 1.6225982904434204\n",
            "epoch: 9/10,    batch: 11214/15469    Discriminator_loss: 0.21996493637561798  Generator_loss: 1.6243937015533447\n",
            "epoch: 9/10,    batch: 11215/15469    Discriminator_loss: 0.21949706971645355  Generator_loss: 1.6267719268798828\n",
            "epoch: 9/10,    batch: 11216/15469    Discriminator_loss: 0.21875478327274323  Generator_loss: 1.629631519317627\n",
            "epoch: 9/10,    batch: 11217/15469    Discriminator_loss: 0.2180079072713852  Generator_loss: 1.632878303527832\n",
            "epoch: 9/10,    batch: 11218/15469    Discriminator_loss: 0.2171742469072342  Generator_loss: 1.6364381313323975\n",
            "epoch: 9/10,    batch: 11219/15469    Discriminator_loss: 0.21627576649188995  Generator_loss: 1.640271544456482\n",
            "epoch: 9/10,    batch: 11220/15469    Discriminator_loss: 0.21532940864562988  Generator_loss: 1.6443017721176147\n",
            "epoch: 9/10,    batch: 11221/15469    Discriminator_loss: 0.21433202922344208  Generator_loss: 1.6485408544540405\n",
            "epoch: 9/10,    batch: 11222/15469    Discriminator_loss: 0.21330967545509338  Generator_loss: 1.652878999710083\n",
            "epoch: 9/10,    batch: 11223/15469    Discriminator_loss: 0.21225979924201965  Generator_loss: 1.6573445796966553\n",
            "epoch: 9/10,    batch: 11224/15469    Discriminator_loss: 0.2111932933330536  Generator_loss: 1.6619089841842651\n",
            "epoch: 9/10,    batch: 11225/15469    Discriminator_loss: 0.21011367440223694  Generator_loss: 1.6665449142456055\n",
            "epoch: 9/10,    batch: 11226/15469    Discriminator_loss: 0.20902714133262634  Generator_loss: 1.6712355613708496\n",
            "epoch: 9/10,    batch: 11227/15469    Discriminator_loss: 0.20793016254901886  Generator_loss: 1.6759732961654663\n",
            "epoch: 9/10,    batch: 11228/15469    Discriminator_loss: 0.2068321853876114  Generator_loss: 1.6807732582092285\n",
            "epoch: 9/10,    batch: 11229/15469    Discriminator_loss: 0.20572978258132935  Generator_loss: 1.6855660676956177\n",
            "epoch: 9/10,    batch: 11230/15469    Discriminator_loss: 0.20462526381015778  Generator_loss: 1.6904127597808838\n",
            "epoch: 9/10,    batch: 11231/15469    Discriminator_loss: 0.20352552831172943  Generator_loss: 1.6952698230743408\n",
            "epoch: 9/10,    batch: 11232/15469    Discriminator_loss: 0.20243100821971893  Generator_loss: 1.7001633644104004\n",
            "epoch: 9/10,    batch: 11233/15469    Discriminator_loss: 0.20133373141288757  Generator_loss: 1.7050626277923584\n",
            "epoch: 9/10,    batch: 11234/15469    Discriminator_loss: 0.20024514198303223  Generator_loss: 1.7099621295928955\n",
            "epoch: 9/10,    batch: 11235/15469    Discriminator_loss: 0.19915956258773804  Generator_loss: 1.714874267578125\n",
            "epoch: 9/10,    batch: 11236/15469    Discriminator_loss: 0.1980745494365692  Generator_loss: 1.719801664352417\n",
            "epoch: 9/10,    batch: 11237/15469    Discriminator_loss: 0.19699925184249878  Generator_loss: 1.724725604057312\n",
            "epoch: 9/10,    batch: 11238/15469    Discriminator_loss: 0.19592663645744324  Generator_loss: 1.7296743392944336\n",
            "epoch: 9/10,    batch: 11239/15469    Discriminator_loss: 0.19485926628112793  Generator_loss: 1.7346235513687134\n",
            "epoch: 9/10,    batch: 11240/15469    Discriminator_loss: 0.19379812479019165  Generator_loss: 1.739574670791626\n",
            "epoch: 9/10,    batch: 11241/15469    Discriminator_loss: 0.19274242222309113  Generator_loss: 1.7445127964019775\n",
            "epoch: 9/10,    batch: 11242/15469    Discriminator_loss: 0.19169221818447113  Generator_loss: 1.7494860887527466\n",
            "epoch: 9/10,    batch: 11243/15469    Discriminator_loss: 0.19064530730247498  Generator_loss: 1.754450798034668\n",
            "epoch: 9/10,    batch: 11244/15469    Discriminator_loss: 0.18959879875183105  Generator_loss: 1.7594146728515625\n",
            "epoch: 9/10,    batch: 11245/15469    Discriminator_loss: 0.18856315314769745  Generator_loss: 1.7643826007843018\n",
            "epoch: 9/10,    batch: 11246/15469    Discriminator_loss: 0.18753372132778168  Generator_loss: 1.769374132156372\n",
            "epoch: 9/10,    batch: 11247/15469    Discriminator_loss: 0.18650734424591064  Generator_loss: 1.7743511199951172\n",
            "epoch: 9/10,    batch: 11248/15469    Discriminator_loss: 0.18548224866390228  Generator_loss: 1.779350996017456\n",
            "epoch: 9/10,    batch: 11249/15469    Discriminator_loss: 0.1844671368598938  Generator_loss: 1.784351110458374\n",
            "epoch: 9/10,    batch: 11250/15469    Discriminator_loss: 0.1834597885608673  Generator_loss: 1.7893372774124146\n",
            "epoch: 9/10,    batch: 11251/15469    Discriminator_loss: 0.1824563443660736  Generator_loss: 1.7943553924560547\n",
            "epoch: 9/10,    batch: 11252/15469    Discriminator_loss: 0.1814579963684082  Generator_loss: 1.7993507385253906\n",
            "epoch: 9/10,    batch: 11253/15469    Discriminator_loss: 0.18046075105667114  Generator_loss: 1.8043795824050903\n",
            "epoch: 9/10,    batch: 11254/15469    Discriminator_loss: 0.17946825921535492  Generator_loss: 1.8094028234481812\n",
            "epoch: 9/10,    batch: 11255/15469    Discriminator_loss: 0.17848043143749237  Generator_loss: 1.8144285678863525\n",
            "epoch: 9/10,    batch: 11256/15469    Discriminator_loss: 0.17750205099582672  Generator_loss: 1.8194888830184937\n",
            "epoch: 9/10,    batch: 11257/15469    Discriminator_loss: 0.17652678489685059  Generator_loss: 1.8245041370391846\n",
            "epoch: 9/10,    batch: 11258/15469    Discriminator_loss: 0.17555585503578186  Generator_loss: 1.8295600414276123\n",
            "epoch: 9/10,    batch: 11259/15469    Discriminator_loss: 0.17459064722061157  Generator_loss: 1.834606409072876\n",
            "epoch: 9/10,    batch: 11260/15469    Discriminator_loss: 0.17363087832927704  Generator_loss: 1.8396588563919067\n",
            "epoch: 9/10,    batch: 11261/15469    Discriminator_loss: 0.17267148196697235  Generator_loss: 1.8447279930114746\n",
            "epoch: 9/10,    batch: 11262/15469    Discriminator_loss: 0.17172037065029144  Generator_loss: 1.8497724533081055\n",
            "epoch: 9/10,    batch: 11263/15469    Discriminator_loss: 0.1707678884267807  Generator_loss: 1.8548595905303955\n",
            "epoch: 9/10,    batch: 11264/15469    Discriminator_loss: 0.16983254253864288  Generator_loss: 1.85992431640625\n",
            "epoch: 9/10,    batch: 11265/15469    Discriminator_loss: 0.16889391839504242  Generator_loss: 1.8650016784667969\n",
            "epoch: 9/10,    batch: 11266/15469    Discriminator_loss: 0.16795627772808075  Generator_loss: 1.8700969219207764\n",
            "epoch: 9/10,    batch: 11267/15469    Discriminator_loss: 0.16702046990394592  Generator_loss: 1.8751935958862305\n",
            "epoch: 9/10,    batch: 11268/15469    Discriminator_loss: 0.16609467566013336  Generator_loss: 1.880307912826538\n",
            "epoch: 9/10,    batch: 11269/15469    Discriminator_loss: 0.1651710569858551  Generator_loss: 1.8853808641433716\n",
            "epoch: 9/10,    batch: 11270/15469    Discriminator_loss: 0.16426154971122742  Generator_loss: 1.8904863595962524\n",
            "epoch: 9/10,    batch: 11271/15469    Discriminator_loss: 0.16335131227970123  Generator_loss: 1.8955886363983154\n",
            "epoch: 9/10,    batch: 11272/15469    Discriminator_loss: 0.1624448597431183  Generator_loss: 1.900683879852295\n",
            "epoch: 9/10,    batch: 11273/15469    Discriminator_loss: 0.16154342889785767  Generator_loss: 1.9058035612106323\n",
            "epoch: 9/10,    batch: 11274/15469    Discriminator_loss: 0.16065503656864166  Generator_loss: 1.9109009504318237\n",
            "epoch: 9/10,    batch: 11275/15469    Discriminator_loss: 0.15976393222808838  Generator_loss: 1.9160226583480835\n",
            "epoch: 9/10,    batch: 11276/15469    Discriminator_loss: 0.15887776017189026  Generator_loss: 1.9211506843566895\n",
            "epoch: 9/10,    batch: 11277/15469    Discriminator_loss: 0.1580006629228592  Generator_loss: 1.9262654781341553\n",
            "epoch: 9/10,    batch: 11278/15469    Discriminator_loss: 0.1571255475282669  Generator_loss: 1.9313945770263672\n",
            "epoch: 9/10,    batch: 11279/15469    Discriminator_loss: 0.15625183284282684  Generator_loss: 1.9365143775939941\n",
            "epoch: 9/10,    batch: 11280/15469    Discriminator_loss: 0.15539157390594482  Generator_loss: 1.9416558742523193\n",
            "epoch: 9/10,    batch: 11281/15469    Discriminator_loss: 0.15452717244625092  Generator_loss: 1.9467604160308838\n",
            "epoch: 9/10,    batch: 11282/15469    Discriminator_loss: 0.15366971492767334  Generator_loss: 1.9519059658050537\n",
            "epoch: 9/10,    batch: 11283/15469    Discriminator_loss: 0.15282021462917328  Generator_loss: 1.9570372104644775\n",
            "epoch: 9/10,    batch: 11284/15469    Discriminator_loss: 0.1519741415977478  Generator_loss: 1.9621654748916626\n",
            "epoch: 9/10,    batch: 11285/15469    Discriminator_loss: 0.15113453567028046  Generator_loss: 1.9673134088516235\n",
            "epoch: 9/10,    batch: 11286/15469    Discriminator_loss: 0.1503012776374817  Generator_loss: 1.9724245071411133\n",
            "epoch: 9/10,    batch: 11287/15469    Discriminator_loss: 0.14946836233139038  Generator_loss: 1.9775722026824951\n",
            "epoch: 9/10,    batch: 11288/15469    Discriminator_loss: 0.14864280819892883  Generator_loss: 1.9827003479003906\n",
            "epoch: 9/10,    batch: 11289/15469    Discriminator_loss: 0.14782169461250305  Generator_loss: 1.98784339427948\n",
            "epoch: 9/10,    batch: 11290/15469    Discriminator_loss: 0.1470058560371399  Generator_loss: 1.9929746389389038\n",
            "epoch: 9/10,    batch: 11291/15469    Discriminator_loss: 0.14619550108909607  Generator_loss: 1.998093843460083\n",
            "epoch: 9/10,    batch: 11292/15469    Discriminator_loss: 0.14538809657096863  Generator_loss: 2.0032405853271484\n",
            "epoch: 9/10,    batch: 11293/15469    Discriminator_loss: 0.14458537101745605  Generator_loss: 2.008369207382202\n",
            "epoch: 9/10,    batch: 11294/15469    Discriminator_loss: 0.14378860592842102  Generator_loss: 2.0134897232055664\n",
            "epoch: 9/10,    batch: 11295/15469    Discriminator_loss: 0.14299793541431427  Generator_loss: 2.018655776977539\n",
            "epoch: 9/10,    batch: 11296/15469    Discriminator_loss: 0.14221061766147614  Generator_loss: 2.0237646102905273\n",
            "epoch: 9/10,    batch: 11297/15469    Discriminator_loss: 0.14142604172229767  Generator_loss: 2.02890682220459\n",
            "epoch: 9/10,    batch: 11298/15469    Discriminator_loss: 0.14064815640449524  Generator_loss: 2.03402042388916\n",
            "epoch: 9/10,    batch: 11299/15469    Discriminator_loss: 0.13987505435943604  Generator_loss: 2.0391602516174316\n",
            "epoch: 9/10,    batch: 11300/15469    Discriminator_loss: 0.13910998404026031  Generator_loss: 2.044264316558838\n",
            "epoch: 9/10,    batch: 11301/15469    Discriminator_loss: 0.13834291696548462  Generator_loss: 2.0493993759155273\n",
            "epoch: 9/10,    batch: 11302/15469    Discriminator_loss: 0.1375890076160431  Generator_loss: 2.054506778717041\n",
            "epoch: 9/10,    batch: 11303/15469    Discriminator_loss: 0.1368362009525299  Generator_loss: 2.0596113204956055\n",
            "epoch: 9/10,    batch: 11304/15469    Discriminator_loss: 0.13608616590499878  Generator_loss: 2.0647521018981934\n",
            "epoch: 9/10,    batch: 11305/15469    Discriminator_loss: 0.13534119725227356  Generator_loss: 2.0698649883270264\n",
            "epoch: 9/10,    batch: 11306/15469    Discriminator_loss: 0.1346120983362198  Generator_loss: 2.0749564170837402\n",
            "epoch: 9/10,    batch: 11307/15469    Discriminator_loss: 0.13387273252010345  Generator_loss: 2.0800857543945312\n",
            "epoch: 9/10,    batch: 11308/15469    Discriminator_loss: 0.1331399828195572  Generator_loss: 2.0851612091064453\n",
            "epoch: 9/10,    batch: 11309/15469    Discriminator_loss: 0.13241641223430634  Generator_loss: 2.0902676582336426\n",
            "epoch: 9/10,    batch: 11310/15469    Discriminator_loss: 0.131695955991745  Generator_loss: 2.095360279083252\n",
            "epoch: 9/10,    batch: 11311/15469    Discriminator_loss: 0.13098035752773285  Generator_loss: 2.100482940673828\n",
            "epoch: 9/10,    batch: 11312/15469    Discriminator_loss: 0.13026876747608185  Generator_loss: 2.10555362701416\n",
            "epoch: 9/10,    batch: 11313/15469    Discriminator_loss: 0.12956568598747253  Generator_loss: 2.110654354095459\n",
            "epoch: 9/10,    batch: 11314/15469    Discriminator_loss: 0.12886273860931396  Generator_loss: 2.1157281398773193\n",
            "epoch: 9/10,    batch: 11315/15469    Discriminator_loss: 0.1281604915857315  Generator_loss: 2.1208059787750244\n",
            "epoch: 9/10,    batch: 11316/15469    Discriminator_loss: 0.12747251987457275  Generator_loss: 2.1258764266967773\n",
            "epoch: 9/10,    batch: 11317/15469    Discriminator_loss: 0.12678030133247375  Generator_loss: 2.1309621334075928\n",
            "epoch: 9/10,    batch: 11318/15469    Discriminator_loss: 0.12609827518463135  Generator_loss: 2.1360087394714355\n",
            "epoch: 9/10,    batch: 11319/15469    Discriminator_loss: 0.12542179226875305  Generator_loss: 2.1410675048828125\n",
            "epoch: 9/10,    batch: 11320/15469    Discriminator_loss: 0.12474334239959717  Generator_loss: 2.1461524963378906\n",
            "epoch: 9/10,    batch: 11321/15469    Discriminator_loss: 0.12407396733760834  Generator_loss: 2.1512231826782227\n",
            "epoch: 9/10,    batch: 11322/15469    Discriminator_loss: 0.12340827286243439  Generator_loss: 2.1562561988830566\n",
            "epoch: 9/10,    batch: 11323/15469    Discriminator_loss: 0.12274807691574097  Generator_loss: 2.1612956523895264\n",
            "epoch: 9/10,    batch: 11324/15469    Discriminator_loss: 0.12209168076515198  Generator_loss: 2.166323184967041\n",
            "epoch: 9/10,    batch: 11325/15469    Discriminator_loss: 0.12143418192863464  Generator_loss: 2.1713898181915283\n",
            "epoch: 9/10,    batch: 11326/15469    Discriminator_loss: 0.12078939378261566  Generator_loss: 2.176405906677246\n",
            "epoch: 9/10,    batch: 11327/15469    Discriminator_loss: 0.12014439702033997  Generator_loss: 2.1814823150634766\n",
            "epoch: 9/10,    batch: 11328/15469    Discriminator_loss: 0.11950523406267166  Generator_loss: 2.1864678859710693\n",
            "epoch: 9/10,    batch: 11329/15469    Discriminator_loss: 0.11886914074420929  Generator_loss: 2.191478729248047\n",
            "epoch: 9/10,    batch: 11330/15469    Discriminator_loss: 0.11823435127735138  Generator_loss: 2.196502685546875\n",
            "epoch: 9/10,    batch: 11331/15469    Discriminator_loss: 0.11760596930980682  Generator_loss: 2.2015209197998047\n",
            "epoch: 9/10,    batch: 11332/15469    Discriminator_loss: 0.11698336899280548  Generator_loss: 2.206529140472412\n",
            "epoch: 9/10,    batch: 11333/15469    Discriminator_loss: 0.11636818200349808  Generator_loss: 2.21152400970459\n",
            "epoch: 9/10,    batch: 11334/15469    Discriminator_loss: 0.1157512366771698  Generator_loss: 2.2165398597717285\n",
            "epoch: 9/10,    batch: 11335/15469    Discriminator_loss: 0.11513972282409668  Generator_loss: 2.2214980125427246\n",
            "epoch: 9/10,    batch: 11336/15469    Discriminator_loss: 0.11453039199113846  Generator_loss: 2.2264914512634277\n",
            "epoch: 9/10,    batch: 11337/15469    Discriminator_loss: 0.11392773687839508  Generator_loss: 2.231487274169922\n",
            "epoch: 9/10,    batch: 11338/15469    Discriminator_loss: 0.1133299395442009  Generator_loss: 2.2364373207092285\n",
            "epoch: 9/10,    batch: 11339/15469    Discriminator_loss: 0.11273461580276489  Generator_loss: 2.241420030593872\n",
            "epoch: 9/10,    batch: 11340/15469    Discriminator_loss: 0.11214471608400345  Generator_loss: 2.2463784217834473\n",
            "epoch: 9/10,    batch: 11341/15469    Discriminator_loss: 0.11155624687671661  Generator_loss: 2.2513694763183594\n",
            "epoch: 9/10,    batch: 11342/15469    Discriminator_loss: 0.11097131669521332  Generator_loss: 2.2563300132751465\n",
            "epoch: 9/10,    batch: 11343/15469    Discriminator_loss: 0.11039227247238159  Generator_loss: 2.261253595352173\n",
            "epoch: 9/10,    batch: 11344/15469    Discriminator_loss: 0.10981842130422592  Generator_loss: 2.266204595565796\n",
            "epoch: 9/10,    batch: 11345/15469    Discriminator_loss: 0.10924343019723892  Generator_loss: 2.271122455596924\n",
            "epoch: 9/10,    batch: 11346/15469    Discriminator_loss: 0.10867662727832794  Generator_loss: 2.2760872840881348\n",
            "epoch: 9/10,    batch: 11347/15469    Discriminator_loss: 0.10810952633619308  Generator_loss: 2.280993938446045\n",
            "epoch: 9/10,    batch: 11348/15469    Discriminator_loss: 0.10755207389593124  Generator_loss: 2.285916328430176\n",
            "epoch: 9/10,    batch: 11349/15469    Discriminator_loss: 0.10699397325515747  Generator_loss: 2.2908475399017334\n",
            "epoch: 9/10,    batch: 11350/15469    Discriminator_loss: 0.10644032061100006  Generator_loss: 2.295759439468384\n",
            "epoch: 9/10,    batch: 11351/15469    Discriminator_loss: 0.10588756948709488  Generator_loss: 2.3006463050842285\n",
            "epoch: 9/10,    batch: 11352/15469    Discriminator_loss: 0.10534434020519257  Generator_loss: 2.3055596351623535\n",
            "epoch: 9/10,    batch: 11353/15469    Discriminator_loss: 0.10480006039142609  Generator_loss: 2.3104803562164307\n",
            "epoch: 9/10,    batch: 11354/15469    Discriminator_loss: 0.10425848513841629  Generator_loss: 2.3153629302978516\n",
            "epoch: 9/10,    batch: 11355/15469    Discriminator_loss: 0.1037253588438034  Generator_loss: 2.320220947265625\n",
            "epoch: 9/10,    batch: 11356/15469    Discriminator_loss: 0.10319285094738007  Generator_loss: 2.325113296508789\n",
            "epoch: 9/10,    batch: 11357/15469    Discriminator_loss: 0.10266190022230148  Generator_loss: 2.330003261566162\n",
            "epoch: 9/10,    batch: 11358/15469    Discriminator_loss: 0.10213830322027206  Generator_loss: 2.3348634243011475\n",
            "epoch: 9/10,    batch: 11359/15469    Discriminator_loss: 0.10161780565977097  Generator_loss: 2.339733600616455\n",
            "epoch: 9/10,    batch: 11360/15469    Discriminator_loss: 0.10109858214855194  Generator_loss: 2.3445565700531006\n",
            "epoch: 9/10,    batch: 11361/15469    Discriminator_loss: 0.10058411210775375  Generator_loss: 2.3494184017181396\n",
            "epoch: 9/10,    batch: 11362/15469    Discriminator_loss: 0.10007294267416  Generator_loss: 2.3543102741241455\n",
            "epoch: 9/10,    batch: 11363/15469    Discriminator_loss: 0.09956325590610504  Generator_loss: 2.359123468399048\n",
            "epoch: 9/10,    batch: 11364/15469    Discriminator_loss: 0.09905898571014404  Generator_loss: 2.3639566898345947\n",
            "epoch: 9/10,    batch: 11365/15469    Discriminator_loss: 0.09855543822050095  Generator_loss: 2.3687686920166016\n",
            "epoch: 9/10,    batch: 11366/15469    Discriminator_loss: 0.09805573523044586  Generator_loss: 2.3735949993133545\n",
            "epoch: 9/10,    batch: 11367/15469    Discriminator_loss: 0.09756346046924591  Generator_loss: 2.3784217834472656\n",
            "epoch: 9/10,    batch: 11368/15469    Discriminator_loss: 0.09707048535346985  Generator_loss: 2.383227825164795\n",
            "epoch: 9/10,    batch: 11369/15469    Discriminator_loss: 0.09658268094062805  Generator_loss: 2.388023853302002\n",
            "epoch: 9/10,    batch: 11370/15469    Discriminator_loss: 0.09609632194042206  Generator_loss: 2.392855167388916\n",
            "epoch: 9/10,    batch: 11371/15469    Discriminator_loss: 0.09561477601528168  Generator_loss: 2.397594928741455\n",
            "epoch: 9/10,    batch: 11372/15469    Discriminator_loss: 0.09513166546821594  Generator_loss: 2.4024059772491455\n",
            "epoch: 9/10,    batch: 11373/15469    Discriminator_loss: 0.09465897083282471  Generator_loss: 2.4071884155273438\n",
            "epoch: 9/10,    batch: 11374/15469    Discriminator_loss: 0.09418600052595139  Generator_loss: 2.411970615386963\n",
            "epoch: 9/10,    batch: 11375/15469    Discriminator_loss: 0.09371373057365417  Generator_loss: 2.41672420501709\n",
            "epoch: 9/10,    batch: 11376/15469    Discriminator_loss: 0.0932486355304718  Generator_loss: 2.4214978218078613\n",
            "epoch: 9/10,    batch: 11377/15469    Discriminator_loss: 0.09278371930122375  Generator_loss: 2.4262585639953613\n",
            "epoch: 9/10,    batch: 11378/15469    Discriminator_loss: 0.09232300519943237  Generator_loss: 2.431011199951172\n",
            "epoch: 9/10,    batch: 11379/15469    Discriminator_loss: 0.09186458587646484  Generator_loss: 2.435760021209717\n",
            "epoch: 9/10,    batch: 11380/15469    Discriminator_loss: 0.09140942990779877  Generator_loss: 2.4404819011688232\n",
            "epoch: 9/10,    batch: 11381/15469    Discriminator_loss: 0.09095646440982819  Generator_loss: 2.4452104568481445\n",
            "epoch: 9/10,    batch: 11382/15469    Discriminator_loss: 0.09051015973091125  Generator_loss: 2.4499456882476807\n",
            "epoch: 9/10,    batch: 11383/15469    Discriminator_loss: 0.09006324410438538  Generator_loss: 2.4546637535095215\n",
            "epoch: 9/10,    batch: 11384/15469    Discriminator_loss: 0.08961883187294006  Generator_loss: 2.4593896865844727\n",
            "epoch: 9/10,    batch: 11385/15469    Discriminator_loss: 0.08917995542287827  Generator_loss: 2.464085817337036\n",
            "epoch: 9/10,    batch: 11386/15469    Discriminator_loss: 0.0887511745095253  Generator_loss: 2.468777656555176\n",
            "epoch: 9/10,    batch: 11387/15469    Discriminator_loss: 0.08832378685474396  Generator_loss: 2.473491668701172\n",
            "epoch: 9/10,    batch: 11388/15469    Discriminator_loss: 0.08789116144180298  Generator_loss: 2.478156328201294\n",
            "epoch: 9/10,    batch: 11389/15469    Discriminator_loss: 0.08746402710676193  Generator_loss: 2.4828686714172363\n",
            "epoch: 9/10,    batch: 11390/15469    Discriminator_loss: 0.087041936814785  Generator_loss: 2.487544536590576\n",
            "epoch: 9/10,    batch: 11391/15469    Discriminator_loss: 0.08661795407533646  Generator_loss: 2.4921860694885254\n",
            "epoch: 9/10,    batch: 11392/15469    Discriminator_loss: 0.0861954540014267  Generator_loss: 2.4968743324279785\n",
            "epoch: 9/10,    batch: 11393/15469    Discriminator_loss: 0.08577865362167358  Generator_loss: 2.501509666442871\n",
            "epoch: 9/10,    batch: 11394/15469    Discriminator_loss: 0.0853671282529831  Generator_loss: 2.506165027618408\n",
            "epoch: 9/10,    batch: 11395/15469    Discriminator_loss: 0.08495622128248215  Generator_loss: 2.5108132362365723\n",
            "epoch: 9/10,    batch: 11396/15469    Discriminator_loss: 0.08453961461782455  Generator_loss: 2.5154552459716797\n",
            "epoch: 9/10,    batch: 11397/15469    Discriminator_loss: 0.08413363993167877  Generator_loss: 2.5200812816619873\n",
            "epoch: 9/10,    batch: 11398/15469    Discriminator_loss: 0.08373857289552689  Generator_loss: 2.524721622467041\n",
            "epoch: 9/10,    batch: 11399/15469    Discriminator_loss: 0.08333742618560791  Generator_loss: 2.5293359756469727\n",
            "epoch: 9/10,    batch: 11400/15469    Discriminator_loss: 0.08293739706277847  Generator_loss: 2.533951759338379\n",
            "epoch: 9/10,    batch: 11401/15469    Discriminator_loss: 0.08254633843898773  Generator_loss: 2.5385518074035645\n",
            "epoch: 9/10,    batch: 11402/15469    Discriminator_loss: 0.08216522634029388  Generator_loss: 2.543161630630493\n",
            "epoch: 9/10,    batch: 11403/15469    Discriminator_loss: 0.08177316188812256  Generator_loss: 2.5477380752563477\n",
            "epoch: 9/10,    batch: 11404/15469    Discriminator_loss: 0.08139163255691528  Generator_loss: 2.552321434020996\n",
            "epoch: 9/10,    batch: 11405/15469    Discriminator_loss: 0.08100270479917526  Generator_loss: 2.556899070739746\n",
            "epoch: 9/10,    batch: 11406/15469    Discriminator_loss: 0.08061888068914413  Generator_loss: 2.561495304107666\n",
            "epoch: 9/10,    batch: 11407/15469    Discriminator_loss: 0.08022551983594894  Generator_loss: 2.566037654876709\n",
            "epoch: 9/10,    batch: 11408/15469    Discriminator_loss: 0.07984195649623871  Generator_loss: 2.5705976486206055\n",
            "epoch: 9/10,    batch: 11409/15469    Discriminator_loss: 0.07950498908758163  Generator_loss: 2.5751702785491943\n",
            "epoch: 9/10,    batch: 11410/15469    Discriminator_loss: 0.07913123816251755  Generator_loss: 2.5796868801116943\n",
            "epoch: 9/10,    batch: 11411/15469    Discriminator_loss: 0.0787620097398758  Generator_loss: 2.584237575531006\n",
            "epoch: 9/10,    batch: 11412/15469    Discriminator_loss: 0.07840856164693832  Generator_loss: 2.5887622833251953\n",
            "epoch: 9/10,    batch: 11413/15469    Discriminator_loss: 0.07801183313131332  Generator_loss: 2.5932881832122803\n",
            "epoch: 9/10,    batch: 11414/15469    Discriminator_loss: 0.07765840739011765  Generator_loss: 2.597818374633789\n",
            "epoch: 9/10,    batch: 11415/15469    Discriminator_loss: 0.07731499522924423  Generator_loss: 2.6023011207580566\n",
            "epoch: 9/10,    batch: 11416/15469    Discriminator_loss: 0.07701816409826279  Generator_loss: 2.606801986694336\n",
            "epoch: 9/10,    batch: 11417/15469    Discriminator_loss: 0.07664158940315247  Generator_loss: 2.611337661743164\n",
            "epoch: 9/10,    batch: 11418/15469    Discriminator_loss: 0.07621195167303085  Generator_loss: 2.6157870292663574\n",
            "epoch: 9/10,    batch: 11419/15469    Discriminator_loss: 0.07598094642162323  Generator_loss: 2.620274066925049\n",
            "epoch: 9/10,    batch: 11420/15469    Discriminator_loss: 0.07546878606081009  Generator_loss: 2.624744415283203\n",
            "epoch: 9/10,    batch: 11421/15469    Discriminator_loss: 0.07519591599702835  Generator_loss: 2.629183292388916\n",
            "epoch: 9/10,    batch: 11422/15469    Discriminator_loss: 0.07473936676979065  Generator_loss: 2.633683681488037\n",
            "epoch: 9/10,    batch: 11423/15469    Discriminator_loss: 0.07438772916793823  Generator_loss: 2.638123035430908\n",
            "epoch: 9/10,    batch: 11424/15469    Discriminator_loss: 0.07404319196939468  Generator_loss: 2.6425716876983643\n",
            "epoch: 9/10,    batch: 11425/15469    Discriminator_loss: 0.07370308041572571  Generator_loss: 2.6469578742980957\n",
            "epoch: 9/10,    batch: 11426/15469    Discriminator_loss: 0.07336467504501343  Generator_loss: 2.6514105796813965\n",
            "epoch: 9/10,    batch: 11427/15469    Discriminator_loss: 0.07319403439760208  Generator_loss: 2.65582275390625\n",
            "epoch: 9/10,    batch: 11428/15469    Discriminator_loss: 2.1119256019592285  Generator_loss: 2.6519246101379395\n",
            "epoch: 9/10,    batch: 11429/15469    Discriminator_loss: 2.769101858139038  Generator_loss: 2.6389315128326416\n",
            "epoch: 9/10,    batch: 11430/15469    Discriminator_loss: 2.7521777153015137  Generator_loss: 2.6189064979553223\n",
            "epoch: 9/10,    batch: 11431/15469    Discriminator_loss: 2.7336175441741943  Generator_loss: 2.5934996604919434\n",
            "epoch: 9/10,    batch: 11432/15469    Discriminator_loss: 2.7043373584747314  Generator_loss: 2.564197063446045\n",
            "epoch: 9/10,    batch: 11433/15469    Discriminator_loss: 2.6657090187072754  Generator_loss: 2.5322487354278564\n",
            "epoch: 9/10,    batch: 11434/15469    Discriminator_loss: 2.6381242275238037  Generator_loss: 2.4984564781188965\n",
            "epoch: 9/10,    batch: 11435/15469    Discriminator_loss: 2.6251230239868164  Generator_loss: 2.4633853435516357\n",
            "epoch: 9/10,    batch: 11436/15469    Discriminator_loss: 2.617814779281616  Generator_loss: 2.4274468421936035\n",
            "epoch: 9/10,    batch: 11437/15469    Discriminator_loss: 2.5848348140716553  Generator_loss: 2.391063690185547\n",
            "epoch: 9/10,    batch: 11438/15469    Discriminator_loss: 2.543795585632324  Generator_loss: 2.354692220687866\n",
            "epoch: 9/10,    batch: 11439/15469    Discriminator_loss: 2.5013835430145264  Generator_loss: 2.31856107711792\n",
            "epoch: 9/10,    batch: 11440/15469    Discriminator_loss: 2.463117837905884  Generator_loss: 2.2828519344329834\n",
            "epoch: 9/10,    batch: 11441/15469    Discriminator_loss: 2.430082082748413  Generator_loss: 2.247802257537842\n",
            "epoch: 9/10,    batch: 11442/15469    Discriminator_loss: 2.3970279693603516  Generator_loss: 2.2134289741516113\n",
            "epoch: 9/10,    batch: 11443/15469    Discriminator_loss: 2.36106014251709  Generator_loss: 2.1798532009124756\n",
            "epoch: 9/10,    batch: 11444/15469    Discriminator_loss: 2.327484607696533  Generator_loss: 2.147066116333008\n",
            "epoch: 9/10,    batch: 11445/15469    Discriminator_loss: 2.2930848598480225  Generator_loss: 2.1150472164154053\n",
            "epoch: 9/10,    batch: 11446/15469    Discriminator_loss: 2.264528274536133  Generator_loss: 2.08381986618042\n",
            "epoch: 9/10,    batch: 11447/15469    Discriminator_loss: 2.2342114448547363  Generator_loss: 2.0534307956695557\n",
            "epoch: 9/10,    batch: 11448/15469    Discriminator_loss: 2.2099504470825195  Generator_loss: 2.0237979888916016\n",
            "epoch: 9/10,    batch: 11449/15469    Discriminator_loss: 2.1798338890075684  Generator_loss: 1.9949893951416016\n",
            "epoch: 9/10,    batch: 11450/15469    Discriminator_loss: 2.154646396636963  Generator_loss: 1.966908574104309\n",
            "epoch: 9/10,    batch: 11451/15469    Discriminator_loss: 2.1328775882720947  Generator_loss: 1.939560890197754\n",
            "epoch: 9/10,    batch: 11452/15469    Discriminator_loss: 2.10968279838562  Generator_loss: 1.9129345417022705\n",
            "epoch: 9/10,    batch: 11453/15469    Discriminator_loss: 2.086474895477295  Generator_loss: 1.8870084285736084\n",
            "epoch: 9/10,    batch: 11454/15469    Discriminator_loss: 2.065175771713257  Generator_loss: 1.8617379665374756\n",
            "epoch: 9/10,    batch: 11455/15469    Discriminator_loss: 2.0421242713928223  Generator_loss: 1.8371264934539795\n",
            "epoch: 9/10,    batch: 11456/15469    Discriminator_loss: 2.019266128540039  Generator_loss: 1.8131392002105713\n",
            "epoch: 9/10,    batch: 11457/15469    Discriminator_loss: 1.9993412494659424  Generator_loss: 1.7897382974624634\n",
            "epoch: 9/10,    batch: 11458/15469    Discriminator_loss: 1.9805235862731934  Generator_loss: 1.7669167518615723\n",
            "epoch: 9/10,    batch: 11459/15469    Discriminator_loss: 1.9580479860305786  Generator_loss: 1.7446433305740356\n",
            "epoch: 9/10,    batch: 11460/15469    Discriminator_loss: 1.9429668188095093  Generator_loss: 1.7229440212249756\n",
            "epoch: 9/10,    batch: 11461/15469    Discriminator_loss: 1.9233930110931396  Generator_loss: 1.7017323970794678\n",
            "epoch: 9/10,    batch: 11462/15469    Discriminator_loss: 1.9059879779815674  Generator_loss: 1.680984377861023\n",
            "epoch: 9/10,    batch: 11463/15469    Discriminator_loss: 1.8908476829528809  Generator_loss: 1.660731315612793\n",
            "epoch: 9/10,    batch: 11464/15469    Discriminator_loss: 1.8760372400283813  Generator_loss: 1.640913486480713\n",
            "epoch: 9/10,    batch: 11465/15469    Discriminator_loss: 1.8583695888519287  Generator_loss: 1.6215336322784424\n",
            "epoch: 9/10,    batch: 11466/15469    Discriminator_loss: 1.847826600074768  Generator_loss: 1.6025540828704834\n",
            "epoch: 9/10,    batch: 11467/15469    Discriminator_loss: 0.4954296052455902  Generator_loss: 1.5875341892242432\n",
            "epoch: 9/10,    batch: 11468/15469    Discriminator_loss: 0.23044630885124207  Generator_loss: 1.5764871835708618\n",
            "epoch: 9/10,    batch: 11469/15469    Discriminator_loss: 0.23287732899188995  Generator_loss: 1.5686171054840088\n",
            "epoch: 9/10,    batch: 11470/15469    Discriminator_loss: 0.2346608191728592  Generator_loss: 1.5633368492126465\n",
            "epoch: 9/10,    batch: 11471/15469    Discriminator_loss: 0.2356693595647812  Generator_loss: 1.5601693391799927\n",
            "epoch: 9/10,    batch: 11472/15469    Discriminator_loss: 0.23627682030200958  Generator_loss: 1.558712124824524\n",
            "epoch: 9/10,    batch: 11473/15469    Discriminator_loss: 0.23647135496139526  Generator_loss: 1.558614730834961\n",
            "epoch: 9/10,    batch: 11474/15469    Discriminator_loss: 0.23630410432815552  Generator_loss: 1.5596802234649658\n",
            "epoch: 9/10,    batch: 11475/15469    Discriminator_loss: 0.23589886724948883  Generator_loss: 1.5616400241851807\n",
            "epoch: 9/10,    batch: 11476/15469    Discriminator_loss: 0.33691757917404175  Generator_loss: 1.564052939414978\n",
            "epoch: 9/10,    batch: 11477/15469    Discriminator_loss: 1.7137646675109863  Generator_loss: 1.562972068786621\n",
            "epoch: 9/10,    batch: 11478/15469    Discriminator_loss: 1.8011834621429443  Generator_loss: 1.5588561296463013\n",
            "epoch: 9/10,    batch: 11479/15469    Discriminator_loss: 1.7976982593536377  Generator_loss: 1.552297592163086\n",
            "epoch: 9/10,    batch: 11480/15469    Discriminator_loss: 1.8003615140914917  Generator_loss: 1.5438346862792969\n",
            "epoch: 9/10,    batch: 11481/15469    Discriminator_loss: 1.7915973663330078  Generator_loss: 1.533825159072876\n",
            "epoch: 9/10,    batch: 11482/15469    Discriminator_loss: 1.7860240936279297  Generator_loss: 1.5225962400436401\n",
            "epoch: 9/10,    batch: 11483/15469    Discriminator_loss: 1.7723493576049805  Generator_loss: 1.510420560836792\n",
            "epoch: 9/10,    batch: 11484/15469    Discriminator_loss: 1.7425841093063354  Generator_loss: 1.4973593950271606\n",
            "epoch: 9/10,    batch: 11485/15469    Discriminator_loss: 1.7071911096572876  Generator_loss: 1.4835219383239746\n",
            "epoch: 9/10,    batch: 11486/15469    Discriminator_loss: 1.6881035566329956  Generator_loss: 1.469214916229248\n",
            "epoch: 9/10,    batch: 11487/15469    Discriminator_loss: 1.6689696311950684  Generator_loss: 1.4544258117675781\n",
            "epoch: 9/10,    batch: 11488/15469    Discriminator_loss: 1.6665104627609253  Generator_loss: 1.4392904043197632\n",
            "epoch: 9/10,    batch: 11489/15469    Discriminator_loss: 1.659682273864746  Generator_loss: 1.4239575862884521\n",
            "epoch: 9/10,    batch: 11490/15469    Discriminator_loss: 1.6452642679214478  Generator_loss: 1.40837824344635\n",
            "epoch: 9/10,    batch: 11491/15469    Discriminator_loss: 1.63226318359375  Generator_loss: 1.392647385597229\n",
            "epoch: 9/10,    batch: 11492/15469    Discriminator_loss: 1.6138877868652344  Generator_loss: 1.3767791986465454\n",
            "epoch: 9/10,    batch: 11493/15469    Discriminator_loss: 1.6074693202972412  Generator_loss: 1.360793113708496\n",
            "epoch: 9/10,    batch: 11494/15469    Discriminator_loss: 1.5880736112594604  Generator_loss: 1.344622254371643\n",
            "epoch: 9/10,    batch: 11495/15469    Discriminator_loss: 1.5737287998199463  Generator_loss: 1.3282470703125\n",
            "epoch: 9/10,    batch: 11496/15469    Discriminator_loss: 1.5471738576889038  Generator_loss: 1.31150484085083\n",
            "epoch: 9/10,    batch: 11497/15469    Discriminator_loss: 1.5290988683700562  Generator_loss: 1.2943851947784424\n",
            "epoch: 9/10,    batch: 11498/15469    Discriminator_loss: 1.5006260871887207  Generator_loss: 1.2768123149871826\n",
            "epoch: 9/10,    batch: 11499/15469    Discriminator_loss: 1.4859111309051514  Generator_loss: 1.2587144374847412\n",
            "epoch: 9/10,    batch: 11500/15469    Discriminator_loss: 1.4623486995697021  Generator_loss: 1.2400178909301758\n",
            "epoch: 9/10,    batch: 11501/15469    Discriminator_loss: 1.4363315105438232  Generator_loss: 1.2207753658294678\n",
            "epoch: 9/10,    batch: 11502/15469    Discriminator_loss: 1.3955800533294678  Generator_loss: 1.2008171081542969\n",
            "epoch: 9/10,    batch: 11503/15469    Discriminator_loss: 1.3518013954162598  Generator_loss: 1.1804099082946777\n",
            "epoch: 9/10,    batch: 11504/15469    Discriminator_loss: 1.3138484954833984  Generator_loss: 1.1595356464385986\n",
            "epoch: 9/10,    batch: 11505/15469    Discriminator_loss: 1.2920160293579102  Generator_loss: 1.1384050846099854\n",
            "epoch: 9/10,    batch: 11506/15469    Discriminator_loss: 1.2514839172363281  Generator_loss: 1.117199420928955\n",
            "epoch: 9/10,    batch: 11507/15469    Discriminator_loss: 1.2296428680419922  Generator_loss: 1.0962059497833252\n",
            "epoch: 9/10,    batch: 11508/15469    Discriminator_loss: 1.1995375156402588  Generator_loss: 1.0755350589752197\n",
            "epoch: 9/10,    batch: 11509/15469    Discriminator_loss: 1.1714816093444824  Generator_loss: 1.0554203987121582\n",
            "epoch: 9/10,    batch: 11510/15469    Discriminator_loss: 1.1013481616973877  Generator_loss: 1.035926342010498\n",
            "epoch: 9/10,    batch: 11511/15469    Discriminator_loss: 1.028327226638794  Generator_loss: 1.0172290802001953\n",
            "epoch: 9/10,    batch: 11512/15469    Discriminator_loss: 0.9893423318862915  Generator_loss: 0.998989462852478\n",
            "epoch: 9/10,    batch: 11513/15469    Discriminator_loss: 0.9444093704223633  Generator_loss: 0.9811407923698425\n",
            "epoch: 9/10,    batch: 11514/15469    Discriminator_loss: 0.7300844192504883  Generator_loss: 0.9639720320701599\n",
            "epoch: 9/10,    batch: 11515/15469    Discriminator_loss: 0.6192660927772522  Generator_loss: 0.9493182301521301\n",
            "epoch: 9/10,    batch: 11516/15469    Discriminator_loss: 0.5500102639198303  Generator_loss: 0.937969982624054\n",
            "epoch: 9/10,    batch: 11517/15469    Discriminator_loss: 0.532405436038971  Generator_loss: 0.9300950765609741\n",
            "epoch: 9/10,    batch: 11518/15469    Discriminator_loss: 0.5242974162101746  Generator_loss: 0.9253388047218323\n",
            "epoch: 9/10,    batch: 11519/15469    Discriminator_loss: 0.5126668810844421  Generator_loss: 0.923527717590332\n",
            "epoch: 9/10,    batch: 11520/15469    Discriminator_loss: 0.512251079082489  Generator_loss: 0.9241463541984558\n",
            "epoch: 9/10,    batch: 11521/15469    Discriminator_loss: 0.5111489295959473  Generator_loss: 0.926711916923523\n",
            "epoch: 9/10,    batch: 11522/15469    Discriminator_loss: 0.5080233216285706  Generator_loss: 0.9311970472335815\n",
            "epoch: 9/10,    batch: 11523/15469    Discriminator_loss: 0.5042192935943604  Generator_loss: 0.9366762042045593\n",
            "epoch: 9/10,    batch: 11524/15469    Discriminator_loss: 0.5001757144927979  Generator_loss: 0.9440462589263916\n",
            "epoch: 9/10,    batch: 11525/15469    Discriminator_loss: 0.496477872133255  Generator_loss: 0.9514432549476624\n",
            "epoch: 9/10,    batch: 11526/15469    Discriminator_loss: 0.49132978916168213  Generator_loss: 0.9591559767723083\n",
            "epoch: 9/10,    batch: 11527/15469    Discriminator_loss: 0.48655083775520325  Generator_loss: 0.9674066305160522\n",
            "epoch: 9/10,    batch: 11528/15469    Discriminator_loss: 0.4820765554904938  Generator_loss: 0.9753493070602417\n",
            "epoch: 9/10,    batch: 11529/15469    Discriminator_loss: 0.4783065617084503  Generator_loss: 0.9828886389732361\n",
            "epoch: 9/10,    batch: 11530/15469    Discriminator_loss: 0.47484880685806274  Generator_loss: 0.9892487525939941\n",
            "epoch: 9/10,    batch: 11531/15469    Discriminator_loss: 0.47296643257141113  Generator_loss: 0.9936612248420715\n",
            "epoch: 9/10,    batch: 11532/15469    Discriminator_loss: 0.4725269079208374  Generator_loss: 0.9962846040725708\n",
            "epoch: 9/10,    batch: 11533/15469    Discriminator_loss: 0.4742915630340576  Generator_loss: 0.9953353404998779\n",
            "epoch: 9/10,    batch: 11534/15469    Discriminator_loss: 0.4790513813495636  Generator_loss: 0.9903773069381714\n",
            "epoch: 9/10,    batch: 11535/15469    Discriminator_loss: 0.48843488097190857  Generator_loss: 0.9808492660522461\n",
            "epoch: 9/10,    batch: 11536/15469    Discriminator_loss: 0.504397988319397  Generator_loss: 0.965557336807251\n",
            "epoch: 9/10,    batch: 11537/15469    Discriminator_loss: 0.5257918834686279  Generator_loss: 0.9464629292488098\n",
            "epoch: 9/10,    batch: 11538/15469    Discriminator_loss: 0.5518029928207397  Generator_loss: 0.9296600818634033\n",
            "epoch: 9/10,    batch: 11539/15469    Discriminator_loss: 0.5717573165893555  Generator_loss: 0.9313794374465942\n",
            "epoch: 9/10,    batch: 11540/15469    Discriminator_loss: 0.5636954307556152  Generator_loss: 0.9692495465278625\n",
            "epoch: 9/10,    batch: 11541/15469    Discriminator_loss: 0.5110278129577637  Generator_loss: 1.0583895444869995\n",
            "epoch: 9/10,    batch: 11542/15469    Discriminator_loss: 0.42460697889328003  Generator_loss: 1.200268030166626\n",
            "epoch: 9/10,    batch: 11543/15469    Discriminator_loss: 0.33517611026763916  Generator_loss: 1.373189926147461\n",
            "epoch: 9/10,    batch: 11544/15469    Discriminator_loss: 0.267858624458313  Generator_loss: 1.5373889207839966\n",
            "epoch: 9/10,    batch: 11545/15469    Discriminator_loss: 0.2282278835773468  Generator_loss: 1.6576740741729736\n",
            "epoch: 9/10,    batch: 11546/15469    Discriminator_loss: 0.20909562706947327  Generator_loss: 1.7231028079986572\n",
            "epoch: 9/10,    batch: 11547/15469    Discriminator_loss: 0.20139440894126892  Generator_loss: 1.7467094659805298\n",
            "epoch: 9/10,    batch: 11548/15469    Discriminator_loss: 0.19912035763263702  Generator_loss: 1.748744010925293\n",
            "epoch: 9/10,    batch: 11549/15469    Discriminator_loss: 0.19899387657642365  Generator_loss: 1.7429938316345215\n",
            "epoch: 9/10,    batch: 11550/15469    Discriminator_loss: 0.19902661442756653  Generator_loss: 1.7366116046905518\n",
            "epoch: 9/10,    batch: 11551/15469    Discriminator_loss: 0.19924981892108917  Generator_loss: 1.732351303100586\n",
            "epoch: 9/10,    batch: 11552/15469    Discriminator_loss: 0.19924476742744446  Generator_loss: 1.7289328575134277\n",
            "epoch: 9/10,    batch: 11553/15469    Discriminator_loss: 0.1992679238319397  Generator_loss: 1.7266390323638916\n",
            "epoch: 9/10,    batch: 11554/15469    Discriminator_loss: 0.19928695261478424  Generator_loss: 1.7251157760620117\n",
            "epoch: 9/10,    batch: 11555/15469    Discriminator_loss: 0.1991310715675354  Generator_loss: 1.7244415283203125\n",
            "epoch: 9/10,    batch: 11556/15469    Discriminator_loss: 0.19887419044971466  Generator_loss: 1.724132776260376\n",
            "epoch: 9/10,    batch: 11557/15469    Discriminator_loss: 0.19857566058635712  Generator_loss: 1.7247525453567505\n",
            "epoch: 9/10,    batch: 11558/15469    Discriminator_loss: 0.19826219975948334  Generator_loss: 1.725868582725525\n",
            "epoch: 9/10,    batch: 11559/15469    Discriminator_loss: 0.19778992235660553  Generator_loss: 1.7272851467132568\n",
            "epoch: 9/10,    batch: 11560/15469    Discriminator_loss: 0.1974131166934967  Generator_loss: 1.7289800643920898\n",
            "epoch: 9/10,    batch: 11561/15469    Discriminator_loss: 0.19686365127563477  Generator_loss: 1.731095314025879\n",
            "epoch: 9/10,    batch: 11562/15469    Discriminator_loss: 0.19633087515830994  Generator_loss: 1.7335753440856934\n",
            "epoch: 9/10,    batch: 11563/15469    Discriminator_loss: 0.1957119256258011  Generator_loss: 1.7361352443695068\n",
            "epoch: 9/10,    batch: 11564/15469    Discriminator_loss: 0.19512279331684113  Generator_loss: 1.7386889457702637\n",
            "epoch: 9/10,    batch: 11565/15469    Discriminator_loss: 0.19444118440151215  Generator_loss: 1.741602897644043\n",
            "epoch: 9/10,    batch: 11566/15469    Discriminator_loss: 0.1938152015209198  Generator_loss: 1.7447926998138428\n",
            "epoch: 9/10,    batch: 11567/15469    Discriminator_loss: 0.19306586682796478  Generator_loss: 1.7477093935012817\n",
            "epoch: 9/10,    batch: 11568/15469    Discriminator_loss: 0.19237305223941803  Generator_loss: 1.7509136199951172\n",
            "epoch: 9/10,    batch: 11569/15469    Discriminator_loss: 0.19159527122974396  Generator_loss: 1.7546614408493042\n",
            "epoch: 9/10,    batch: 11570/15469    Discriminator_loss: 0.19074521958827972  Generator_loss: 1.758754014968872\n",
            "epoch: 9/10,    batch: 11571/15469    Discriminator_loss: 0.18985389173030853  Generator_loss: 1.7632169723510742\n",
            "epoch: 9/10,    batch: 11572/15469    Discriminator_loss: 0.18886472284793854  Generator_loss: 1.7679338455200195\n",
            "epoch: 9/10,    batch: 11573/15469    Discriminator_loss: 0.18779969215393066  Generator_loss: 1.773298740386963\n",
            "epoch: 9/10,    batch: 11574/15469    Discriminator_loss: 0.18666867911815643  Generator_loss: 1.7791337966918945\n",
            "epoch: 9/10,    batch: 11575/15469    Discriminator_loss: 0.18540731072425842  Generator_loss: 1.7853682041168213\n",
            "epoch: 9/10,    batch: 11576/15469    Discriminator_loss: 0.18408513069152832  Generator_loss: 1.7920968532562256\n",
            "epoch: 9/10,    batch: 11577/15469    Discriminator_loss: 0.18267133831977844  Generator_loss: 1.7995420694351196\n",
            "epoch: 9/10,    batch: 11578/15469    Discriminator_loss: 0.1811424195766449  Generator_loss: 1.8074626922607422\n",
            "epoch: 9/10,    batch: 11579/15469    Discriminator_loss: 0.1795029193162918  Generator_loss: 1.8160953521728516\n",
            "epoch: 9/10,    batch: 11580/15469    Discriminator_loss: 0.17776502668857574  Generator_loss: 1.825178623199463\n",
            "epoch: 9/10,    batch: 11581/15469    Discriminator_loss: 0.17595572769641876  Generator_loss: 1.8349356651306152\n",
            "epoch: 9/10,    batch: 11582/15469    Discriminator_loss: 0.1740766316652298  Generator_loss: 1.8450264930725098\n",
            "epoch: 9/10,    batch: 11583/15469    Discriminator_loss: 0.17213773727416992  Generator_loss: 1.8553483486175537\n",
            "epoch: 9/10,    batch: 11584/15469    Discriminator_loss: 0.1701909750699997  Generator_loss: 1.8658490180969238\n",
            "epoch: 9/10,    batch: 11585/15469    Discriminator_loss: 0.1682511568069458  Generator_loss: 1.8761825561523438\n",
            "epoch: 9/10,    batch: 11586/15469    Discriminator_loss: 0.16637296974658966  Generator_loss: 1.8863813877105713\n",
            "epoch: 9/10,    batch: 11587/15469    Discriminator_loss: 0.16455288231372833  Generator_loss: 1.8961689472198486\n",
            "epoch: 9/10,    batch: 11588/15469    Discriminator_loss: 0.16282957792282104  Generator_loss: 1.9054975509643555\n",
            "epoch: 9/10,    batch: 11589/15469    Discriminator_loss: 0.16119641065597534  Generator_loss: 1.9144055843353271\n",
            "epoch: 9/10,    batch: 11590/15469    Discriminator_loss: 0.15964171290397644  Generator_loss: 1.9229280948638916\n",
            "epoch: 9/10,    batch: 11591/15469    Discriminator_loss: 0.15816763043403625  Generator_loss: 1.931148886680603\n",
            "epoch: 9/10,    batch: 11592/15469    Discriminator_loss: 0.156771719455719  Generator_loss: 1.9390428066253662\n",
            "epoch: 9/10,    batch: 11593/15469    Discriminator_loss: 0.15542685985565186  Generator_loss: 1.9467110633850098\n",
            "epoch: 9/10,    batch: 11594/15469    Discriminator_loss: 0.15413230657577515  Generator_loss: 1.9541999101638794\n",
            "epoch: 9/10,    batch: 11595/15469    Discriminator_loss: 0.15286676585674286  Generator_loss: 1.9615975618362427\n",
            "epoch: 9/10,    batch: 11596/15469    Discriminator_loss: 0.15163010358810425  Generator_loss: 1.9688833951950073\n",
            "epoch: 9/10,    batch: 11597/15469    Discriminator_loss: 0.15041881799697876  Generator_loss: 1.9761500358581543\n",
            "epoch: 9/10,    batch: 11598/15469    Discriminator_loss: 0.1492231786251068  Generator_loss: 1.9834187030792236\n",
            "epoch: 9/10,    batch: 11599/15469    Discriminator_loss: 0.1480356752872467  Generator_loss: 1.9907276630401611\n",
            "epoch: 9/10,    batch: 11600/15469    Discriminator_loss: 0.1468523144721985  Generator_loss: 1.9980875253677368\n",
            "epoch: 9/10,    batch: 11601/15469    Discriminator_loss: 0.14567160606384277  Generator_loss: 2.005467414855957\n",
            "epoch: 9/10,    batch: 11602/15469    Discriminator_loss: 0.14450067281723022  Generator_loss: 2.0129473209381104\n",
            "epoch: 9/10,    batch: 11603/15469    Discriminator_loss: 0.14332634210586548  Generator_loss: 2.0203659534454346\n",
            "epoch: 9/10,    batch: 11604/15469    Discriminator_loss: 0.1421491801738739  Generator_loss: 2.0279436111450195\n",
            "epoch: 9/10,    batch: 11605/15469    Discriminator_loss: 0.14099304378032684  Generator_loss: 2.0355539321899414\n",
            "epoch: 9/10,    batch: 11606/15469    Discriminator_loss: 0.13984151184558868  Generator_loss: 2.0432779788970947\n",
            "epoch: 9/10,    batch: 11607/15469    Discriminator_loss: 0.13866794109344482  Generator_loss: 2.050952196121216\n",
            "epoch: 9/10,    batch: 11608/15469    Discriminator_loss: 0.13750898838043213  Generator_loss: 2.0586585998535156\n",
            "epoch: 9/10,    batch: 11609/15469    Discriminator_loss: 0.13637444376945496  Generator_loss: 2.0663933753967285\n",
            "epoch: 9/10,    batch: 11610/15469    Discriminator_loss: 0.13521412014961243  Generator_loss: 2.074490547180176\n",
            "epoch: 9/10,    batch: 11611/15469    Discriminator_loss: 0.13408222794532776  Generator_loss: 2.082228660583496\n",
            "epoch: 9/10,    batch: 11612/15469    Discriminator_loss: 0.13294415175914764  Generator_loss: 2.090245008468628\n",
            "epoch: 9/10,    batch: 11613/15469    Discriminator_loss: 0.1317964345216751  Generator_loss: 2.098085641860962\n",
            "epoch: 9/10,    batch: 11614/15469    Discriminator_loss: 0.13071008026599884  Generator_loss: 2.106081008911133\n",
            "epoch: 9/10,    batch: 11615/15469    Discriminator_loss: 0.12957407534122467  Generator_loss: 2.1142220497131348\n",
            "epoch: 9/10,    batch: 11616/15469    Discriminator_loss: 0.12844610214233398  Generator_loss: 2.1222336292266846\n",
            "epoch: 9/10,    batch: 11617/15469    Discriminator_loss: 0.12735354900360107  Generator_loss: 2.130460739135742\n",
            "epoch: 9/10,    batch: 11618/15469    Discriminator_loss: 0.12625443935394287  Generator_loss: 2.1385655403137207\n",
            "epoch: 9/10,    batch: 11619/15469    Discriminator_loss: 0.1251443326473236  Generator_loss: 2.146636724472046\n",
            "epoch: 9/10,    batch: 11620/15469    Discriminator_loss: 0.1240561306476593  Generator_loss: 2.154935836791992\n",
            "epoch: 9/10,    batch: 11621/15469    Discriminator_loss: 0.12297570705413818  Generator_loss: 2.163236379623413\n",
            "epoch: 9/10,    batch: 11622/15469    Discriminator_loss: 0.121894471347332  Generator_loss: 2.171354055404663\n",
            "epoch: 9/10,    batch: 11623/15469    Discriminator_loss: 0.12082679569721222  Generator_loss: 2.179882049560547\n",
            "epoch: 9/10,    batch: 11624/15469    Discriminator_loss: 0.11975586414337158  Generator_loss: 2.1882262229919434\n",
            "epoch: 9/10,    batch: 11625/15469    Discriminator_loss: 0.11869543045759201  Generator_loss: 2.1964898109436035\n",
            "epoch: 9/10,    batch: 11626/15469    Discriminator_loss: 0.11763342469930649  Generator_loss: 2.204954147338867\n",
            "epoch: 9/10,    batch: 11627/15469    Discriminator_loss: 0.11659398674964905  Generator_loss: 2.2133665084838867\n",
            "epoch: 9/10,    batch: 11628/15469    Discriminator_loss: 0.11556904017925262  Generator_loss: 2.2217562198638916\n",
            "epoch: 9/10,    batch: 11629/15469    Discriminator_loss: 0.11452964693307877  Generator_loss: 2.2302398681640625\n",
            "epoch: 9/10,    batch: 11630/15469    Discriminator_loss: 0.11351287364959717  Generator_loss: 2.238617420196533\n",
            "epoch: 9/10,    batch: 11631/15469    Discriminator_loss: 0.11249668151140213  Generator_loss: 2.246964931488037\n",
            "epoch: 9/10,    batch: 11632/15469    Discriminator_loss: 0.11150409281253815  Generator_loss: 2.2554893493652344\n",
            "epoch: 9/10,    batch: 11633/15469    Discriminator_loss: 0.11050844192504883  Generator_loss: 2.264021396636963\n",
            "epoch: 9/10,    batch: 11634/15469    Discriminator_loss: 0.10953263938426971  Generator_loss: 2.272353172302246\n",
            "epoch: 9/10,    batch: 11635/15469    Discriminator_loss: 0.10855958610773087  Generator_loss: 2.2807869911193848\n",
            "epoch: 9/10,    batch: 11636/15469    Discriminator_loss: 0.10759058594703674  Generator_loss: 2.289367198944092\n",
            "epoch: 9/10,    batch: 11637/15469    Discriminator_loss: 0.10664387792348862  Generator_loss: 2.297801971435547\n",
            "epoch: 9/10,    batch: 11638/15469    Discriminator_loss: 0.10569600015878677  Generator_loss: 2.3059747219085693\n",
            "epoch: 9/10,    batch: 11639/15469    Discriminator_loss: 0.1047775074839592  Generator_loss: 2.31449556350708\n",
            "epoch: 9/10,    batch: 11640/15469    Discriminator_loss: 0.10385733097791672  Generator_loss: 2.322883129119873\n",
            "epoch: 9/10,    batch: 11641/15469    Discriminator_loss: 0.1029382050037384  Generator_loss: 2.331296443939209\n",
            "epoch: 9/10,    batch: 11642/15469    Discriminator_loss: 0.10203030705451965  Generator_loss: 2.339482069015503\n",
            "epoch: 9/10,    batch: 11643/15469    Discriminator_loss: 0.10113148391246796  Generator_loss: 2.347925901412964\n",
            "epoch: 9/10,    batch: 11644/15469    Discriminator_loss: 0.10026413202285767  Generator_loss: 2.356274127960205\n",
            "epoch: 9/10,    batch: 11645/15469    Discriminator_loss: 0.0993986427783966  Generator_loss: 2.3644871711730957\n",
            "epoch: 9/10,    batch: 11646/15469    Discriminator_loss: 0.09852586686611176  Generator_loss: 2.372744560241699\n",
            "epoch: 9/10,    batch: 11647/15469    Discriminator_loss: 0.09767968952655792  Generator_loss: 2.3810696601867676\n",
            "epoch: 9/10,    batch: 11648/15469    Discriminator_loss: 0.09683284163475037  Generator_loss: 2.389352798461914\n",
            "epoch: 9/10,    batch: 11649/15469    Discriminator_loss: 0.096001997590065  Generator_loss: 2.397672653198242\n",
            "epoch: 9/10,    batch: 11650/15469    Discriminator_loss: 0.09517335891723633  Generator_loss: 2.40580415725708\n",
            "epoch: 9/10,    batch: 11651/15469    Discriminator_loss: 0.09436411410570145  Generator_loss: 2.4139342308044434\n",
            "epoch: 9/10,    batch: 11652/15469    Discriminator_loss: 0.09355462342500687  Generator_loss: 2.4221181869506836\n",
            "epoch: 9/10,    batch: 11653/15469    Discriminator_loss: 0.09275691956281662  Generator_loss: 2.430327892303467\n",
            "epoch: 9/10,    batch: 11654/15469    Discriminator_loss: 0.0919848307967186  Generator_loss: 2.4385437965393066\n",
            "epoch: 9/10,    batch: 11655/15469    Discriminator_loss: 0.09119106084108353  Generator_loss: 2.4466187953948975\n",
            "epoch: 9/10,    batch: 11656/15469    Discriminator_loss: 0.09043598175048828  Generator_loss: 2.4547505378723145\n",
            "epoch: 9/10,    batch: 11657/15469    Discriminator_loss: 0.08966527879238129  Generator_loss: 2.462794780731201\n",
            "epoch: 9/10,    batch: 11658/15469    Discriminator_loss: 0.08890756964683533  Generator_loss: 2.470839023590088\n",
            "epoch: 9/10,    batch: 11659/15469    Discriminator_loss: 0.08816978335380554  Generator_loss: 2.4789059162139893\n",
            "epoch: 9/10,    batch: 11660/15469    Discriminator_loss: 0.08743394166231155  Generator_loss: 2.486816883087158\n",
            "epoch: 9/10,    batch: 11661/15469    Discriminator_loss: 0.08670824021100998  Generator_loss: 2.495025157928467\n",
            "epoch: 9/10,    batch: 11662/15469    Discriminator_loss: 0.08599226921796799  Generator_loss: 2.5028934478759766\n",
            "epoch: 9/10,    batch: 11663/15469    Discriminator_loss: 0.08528508991003036  Generator_loss: 2.5108563899993896\n",
            "epoch: 9/10,    batch: 11664/15469    Discriminator_loss: 0.08455967158079147  Generator_loss: 2.5188961029052734\n",
            "epoch: 9/10,    batch: 11665/15469    Discriminator_loss: 0.08386597037315369  Generator_loss: 2.526926040649414\n",
            "epoch: 9/10,    batch: 11666/15469    Discriminator_loss: 0.08318851888179779  Generator_loss: 2.534696578979492\n",
            "epoch: 9/10,    batch: 11667/15469    Discriminator_loss: 0.08250531554222107  Generator_loss: 2.5425121784210205\n",
            "epoch: 9/10,    batch: 11668/15469    Discriminator_loss: 0.08184610307216644  Generator_loss: 2.550401449203491\n",
            "epoch: 9/10,    batch: 11669/15469    Discriminator_loss: 0.08118084818124771  Generator_loss: 2.5582501888275146\n",
            "epoch: 9/10,    batch: 11670/15469    Discriminator_loss: 0.08053556084632874  Generator_loss: 2.5659942626953125\n",
            "epoch: 9/10,    batch: 11671/15469    Discriminator_loss: 0.07988687604665756  Generator_loss: 2.5738143920898438\n",
            "epoch: 9/10,    batch: 11672/15469    Discriminator_loss: 0.07924316823482513  Generator_loss: 2.5814855098724365\n",
            "epoch: 9/10,    batch: 11673/15469    Discriminator_loss: 0.07861532270908356  Generator_loss: 2.589230537414551\n",
            "epoch: 9/10,    batch: 11674/15469    Discriminator_loss: 0.07801046967506409  Generator_loss: 2.596818208694458\n",
            "epoch: 9/10,    batch: 11675/15469    Discriminator_loss: 0.07743839174509048  Generator_loss: 2.604461669921875\n",
            "epoch: 9/10,    batch: 11676/15469    Discriminator_loss: 0.076807901263237  Generator_loss: 2.6120240688323975\n",
            "epoch: 9/10,    batch: 11677/15469    Discriminator_loss: 0.07620983570814133  Generator_loss: 2.6195991039276123\n",
            "epoch: 9/10,    batch: 11678/15469    Discriminator_loss: 0.07563550025224686  Generator_loss: 2.6270735263824463\n",
            "epoch: 9/10,    batch: 11679/15469    Discriminator_loss: 0.07509360462427139  Generator_loss: 2.634474039077759\n",
            "epoch: 9/10,    batch: 11680/15469    Discriminator_loss: 0.07452474534511566  Generator_loss: 2.641679048538208\n",
            "epoch: 9/10,    batch: 11681/15469    Discriminator_loss: 0.07402198016643524  Generator_loss: 2.64900279045105\n",
            "epoch: 9/10,    batch: 11682/15469    Discriminator_loss: 0.07347919791936874  Generator_loss: 2.6562023162841797\n",
            "epoch: 9/10,    batch: 11683/15469    Discriminator_loss: 0.07306791841983795  Generator_loss: 2.663343906402588\n",
            "epoch: 9/10,    batch: 11684/15469    Discriminator_loss: 0.5690318942070007  Generator_loss: 2.663733959197998\n",
            "epoch: 9/10,    batch: 11685/15469    Discriminator_loss: 2.9432270526885986  Generator_loss: 2.6347484588623047\n",
            "epoch: 9/10,    batch: 11686/15469    Discriminator_loss: 2.864264488220215  Generator_loss: 2.586308002471924\n",
            "epoch: 9/10,    batch: 11687/15469    Discriminator_loss: 2.732769012451172  Generator_loss: 2.5254926681518555\n",
            "epoch: 9/10,    batch: 11688/15469    Discriminator_loss: 2.639651298522949  Generator_loss: 2.455702543258667\n",
            "epoch: 9/10,    batch: 11689/15469    Discriminator_loss: 2.500544786453247  Generator_loss: 2.3802499771118164\n",
            "epoch: 9/10,    batch: 11690/15469    Discriminator_loss: 2.3998970985412598  Generator_loss: 2.3020753860473633\n",
            "epoch: 9/10,    batch: 11691/15469    Discriminator_loss: 0.1563655287027359  Generator_loss: 2.240302085876465\n",
            "epoch: 9/10,    batch: 11692/15469    Discriminator_loss: 0.11616683006286621  Generator_loss: 2.1928985118865967\n",
            "epoch: 9/10,    batch: 11693/15469    Discriminator_loss: 0.12134716659784317  Generator_loss: 2.1570117473602295\n",
            "epoch: 9/10,    batch: 11694/15469    Discriminator_loss: 0.125423401594162  Generator_loss: 2.129876136779785\n",
            "epoch: 9/10,    batch: 11695/15469    Discriminator_loss: 0.12858735024929047  Generator_loss: 2.1096339225769043\n",
            "epoch: 9/10,    batch: 11696/15469    Discriminator_loss: 0.13101091980934143  Generator_loss: 2.094334125518799\n",
            "epoch: 9/10,    batch: 11697/15469    Discriminator_loss: 0.13286268711090088  Generator_loss: 2.082826614379883\n",
            "epoch: 9/10,    batch: 11698/15469    Discriminator_loss: 0.13424941897392273  Generator_loss: 2.0744287967681885\n",
            "epoch: 9/10,    batch: 11699/15469    Discriminator_loss: 0.13524524867534637  Generator_loss: 2.0683553218841553\n",
            "epoch: 9/10,    batch: 11700/15469    Discriminator_loss: 0.13595432043075562  Generator_loss: 2.0642411708831787\n",
            "epoch: 9/10,    batch: 11701/15469    Discriminator_loss: 0.13643302023410797  Generator_loss: 2.061595916748047\n",
            "epoch: 9/10,    batch: 11702/15469    Discriminator_loss: 0.13670958578586578  Generator_loss: 2.0601611137390137\n",
            "epoch: 9/10,    batch: 11703/15469    Discriminator_loss: 0.13681358098983765  Generator_loss: 2.0596678256988525\n",
            "epoch: 9/10,    batch: 11704/15469    Discriminator_loss: 0.13678662478923798  Generator_loss: 2.0601797103881836\n",
            "epoch: 9/10,    batch: 11705/15469    Discriminator_loss: 0.13665549457073212  Generator_loss: 2.0612943172454834\n",
            "epoch: 9/10,    batch: 11706/15469    Discriminator_loss: 0.13642024993896484  Generator_loss: 2.0630440711975098\n",
            "epoch: 9/10,    batch: 11707/15469    Discriminator_loss: 0.13611385226249695  Generator_loss: 2.0652928352355957\n",
            "epoch: 9/10,    batch: 11708/15469    Discriminator_loss: 0.1357598751783371  Generator_loss: 2.067932367324829\n",
            "epoch: 9/10,    batch: 11709/15469    Discriminator_loss: 0.13533158600330353  Generator_loss: 2.0708680152893066\n",
            "epoch: 9/10,    batch: 11710/15469    Discriminator_loss: 0.13486862182617188  Generator_loss: 2.0741071701049805\n",
            "epoch: 9/10,    batch: 11711/15469    Discriminator_loss: 0.13437382876873016  Generator_loss: 2.077662467956543\n",
            "epoch: 9/10,    batch: 11712/15469    Discriminator_loss: 0.13384155929088593  Generator_loss: 2.0813965797424316\n",
            "epoch: 9/10,    batch: 11713/15469    Discriminator_loss: 0.1332835853099823  Generator_loss: 2.085313320159912\n",
            "epoch: 9/10,    batch: 11714/15469    Discriminator_loss: 0.13270685076713562  Generator_loss: 2.089397430419922\n",
            "epoch: 9/10,    batch: 11715/15469    Discriminator_loss: 0.13211573660373688  Generator_loss: 2.0936098098754883\n",
            "epoch: 9/10,    batch: 11716/15469    Discriminator_loss: 0.13150697946548462  Generator_loss: 2.0979599952697754\n",
            "epoch: 9/10,    batch: 11717/15469    Discriminator_loss: 0.13088694214820862  Generator_loss: 2.1024010181427\n",
            "epoch: 9/10,    batch: 11718/15469    Discriminator_loss: 0.13025236129760742  Generator_loss: 2.106931209564209\n",
            "epoch: 9/10,    batch: 11719/15469    Discriminator_loss: 0.1296124905347824  Generator_loss: 2.1115658283233643\n",
            "epoch: 9/10,    batch: 11720/15469    Discriminator_loss: 0.12896586954593658  Generator_loss: 2.1162335872650146\n",
            "epoch: 9/10,    batch: 11721/15469    Discriminator_loss: 0.12831450998783112  Generator_loss: 2.1209843158721924\n",
            "epoch: 9/10,    batch: 11722/15469    Discriminator_loss: 0.12766143679618835  Generator_loss: 2.125753879547119\n",
            "epoch: 9/10,    batch: 11723/15469    Discriminator_loss: 0.12700366973876953  Generator_loss: 2.130617141723633\n",
            "epoch: 9/10,    batch: 11724/15469    Discriminator_loss: 0.12633971869945526  Generator_loss: 2.1355485916137695\n",
            "epoch: 9/10,    batch: 11725/15469    Discriminator_loss: 0.1256864219903946  Generator_loss: 2.140427827835083\n",
            "epoch: 9/10,    batch: 11726/15469    Discriminator_loss: 0.1250205934047699  Generator_loss: 2.145379066467285\n",
            "epoch: 9/10,    batch: 11727/15469    Discriminator_loss: 0.12436085194349289  Generator_loss: 2.1503524780273438\n",
            "epoch: 9/10,    batch: 11728/15469    Discriminator_loss: 0.12370415031909943  Generator_loss: 2.15535306930542\n",
            "epoch: 9/10,    batch: 11729/15469    Discriminator_loss: 0.12305113673210144  Generator_loss: 2.160409450531006\n",
            "epoch: 9/10,    batch: 11730/15469    Discriminator_loss: 0.12238403409719467  Generator_loss: 2.165424346923828\n",
            "epoch: 9/10,    batch: 11731/15469    Discriminator_loss: 0.1217251867055893  Generator_loss: 2.17050838470459\n",
            "epoch: 9/10,    batch: 11732/15469    Discriminator_loss: 0.12107257544994354  Generator_loss: 2.1755616664886475\n",
            "epoch: 9/10,    batch: 11733/15469    Discriminator_loss: 0.12041976302862167  Generator_loss: 2.180665969848633\n",
            "epoch: 9/10,    batch: 11734/15469    Discriminator_loss: 0.11976676434278488  Generator_loss: 2.18577241897583\n",
            "epoch: 9/10,    batch: 11735/15469    Discriminator_loss: 0.1191195398569107  Generator_loss: 2.1908485889434814\n",
            "epoch: 9/10,    batch: 11736/15469    Discriminator_loss: 0.11847954243421555  Generator_loss: 2.1959285736083984\n",
            "epoch: 9/10,    batch: 11737/15469    Discriminator_loss: 0.11783633381128311  Generator_loss: 2.201042652130127\n",
            "epoch: 9/10,    batch: 11738/15469    Discriminator_loss: 0.1171988770365715  Generator_loss: 2.206173896789551\n",
            "epoch: 9/10,    batch: 11739/15469    Discriminator_loss: 0.11656277626752853  Generator_loss: 2.2112879753112793\n",
            "epoch: 9/10,    batch: 11740/15469    Discriminator_loss: 0.11593876034021378  Generator_loss: 2.216390609741211\n",
            "epoch: 9/10,    batch: 11741/15469    Discriminator_loss: 0.1153089851140976  Generator_loss: 2.221492052078247\n",
            "epoch: 9/10,    batch: 11742/15469    Discriminator_loss: 0.11468818038702011  Generator_loss: 2.2265872955322266\n",
            "epoch: 9/10,    batch: 11743/15469    Discriminator_loss: 0.11407463252544403  Generator_loss: 2.2316927909851074\n",
            "epoch: 9/10,    batch: 11744/15469    Discriminator_loss: 0.11346195638179779  Generator_loss: 2.2367405891418457\n",
            "epoch: 9/10,    batch: 11745/15469    Discriminator_loss: 0.11285276710987091  Generator_loss: 2.241819381713867\n",
            "epoch: 9/10,    batch: 11746/15469    Discriminator_loss: 0.11224967241287231  Generator_loss: 2.2468526363372803\n",
            "epoch: 9/10,    batch: 11747/15469    Discriminator_loss: 0.11165465414524078  Generator_loss: 2.25193452835083\n",
            "epoch: 9/10,    batch: 11748/15469    Discriminator_loss: 0.11106367409229279  Generator_loss: 2.2569363117218018\n",
            "epoch: 9/10,    batch: 11749/15469    Discriminator_loss: 0.11047935485839844  Generator_loss: 2.2619307041168213\n",
            "epoch: 9/10,    batch: 11750/15469    Discriminator_loss: 0.1099010556936264  Generator_loss: 2.2668938636779785\n",
            "epoch: 9/10,    batch: 11751/15469    Discriminator_loss: 0.10932731628417969  Generator_loss: 2.271867036819458\n",
            "epoch: 9/10,    batch: 11752/15469    Discriminator_loss: 0.10875789821147919  Generator_loss: 2.276754856109619\n",
            "epoch: 9/10,    batch: 11753/15469    Discriminator_loss: 0.10820131003856659  Generator_loss: 2.281625270843506\n",
            "epoch: 9/10,    batch: 11754/15469    Discriminator_loss: 0.10764645040035248  Generator_loss: 2.2864809036254883\n",
            "epoch: 9/10,    batch: 11755/15469    Discriminator_loss: 0.10710028558969498  Generator_loss: 2.291303873062134\n",
            "epoch: 9/10,    batch: 11756/15469    Discriminator_loss: 0.10656464099884033  Generator_loss: 2.2960798740386963\n",
            "epoch: 9/10,    batch: 11757/15469    Discriminator_loss: 0.10603374987840652  Generator_loss: 2.300814151763916\n",
            "epoch: 9/10,    batch: 11758/15469    Discriminator_loss: 0.1055087298154831  Generator_loss: 2.3054816722869873\n",
            "epoch: 9/10,    batch: 11759/15469    Discriminator_loss: 0.10499541461467743  Generator_loss: 2.3101377487182617\n",
            "epoch: 9/10,    batch: 11760/15469    Discriminator_loss: 0.10448834300041199  Generator_loss: 2.3147268295288086\n",
            "epoch: 9/10,    batch: 11761/15469    Discriminator_loss: 0.10399273037910461  Generator_loss: 2.319279670715332\n",
            "epoch: 9/10,    batch: 11762/15469    Discriminator_loss: 0.10350372642278671  Generator_loss: 2.323753595352173\n",
            "epoch: 9/10,    batch: 11763/15469    Discriminator_loss: 0.10302199423313141  Generator_loss: 2.3282079696655273\n",
            "epoch: 9/10,    batch: 11764/15469    Discriminator_loss: 0.10254541784524918  Generator_loss: 2.332573413848877\n",
            "epoch: 9/10,    batch: 11765/15469    Discriminator_loss: 0.1020771935582161  Generator_loss: 2.3369193077087402\n",
            "epoch: 9/10,    batch: 11766/15469    Discriminator_loss: 0.10161955654621124  Generator_loss: 2.341193675994873\n",
            "epoch: 9/10,    batch: 11767/15469    Discriminator_loss: 0.1011664867401123  Generator_loss: 2.345425605773926\n",
            "epoch: 9/10,    batch: 11768/15469    Discriminator_loss: 0.10072129219770432  Generator_loss: 2.3496170043945312\n",
            "epoch: 9/10,    batch: 11769/15469    Discriminator_loss: 0.10028018802404404  Generator_loss: 2.3537826538085938\n",
            "epoch: 9/10,    batch: 11770/15469    Discriminator_loss: 0.09984802454710007  Generator_loss: 2.3579039573669434\n",
            "epoch: 9/10,    batch: 11771/15469    Discriminator_loss: 0.09941918402910233  Generator_loss: 2.3620076179504395\n",
            "epoch: 9/10,    batch: 11772/15469    Discriminator_loss: 0.09899236261844635  Generator_loss: 2.3660972118377686\n",
            "epoch: 9/10,    batch: 11773/15469    Discriminator_loss: 0.09857168793678284  Generator_loss: 2.370145320892334\n",
            "epoch: 9/10,    batch: 11774/15469    Discriminator_loss: 0.09815537184476852  Generator_loss: 2.374239683151245\n",
            "epoch: 9/10,    batch: 11775/15469    Discriminator_loss: 0.0977381095290184  Generator_loss: 2.37825870513916\n",
            "epoch: 9/10,    batch: 11776/15469    Discriminator_loss: 0.09732328355312347  Generator_loss: 2.3823180198669434\n",
            "epoch: 9/10,    batch: 11777/15469    Discriminator_loss: 0.09690627455711365  Generator_loss: 2.3863654136657715\n",
            "epoch: 9/10,    batch: 11778/15469    Discriminator_loss: 0.09649302065372467  Generator_loss: 2.3904714584350586\n",
            "epoch: 9/10,    batch: 11779/15469    Discriminator_loss: 0.09608056396245956  Generator_loss: 2.39454984664917\n",
            "epoch: 9/10,    batch: 11780/15469    Discriminator_loss: 0.09566882252693176  Generator_loss: 2.398655891418457\n",
            "epoch: 9/10,    batch: 11781/15469    Discriminator_loss: 0.0952516421675682  Generator_loss: 2.4028193950653076\n",
            "epoch: 9/10,    batch: 11782/15469    Discriminator_loss: 0.09483851492404938  Generator_loss: 2.4069995880126953\n",
            "epoch: 9/10,    batch: 11783/15469    Discriminator_loss: 0.09442311525344849  Generator_loss: 2.4111804962158203\n",
            "epoch: 9/10,    batch: 11784/15469    Discriminator_loss: 0.0940028727054596  Generator_loss: 2.415417194366455\n",
            "epoch: 9/10,    batch: 11785/15469    Discriminator_loss: 0.09358179569244385  Generator_loss: 2.4196736812591553\n",
            "epoch: 9/10,    batch: 11786/15469    Discriminator_loss: 0.0931638702750206  Generator_loss: 2.4240031242370605\n",
            "epoch: 9/10,    batch: 11787/15469    Discriminator_loss: 0.0927421823143959  Generator_loss: 2.4283385276794434\n",
            "epoch: 9/10,    batch: 11788/15469    Discriminator_loss: 0.09231623262166977  Generator_loss: 2.4327704906463623\n",
            "epoch: 9/10,    batch: 11789/15469    Discriminator_loss: 0.0918910801410675  Generator_loss: 2.4371824264526367\n",
            "epoch: 9/10,    batch: 11790/15469    Discriminator_loss: 0.09146393835544586  Generator_loss: 2.4416565895080566\n",
            "epoch: 9/10,    batch: 11791/15469    Discriminator_loss: 0.09103424102067947  Generator_loss: 2.4461498260498047\n",
            "epoch: 9/10,    batch: 11792/15469    Discriminator_loss: 0.09060528129339218  Generator_loss: 2.450653076171875\n",
            "epoch: 9/10,    batch: 11793/15469    Discriminator_loss: 0.09017714858055115  Generator_loss: 2.4552359580993652\n",
            "epoch: 9/10,    batch: 11794/15469    Discriminator_loss: 0.08974385261535645  Generator_loss: 2.459855079650879\n",
            "epoch: 9/10,    batch: 11795/15469    Discriminator_loss: 0.08930603414773941  Generator_loss: 2.4645214080810547\n",
            "epoch: 9/10,    batch: 11796/15469    Discriminator_loss: 0.08888319134712219  Generator_loss: 2.46917986869812\n",
            "epoch: 9/10,    batch: 11797/15469    Discriminator_loss: 0.08844353258609772  Generator_loss: 2.4738705158233643\n",
            "epoch: 9/10,    batch: 11798/15469    Discriminator_loss: 0.08801169693470001  Generator_loss: 2.478633165359497\n",
            "epoch: 9/10,    batch: 11799/15469    Discriminator_loss: 0.08757779002189636  Generator_loss: 2.483414888381958\n",
            "epoch: 9/10,    batch: 11800/15469    Discriminator_loss: 0.08714577555656433  Generator_loss: 2.4882290363311768\n",
            "epoch: 9/10,    batch: 11801/15469    Discriminator_loss: 0.0867190808057785  Generator_loss: 2.493061065673828\n",
            "epoch: 9/10,    batch: 11802/15469    Discriminator_loss: 0.08628367632627487  Generator_loss: 2.4979257583618164\n",
            "epoch: 9/10,    batch: 11803/15469    Discriminator_loss: 1.577201008796692  Generator_loss: 2.493741989135742\n",
            "epoch: 9/10,    batch: 11804/15469    Discriminator_loss: 2.7225050926208496  Generator_loss: 2.476212978363037\n",
            "epoch: 9/10,    batch: 11805/15469    Discriminator_loss: 2.6915323734283447  Generator_loss: 2.449219226837158\n",
            "epoch: 9/10,    batch: 11806/15469    Discriminator_loss: 2.6111481189727783  Generator_loss: 2.415926933288574\n",
            "epoch: 9/10,    batch: 11807/15469    Discriminator_loss: 2.560211181640625  Generator_loss: 2.378600597381592\n",
            "epoch: 9/10,    batch: 11808/15469    Discriminator_loss: 2.530060052871704  Generator_loss: 2.33882474899292\n",
            "epoch: 9/10,    batch: 11809/15469    Discriminator_loss: 2.460937023162842  Generator_loss: 2.2979233264923096\n",
            "epoch: 9/10,    batch: 11810/15469    Discriminator_loss: 2.373035192489624  Generator_loss: 2.2567496299743652\n",
            "epoch: 9/10,    batch: 11811/15469    Discriminator_loss: 2.32092547416687  Generator_loss: 2.215871810913086\n",
            "epoch: 9/10,    batch: 11812/15469    Discriminator_loss: 2.256227970123291  Generator_loss: 2.1756210327148438\n",
            "epoch: 9/10,    batch: 11813/15469    Discriminator_loss: 2.203120708465576  Generator_loss: 2.136115550994873\n",
            "epoch: 9/10,    batch: 11814/15469    Discriminator_loss: 2.1565587520599365  Generator_loss: 2.0975964069366455\n",
            "epoch: 9/10,    batch: 11815/15469    Discriminator_loss: 2.1152265071868896  Generator_loss: 2.059931755065918\n",
            "epoch: 9/10,    batch: 11816/15469    Discriminator_loss: 2.0369958877563477  Generator_loss: 2.0233731269836426\n",
            "epoch: 9/10,    batch: 11817/15469    Discriminator_loss: 1.9861255884170532  Generator_loss: 1.9877266883850098\n",
            "epoch: 9/10,    batch: 11818/15469    Discriminator_loss: 1.9550786018371582  Generator_loss: 1.9530210494995117\n",
            "epoch: 9/10,    batch: 11819/15469    Discriminator_loss: 1.9101885557174683  Generator_loss: 1.9191076755523682\n",
            "epoch: 9/10,    batch: 11820/15469    Discriminator_loss: 1.85050368309021  Generator_loss: 1.8860580921173096\n",
            "epoch: 9/10,    batch: 11821/15469    Discriminator_loss: 1.8092008829116821  Generator_loss: 1.853774070739746\n",
            "epoch: 9/10,    batch: 11822/15469    Discriminator_loss: 1.7663142681121826  Generator_loss: 1.8222177028656006\n",
            "epoch: 9/10,    batch: 11823/15469    Discriminator_loss: 1.7234833240509033  Generator_loss: 1.7913576364517212\n",
            "epoch: 9/10,    batch: 11824/15469    Discriminator_loss: 1.6966674327850342  Generator_loss: 1.7608916759490967\n",
            "epoch: 9/10,    batch: 11825/15469    Discriminator_loss: 1.6475749015808105  Generator_loss: 1.7309892177581787\n",
            "epoch: 9/10,    batch: 11826/15469    Discriminator_loss: 1.6134114265441895  Generator_loss: 1.7015209197998047\n",
            "epoch: 9/10,    batch: 11827/15469    Discriminator_loss: 1.578674554824829  Generator_loss: 1.6722257137298584\n",
            "epoch: 9/10,    batch: 11828/15469    Discriminator_loss: 1.542426347732544  Generator_loss: 1.643470287322998\n",
            "epoch: 9/10,    batch: 11829/15469    Discriminator_loss: 1.508363962173462  Generator_loss: 1.6149818897247314\n",
            "epoch: 9/10,    batch: 11830/15469    Discriminator_loss: 1.4827113151550293  Generator_loss: 1.5870858430862427\n",
            "epoch: 9/10,    batch: 11831/15469    Discriminator_loss: 1.4438320398330688  Generator_loss: 1.5595886707305908\n",
            "epoch: 9/10,    batch: 11832/15469    Discriminator_loss: 1.4170920848846436  Generator_loss: 1.532674789428711\n",
            "epoch: 9/10,    batch: 11833/15469    Discriminator_loss: 1.3825210332870483  Generator_loss: 1.5065182447433472\n",
            "epoch: 9/10,    batch: 11834/15469    Discriminator_loss: 1.3522411584854126  Generator_loss: 1.4810925722122192\n",
            "epoch: 9/10,    batch: 11835/15469    Discriminator_loss: 1.3248955011367798  Generator_loss: 1.4567747116088867\n",
            "epoch: 9/10,    batch: 11836/15469    Discriminator_loss: 1.292296051979065  Generator_loss: 1.4335792064666748\n",
            "epoch: 9/10,    batch: 11837/15469    Discriminator_loss: 1.2736806869506836  Generator_loss: 1.4113893508911133\n",
            "epoch: 9/10,    batch: 11838/15469    Discriminator_loss: 0.7268344759941101  Generator_loss: 1.3928289413452148\n",
            "epoch: 9/10,    batch: 11839/15469    Discriminator_loss: 0.2890351116657257  Generator_loss: 1.3793162107467651\n",
            "epoch: 9/10,    batch: 11840/15469    Discriminator_loss: 0.2927912771701813  Generator_loss: 1.370230793952942\n",
            "epoch: 9/10,    batch: 11841/15469    Discriminator_loss: 0.2952048182487488  Generator_loss: 1.3647547960281372\n",
            "epoch: 9/10,    batch: 11842/15469    Discriminator_loss: 0.2964978814125061  Generator_loss: 1.362324595451355\n",
            "epoch: 9/10,    batch: 11843/15469    Discriminator_loss: 0.29687488079071045  Generator_loss: 1.3623570203781128\n",
            "epoch: 9/10,    batch: 11844/15469    Discriminator_loss: 0.2964651584625244  Generator_loss: 1.3644168376922607\n",
            "epoch: 9/10,    batch: 11845/15469    Discriminator_loss: 0.2954257130622864  Generator_loss: 1.3682420253753662\n",
            "epoch: 9/10,    batch: 11846/15469    Discriminator_loss: 0.29388144612312317  Generator_loss: 1.3733406066894531\n",
            "epoch: 9/10,    batch: 11847/15469    Discriminator_loss: 0.29192930459976196  Generator_loss: 1.3796195983886719\n",
            "epoch: 9/10,    batch: 11848/15469    Discriminator_loss: 0.28965863585472107  Generator_loss: 1.3867952823638916\n",
            "epoch: 9/10,    batch: 11849/15469    Discriminator_loss: 0.2871442139148712  Generator_loss: 1.3946583271026611\n",
            "epoch: 9/10,    batch: 11850/15469    Discriminator_loss: 0.2844252288341522  Generator_loss: 1.4031370878219604\n",
            "epoch: 9/10,    batch: 11851/15469    Discriminator_loss: 0.28156641125679016  Generator_loss: 1.4121181964874268\n",
            "epoch: 9/10,    batch: 11852/15469    Discriminator_loss: 0.27860042452812195  Generator_loss: 1.4214885234832764\n",
            "epoch: 9/10,    batch: 11853/15469    Discriminator_loss: 0.2755447030067444  Generator_loss: 1.4311954975128174\n",
            "epoch: 9/10,    batch: 11854/15469    Discriminator_loss: 0.2724432647228241  Generator_loss: 1.441152572631836\n",
            "epoch: 9/10,    batch: 11855/15469    Discriminator_loss: 0.26929402351379395  Generator_loss: 1.4513578414916992\n",
            "epoch: 9/10,    batch: 11856/15469    Discriminator_loss: 0.26613858342170715  Generator_loss: 1.4617195129394531\n",
            "epoch: 9/10,    batch: 11857/15469    Discriminator_loss: 0.2629593014717102  Generator_loss: 1.4722650051116943\n",
            "epoch: 9/10,    batch: 11858/15469    Discriminator_loss: 0.25978103280067444  Generator_loss: 1.4829403162002563\n",
            "epoch: 9/10,    batch: 11859/15469    Discriminator_loss: 0.2566194236278534  Generator_loss: 1.4937398433685303\n",
            "epoch: 9/10,    batch: 11860/15469    Discriminator_loss: 0.25345367193222046  Generator_loss: 1.5045955181121826\n",
            "epoch: 9/10,    batch: 11861/15469    Discriminator_loss: 0.2503238320350647  Generator_loss: 1.5155448913574219\n",
            "epoch: 9/10,    batch: 11862/15469    Discriminator_loss: 0.24720975756645203  Generator_loss: 1.5265612602233887\n",
            "epoch: 9/10,    batch: 11863/15469    Discriminator_loss: 0.24413146078586578  Generator_loss: 1.5376391410827637\n",
            "epoch: 9/10,    batch: 11864/15469    Discriminator_loss: 0.2410658597946167  Generator_loss: 1.5488002300262451\n",
            "epoch: 9/10,    batch: 11865/15469    Discriminator_loss: 0.2380332201719284  Generator_loss: 1.5599818229675293\n",
            "epoch: 9/10,    batch: 11866/15469    Discriminator_loss: 0.2350333034992218  Generator_loss: 1.571205973625183\n",
            "epoch: 9/10,    batch: 11867/15469    Discriminator_loss: 0.2320779711008072  Generator_loss: 1.5824403762817383\n",
            "epoch: 9/10,    batch: 11868/15469    Discriminator_loss: 0.22916433215141296  Generator_loss: 1.5936925411224365\n",
            "epoch: 9/10,    batch: 11869/15469    Discriminator_loss: 0.22625303268432617  Generator_loss: 1.6049997806549072\n",
            "epoch: 9/10,    batch: 11870/15469    Discriminator_loss: 0.223404198884964  Generator_loss: 1.61627197265625\n",
            "epoch: 9/10,    batch: 11871/15469    Discriminator_loss: 0.22060298919677734  Generator_loss: 1.6275615692138672\n",
            "epoch: 9/10,    batch: 11872/15469    Discriminator_loss: 0.21781933307647705  Generator_loss: 1.6388566493988037\n",
            "epoch: 9/10,    batch: 11873/15469    Discriminator_loss: 0.21509315073490143  Generator_loss: 1.650181531906128\n",
            "epoch: 9/10,    batch: 11874/15469    Discriminator_loss: 0.212397038936615  Generator_loss: 1.6614147424697876\n",
            "epoch: 9/10,    batch: 11875/15469    Discriminator_loss: 0.20975613594055176  Generator_loss: 1.672681212425232\n",
            "epoch: 9/10,    batch: 11876/15469    Discriminator_loss: 0.20714622735977173  Generator_loss: 1.68392014503479\n",
            "epoch: 9/10,    batch: 11877/15469    Discriminator_loss: 0.20458391308784485  Generator_loss: 1.6951009035110474\n",
            "epoch: 9/10,    batch: 11878/15469    Discriminator_loss: 0.20205482840538025  Generator_loss: 1.7062512636184692\n",
            "epoch: 9/10,    batch: 11879/15469    Discriminator_loss: 0.1995786428451538  Generator_loss: 1.7174009084701538\n",
            "epoch: 9/10,    batch: 11880/15469    Discriminator_loss: 0.19713756442070007  Generator_loss: 1.728506088256836\n",
            "epoch: 9/10,    batch: 11881/15469    Discriminator_loss: 0.19473695755004883  Generator_loss: 1.7395222187042236\n",
            "epoch: 9/10,    batch: 11882/15469    Discriminator_loss: 0.19238555431365967  Generator_loss: 1.7505137920379639\n",
            "epoch: 9/10,    batch: 11883/15469    Discriminator_loss: 0.19006678462028503  Generator_loss: 1.7614963054656982\n",
            "epoch: 9/10,    batch: 11884/15469    Discriminator_loss: 0.18779022991657257  Generator_loss: 1.7723915576934814\n",
            "epoch: 9/10,    batch: 11885/15469    Discriminator_loss: 0.18556320667266846  Generator_loss: 1.7833117246627808\n",
            "epoch: 9/10,    batch: 11886/15469    Discriminator_loss: 0.18336275219917297  Generator_loss: 1.7940315008163452\n",
            "epoch: 9/10,    batch: 11887/15469    Discriminator_loss: 0.18121139705181122  Generator_loss: 1.8048250675201416\n",
            "epoch: 9/10,    batch: 11888/15469    Discriminator_loss: 0.17909429967403412  Generator_loss: 1.815502405166626\n",
            "epoch: 9/10,    batch: 11889/15469    Discriminator_loss: 0.1770201325416565  Generator_loss: 1.8260760307312012\n",
            "epoch: 9/10,    batch: 11890/15469    Discriminator_loss: 0.17497101426124573  Generator_loss: 1.8366914987564087\n",
            "epoch: 9/10,    batch: 11891/15469    Discriminator_loss: 0.1729755401611328  Generator_loss: 1.8471983671188354\n",
            "epoch: 9/10,    batch: 11892/15469    Discriminator_loss: 0.1710054576396942  Generator_loss: 1.8576539754867554\n",
            "epoch: 9/10,    batch: 11893/15469    Discriminator_loss: 0.16907520592212677  Generator_loss: 1.8679966926574707\n",
            "epoch: 9/10,    batch: 11894/15469    Discriminator_loss: 0.1671862006187439  Generator_loss: 1.8783379793167114\n",
            "epoch: 9/10,    batch: 11895/15469    Discriminator_loss: 0.16533702611923218  Generator_loss: 1.8885414600372314\n",
            "epoch: 9/10,    batch: 11896/15469    Discriminator_loss: 0.16349905729293823  Generator_loss: 1.8987066745758057\n",
            "epoch: 9/10,    batch: 11897/15469    Discriminator_loss: 0.16172608733177185  Generator_loss: 1.9088404178619385\n",
            "epoch: 9/10,    batch: 11898/15469    Discriminator_loss: 0.15996479988098145  Generator_loss: 1.918806552886963\n",
            "epoch: 9/10,    batch: 11899/15469    Discriminator_loss: 0.1582561433315277  Generator_loss: 1.928720474243164\n",
            "epoch: 9/10,    batch: 11900/15469    Discriminator_loss: 0.1565777063369751  Generator_loss: 1.938564419746399\n",
            "epoch: 9/10,    batch: 11901/15469    Discriminator_loss: 0.15491637587547302  Generator_loss: 1.9482769966125488\n",
            "epoch: 9/10,    batch: 11902/15469    Discriminator_loss: 0.15331542491912842  Generator_loss: 1.9579973220825195\n",
            "epoch: 9/10,    batch: 11903/15469    Discriminator_loss: 0.1517210602760315  Generator_loss: 1.9676010608673096\n",
            "epoch: 9/10,    batch: 11904/15469    Discriminator_loss: 0.15018653869628906  Generator_loss: 1.9770004749298096\n",
            "epoch: 9/10,    batch: 11905/15469    Discriminator_loss: 0.14866822957992554  Generator_loss: 1.9863908290863037\n",
            "epoch: 9/10,    batch: 11906/15469    Discriminator_loss: 0.1471908986568451  Generator_loss: 1.9956530332565308\n",
            "epoch: 9/10,    batch: 11907/15469    Discriminator_loss: 0.14573949575424194  Generator_loss: 2.0047407150268555\n",
            "epoch: 9/10,    batch: 11908/15469    Discriminator_loss: 0.14432445168495178  Generator_loss: 2.013772964477539\n",
            "epoch: 9/10,    batch: 11909/15469    Discriminator_loss: 0.14294245839118958  Generator_loss: 2.0226478576660156\n",
            "epoch: 9/10,    batch: 11910/15469    Discriminator_loss: 0.1416091024875641  Generator_loss: 2.0315003395080566\n",
            "epoch: 9/10,    batch: 11911/15469    Discriminator_loss: 0.140314519405365  Generator_loss: 2.0400753021240234\n",
            "epoch: 9/10,    batch: 11912/15469    Discriminator_loss: 0.13901127874851227  Generator_loss: 2.048549175262451\n",
            "epoch: 9/10,    batch: 11913/15469    Discriminator_loss: 0.1377878040075302  Generator_loss: 2.056833028793335\n",
            "epoch: 9/10,    batch: 11914/15469    Discriminator_loss: 0.13658034801483154  Generator_loss: 2.065134048461914\n",
            "epoch: 9/10,    batch: 11915/15469    Discriminator_loss: 0.13542470335960388  Generator_loss: 2.0730061531066895\n",
            "epoch: 9/10,    batch: 11916/15469    Discriminator_loss: 0.13427215814590454  Generator_loss: 2.08071231842041\n",
            "epoch: 9/10,    batch: 11917/15469    Discriminator_loss: 0.13319630920886993  Generator_loss: 2.088390350341797\n",
            "epoch: 9/10,    batch: 11918/15469    Discriminator_loss: 0.13212919235229492  Generator_loss: 2.09578537940979\n",
            "epoch: 9/10,    batch: 11919/15469    Discriminator_loss: 0.1311233639717102  Generator_loss: 2.1028242111206055\n",
            "epoch: 9/10,    batch: 11920/15469    Discriminator_loss: 0.13017623126506805  Generator_loss: 2.1099345684051514\n",
            "epoch: 9/10,    batch: 11921/15469    Discriminator_loss: 0.12921684980392456  Generator_loss: 2.1165900230407715\n",
            "epoch: 9/10,    batch: 11922/15469    Discriminator_loss: 0.12835173308849335  Generator_loss: 2.123004674911499\n",
            "epoch: 9/10,    batch: 11923/15469    Discriminator_loss: 0.12752100825309753  Generator_loss: 2.1291046142578125\n",
            "epoch: 9/10,    batch: 11924/15469    Discriminator_loss: 0.12674890458583832  Generator_loss: 2.134648323059082\n",
            "epoch: 9/10,    batch: 11925/15469    Discriminator_loss: 0.12602058053016663  Generator_loss: 2.140122413635254\n",
            "epoch: 9/10,    batch: 11926/15469    Discriminator_loss: 0.12533564865589142  Generator_loss: 2.144908905029297\n",
            "epoch: 9/10,    batch: 11927/15469    Discriminator_loss: 0.12474104017019272  Generator_loss: 2.149625301361084\n",
            "epoch: 9/10,    batch: 11928/15469    Discriminator_loss: 0.12417536973953247  Generator_loss: 2.1536197662353516\n",
            "epoch: 9/10,    batch: 11929/15469    Discriminator_loss: 0.12369908392429352  Generator_loss: 2.1573894023895264\n",
            "epoch: 9/10,    batch: 11930/15469    Discriminator_loss: 0.12328826636075974  Generator_loss: 2.1606459617614746\n",
            "epoch: 9/10,    batch: 11931/15469    Discriminator_loss: 0.12295319139957428  Generator_loss: 2.1631007194519043\n",
            "epoch: 9/10,    batch: 11932/15469    Discriminator_loss: 0.12267836183309555  Generator_loss: 2.1651487350463867\n",
            "epoch: 9/10,    batch: 11933/15469    Discriminator_loss: 0.12250922620296478  Generator_loss: 2.166651964187622\n",
            "epoch: 9/10,    batch: 11934/15469    Discriminator_loss: 0.12239212542772293  Generator_loss: 2.1673121452331543\n",
            "epoch: 9/10,    batch: 11935/15469    Discriminator_loss: 0.12239770591259003  Generator_loss: 2.1673803329467773\n",
            "epoch: 9/10,    batch: 11936/15469    Discriminator_loss: 0.12243010848760605  Generator_loss: 2.167092800140381\n",
            "epoch: 9/10,    batch: 11937/15469    Discriminator_loss: 0.12255126982927322  Generator_loss: 2.1660983562469482\n",
            "epoch: 9/10,    batch: 11938/15469    Discriminator_loss: 0.12274707108736038  Generator_loss: 2.164767265319824\n",
            "epoch: 9/10,    batch: 11939/15469    Discriminator_loss: 0.12297210097312927  Generator_loss: 2.1628897190093994\n",
            "epoch: 9/10,    batch: 11940/15469    Discriminator_loss: 0.12327729165554047  Generator_loss: 2.1610894203186035\n",
            "epoch: 9/10,    batch: 11941/15469    Discriminator_loss: 0.12357204407453537  Generator_loss: 2.158715009689331\n",
            "epoch: 9/10,    batch: 11942/15469    Discriminator_loss: 0.12386094033718109  Generator_loss: 2.156431198120117\n",
            "epoch: 9/10,    batch: 11943/15469    Discriminator_loss: 0.12420590221881866  Generator_loss: 2.153960704803467\n",
            "epoch: 9/10,    batch: 11944/15469    Discriminator_loss: 0.12451411038637161  Generator_loss: 2.1515045166015625\n",
            "epoch: 9/10,    batch: 11945/15469    Discriminator_loss: 0.12481100112199783  Generator_loss: 2.1494221687316895\n",
            "epoch: 9/10,    batch: 11946/15469    Discriminator_loss: 0.1251276582479477  Generator_loss: 2.147202968597412\n",
            "epoch: 9/10,    batch: 11947/15469    Discriminator_loss: 0.12544623017311096  Generator_loss: 2.144970417022705\n",
            "epoch: 9/10,    batch: 11948/15469    Discriminator_loss: 0.1258103996515274  Generator_loss: 2.1424150466918945\n",
            "epoch: 9/10,    batch: 11949/15469    Discriminator_loss: 0.12618032097816467  Generator_loss: 2.139620065689087\n",
            "epoch: 9/10,    batch: 11950/15469    Discriminator_loss: 0.12663397192955017  Generator_loss: 2.1365585327148438\n",
            "epoch: 9/10,    batch: 11951/15469    Discriminator_loss: 0.927940845489502  Generator_loss: 2.126166343688965\n",
            "epoch: 9/10,    batch: 11952/15469    Discriminator_loss: 2.0395967960357666  Generator_loss: 2.1005492210388184\n",
            "epoch: 9/10,    batch: 11953/15469    Discriminator_loss: 2.0259079933166504  Generator_loss: 2.061781167984009\n",
            "epoch: 9/10,    batch: 11954/15469    Discriminator_loss: 2.0059471130371094  Generator_loss: 2.012300968170166\n",
            "epoch: 9/10,    batch: 11955/15469    Discriminator_loss: 1.9867905378341675  Generator_loss: 1.9539844989776611\n",
            "epoch: 9/10,    batch: 11956/15469    Discriminator_loss: 1.9538960456848145  Generator_loss: 1.8884283304214478\n",
            "epoch: 9/10,    batch: 11957/15469    Discriminator_loss: 1.9161765575408936  Generator_loss: 1.817838191986084\n",
            "epoch: 9/10,    batch: 11958/15469    Discriminator_loss: 1.8854228258132935  Generator_loss: 1.744030475616455\n",
            "epoch: 9/10,    batch: 11959/15469    Discriminator_loss: 1.8559906482696533  Generator_loss: 1.669318675994873\n",
            "epoch: 9/10,    batch: 11960/15469    Discriminator_loss: 1.8374205827713013  Generator_loss: 1.5956976413726807\n",
            "epoch: 9/10,    batch: 11961/15469    Discriminator_loss: 0.9324973821640015  Generator_loss: 1.5317269563674927\n",
            "epoch: 9/10,    batch: 11962/15469    Discriminator_loss: 0.2545984089374542  Generator_loss: 1.4824177026748657\n",
            "epoch: 9/10,    batch: 11963/15469    Discriminator_loss: 0.26700806617736816  Generator_loss: 1.445874810218811\n",
            "epoch: 9/10,    batch: 11964/15469    Discriminator_loss: 0.2764953672885895  Generator_loss: 1.4196562767028809\n",
            "epoch: 9/10,    batch: 11965/15469    Discriminator_loss: 0.28312793374061584  Generator_loss: 1.401970386505127\n",
            "epoch: 9/10,    batch: 11966/15469    Discriminator_loss: 0.2877565324306488  Generator_loss: 1.3908319473266602\n",
            "epoch: 9/10,    batch: 11967/15469    Discriminator_loss: 0.2903466820716858  Generator_loss: 1.3852747678756714\n",
            "epoch: 9/10,    batch: 11968/15469    Discriminator_loss: 0.29134759306907654  Generator_loss: 1.3838688135147095\n",
            "epoch: 9/10,    batch: 11969/15469    Discriminator_loss: 0.29128679633140564  Generator_loss: 1.385911464691162\n",
            "epoch: 9/10,    batch: 11970/15469    Discriminator_loss: 0.2901310622692108  Generator_loss: 1.3907806873321533\n",
            "epoch: 9/10,    batch: 11971/15469    Discriminator_loss: 0.2880392372608185  Generator_loss: 1.397552490234375\n",
            "epoch: 9/10,    batch: 11972/15469    Discriminator_loss: 0.28533387184143066  Generator_loss: 1.4068453311920166\n",
            "epoch: 9/10,    batch: 11973/15469    Discriminator_loss: 0.28196340799331665  Generator_loss: 1.4177271127700806\n",
            "epoch: 9/10,    batch: 11974/15469    Discriminator_loss: 0.2779782712459564  Generator_loss: 1.430711269378662\n",
            "epoch: 9/10,    batch: 11975/15469    Discriminator_loss: 0.2735339105129242  Generator_loss: 1.4453027248382568\n",
            "epoch: 9/10,    batch: 11976/15469    Discriminator_loss: 0.2687166631221771  Generator_loss: 1.4614098072052002\n",
            "epoch: 9/10,    batch: 11977/15469    Discriminator_loss: 0.2634773552417755  Generator_loss: 1.4791550636291504\n",
            "epoch: 9/10,    batch: 11978/15469    Discriminator_loss: 0.2578968405723572  Generator_loss: 1.4983853101730347\n",
            "epoch: 9/10,    batch: 11979/15469    Discriminator_loss: 0.2520572543144226  Generator_loss: 1.5191295146942139\n",
            "epoch: 9/10,    batch: 11980/15469    Discriminator_loss: 0.24597856402397156  Generator_loss: 1.541337490081787\n",
            "epoch: 9/10,    batch: 11981/15469    Discriminator_loss: 0.23966753482818604  Generator_loss: 1.5648789405822754\n",
            "epoch: 9/10,    batch: 11982/15469    Discriminator_loss: 0.23317885398864746  Generator_loss: 1.5900790691375732\n",
            "epoch: 9/10,    batch: 11983/15469    Discriminator_loss: 0.22650516033172607  Generator_loss: 1.6167305707931519\n",
            "epoch: 9/10,    batch: 11984/15469    Discriminator_loss: 0.2196769416332245  Generator_loss: 1.6450114250183105\n",
            "epoch: 9/10,    batch: 11985/15469    Discriminator_loss: 0.2127225697040558  Generator_loss: 1.6748219728469849\n",
            "epoch: 9/10,    batch: 11986/15469    Discriminator_loss: 0.20565244555473328  Generator_loss: 1.7061703205108643\n",
            "epoch: 9/10,    batch: 11987/15469    Discriminator_loss: 0.1985795944929123  Generator_loss: 1.7391397953033447\n",
            "epoch: 9/10,    batch: 11988/15469    Discriminator_loss: 0.1914437711238861  Generator_loss: 1.7732220888137817\n",
            "epoch: 9/10,    batch: 11989/15469    Discriminator_loss: 0.18441665172576904  Generator_loss: 1.8084261417388916\n",
            "epoch: 9/10,    batch: 11990/15469    Discriminator_loss: 0.17752155661582947  Generator_loss: 1.843956470489502\n",
            "epoch: 9/10,    batch: 11991/15469    Discriminator_loss: 0.17086002230644226  Generator_loss: 1.8797649145126343\n",
            "epoch: 9/10,    batch: 11992/15469    Discriminator_loss: 0.16449761390686035  Generator_loss: 1.91517972946167\n",
            "epoch: 9/10,    batch: 11993/15469    Discriminator_loss: 0.15854232013225555  Generator_loss: 1.9491263628005981\n",
            "epoch: 9/10,    batch: 11994/15469    Discriminator_loss: 0.15304964780807495  Generator_loss: 1.981574535369873\n",
            "epoch: 9/10,    batch: 11995/15469    Discriminator_loss: 0.1480436474084854  Generator_loss: 2.0118231773376465\n",
            "epoch: 9/10,    batch: 11996/15469    Discriminator_loss: 0.14354373514652252  Generator_loss: 2.0395700931549072\n",
            "epoch: 9/10,    batch: 11997/15469    Discriminator_loss: 0.13955888152122498  Generator_loss: 2.064716339111328\n",
            "epoch: 9/10,    batch: 11998/15469    Discriminator_loss: 0.13603392243385315  Generator_loss: 2.0870416164398193\n",
            "epoch: 9/10,    batch: 11999/15469    Discriminator_loss: 0.13296936452388763  Generator_loss: 2.10689115524292\n",
            "epoch: 9/10,    batch: 12000/15469    Discriminator_loss: 0.1302458792924881  Generator_loss: 2.1246800422668457\n",
            "epoch: 9/10,    batch: 12001/15469    Discriminator_loss: 0.12785227596759796  Generator_loss: 2.1405115127563477\n",
            "epoch: 9/10,    batch: 12002/15469    Discriminator_loss: 0.12570081651210785  Generator_loss: 2.154940128326416\n",
            "epoch: 9/10,    batch: 12003/15469    Discriminator_loss: 0.12376271933317184  Generator_loss: 2.1681647300720215\n",
            "epoch: 9/10,    batch: 12004/15469    Discriminator_loss: 0.1219867393374443  Generator_loss: 2.180513858795166\n",
            "epoch: 9/10,    batch: 12005/15469    Discriminator_loss: 0.12033158540725708  Generator_loss: 2.1922051906585693\n",
            "epoch: 9/10,    batch: 12006/15469    Discriminator_loss: 0.11881119757890701  Generator_loss: 2.2031478881835938\n",
            "epoch: 9/10,    batch: 12007/15469    Discriminator_loss: 0.11738989502191544  Generator_loss: 2.2136106491088867\n",
            "epoch: 9/10,    batch: 12008/15469    Discriminator_loss: 0.11604983359575272  Generator_loss: 2.2236833572387695\n",
            "epoch: 9/10,    batch: 12009/15469    Discriminator_loss: 0.11481404304504395  Generator_loss: 2.233099937438965\n",
            "epoch: 9/10,    batch: 12010/15469    Discriminator_loss: 0.11366111785173416  Generator_loss: 2.2420506477355957\n",
            "epoch: 9/10,    batch: 12011/15469    Discriminator_loss: 0.11258228123188019  Generator_loss: 2.250575304031372\n",
            "epoch: 9/10,    batch: 12012/15469    Discriminator_loss: 0.11157671362161636  Generator_loss: 2.2586536407470703\n",
            "epoch: 9/10,    batch: 12013/15469    Discriminator_loss: 0.1106506884098053  Generator_loss: 2.266195058822632\n",
            "epoch: 9/10,    batch: 12014/15469    Discriminator_loss: 0.1097768098115921  Generator_loss: 2.2734081745147705\n",
            "epoch: 9/10,    batch: 12015/15469    Discriminator_loss: 0.10895633697509766  Generator_loss: 2.280165672302246\n",
            "epoch: 9/10,    batch: 12016/15469    Discriminator_loss: 0.10818187147378922  Generator_loss: 2.2867705821990967\n",
            "epoch: 9/10,    batch: 12017/15469    Discriminator_loss: 0.10745596140623093  Generator_loss: 2.293057918548584\n",
            "epoch: 9/10,    batch: 12018/15469    Discriminator_loss: 0.10673360526561737  Generator_loss: 2.2991929054260254\n",
            "epoch: 9/10,    batch: 12019/15469    Discriminator_loss: 0.10604849457740784  Generator_loss: 2.305140495300293\n",
            "epoch: 9/10,    batch: 12020/15469    Discriminator_loss: 0.10538316518068314  Generator_loss: 2.311180830001831\n",
            "epoch: 9/10,    batch: 12021/15469    Discriminator_loss: 0.10469886660575867  Generator_loss: 2.3171327114105225\n",
            "epoch: 9/10,    batch: 12022/15469    Discriminator_loss: 0.1040337085723877  Generator_loss: 2.3232192993164062\n",
            "epoch: 9/10,    batch: 12023/15469    Discriminator_loss: 0.10335013270378113  Generator_loss: 2.329379081726074\n",
            "epoch: 9/10,    batch: 12024/15469    Discriminator_loss: 0.10266277939081192  Generator_loss: 2.335634469985962\n",
            "epoch: 9/10,    batch: 12025/15469    Discriminator_loss: 0.10196894407272339  Generator_loss: 2.3421225547790527\n",
            "epoch: 9/10,    batch: 12026/15469    Discriminator_loss: 0.10125218331813812  Generator_loss: 2.348641872406006\n",
            "epoch: 9/10,    batch: 12027/15469    Discriminator_loss: 0.10052995383739471  Generator_loss: 2.3553550243377686\n",
            "epoch: 9/10,    batch: 12028/15469    Discriminator_loss: 0.09979483485221863  Generator_loss: 2.3623251914978027\n",
            "epoch: 9/10,    batch: 12029/15469    Discriminator_loss: 0.09904995560646057  Generator_loss: 2.3693995475769043\n",
            "epoch: 9/10,    batch: 12030/15469    Discriminator_loss: 0.09830812364816666  Generator_loss: 2.3765878677368164\n",
            "epoch: 9/10,    batch: 12031/15469    Discriminator_loss: 0.09754577279090881  Generator_loss: 2.3839640617370605\n",
            "epoch: 9/10,    batch: 12032/15469    Discriminator_loss: 0.09676621854305267  Generator_loss: 2.3914644718170166\n",
            "epoch: 9/10,    batch: 12033/15469    Discriminator_loss: 0.09598414599895477  Generator_loss: 2.399172306060791\n",
            "epoch: 9/10,    batch: 12034/15469    Discriminator_loss: 0.09519293904304504  Generator_loss: 2.4070518016815186\n",
            "epoch: 9/10,    batch: 12035/15469    Discriminator_loss: 0.09438657015562057  Generator_loss: 2.4151105880737305\n",
            "epoch: 9/10,    batch: 12036/15469    Discriminator_loss: 0.0935806930065155  Generator_loss: 2.423281669616699\n",
            "epoch: 9/10,    batch: 12037/15469    Discriminator_loss: 0.09276333451271057  Generator_loss: 2.4315452575683594\n",
            "epoch: 9/10,    batch: 12038/15469    Discriminator_loss: 0.09194793552160263  Generator_loss: 2.4399561882019043\n",
            "epoch: 9/10,    batch: 12039/15469    Discriminator_loss: 0.0911305695772171  Generator_loss: 2.4484434127807617\n",
            "epoch: 9/10,    batch: 12040/15469    Discriminator_loss: 0.09031835198402405  Generator_loss: 2.4569334983825684\n",
            "epoch: 9/10,    batch: 12041/15469    Discriminator_loss: 0.08951158076524734  Generator_loss: 2.465470790863037\n",
            "epoch: 9/10,    batch: 12042/15469    Discriminator_loss: 0.08871351927518845  Generator_loss: 2.47399640083313\n",
            "epoch: 9/10,    batch: 12043/15469    Discriminator_loss: 0.08792264759540558  Generator_loss: 2.482532024383545\n",
            "epoch: 9/10,    batch: 12044/15469    Discriminator_loss: 0.08714177459478378  Generator_loss: 2.491025447845459\n",
            "epoch: 9/10,    batch: 12045/15469    Discriminator_loss: 0.08637063205242157  Generator_loss: 2.499505043029785\n",
            "epoch: 9/10,    batch: 12046/15469    Discriminator_loss: 0.0856078639626503  Generator_loss: 2.5079784393310547\n",
            "epoch: 9/10,    batch: 12047/15469    Discriminator_loss: 0.08485757559537888  Generator_loss: 2.516434907913208\n",
            "epoch: 9/10,    batch: 12048/15469    Discriminator_loss: 0.0841127410531044  Generator_loss: 2.5248427391052246\n",
            "epoch: 9/10,    batch: 12049/15469    Discriminator_loss: 0.08337493985891342  Generator_loss: 2.5332841873168945\n",
            "epoch: 9/10,    batch: 12050/15469    Discriminator_loss: 0.08264388889074326  Generator_loss: 2.541701078414917\n",
            "epoch: 9/10,    batch: 12051/15469    Discriminator_loss: 0.08191624283790588  Generator_loss: 2.5501749515533447\n",
            "epoch: 9/10,    batch: 12052/15469    Discriminator_loss: 0.08119746297597885  Generator_loss: 2.558642625808716\n",
            "epoch: 9/10,    batch: 12053/15469    Discriminator_loss: 0.08047926425933838  Generator_loss: 2.567143678665161\n",
            "epoch: 9/10,    batch: 12054/15469    Discriminator_loss: 0.07976949959993362  Generator_loss: 2.5756783485412598\n",
            "epoch: 9/10,    batch: 12055/15469    Discriminator_loss: 0.07906363159418106  Generator_loss: 2.584254503250122\n",
            "epoch: 9/10,    batch: 12056/15469    Discriminator_loss: 0.07835976034402847  Generator_loss: 2.592824697494507\n",
            "epoch: 9/10,    batch: 12057/15469    Discriminator_loss: 0.07766147702932358  Generator_loss: 2.6014442443847656\n",
            "epoch: 9/10,    batch: 12058/15469    Discriminator_loss: 0.07696626335382462  Generator_loss: 2.610034704208374\n",
            "epoch: 9/10,    batch: 12059/15469    Discriminator_loss: 0.07627719640731812  Generator_loss: 2.618729829788208\n",
            "epoch: 9/10,    batch: 12060/15469    Discriminator_loss: 0.0755956619977951  Generator_loss: 2.627384901046753\n",
            "epoch: 9/10,    batch: 12061/15469    Discriminator_loss: 0.07491602003574371  Generator_loss: 2.636077404022217\n",
            "epoch: 9/10,    batch: 12062/15469    Discriminator_loss: 0.07424375414848328  Generator_loss: 2.6447694301605225\n",
            "epoch: 9/10,    batch: 12063/15469    Discriminator_loss: 0.07357712090015411  Generator_loss: 2.6535093784332275\n",
            "epoch: 9/10,    batch: 12064/15469    Discriminator_loss: 0.07291307300329208  Generator_loss: 2.662252902984619\n",
            "epoch: 9/10,    batch: 12065/15469    Discriminator_loss: 0.07226642966270447  Generator_loss: 2.6709561347961426\n",
            "epoch: 9/10,    batch: 12066/15469    Discriminator_loss: 0.07161995768547058  Generator_loss: 2.679680347442627\n",
            "epoch: 9/10,    batch: 12067/15469    Discriminator_loss: 0.0709790363907814  Generator_loss: 2.6884219646453857\n",
            "epoch: 9/10,    batch: 12068/15469    Discriminator_loss: 0.07033707201480865  Generator_loss: 2.697166919708252\n",
            "epoch: 9/10,    batch: 12069/15469    Discriminator_loss: 0.06970793008804321  Generator_loss: 2.7059035301208496\n",
            "epoch: 9/10,    batch: 12070/15469    Discriminator_loss: 0.06911046802997589  Generator_loss: 2.7146315574645996\n",
            "epoch: 9/10,    batch: 12071/15469    Discriminator_loss: 0.06848111748695374  Generator_loss: 2.723391056060791\n",
            "epoch: 9/10,    batch: 12072/15469    Discriminator_loss: 0.06791999936103821  Generator_loss: 2.732149362564087\n",
            "epoch: 9/10,    batch: 12073/15469    Discriminator_loss: 1.704359531402588  Generator_loss: 2.7197117805480957\n",
            "epoch: 9/10,    batch: 12074/15469    Discriminator_loss: 3.1676340103149414  Generator_loss: 2.6784372329711914\n",
            "epoch: 9/10,    batch: 12075/15469    Discriminator_loss: 3.1560921669006348  Generator_loss: 2.6182570457458496\n",
            "epoch: 9/10,    batch: 12076/15469    Discriminator_loss: 3.0428004264831543  Generator_loss: 2.5491809844970703\n",
            "epoch: 9/10,    batch: 12077/15469    Discriminator_loss: 2.949909210205078  Generator_loss: 2.476980686187744\n",
            "epoch: 9/10,    batch: 12078/15469    Discriminator_loss: 2.845827341079712  Generator_loss: 2.4052343368530273\n",
            "epoch: 9/10,    batch: 12079/15469    Discriminator_loss: 2.744319200515747  Generator_loss: 2.3354740142822266\n",
            "epoch: 9/10,    batch: 12080/15469    Discriminator_loss: 2.664656400680542  Generator_loss: 2.268472671508789\n",
            "epoch: 9/10,    batch: 12081/15469    Discriminator_loss: 2.5763344764709473  Generator_loss: 2.204493284225464\n",
            "epoch: 9/10,    batch: 12082/15469    Discriminator_loss: 2.487922191619873  Generator_loss: 2.143794298171997\n",
            "epoch: 9/10,    batch: 12083/15469    Discriminator_loss: 2.4100797176361084  Generator_loss: 2.0859436988830566\n",
            "epoch: 9/10,    batch: 12084/15469    Discriminator_loss: 2.336024522781372  Generator_loss: 2.0308125019073486\n",
            "epoch: 9/10,    batch: 12085/15469    Discriminator_loss: 2.270557403564453  Generator_loss: 1.9781533479690552\n",
            "epoch: 9/10,    batch: 12086/15469    Discriminator_loss: 2.215388536453247  Generator_loss: 1.927621603012085\n",
            "epoch: 9/10,    batch: 12087/15469    Discriminator_loss: 2.163179874420166  Generator_loss: 1.8790063858032227\n",
            "epoch: 9/10,    batch: 12088/15469    Discriminator_loss: 2.1115384101867676  Generator_loss: 1.8318560123443604\n",
            "epoch: 9/10,    batch: 12089/15469    Discriminator_loss: 2.0584089756011963  Generator_loss: 1.7862293720245361\n",
            "epoch: 9/10,    batch: 12090/15469    Discriminator_loss: 2.0091214179992676  Generator_loss: 1.7419192790985107\n",
            "epoch: 9/10,    batch: 12091/15469    Discriminator_loss: 1.9682731628417969  Generator_loss: 1.698926568031311\n",
            "epoch: 9/10,    batch: 12092/15469    Discriminator_loss: 1.9308146238327026  Generator_loss: 1.6570836305618286\n",
            "epoch: 9/10,    batch: 12093/15469    Discriminator_loss: 2.033003330230713  Generator_loss: 1.6147750616073608\n",
            "epoch: 9/10,    batch: 12094/15469    Discriminator_loss: 2.19230318069458  Generator_loss: 1.5706381797790527\n",
            "epoch: 9/10,    batch: 12095/15469    Discriminator_loss: 2.093202590942383  Generator_loss: 1.525595784187317\n",
            "epoch: 9/10,    batch: 12096/15469    Discriminator_loss: 2.1410608291625977  Generator_loss: 1.4799878597259521\n",
            "epoch: 9/10,    batch: 12097/15469    Discriminator_loss: 1.9214972257614136  Generator_loss: 1.4347922801971436\n",
            "epoch: 9/10,    batch: 12098/15469    Discriminator_loss: 1.7262176275253296  Generator_loss: 1.3907644748687744\n",
            "epoch: 9/10,    batch: 12099/15469    Discriminator_loss: 1.6315698623657227  Generator_loss: 1.3479185104370117\n",
            "epoch: 9/10,    batch: 12100/15469    Discriminator_loss: 1.5089623928070068  Generator_loss: 1.3060219287872314\n",
            "epoch: 9/10,    batch: 12101/15469    Discriminator_loss: 1.392857551574707  Generator_loss: 1.2648402452468872\n",
            "epoch: 9/10,    batch: 12102/15469    Discriminator_loss: 1.2371349334716797  Generator_loss: 1.2233467102050781\n",
            "epoch: 9/10,    batch: 12103/15469    Discriminator_loss: 0.8403724431991577  Generator_loss: 1.1788811683654785\n",
            "epoch: 9/10,    batch: 12104/15469    Discriminator_loss: 0.41877496242523193  Generator_loss: 1.1417399644851685\n",
            "epoch: 9/10,    batch: 12105/15469    Discriminator_loss: 0.39540180563926697  Generator_loss: 1.113415241241455\n",
            "epoch: 9/10,    batch: 12106/15469    Discriminator_loss: 0.40656375885009766  Generator_loss: 1.092822790145874\n",
            "epoch: 9/10,    batch: 12107/15469    Discriminator_loss: 0.4152282774448395  Generator_loss: 1.0789252519607544\n",
            "epoch: 9/10,    batch: 12108/15469    Discriminator_loss: 0.4208216369152069  Generator_loss: 1.0709507465362549\n",
            "epoch: 9/10,    batch: 12109/15469    Discriminator_loss: 0.4234771430492401  Generator_loss: 1.0683887004852295\n",
            "epoch: 9/10,    batch: 12110/15469    Discriminator_loss: 0.42368757724761963  Generator_loss: 1.0699594020843506\n",
            "epoch: 9/10,    batch: 12111/15469    Discriminator_loss: 0.4217952787876129  Generator_loss: 1.075486183166504\n",
            "epoch: 9/10,    batch: 12112/15469    Discriminator_loss: 0.41806769371032715  Generator_loss: 1.0842809677124023\n",
            "epoch: 9/10,    batch: 12113/15469    Discriminator_loss: 0.4129777252674103  Generator_loss: 1.0953302383422852\n",
            "epoch: 9/10,    batch: 12114/15469    Discriminator_loss: 0.4069099426269531  Generator_loss: 1.1085755825042725\n",
            "epoch: 9/10,    batch: 12115/15469    Discriminator_loss: 0.40009889006614685  Generator_loss: 1.123046636581421\n",
            "epoch: 9/10,    batch: 12116/15469    Discriminator_loss: 0.39278140664100647  Generator_loss: 1.1386977434158325\n",
            "epoch: 9/10,    batch: 12117/15469    Discriminator_loss: 0.3855143189430237  Generator_loss: 1.1546344757080078\n",
            "epoch: 9/10,    batch: 12118/15469    Discriminator_loss: 0.3780902326107025  Generator_loss: 1.1710138320922852\n",
            "epoch: 9/10,    batch: 12119/15469    Discriminator_loss: 0.3708297908306122  Generator_loss: 1.18723464012146\n",
            "epoch: 9/10,    batch: 12120/15469    Discriminator_loss: 0.36382177472114563  Generator_loss: 1.2033843994140625\n",
            "epoch: 9/10,    batch: 12121/15469    Discriminator_loss: 0.3569716513156891  Generator_loss: 1.218903660774231\n",
            "epoch: 9/10,    batch: 12122/15469    Discriminator_loss: 0.3505822718143463  Generator_loss: 1.2341957092285156\n",
            "epoch: 9/10,    batch: 12123/15469    Discriminator_loss: 0.3442048132419586  Generator_loss: 1.2491495609283447\n",
            "epoch: 9/10,    batch: 12124/15469    Discriminator_loss: 0.33825162053108215  Generator_loss: 1.263678789138794\n",
            "epoch: 9/10,    batch: 12125/15469    Discriminator_loss: 0.33248838782310486  Generator_loss: 1.2779896259307861\n",
            "epoch: 9/10,    batch: 12126/15469    Discriminator_loss: 0.32677674293518066  Generator_loss: 1.2920331954956055\n",
            "epoch: 9/10,    batch: 12127/15469    Discriminator_loss: 0.321397989988327  Generator_loss: 1.3058362007141113\n",
            "epoch: 9/10,    batch: 12128/15469    Discriminator_loss: 0.3162091374397278  Generator_loss: 1.3197275400161743\n",
            "epoch: 9/10,    batch: 12129/15469    Discriminator_loss: 0.31104904413223267  Generator_loss: 1.3327710628509521\n",
            "epoch: 9/10,    batch: 12130/15469    Discriminator_loss: 0.3063686490058899  Generator_loss: 1.3459137678146362\n",
            "epoch: 9/10,    batch: 12131/15469    Discriminator_loss: 0.3018277883529663  Generator_loss: 1.3582934141159058\n",
            "epoch: 9/10,    batch: 12132/15469    Discriminator_loss: 0.2975778579711914  Generator_loss: 1.3703999519348145\n",
            "epoch: 9/10,    batch: 12133/15469    Discriminator_loss: 0.293628454208374  Generator_loss: 1.3817018270492554\n",
            "epoch: 9/10,    batch: 12134/15469    Discriminator_loss: 0.2899685800075531  Generator_loss: 1.3922840356826782\n",
            "epoch: 9/10,    batch: 12135/15469    Discriminator_loss: 0.2866041660308838  Generator_loss: 1.4022367000579834\n",
            "epoch: 9/10,    batch: 12136/15469    Discriminator_loss: 0.28352147340774536  Generator_loss: 1.411695122718811\n",
            "epoch: 9/10,    batch: 12137/15469    Discriminator_loss: 0.28059354424476624  Generator_loss: 1.420498251914978\n",
            "epoch: 9/10,    batch: 12138/15469    Discriminator_loss: 0.27781546115875244  Generator_loss: 1.4289240837097168\n",
            "epoch: 9/10,    batch: 12139/15469    Discriminator_loss: 0.2752586603164673  Generator_loss: 1.4370006322860718\n",
            "epoch: 9/10,    batch: 12140/15469    Discriminator_loss: 0.2727828025817871  Generator_loss: 1.4449611902236938\n",
            "epoch: 9/10,    batch: 12141/15469    Discriminator_loss: 0.27036938071250916  Generator_loss: 1.452810287475586\n",
            "epoch: 9/10,    batch: 12142/15469    Discriminator_loss: 0.2680107355117798  Generator_loss: 1.460357427597046\n",
            "epoch: 9/10,    batch: 12143/15469    Discriminator_loss: 0.2657216489315033  Generator_loss: 1.467978835105896\n",
            "epoch: 9/10,    batch: 12144/15469    Discriminator_loss: 0.26355671882629395  Generator_loss: 1.4754176139831543\n",
            "epoch: 9/10,    batch: 12145/15469    Discriminator_loss: 0.26143020391464233  Generator_loss: 1.4824802875518799\n",
            "epoch: 9/10,    batch: 12146/15469    Discriminator_loss: 0.25953370332717896  Generator_loss: 1.488950252532959\n",
            "epoch: 9/10,    batch: 12147/15469    Discriminator_loss: 0.2579389214515686  Generator_loss: 1.4947372674942017\n",
            "epoch: 9/10,    batch: 12148/15469    Discriminator_loss: 0.2567236125469208  Generator_loss: 1.499380350112915\n",
            "epoch: 9/10,    batch: 12149/15469    Discriminator_loss: 0.2559472620487213  Generator_loss: 1.5024365186691284\n",
            "epoch: 9/10,    batch: 12150/15469    Discriminator_loss: 0.2558913230895996  Generator_loss: 1.5029127597808838\n",
            "epoch: 9/10,    batch: 12151/15469    Discriminator_loss: 0.25743234157562256  Generator_loss: 1.499160647392273\n",
            "epoch: 9/10,    batch: 12152/15469    Discriminator_loss: 0.26188594102859497  Generator_loss: 1.4875953197479248\n",
            "epoch: 9/10,    batch: 12153/15469    Discriminator_loss: 0.2761954367160797  Generator_loss: 1.4540072679519653\n",
            "epoch: 9/10,    batch: 12154/15469    Discriminator_loss: 0.33563995361328125  Generator_loss: 1.359749674797058\n",
            "epoch: 9/10,    batch: 12155/15469    Discriminator_loss: 0.4916597604751587  Generator_loss: 1.2287161350250244\n",
            "epoch: 9/10,    batch: 12156/15469    Discriminator_loss: 0.5780303478240967  Generator_loss: 1.2557544708251953\n",
            "epoch: 9/10,    batch: 12157/15469    Discriminator_loss: 0.407179594039917  Generator_loss: 1.5415575504302979\n",
            "epoch: 9/10,    batch: 12158/15469    Discriminator_loss: 0.2251744568347931  Generator_loss: 1.9308886528015137\n",
            "epoch: 9/10,    batch: 12159/15469    Discriminator_loss: 0.14257541298866272  Generator_loss: 2.201308250427246\n",
            "epoch: 9/10,    batch: 12160/15469    Discriminator_loss: 0.11658352613449097  Generator_loss: 2.295517921447754\n",
            "epoch: 9/10,    batch: 12161/15469    Discriminator_loss: 0.11268848180770874  Generator_loss: 2.2846016883850098\n",
            "epoch: 9/10,    batch: 12162/15469    Discriminator_loss: 0.11587031185626984  Generator_loss: 2.2425763607025146\n",
            "epoch: 9/10,    batch: 12163/15469    Discriminator_loss: 0.11981825530529022  Generator_loss: 2.2043018341064453\n",
            "epoch: 9/10,    batch: 12164/15469    Discriminator_loss: 0.12213198840618134  Generator_loss: 2.1838128566741943\n",
            "epoch: 9/10,    batch: 12165/15469    Discriminator_loss: 0.12263251841068268  Generator_loss: 2.1774821281433105\n",
            "epoch: 9/10,    batch: 12166/15469    Discriminator_loss: 0.12199901044368744  Generator_loss: 2.1803786754608154\n",
            "epoch: 9/10,    batch: 12167/15469    Discriminator_loss: 0.12087063491344452  Generator_loss: 2.1863560676574707\n",
            "epoch: 9/10,    batch: 12168/15469    Discriminator_loss: 0.11990834027528763  Generator_loss: 2.192756175994873\n",
            "epoch: 9/10,    batch: 12169/15469    Discriminator_loss: 0.11905026435852051  Generator_loss: 2.198695182800293\n",
            "epoch: 9/10,    batch: 12170/15469    Discriminator_loss: 0.1185268834233284  Generator_loss: 2.2021102905273438\n",
            "epoch: 9/10,    batch: 12171/15469    Discriminator_loss: 0.11816366761922836  Generator_loss: 2.2040257453918457\n",
            "epoch: 9/10,    batch: 12172/15469    Discriminator_loss: 0.11792553961277008  Generator_loss: 2.205705404281616\n",
            "epoch: 9/10,    batch: 12173/15469    Discriminator_loss: 0.117738276720047  Generator_loss: 2.2064900398254395\n",
            "epoch: 9/10,    batch: 12174/15469    Discriminator_loss: 0.11756417155265808  Generator_loss: 2.2079124450683594\n",
            "epoch: 9/10,    batch: 12175/15469    Discriminator_loss: 0.11727390438318253  Generator_loss: 2.209801197052002\n",
            "epoch: 9/10,    batch: 12176/15469    Discriminator_loss: 0.11692963540554047  Generator_loss: 2.2119245529174805\n",
            "epoch: 9/10,    batch: 12177/15469    Discriminator_loss: 0.11654005944728851  Generator_loss: 2.214987277984619\n",
            "epoch: 9/10,    batch: 12178/15469    Discriminator_loss: 0.11611078679561615  Generator_loss: 2.2183868885040283\n",
            "epoch: 9/10,    batch: 12179/15469    Discriminator_loss: 0.11563640087842941  Generator_loss: 2.2220213413238525\n",
            "epoch: 9/10,    batch: 12180/15469    Discriminator_loss: 0.11515536904335022  Generator_loss: 2.2257518768310547\n",
            "epoch: 9/10,    batch: 12181/15469    Discriminator_loss: 0.1146678626537323  Generator_loss: 2.2296717166900635\n",
            "epoch: 9/10,    batch: 12182/15469    Discriminator_loss: 0.11419026553630829  Generator_loss: 2.2334446907043457\n",
            "epoch: 9/10,    batch: 12183/15469    Discriminator_loss: 0.1137193813920021  Generator_loss: 2.237236976623535\n",
            "epoch: 9/10,    batch: 12184/15469    Discriminator_loss: 0.11324462294578552  Generator_loss: 2.241027593612671\n",
            "epoch: 9/10,    batch: 12185/15469    Discriminator_loss: 0.11278262734413147  Generator_loss: 2.244736671447754\n",
            "epoch: 9/10,    batch: 12186/15469    Discriminator_loss: 0.11231853067874908  Generator_loss: 2.248464345932007\n",
            "epoch: 9/10,    batch: 12187/15469    Discriminator_loss: 0.11187097430229187  Generator_loss: 2.2523880004882812\n",
            "epoch: 9/10,    batch: 12188/15469    Discriminator_loss: 0.11139801144599915  Generator_loss: 2.256194591522217\n",
            "epoch: 9/10,    batch: 12189/15469    Discriminator_loss: 0.11091917753219604  Generator_loss: 2.2601749897003174\n",
            "epoch: 9/10,    batch: 12190/15469    Discriminator_loss: 0.1104496493935585  Generator_loss: 2.2641963958740234\n",
            "epoch: 9/10,    batch: 12191/15469    Discriminator_loss: 0.10994267463684082  Generator_loss: 2.268378734588623\n",
            "epoch: 9/10,    batch: 12192/15469    Discriminator_loss: 0.10943438112735748  Generator_loss: 2.2726190090179443\n",
            "epoch: 9/10,    batch: 12193/15469    Discriminator_loss: 0.10890534520149231  Generator_loss: 2.277184009552002\n",
            "epoch: 9/10,    batch: 12194/15469    Discriminator_loss: 0.10836347937583923  Generator_loss: 2.2817442417144775\n",
            "epoch: 9/10,    batch: 12195/15469    Discriminator_loss: 0.10779669135808945  Generator_loss: 2.286707878112793\n",
            "epoch: 9/10,    batch: 12196/15469    Discriminator_loss: 0.10720929503440857  Generator_loss: 2.291750907897949\n",
            "epoch: 9/10,    batch: 12197/15469    Discriminator_loss: 0.10659424215555191  Generator_loss: 2.2971580028533936\n",
            "epoch: 9/10,    batch: 12198/15469    Discriminator_loss: 0.10598128288984299  Generator_loss: 2.3026342391967773\n",
            "epoch: 9/10,    batch: 12199/15469    Discriminator_loss: 0.10535416007041931  Generator_loss: 2.308278799057007\n",
            "epoch: 9/10,    batch: 12200/15469    Discriminator_loss: 0.1047200858592987  Generator_loss: 2.3140668869018555\n",
            "epoch: 9/10,    batch: 12201/15469    Discriminator_loss: 0.10406837612390518  Generator_loss: 2.3200440406799316\n",
            "epoch: 9/10,    batch: 12202/15469    Discriminator_loss: 0.10341227799654007  Generator_loss: 2.326051712036133\n",
            "epoch: 9/10,    batch: 12203/15469    Discriminator_loss: 0.10274283587932587  Generator_loss: 2.3320930004119873\n",
            "epoch: 9/10,    batch: 12204/15469    Discriminator_loss: 0.1020880863070488  Generator_loss: 2.3382248878479004\n",
            "epoch: 9/10,    batch: 12205/15469    Discriminator_loss: 0.1014205813407898  Generator_loss: 2.3442845344543457\n",
            "epoch: 9/10,    batch: 12206/15469    Discriminator_loss: 0.1007690504193306  Generator_loss: 2.350403308868408\n",
            "epoch: 9/10,    batch: 12207/15469    Discriminator_loss: 0.10011515021324158  Generator_loss: 2.356623888015747\n",
            "epoch: 9/10,    batch: 12208/15469    Discriminator_loss: 0.09947388619184494  Generator_loss: 2.3627524375915527\n",
            "epoch: 9/10,    batch: 12209/15469    Discriminator_loss: 0.09883373230695724  Generator_loss: 2.368990898132324\n",
            "epoch: 9/10,    batch: 12210/15469    Discriminator_loss: 0.09817194193601608  Generator_loss: 2.3752682209014893\n",
            "epoch: 9/10,    batch: 12211/15469    Discriminator_loss: 0.0975167453289032  Generator_loss: 2.381542921066284\n",
            "epoch: 9/10,    batch: 12212/15469    Discriminator_loss: 0.09687426686286926  Generator_loss: 2.387885093688965\n",
            "epoch: 9/10,    batch: 12213/15469    Discriminator_loss: 0.09620675444602966  Generator_loss: 2.394357442855835\n",
            "epoch: 9/10,    batch: 12214/15469    Discriminator_loss: 0.09555976092815399  Generator_loss: 2.4009108543395996\n",
            "epoch: 9/10,    batch: 12215/15469    Discriminator_loss: 0.09490612894296646  Generator_loss: 2.4074244499206543\n",
            "epoch: 9/10,    batch: 12216/15469    Discriminator_loss: 0.09426278620958328  Generator_loss: 2.414011001586914\n",
            "epoch: 9/10,    batch: 12217/15469    Discriminator_loss: 0.09366726875305176  Generator_loss: 2.4205546379089355\n",
            "epoch: 9/10,    batch: 12218/15469    Discriminator_loss: 0.09338673204183578  Generator_loss: 2.4271578788757324\n",
            "epoch: 9/10,    batch: 12219/15469    Discriminator_loss: 0.0928715169429779  Generator_loss: 2.433732509613037\n",
            "epoch: 9/10,    batch: 12220/15469    Discriminator_loss: 0.09229951351881027  Generator_loss: 2.440321207046509\n",
            "epoch: 9/10,    batch: 12221/15469    Discriminator_loss: 0.0916997492313385  Generator_loss: 2.4470343589782715\n",
            "epoch: 9/10,    batch: 12222/15469    Discriminator_loss: 0.09098506718873978  Generator_loss: 2.4535465240478516\n",
            "epoch: 9/10,    batch: 12223/15469    Discriminator_loss: 0.09059444069862366  Generator_loss: 2.460153818130493\n",
            "epoch: 9/10,    batch: 12224/15469    Discriminator_loss: 0.09055151045322418  Generator_loss: 2.4667396545410156\n",
            "epoch: 9/10,    batch: 12225/15469    Discriminator_loss: 0.09030000865459442  Generator_loss: 2.4733309745788574\n",
            "epoch: 9/10,    batch: 12226/15469    Discriminator_loss: 0.08948659896850586  Generator_loss: 2.4798920154571533\n",
            "epoch: 9/10,    batch: 12227/15469    Discriminator_loss: 0.08970978856086731  Generator_loss: 2.48642635345459\n",
            "epoch: 9/10,    batch: 12228/15469    Discriminator_loss: 0.08854416757822037  Generator_loss: 2.4929890632629395\n",
            "epoch: 9/10,    batch: 12229/15469    Discriminator_loss: 0.08803530782461166  Generator_loss: 2.499462604522705\n",
            "epoch: 9/10,    batch: 12230/15469    Discriminator_loss: 0.08712057024240494  Generator_loss: 2.505906581878662\n",
            "epoch: 9/10,    batch: 12231/15469    Discriminator_loss: 0.08684082329273224  Generator_loss: 2.512421131134033\n",
            "epoch: 9/10,    batch: 12232/15469    Discriminator_loss: 0.08654218912124634  Generator_loss: 2.5187623500823975\n",
            "epoch: 9/10,    batch: 12233/15469    Discriminator_loss: 0.08584355562925339  Generator_loss: 2.5251505374908447\n",
            "epoch: 9/10,    batch: 12234/15469    Discriminator_loss: 0.08729396760463715  Generator_loss: 2.5314736366271973\n",
            "epoch: 9/10,    batch: 12235/15469    Discriminator_loss: 0.08540673553943634  Generator_loss: 2.537821054458618\n",
            "epoch: 9/10,    batch: 12236/15469    Discriminator_loss: 0.08527986705303192  Generator_loss: 2.544020414352417\n",
            "epoch: 9/10,    batch: 12237/15469    Discriminator_loss: 0.08461998403072357  Generator_loss: 2.5502429008483887\n",
            "epoch: 9/10,    batch: 12238/15469    Discriminator_loss: 0.0868501365184784  Generator_loss: 2.556384325027466\n",
            "epoch: 9/10,    batch: 12239/15469    Discriminator_loss: 0.08198967576026917  Generator_loss: 2.5625038146972656\n",
            "epoch: 9/10,    batch: 12240/15469    Discriminator_loss: 2.013052225112915  Generator_loss: 2.5595526695251465\n",
            "epoch: 9/10,    batch: 12241/15469    Discriminator_loss: 2.7069997787475586  Generator_loss: 2.5463342666625977\n",
            "epoch: 9/10,    batch: 12242/15469    Discriminator_loss: 2.6832456588745117  Generator_loss: 2.525480031967163\n",
            "epoch: 9/10,    batch: 12243/15469    Discriminator_loss: 2.661435127258301  Generator_loss: 2.498652935028076\n",
            "epoch: 9/10,    batch: 12244/15469    Discriminator_loss: 2.621533155441284  Generator_loss: 2.467663526535034\n",
            "epoch: 9/10,    batch: 12245/15469    Discriminator_loss: 2.5867295265197754  Generator_loss: 2.4338607788085938\n",
            "epoch: 9/10,    batch: 12246/15469    Discriminator_loss: 2.5610969066619873  Generator_loss: 2.3980298042297363\n",
            "epoch: 9/10,    batch: 12247/15469    Discriminator_loss: 2.5148730278015137  Generator_loss: 2.361032485961914\n",
            "epoch: 9/10,    batch: 12248/15469    Discriminator_loss: 2.4749767780303955  Generator_loss: 2.3236122131347656\n",
            "epoch: 9/10,    batch: 12249/15469    Discriminator_loss: 2.4285941123962402  Generator_loss: 2.2861108779907227\n",
            "epoch: 9/10,    batch: 12250/15469    Discriminator_loss: 2.3904356956481934  Generator_loss: 2.2487645149230957\n",
            "epoch: 9/10,    batch: 12251/15469    Discriminator_loss: 2.3474602699279785  Generator_loss: 2.2118606567382812\n",
            "epoch: 9/10,    batch: 12252/15469    Discriminator_loss: 2.3093433380126953  Generator_loss: 2.175642490386963\n",
            "epoch: 9/10,    batch: 12253/15469    Discriminator_loss: 2.2806928157806396  Generator_loss: 2.1401150226593018\n",
            "epoch: 9/10,    batch: 12254/15469    Discriminator_loss: 2.237563371658325  Generator_loss: 2.1053268909454346\n",
            "epoch: 9/10,    batch: 12255/15469    Discriminator_loss: 2.2092275619506836  Generator_loss: 2.0714058876037598\n",
            "epoch: 9/10,    batch: 12256/15469    Discriminator_loss: 2.1706016063690186  Generator_loss: 2.038349151611328\n",
            "epoch: 9/10,    batch: 12257/15469    Discriminator_loss: 2.1419918537139893  Generator_loss: 2.0062313079833984\n",
            "epoch: 9/10,    batch: 12258/15469    Discriminator_loss: 1.9309911727905273  Generator_loss: 1.9752377271652222\n",
            "epoch: 9/10,    batch: 12259/15469    Discriminator_loss: 0.1537422239780426  Generator_loss: 1.9515810012817383\n",
            "epoch: 9/10,    batch: 12260/15469    Discriminator_loss: 0.15674977004528046  Generator_loss: 1.9336045980453491\n",
            "epoch: 9/10,    batch: 12261/15469    Discriminator_loss: 0.15784168243408203  Generator_loss: 1.92037034034729\n",
            "epoch: 9/10,    batch: 12262/15469    Discriminator_loss: 0.159676656126976  Generator_loss: 1.9108078479766846\n",
            "epoch: 9/10,    batch: 12263/15469    Discriminator_loss: 0.16098874807357788  Generator_loss: 1.9044021368026733\n",
            "epoch: 9/10,    batch: 12264/15469    Discriminator_loss: 0.16187094151973724  Generator_loss: 1.9004647731781006\n",
            "epoch: 9/10,    batch: 12265/15469    Discriminator_loss: 0.16238032281398773  Generator_loss: 1.8985817432403564\n",
            "epoch: 9/10,    batch: 12266/15469    Discriminator_loss: 0.22250992059707642  Generator_loss: 1.896331548690796\n",
            "epoch: 9/10,    batch: 12267/15469    Discriminator_loss: 0.16622240841388702  Generator_loss: 1.8957610130310059\n",
            "epoch: 9/10,    batch: 12268/15469    Discriminator_loss: 0.9648740291595459  Generator_loss: 1.8934036493301392\n",
            "epoch: 9/10,    batch: 12269/15469    Discriminator_loss: 2.012561559677124  Generator_loss: 1.8861005306243896\n",
            "epoch: 9/10,    batch: 12270/15469    Discriminator_loss: 2.001801013946533  Generator_loss: 1.8747074604034424\n",
            "epoch: 9/10,    batch: 12271/15469    Discriminator_loss: 1.99946928024292  Generator_loss: 1.8602063655853271\n",
            "epoch: 9/10,    batch: 12272/15469    Discriminator_loss: 1.9784296751022339  Generator_loss: 1.8432457447052002\n",
            "epoch: 9/10,    batch: 12273/15469    Discriminator_loss: 1.968419075012207  Generator_loss: 1.8243212699890137\n",
            "epoch: 9/10,    batch: 12274/15469    Discriminator_loss: 1.9567766189575195  Generator_loss: 1.8040179014205933\n",
            "epoch: 9/10,    batch: 12275/15469    Discriminator_loss: 1.949617624282837  Generator_loss: 1.7827434539794922\n",
            "epoch: 9/10,    batch: 12276/15469    Discriminator_loss: 1.9503283500671387  Generator_loss: 1.7608706951141357\n",
            "epoch: 9/10,    batch: 12277/15469    Discriminator_loss: 1.9349732398986816  Generator_loss: 1.738485336303711\n",
            "epoch: 9/10,    batch: 12278/15469    Discriminator_loss: 1.9206390380859375  Generator_loss: 1.7159167528152466\n",
            "epoch: 9/10,    batch: 12279/15469    Discriminator_loss: 1.8963112831115723  Generator_loss: 1.693282127380371\n",
            "epoch: 9/10,    batch: 12280/15469    Discriminator_loss: 1.881899118423462  Generator_loss: 1.6707022190093994\n",
            "epoch: 9/10,    batch: 12281/15469    Discriminator_loss: 0.463478147983551  Generator_loss: 1.6529591083526611\n",
            "epoch: 9/10,    batch: 12282/15469    Discriminator_loss: 0.2145448923110962  Generator_loss: 1.640018105506897\n",
            "epoch: 9/10,    batch: 12283/15469    Discriminator_loss: 0.2171400636434555  Generator_loss: 1.6310259103775024\n",
            "epoch: 9/10,    batch: 12284/15469    Discriminator_loss: 0.21898047626018524  Generator_loss: 1.6252493858337402\n",
            "epoch: 9/10,    batch: 12285/15469    Discriminator_loss: 0.21998728811740875  Generator_loss: 1.622040867805481\n",
            "epoch: 9/10,    batch: 12286/15469    Discriminator_loss: 0.22051621973514557  Generator_loss: 1.6209298372268677\n",
            "epoch: 9/10,    batch: 12287/15469    Discriminator_loss: 0.22057224810123444  Generator_loss: 1.62148118019104\n",
            "epoch: 9/10,    batch: 12288/15469    Discriminator_loss: 0.22023317217826843  Generator_loss: 1.6234362125396729\n",
            "epoch: 9/10,    batch: 12289/15469    Discriminator_loss: 0.219609335064888  Generator_loss: 1.626502275466919\n",
            "epoch: 9/10,    batch: 12290/15469    Discriminator_loss: 0.34562116861343384  Generator_loss: 1.6299468278884888\n",
            "epoch: 9/10,    batch: 12291/15469    Discriminator_loss: 1.5856189727783203  Generator_loss: 1.6293519735336304\n",
            "epoch: 9/10,    batch: 12292/15469    Discriminator_loss: 1.8455371856689453  Generator_loss: 1.624441385269165\n",
            "epoch: 9/10,    batch: 12293/15469    Discriminator_loss: 1.8486239910125732  Generator_loss: 1.6160023212432861\n",
            "epoch: 9/10,    batch: 12294/15469    Discriminator_loss: 1.8404130935668945  Generator_loss: 1.6048038005828857\n",
            "epoch: 9/10,    batch: 12295/15469    Discriminator_loss: 1.828725814819336  Generator_loss: 1.591504454612732\n",
            "epoch: 9/10,    batch: 12296/15469    Discriminator_loss: 1.8230206966400146  Generator_loss: 1.5765693187713623\n",
            "epoch: 9/10,    batch: 12297/15469    Discriminator_loss: 1.8178327083587646  Generator_loss: 1.5604312419891357\n",
            "epoch: 9/10,    batch: 12298/15469    Discriminator_loss: 1.8160988092422485  Generator_loss: 1.5434200763702393\n",
            "epoch: 9/10,    batch: 12299/15469    Discriminator_loss: 1.821927785873413  Generator_loss: 1.525902271270752\n",
            "epoch: 9/10,    batch: 12300/15469    Discriminator_loss: 1.815842866897583  Generator_loss: 1.507979154586792\n",
            "epoch: 9/10,    batch: 12301/15469    Discriminator_loss: 1.8069512844085693  Generator_loss: 1.489858627319336\n",
            "epoch: 9/10,    batch: 12302/15469    Discriminator_loss: 1.7854691743850708  Generator_loss: 1.4716176986694336\n",
            "epoch: 9/10,    batch: 12303/15469    Discriminator_loss: 1.7678115367889404  Generator_loss: 1.453449010848999\n",
            "epoch: 9/10,    batch: 12304/15469    Discriminator_loss: 1.7476974725723267  Generator_loss: 1.435464859008789\n",
            "epoch: 9/10,    batch: 12305/15469    Discriminator_loss: 1.7287678718566895  Generator_loss: 1.4176052808761597\n",
            "epoch: 9/10,    batch: 12306/15469    Discriminator_loss: 1.7094826698303223  Generator_loss: 1.4000377655029297\n",
            "epoch: 9/10,    batch: 12307/15469    Discriminator_loss: 1.6962910890579224  Generator_loss: 1.3827903270721436\n",
            "epoch: 9/10,    batch: 12308/15469    Discriminator_loss: 1.6824955940246582  Generator_loss: 1.3658044338226318\n",
            "epoch: 9/10,    batch: 12309/15469    Discriminator_loss: 1.6689423322677612  Generator_loss: 1.3491530418395996\n",
            "epoch: 9/10,    batch: 12310/15469    Discriminator_loss: 1.6526274681091309  Generator_loss: 1.3328948020935059\n",
            "epoch: 9/10,    batch: 12311/15469    Discriminator_loss: 1.6406210660934448  Generator_loss: 1.3168413639068604\n",
            "epoch: 9/10,    batch: 12312/15469    Discriminator_loss: 1.6296558380126953  Generator_loss: 1.3011236190795898\n",
            "epoch: 9/10,    batch: 12313/15469    Discriminator_loss: 1.6150381565093994  Generator_loss: 1.2856149673461914\n",
            "epoch: 9/10,    batch: 12314/15469    Discriminator_loss: 1.6050158739089966  Generator_loss: 1.2704002857208252\n",
            "epoch: 9/10,    batch: 12315/15469    Discriminator_loss: 1.5907974243164062  Generator_loss: 1.2555007934570312\n",
            "epoch: 9/10,    batch: 12316/15469    Discriminator_loss: 1.5819118022918701  Generator_loss: 1.2407152652740479\n",
            "epoch: 9/10,    batch: 12317/15469    Discriminator_loss: 1.5759388208389282  Generator_loss: 1.2260971069335938\n",
            "epoch: 9/10,    batch: 12318/15469    Discriminator_loss: 1.5607881546020508  Generator_loss: 1.211615800857544\n",
            "epoch: 9/10,    batch: 12319/15469    Discriminator_loss: 1.5492315292358398  Generator_loss: 1.1973905563354492\n",
            "epoch: 9/10,    batch: 12320/15469    Discriminator_loss: 1.5384800434112549  Generator_loss: 1.1832574605941772\n",
            "epoch: 9/10,    batch: 12321/15469    Discriminator_loss: 1.5316259860992432  Generator_loss: 1.169248342514038\n",
            "epoch: 9/10,    batch: 12322/15469    Discriminator_loss: 1.5167171955108643  Generator_loss: 1.1552836894989014\n",
            "epoch: 9/10,    batch: 12323/15469    Discriminator_loss: 1.5092880725860596  Generator_loss: 1.1414872407913208\n",
            "epoch: 9/10,    batch: 12324/15469    Discriminator_loss: 1.5024340152740479  Generator_loss: 1.1277451515197754\n",
            "epoch: 9/10,    batch: 12325/15469    Discriminator_loss: 1.49336576461792  Generator_loss: 1.1140216588974\n",
            "epoch: 9/10,    batch: 12326/15469    Discriminator_loss: 1.4817101955413818  Generator_loss: 1.1003949642181396\n",
            "epoch: 9/10,    batch: 12327/15469    Discriminator_loss: 1.4720326662063599  Generator_loss: 1.0868031978607178\n",
            "epoch: 9/10,    batch: 12328/15469    Discriminator_loss: 1.465696096420288  Generator_loss: 1.0731643438339233\n",
            "epoch: 9/10,    batch: 12329/15469    Discriminator_loss: 1.455897331237793  Generator_loss: 1.059471845626831\n",
            "epoch: 9/10,    batch: 12330/15469    Discriminator_loss: 1.4508213996887207  Generator_loss: 1.0457075834274292\n",
            "epoch: 9/10,    batch: 12331/15469    Discriminator_loss: 1.4434224367141724  Generator_loss: 1.0318493843078613\n",
            "epoch: 9/10,    batch: 12332/15469    Discriminator_loss: 1.4307634830474854  Generator_loss: 1.0181691646575928\n",
            "epoch: 9/10,    batch: 12333/15469    Discriminator_loss: 1.4250890016555786  Generator_loss: 1.004060983657837\n",
            "epoch: 9/10,    batch: 12334/15469    Discriminator_loss: 1.4198225736618042  Generator_loss: 0.9899290800094604\n",
            "epoch: 9/10,    batch: 12335/15469    Discriminator_loss: 1.4095721244812012  Generator_loss: 0.9756890535354614\n",
            "epoch: 9/10,    batch: 12336/15469    Discriminator_loss: 1.4018124341964722  Generator_loss: 0.9614568948745728\n",
            "epoch: 9/10,    batch: 12337/15469    Discriminator_loss: 1.3933448791503906  Generator_loss: 0.9469569325447083\n",
            "epoch: 9/10,    batch: 12338/15469    Discriminator_loss: 1.3856186866760254  Generator_loss: 0.9322689771652222\n",
            "epoch: 9/10,    batch: 12339/15469    Discriminator_loss: 1.3799769878387451  Generator_loss: 0.9173060059547424\n",
            "epoch: 9/10,    batch: 12340/15469    Discriminator_loss: 1.378096580505371  Generator_loss: 0.9024177193641663\n",
            "epoch: 9/10,    batch: 12341/15469    Discriminator_loss: 1.3778226375579834  Generator_loss: 0.8879085183143616\n",
            "epoch: 9/10,    batch: 12342/15469    Discriminator_loss: 1.3672583103179932  Generator_loss: 0.8731637001037598\n",
            "epoch: 9/10,    batch: 12343/15469    Discriminator_loss: 1.353874683380127  Generator_loss: 0.8581358790397644\n",
            "epoch: 9/10,    batch: 12344/15469    Discriminator_loss: 1.3543550968170166  Generator_loss: 0.8432673811912537\n",
            "epoch: 9/10,    batch: 12345/15469    Discriminator_loss: 1.341387152671814  Generator_loss: 0.8282277584075928\n",
            "epoch: 9/10,    batch: 12346/15469    Discriminator_loss: 1.3325271606445312  Generator_loss: 0.8131271600723267\n",
            "epoch: 9/10,    batch: 12347/15469    Discriminator_loss: 1.3275059461593628  Generator_loss: 0.7984454035758972\n",
            "epoch: 9/10,    batch: 12348/15469    Discriminator_loss: 1.323460578918457  Generator_loss: 0.7832481861114502\n",
            "epoch: 9/10,    batch: 12349/15469    Discriminator_loss: 1.3121159076690674  Generator_loss: 0.7684528827667236\n",
            "epoch: 9/10,    batch: 12350/15469    Discriminator_loss: 1.3141355514526367  Generator_loss: 0.7541221380233765\n",
            "epoch: 9/10,    batch: 12351/15469    Discriminator_loss: 1.3027925491333008  Generator_loss: 0.739967942237854\n",
            "epoch: 9/10,    batch: 12352/15469    Discriminator_loss: 1.2956509590148926  Generator_loss: 0.7263509035110474\n",
            "epoch: 9/10,    batch: 12353/15469    Discriminator_loss: 1.2772295475006104  Generator_loss: 0.7128092050552368\n",
            "epoch: 9/10,    batch: 12354/15469    Discriminator_loss: 1.2843961715698242  Generator_loss: 0.7002352476119995\n",
            "epoch: 9/10,    batch: 12355/15469    Discriminator_loss: 1.2966312170028687  Generator_loss: 0.6880501508712769\n",
            "epoch: 9/10,    batch: 12356/15469    Discriminator_loss: 1.2500391006469727  Generator_loss: 0.6765096187591553\n",
            "epoch: 9/10,    batch: 12357/15469    Discriminator_loss: 1.235985279083252  Generator_loss: 0.6656725406646729\n",
            "epoch: 9/10,    batch: 12358/15469    Discriminator_loss: 1.18998122215271  Generator_loss: 0.6538578271865845\n",
            "epoch: 9/10,    batch: 12359/15469    Discriminator_loss: 1.1476401090621948  Generator_loss: 0.6414404511451721\n",
            "epoch: 9/10,    batch: 12360/15469    Discriminator_loss: 0.9532917141914368  Generator_loss: 0.627272367477417\n",
            "epoch: 9/10,    batch: 12361/15469    Discriminator_loss: 0.9008886814117432  Generator_loss: 0.6146427989006042\n",
            "epoch: 9/10,    batch: 12362/15469    Discriminator_loss: 0.8010802865028381  Generator_loss: 0.6104814410209656\n",
            "epoch: 9/10,    batch: 12363/15469    Discriminator_loss: 0.8000476956367493  Generator_loss: 0.6154625415802002\n",
            "epoch: 9/10,    batch: 12364/15469    Discriminator_loss: 0.7919175624847412  Generator_loss: 0.6281636953353882\n",
            "epoch: 9/10,    batch: 12365/15469    Discriminator_loss: 0.7755836248397827  Generator_loss: 0.6481558084487915\n",
            "epoch: 9/10,    batch: 12366/15469    Discriminator_loss: 0.7524011135101318  Generator_loss: 0.6760005950927734\n",
            "epoch: 9/10,    batch: 12367/15469    Discriminator_loss: 0.7215500473976135  Generator_loss: 0.712709903717041\n",
            "epoch: 9/10,    batch: 12368/15469    Discriminator_loss: 0.6823039054870605  Generator_loss: 0.759925127029419\n",
            "epoch: 9/10,    batch: 12369/15469    Discriminator_loss: 0.6339247226715088  Generator_loss: 0.8201016187667847\n",
            "epoch: 9/10,    batch: 12370/15469    Discriminator_loss: 0.5778088569641113  Generator_loss: 0.8925836086273193\n",
            "epoch: 9/10,    batch: 12371/15469    Discriminator_loss: 0.5190746784210205  Generator_loss: 0.9746533632278442\n",
            "epoch: 9/10,    batch: 12372/15469    Discriminator_loss: 0.464628130197525  Generator_loss: 1.0583105087280273\n",
            "epoch: 9/10,    batch: 12373/15469    Discriminator_loss: 0.41923579573631287  Generator_loss: 1.1353487968444824\n",
            "epoch: 9/10,    batch: 12374/15469    Discriminator_loss: 0.38556593656539917  Generator_loss: 1.1978797912597656\n",
            "epoch: 9/10,    batch: 12375/15469    Discriminator_loss: 0.3620992600917816  Generator_loss: 1.24391508102417\n",
            "epoch: 9/10,    batch: 12376/15469    Discriminator_loss: 0.34653612971305847  Generator_loss: 1.274285078048706\n",
            "epoch: 9/10,    batch: 12377/15469    Discriminator_loss: 0.33658817410469055  Generator_loss: 1.292370080947876\n",
            "epoch: 9/10,    batch: 12378/15469    Discriminator_loss: 0.32997632026672363  Generator_loss: 1.3035197257995605\n",
            "epoch: 9/10,    batch: 12379/15469    Discriminator_loss: 0.32422369718551636  Generator_loss: 1.313582181930542\n",
            "epoch: 9/10,    batch: 12380/15469    Discriminator_loss: 0.31828224658966064  Generator_loss: 1.3247630596160889\n",
            "epoch: 9/10,    batch: 12381/15469    Discriminator_loss: 0.31207674741744995  Generator_loss: 1.3383463621139526\n",
            "epoch: 9/10,    batch: 12382/15469    Discriminator_loss: 0.305902361869812  Generator_loss: 1.3532941341400146\n",
            "epoch: 9/10,    batch: 12383/15469    Discriminator_loss: 0.2996070981025696  Generator_loss: 1.3691247701644897\n",
            "epoch: 9/10,    batch: 12384/15469    Discriminator_loss: 0.2937343120574951  Generator_loss: 1.384925127029419\n",
            "epoch: 9/10,    batch: 12385/15469    Discriminator_loss: 0.28810253739356995  Generator_loss: 1.4004539251327515\n",
            "epoch: 9/10,    batch: 12386/15469    Discriminator_loss: 0.28274303674697876  Generator_loss: 1.4155868291854858\n",
            "epoch: 9/10,    batch: 12387/15469    Discriminator_loss: 0.2777795195579529  Generator_loss: 1.430289387702942\n",
            "epoch: 9/10,    batch: 12388/15469    Discriminator_loss: 0.27319422364234924  Generator_loss: 1.4442026615142822\n",
            "epoch: 9/10,    batch: 12389/15469    Discriminator_loss: 0.2689078748226166  Generator_loss: 1.4575932025909424\n",
            "epoch: 9/10,    batch: 12390/15469    Discriminator_loss: 0.2647794783115387  Generator_loss: 1.4704221487045288\n",
            "epoch: 9/10,    batch: 12391/15469    Discriminator_loss: 0.26101168990135193  Generator_loss: 1.4825743436813354\n",
            "epoch: 9/10,    batch: 12392/15469    Discriminator_loss: 0.25747549533843994  Generator_loss: 1.4940142631530762\n",
            "epoch: 9/10,    batch: 12393/15469    Discriminator_loss: 0.2541632354259491  Generator_loss: 1.5050995349884033\n",
            "epoch: 9/10,    batch: 12394/15469    Discriminator_loss: 0.25106170773506165  Generator_loss: 1.5154883861541748\n",
            "epoch: 9/10,    batch: 12395/15469    Discriminator_loss: 0.24824370443820953  Generator_loss: 1.5252635478973389\n",
            "epoch: 9/10,    batch: 12396/15469    Discriminator_loss: 0.24551993608474731  Generator_loss: 1.5346730947494507\n",
            "epoch: 9/10,    batch: 12397/15469    Discriminator_loss: 0.2430422604084015  Generator_loss: 1.5435272455215454\n",
            "epoch: 9/10,    batch: 12398/15469    Discriminator_loss: 0.24066400527954102  Generator_loss: 1.5518031120300293\n",
            "epoch: 9/10,    batch: 12399/15469    Discriminator_loss: 0.23846951127052307  Generator_loss: 1.5597484111785889\n",
            "epoch: 9/10,    batch: 12400/15469    Discriminator_loss: 0.23642051219940186  Generator_loss: 1.567169189453125\n",
            "epoch: 9/10,    batch: 12401/15469    Discriminator_loss: 0.23445895314216614  Generator_loss: 1.5743408203125\n",
            "epoch: 9/10,    batch: 12402/15469    Discriminator_loss: 0.23262915015220642  Generator_loss: 1.5812846422195435\n",
            "epoch: 9/10,    batch: 12403/15469    Discriminator_loss: 0.23088419437408447  Generator_loss: 1.5877065658569336\n",
            "epoch: 9/10,    batch: 12404/15469    Discriminator_loss: 0.22921808063983917  Generator_loss: 1.593914270401001\n",
            "epoch: 9/10,    batch: 12405/15469    Discriminator_loss: 0.2277212142944336  Generator_loss: 1.5996853113174438\n",
            "epoch: 9/10,    batch: 12406/15469    Discriminator_loss: 0.22626399993896484  Generator_loss: 1.6053192615509033\n",
            "epoch: 9/10,    batch: 12407/15469    Discriminator_loss: 0.22489915788173676  Generator_loss: 1.6103031635284424\n",
            "epoch: 9/10,    batch: 12408/15469    Discriminator_loss: 0.22359523177146912  Generator_loss: 1.615356683731079\n",
            "epoch: 9/10,    batch: 12409/15469    Discriminator_loss: 0.2223590761423111  Generator_loss: 1.6202614307403564\n",
            "epoch: 9/10,    batch: 12410/15469    Discriminator_loss: 0.22117091715335846  Generator_loss: 1.6248414516448975\n",
            "epoch: 9/10,    batch: 12411/15469    Discriminator_loss: 0.22014328837394714  Generator_loss: 1.6289918422698975\n",
            "epoch: 9/10,    batch: 12412/15469    Discriminator_loss: 0.21909549832344055  Generator_loss: 1.633012056350708\n",
            "epoch: 9/10,    batch: 12413/15469    Discriminator_loss: 0.2181502729654312  Generator_loss: 1.6366956233978271\n",
            "epoch: 9/10,    batch: 12414/15469    Discriminator_loss: 0.2172279953956604  Generator_loss: 1.6403834819793701\n",
            "epoch: 9/10,    batch: 12415/15469    Discriminator_loss: 0.21632079780101776  Generator_loss: 1.6439849138259888\n",
            "epoch: 9/10,    batch: 12416/15469    Discriminator_loss: 0.2154838889837265  Generator_loss: 1.6474289894104004\n",
            "epoch: 9/10,    batch: 12417/15469    Discriminator_loss: 0.2146988809108734  Generator_loss: 1.6507503986358643\n",
            "epoch: 9/10,    batch: 12418/15469    Discriminator_loss: 0.21382904052734375  Generator_loss: 1.6540459394454956\n",
            "epoch: 9/10,    batch: 12419/15469    Discriminator_loss: 0.21305501461029053  Generator_loss: 1.6574809551239014\n",
            "epoch: 9/10,    batch: 12420/15469    Discriminator_loss: 0.21224668622016907  Generator_loss: 1.6605689525604248\n",
            "epoch: 9/10,    batch: 12421/15469    Discriminator_loss: 0.2114706039428711  Generator_loss: 1.663994312286377\n",
            "epoch: 9/10,    batch: 12422/15469    Discriminator_loss: 0.21070049703121185  Generator_loss: 1.6671119928359985\n",
            "epoch: 9/10,    batch: 12423/15469    Discriminator_loss: 0.20991533994674683  Generator_loss: 1.6704292297363281\n",
            "epoch: 9/10,    batch: 12424/15469    Discriminator_loss: 0.2090541422367096  Generator_loss: 1.6738792657852173\n",
            "epoch: 9/10,    batch: 12425/15469    Discriminator_loss: 0.208261176943779  Generator_loss: 1.677229642868042\n",
            "epoch: 9/10,    batch: 12426/15469    Discriminator_loss: 0.20745670795440674  Generator_loss: 1.680850625038147\n",
            "epoch: 9/10,    batch: 12427/15469    Discriminator_loss: 0.20662614703178406  Generator_loss: 1.6845641136169434\n",
            "epoch: 9/10,    batch: 12428/15469    Discriminator_loss: 0.20581164956092834  Generator_loss: 1.6879364252090454\n",
            "epoch: 9/10,    batch: 12429/15469    Discriminator_loss: 0.2049497812986374  Generator_loss: 1.6916544437408447\n",
            "epoch: 9/10,    batch: 12430/15469    Discriminator_loss: 0.2040615677833557  Generator_loss: 1.695399522781372\n",
            "epoch: 9/10,    batch: 12431/15469    Discriminator_loss: 0.20319019258022308  Generator_loss: 1.6992855072021484\n",
            "epoch: 9/10,    batch: 12432/15469    Discriminator_loss: 0.20230069756507874  Generator_loss: 1.7031354904174805\n",
            "epoch: 9/10,    batch: 12433/15469    Discriminator_loss: 0.2014305293560028  Generator_loss: 1.707155466079712\n",
            "epoch: 9/10,    batch: 12434/15469    Discriminator_loss: 0.20051488280296326  Generator_loss: 1.7111443281173706\n",
            "epoch: 9/10,    batch: 12435/15469    Discriminator_loss: 0.19958069920539856  Generator_loss: 1.715359091758728\n",
            "epoch: 9/10,    batch: 12436/15469    Discriminator_loss: 0.19865742325782776  Generator_loss: 1.7196071147918701\n",
            "epoch: 9/10,    batch: 12437/15469    Discriminator_loss: 0.19768384099006653  Generator_loss: 1.7237926721572876\n",
            "epoch: 9/10,    batch: 12438/15469    Discriminator_loss: 0.19673503935337067  Generator_loss: 1.7282825708389282\n",
            "epoch: 9/10,    batch: 12439/15469    Discriminator_loss: 0.1957242488861084  Generator_loss: 1.7326836585998535\n",
            "epoch: 9/10,    batch: 12440/15469    Discriminator_loss: 0.19476445019245148  Generator_loss: 1.7373242378234863\n",
            "epoch: 9/10,    batch: 12441/15469    Discriminator_loss: 0.19376631081104279  Generator_loss: 1.7418904304504395\n",
            "epoch: 9/10,    batch: 12442/15469    Discriminator_loss: 0.19273929297924042  Generator_loss: 1.7466541528701782\n",
            "epoch: 9/10,    batch: 12443/15469    Discriminator_loss: 0.1917000412940979  Generator_loss: 1.7516415119171143\n",
            "epoch: 9/10,    batch: 12444/15469    Discriminator_loss: 0.19065074622631073  Generator_loss: 1.7565186023712158\n",
            "epoch: 9/10,    batch: 12445/15469    Discriminator_loss: 0.18960298597812653  Generator_loss: 1.7614901065826416\n",
            "epoch: 9/10,    batch: 12446/15469    Discriminator_loss: 0.1885351538658142  Generator_loss: 1.7666683197021484\n",
            "epoch: 9/10,    batch: 12447/15469    Discriminator_loss: 0.18745824694633484  Generator_loss: 1.7717928886413574\n",
            "epoch: 9/10,    batch: 12448/15469    Discriminator_loss: 0.1864069700241089  Generator_loss: 1.7770073413848877\n",
            "epoch: 9/10,    batch: 12449/15469    Discriminator_loss: 0.18531978130340576  Generator_loss: 1.7822825908660889\n",
            "epoch: 9/10,    batch: 12450/15469    Discriminator_loss: 0.1842399686574936  Generator_loss: 1.7875845432281494\n",
            "epoch: 9/10,    batch: 12451/15469    Discriminator_loss: 0.18314898014068604  Generator_loss: 1.7928744554519653\n",
            "epoch: 9/10,    batch: 12452/15469    Discriminator_loss: 0.1820632517337799  Generator_loss: 1.7983251810073853\n",
            "epoch: 9/10,    batch: 12453/15469    Discriminator_loss: 0.18099236488342285  Generator_loss: 1.803713321685791\n",
            "epoch: 9/10,    batch: 12454/15469    Discriminator_loss: 0.1798967868089676  Generator_loss: 1.8091397285461426\n",
            "epoch: 9/10,    batch: 12455/15469    Discriminator_loss: 0.17882004380226135  Generator_loss: 1.8146154880523682\n",
            "epoch: 9/10,    batch: 12456/15469    Discriminator_loss: 0.17775078117847443  Generator_loss: 1.8201258182525635\n",
            "epoch: 9/10,    batch: 12457/15469    Discriminator_loss: 0.17668265104293823  Generator_loss: 1.8256291151046753\n",
            "epoch: 9/10,    batch: 12458/15469    Discriminator_loss: 0.17562709748744965  Generator_loss: 1.831127405166626\n",
            "epoch: 9/10,    batch: 12459/15469    Discriminator_loss: 0.17456048727035522  Generator_loss: 1.8366413116455078\n",
            "epoch: 9/10,    batch: 12460/15469    Discriminator_loss: 0.17350813746452332  Generator_loss: 1.8421411514282227\n",
            "epoch: 9/10,    batch: 12461/15469    Discriminator_loss: 0.17245995998382568  Generator_loss: 1.8476780652999878\n",
            "epoch: 9/10,    batch: 12462/15469    Discriminator_loss: 0.17141182720661163  Generator_loss: 1.8532620668411255\n",
            "epoch: 9/10,    batch: 12463/15469    Discriminator_loss: 0.1703777015209198  Generator_loss: 1.858819603919983\n",
            "epoch: 9/10,    batch: 12464/15469    Discriminator_loss: 0.16933295130729675  Generator_loss: 1.8644027709960938\n",
            "epoch: 9/10,    batch: 12465/15469    Discriminator_loss: 0.16830992698669434  Generator_loss: 1.8699679374694824\n",
            "epoch: 9/10,    batch: 12466/15469    Discriminator_loss: 0.16728751361370087  Generator_loss: 1.8755409717559814\n",
            "epoch: 9/10,    batch: 12467/15469    Discriminator_loss: 0.16626839339733124  Generator_loss: 1.8811545372009277\n",
            "epoch: 9/10,    batch: 12468/15469    Discriminator_loss: 0.16525211930274963  Generator_loss: 1.8867735862731934\n",
            "epoch: 9/10,    batch: 12469/15469    Discriminator_loss: 0.16424885392189026  Generator_loss: 1.8923581838607788\n",
            "epoch: 9/10,    batch: 12470/15469    Discriminator_loss: 0.16325026750564575  Generator_loss: 1.8980193138122559\n",
            "epoch: 9/10,    batch: 12471/15469    Discriminator_loss: 0.16226179897785187  Generator_loss: 1.9036246538162231\n",
            "epoch: 9/10,    batch: 12472/15469    Discriminator_loss: 0.16127130389213562  Generator_loss: 1.9092732667922974\n",
            "epoch: 9/10,    batch: 12473/15469    Discriminator_loss: 0.16028888523578644  Generator_loss: 1.9149186611175537\n",
            "epoch: 9/10,    batch: 12474/15469    Discriminator_loss: 0.1593039333820343  Generator_loss: 1.920580506324768\n",
            "epoch: 9/10,    batch: 12475/15469    Discriminator_loss: 0.1583222895860672  Generator_loss: 1.9262453317642212\n",
            "epoch: 9/10,    batch: 12476/15469    Discriminator_loss: 0.1573619246482849  Generator_loss: 1.931898593902588\n",
            "epoch: 9/10,    batch: 12477/15469    Discriminator_loss: 0.1563919484615326  Generator_loss: 1.9375419616699219\n",
            "epoch: 9/10,    batch: 12478/15469    Discriminator_loss: 0.1554398536682129  Generator_loss: 1.9432804584503174\n",
            "epoch: 9/10,    batch: 12479/15469    Discriminator_loss: 0.15448585152626038  Generator_loss: 1.9489349126815796\n",
            "epoch: 9/10,    batch: 12480/15469    Discriminator_loss: 0.15352919697761536  Generator_loss: 1.9546091556549072\n",
            "epoch: 9/10,    batch: 12481/15469    Discriminator_loss: 0.1525946408510208  Generator_loss: 1.9603010416030884\n",
            "epoch: 9/10,    batch: 12482/15469    Discriminator_loss: 0.15165971219539642  Generator_loss: 1.965988039970398\n",
            "epoch: 9/10,    batch: 12483/15469    Discriminator_loss: 0.15072660148143768  Generator_loss: 1.9717061519622803\n",
            "epoch: 9/10,    batch: 12484/15469    Discriminator_loss: 0.14980173110961914  Generator_loss: 1.9773497581481934\n",
            "epoch: 9/10,    batch: 12485/15469    Discriminator_loss: 0.1488894373178482  Generator_loss: 1.9830657243728638\n",
            "epoch: 9/10,    batch: 12486/15469    Discriminator_loss: 0.14797091484069824  Generator_loss: 1.9888105392456055\n",
            "epoch: 9/10,    batch: 12487/15469    Discriminator_loss: 0.14706221222877502  Generator_loss: 1.994472622871399\n",
            "epoch: 9/10,    batch: 12488/15469    Discriminator_loss: 0.14617136120796204  Generator_loss: 2.0002336502075195\n",
            "epoch: 9/10,    batch: 12489/15469    Discriminator_loss: 0.14528296887874603  Generator_loss: 2.00592041015625\n",
            "epoch: 9/10,    batch: 12490/15469    Discriminator_loss: 0.14439208805561066  Generator_loss: 2.0116682052612305\n",
            "epoch: 9/10,    batch: 12491/15469    Discriminator_loss: 0.14350908994674683  Generator_loss: 2.0173802375793457\n",
            "epoch: 9/10,    batch: 12492/15469    Discriminator_loss: 0.14262646436691284  Generator_loss: 2.023120880126953\n",
            "epoch: 9/10,    batch: 12493/15469    Discriminator_loss: 0.1417594999074936  Generator_loss: 2.0288898944854736\n",
            "epoch: 9/10,    batch: 12494/15469    Discriminator_loss: 0.14088183641433716  Generator_loss: 2.0346879959106445\n",
            "epoch: 9/10,    batch: 12495/15469    Discriminator_loss: 0.14001066982746124  Generator_loss: 2.040426731109619\n",
            "epoch: 9/10,    batch: 12496/15469    Discriminator_loss: 0.1391521692276001  Generator_loss: 2.046234607696533\n",
            "epoch: 9/10,    batch: 12497/15469    Discriminator_loss: 0.13829036056995392  Generator_loss: 2.0521249771118164\n",
            "epoch: 9/10,    batch: 12498/15469    Discriminator_loss: 0.1374475210905075  Generator_loss: 2.057967185974121\n",
            "epoch: 9/10,    batch: 12499/15469    Discriminator_loss: 0.13658837974071503  Generator_loss: 2.0638771057128906\n",
            "epoch: 9/10,    batch: 12500/15469    Discriminator_loss: 0.13571801781654358  Generator_loss: 2.069819211959839\n",
            "epoch: 9/10,    batch: 12501/15469    Discriminator_loss: 0.13487890362739563  Generator_loss: 2.075756072998047\n",
            "epoch: 9/10,    batch: 12502/15469    Discriminator_loss: 0.13402113318443298  Generator_loss: 2.0817651748657227\n",
            "epoch: 9/10,    batch: 12503/15469    Discriminator_loss: 0.13315999507904053  Generator_loss: 2.087790012359619\n",
            "epoch: 9/10,    batch: 12504/15469    Discriminator_loss: 0.13232435286045074  Generator_loss: 2.093846321105957\n",
            "epoch: 9/10,    batch: 12505/15469    Discriminator_loss: 0.13148456811904907  Generator_loss: 2.099951982498169\n",
            "epoch: 9/10,    batch: 12506/15469    Discriminator_loss: 0.1306624561548233  Generator_loss: 2.1061277389526367\n",
            "epoch: 9/10,    batch: 12507/15469    Discriminator_loss: 0.12983770668506622  Generator_loss: 2.1122355461120605\n",
            "epoch: 9/10,    batch: 12508/15469    Discriminator_loss: 0.12904562056064606  Generator_loss: 2.11842679977417\n",
            "epoch: 9/10,    batch: 12509/15469    Discriminator_loss: 0.1281980574131012  Generator_loss: 2.124624729156494\n",
            "epoch: 9/10,    batch: 12510/15469    Discriminator_loss: 0.12731221318244934  Generator_loss: 2.130944013595581\n",
            "epoch: 9/10,    batch: 12511/15469    Discriminator_loss: 1.2155872583389282  Generator_loss: 2.131274700164795\n",
            "epoch: 9/10,    batch: 12512/15469    Discriminator_loss: 2.408505439758301  Generator_loss: 2.120722770690918\n",
            "epoch: 9/10,    batch: 12513/15469    Discriminator_loss: 2.4056813716888428  Generator_loss: 2.1020450592041016\n",
            "epoch: 9/10,    batch: 12514/15469    Discriminator_loss: 2.380422592163086  Generator_loss: 2.077575206756592\n",
            "epoch: 9/10,    batch: 12515/15469    Discriminator_loss: 2.346616268157959  Generator_loss: 2.0490384101867676\n",
            "epoch: 9/10,    batch: 12516/15469    Discriminator_loss: 2.318310022354126  Generator_loss: 2.0179083347320557\n",
            "epoch: 9/10,    batch: 12517/15469    Discriminator_loss: 2.3006250858306885  Generator_loss: 1.9851229190826416\n",
            "epoch: 9/10,    batch: 12518/15469    Discriminator_loss: 2.265401840209961  Generator_loss: 1.9514206647872925\n",
            "epoch: 9/10,    batch: 12519/15469    Discriminator_loss: 2.2263386249542236  Generator_loss: 1.917454719543457\n",
            "epoch: 9/10,    batch: 12520/15469    Discriminator_loss: 2.192317247390747  Generator_loss: 1.8835291862487793\n",
            "epoch: 9/10,    batch: 12521/15469    Discriminator_loss: 2.1673059463500977  Generator_loss: 1.850203514099121\n",
            "epoch: 9/10,    batch: 12522/15469    Discriminator_loss: 2.12367582321167  Generator_loss: 1.8174811601638794\n",
            "epoch: 9/10,    batch: 12523/15469    Discriminator_loss: 2.0998992919921875  Generator_loss: 1.785634160041809\n",
            "epoch: 9/10,    batch: 12524/15469    Discriminator_loss: 2.0768606662750244  Generator_loss: 1.7546898126602173\n",
            "epoch: 9/10,    batch: 12525/15469    Discriminator_loss: 2.056086301803589  Generator_loss: 1.724827527999878\n",
            "epoch: 9/10,    batch: 12526/15469    Discriminator_loss: 2.0321202278137207  Generator_loss: 1.6960322856903076\n",
            "epoch: 9/10,    batch: 12527/15469    Discriminator_loss: 2.0073273181915283  Generator_loss: 1.6682348251342773\n",
            "epoch: 9/10,    batch: 12528/15469    Discriminator_loss: 1.877350926399231  Generator_loss: 1.6417986154556274\n",
            "epoch: 9/10,    batch: 12529/15469    Discriminator_loss: 0.22311747074127197  Generator_loss: 1.621821641921997\n",
            "epoch: 9/10,    batch: 12530/15469    Discriminator_loss: 0.2225194275379181  Generator_loss: 1.6070444583892822\n",
            "epoch: 9/10,    batch: 12531/15469    Discriminator_loss: 0.22564992308616638  Generator_loss: 1.5964573621749878\n",
            "epoch: 9/10,    batch: 12532/15469    Discriminator_loss: 0.22787964344024658  Generator_loss: 1.5891332626342773\n",
            "epoch: 9/10,    batch: 12533/15469    Discriminator_loss: 0.22930578887462616  Generator_loss: 1.584560751914978\n",
            "epoch: 9/10,    batch: 12534/15469    Discriminator_loss: 0.2301611602306366  Generator_loss: 1.5821940898895264\n",
            "epoch: 9/10,    batch: 12535/15469    Discriminator_loss: 0.23051688075065613  Generator_loss: 1.5816564559936523\n",
            "epoch: 9/10,    batch: 12536/15469    Discriminator_loss: 0.23042283952236176  Generator_loss: 1.5825881958007812\n",
            "epoch: 9/10,    batch: 12537/15469    Discriminator_loss: 0.23001070320606232  Generator_loss: 1.5847196578979492\n",
            "epoch: 9/10,    batch: 12538/15469    Discriminator_loss: 0.7234429121017456  Generator_loss: 1.5862607955932617\n",
            "epoch: 9/10,    batch: 12539/15469    Discriminator_loss: 1.922214150428772  Generator_loss: 1.5832936763763428\n",
            "epoch: 9/10,    batch: 12540/15469    Discriminator_loss: 1.9234102964401245  Generator_loss: 1.576683521270752\n",
            "epoch: 9/10,    batch: 12541/15469    Discriminator_loss: 1.919884443283081  Generator_loss: 1.5672385692596436\n",
            "epoch: 9/10,    batch: 12542/15469    Discriminator_loss: 1.9071871042251587  Generator_loss: 1.5555858612060547\n",
            "epoch: 9/10,    batch: 12543/15469    Discriminator_loss: 1.9021916389465332  Generator_loss: 1.542293906211853\n",
            "epoch: 9/10,    batch: 12544/15469    Discriminator_loss: 1.8906126022338867  Generator_loss: 1.5277706384658813\n",
            "epoch: 9/10,    batch: 12545/15469    Discriminator_loss: 1.897491693496704  Generator_loss: 1.5123318433761597\n",
            "epoch: 9/10,    batch: 12546/15469    Discriminator_loss: 1.9039068222045898  Generator_loss: 1.4962208271026611\n",
            "epoch: 9/10,    batch: 12547/15469    Discriminator_loss: 1.89915132522583  Generator_loss: 1.4797381162643433\n",
            "epoch: 9/10,    batch: 12548/15469    Discriminator_loss: 1.88290536403656  Generator_loss: 1.4630897045135498\n",
            "epoch: 9/10,    batch: 12549/15469    Discriminator_loss: 1.8504467010498047  Generator_loss: 1.4464521408081055\n",
            "epoch: 9/10,    batch: 12550/15469    Discriminator_loss: 1.8397107124328613  Generator_loss: 1.429903507232666\n",
            "epoch: 9/10,    batch: 12551/15469    Discriminator_loss: 0.49905675649642944  Generator_loss: 1.4172521829605103\n",
            "epoch: 9/10,    batch: 12552/15469    Discriminator_loss: 0.2794315814971924  Generator_loss: 1.4084316492080688\n",
            "epoch: 9/10,    batch: 12553/15469    Discriminator_loss: 0.28176096081733704  Generator_loss: 1.40268874168396\n",
            "epoch: 9/10,    batch: 12554/15469    Discriminator_loss: 0.28327521681785583  Generator_loss: 1.3994100093841553\n",
            "epoch: 9/10,    batch: 12555/15469    Discriminator_loss: 0.28391769528388977  Generator_loss: 1.398140549659729\n",
            "epoch: 9/10,    batch: 12556/15469    Discriminator_loss: 0.2840513288974762  Generator_loss: 1.3984813690185547\n",
            "epoch: 9/10,    batch: 12557/15469    Discriminator_loss: 0.2837047278881073  Generator_loss: 1.4001743793487549\n",
            "epoch: 9/10,    batch: 12558/15469    Discriminator_loss: 0.2829611897468567  Generator_loss: 1.4029288291931152\n",
            "epoch: 9/10,    batch: 12559/15469    Discriminator_loss: 0.28189995884895325  Generator_loss: 1.4065831899642944\n",
            "epoch: 9/10,    batch: 12560/15469    Discriminator_loss: 0.30387890338897705  Generator_loss: 1.4107987880706787\n",
            "epoch: 9/10,    batch: 12561/15469    Discriminator_loss: 1.4363398551940918  Generator_loss: 1.4121294021606445\n",
            "epoch: 9/10,    batch: 12562/15469    Discriminator_loss: 1.8346469402313232  Generator_loss: 1.4098451137542725\n",
            "epoch: 9/10,    batch: 12563/15469    Discriminator_loss: 1.8369865417480469  Generator_loss: 1.404808521270752\n",
            "epoch: 9/10,    batch: 12564/15469    Discriminator_loss: 1.829569935798645  Generator_loss: 1.3975591659545898\n",
            "epoch: 9/10,    batch: 12565/15469    Discriminator_loss: 1.830298900604248  Generator_loss: 1.388489007949829\n",
            "epoch: 9/10,    batch: 12566/15469    Discriminator_loss: 1.8119163513183594  Generator_loss: 1.3781473636627197\n",
            "epoch: 9/10,    batch: 12567/15469    Discriminator_loss: 1.8098187446594238  Generator_loss: 1.3668391704559326\n",
            "epoch: 9/10,    batch: 12568/15469    Discriminator_loss: 1.8210288286209106  Generator_loss: 1.3548513650894165\n",
            "epoch: 9/10,    batch: 12569/15469    Discriminator_loss: 1.8318350315093994  Generator_loss: 1.3422493934631348\n",
            "epoch: 9/10,    batch: 12570/15469    Discriminator_loss: 1.8237488269805908  Generator_loss: 1.3293497562408447\n",
            "epoch: 9/10,    batch: 12571/15469    Discriminator_loss: 1.7938647270202637  Generator_loss: 1.3163535594940186\n",
            "epoch: 9/10,    batch: 12572/15469    Discriminator_loss: 1.7757279872894287  Generator_loss: 1.303367018699646\n",
            "epoch: 9/10,    batch: 12573/15469    Discriminator_loss: 1.7469578981399536  Generator_loss: 1.2904512882232666\n",
            "epoch: 9/10,    batch: 12574/15469    Discriminator_loss: 1.738246202468872  Generator_loss: 1.2776349782943726\n",
            "epoch: 9/10,    batch: 12575/15469    Discriminator_loss: 1.7162129878997803  Generator_loss: 1.265053391456604\n",
            "epoch: 9/10,    batch: 12576/15469    Discriminator_loss: 1.700621247291565  Generator_loss: 1.2527203559875488\n",
            "epoch: 9/10,    batch: 12577/15469    Discriminator_loss: 1.6872397661209106  Generator_loss: 1.240649938583374\n",
            "epoch: 9/10,    batch: 12578/15469    Discriminator_loss: 1.6665050983428955  Generator_loss: 1.2287743091583252\n",
            "epoch: 9/10,    batch: 12579/15469    Discriminator_loss: 1.6578037738800049  Generator_loss: 1.2172197103500366\n",
            "epoch: 9/10,    batch: 12580/15469    Discriminator_loss: 1.6417787075042725  Generator_loss: 1.2058993577957153\n",
            "epoch: 9/10,    batch: 12581/15469    Discriminator_loss: 1.6217474937438965  Generator_loss: 1.1947760581970215\n",
            "epoch: 9/10,    batch: 12582/15469    Discriminator_loss: 1.6141180992126465  Generator_loss: 1.183814287185669\n",
            "epoch: 9/10,    batch: 12583/15469    Discriminator_loss: 1.6005522012710571  Generator_loss: 1.173133373260498\n",
            "epoch: 9/10,    batch: 12584/15469    Discriminator_loss: 1.5906180143356323  Generator_loss: 1.1626842021942139\n",
            "epoch: 9/10,    batch: 12585/15469    Discriminator_loss: 1.5643999576568604  Generator_loss: 1.1523789167404175\n",
            "epoch: 9/10,    batch: 12586/15469    Discriminator_loss: 1.5527483224868774  Generator_loss: 1.1422669887542725\n",
            "epoch: 9/10,    batch: 12587/15469    Discriminator_loss: 1.543542504310608  Generator_loss: 1.132319688796997\n",
            "epoch: 9/10,    batch: 12588/15469    Discriminator_loss: 1.528904676437378  Generator_loss: 1.1225439310073853\n",
            "epoch: 9/10,    batch: 12589/15469    Discriminator_loss: 1.5246250629425049  Generator_loss: 1.1129910945892334\n",
            "epoch: 9/10,    batch: 12590/15469    Discriminator_loss: 1.5023763179779053  Generator_loss: 1.1035959720611572\n",
            "epoch: 9/10,    batch: 12591/15469    Discriminator_loss: 1.4872751235961914  Generator_loss: 1.0943360328674316\n",
            "epoch: 9/10,    batch: 12592/15469    Discriminator_loss: 1.4725666046142578  Generator_loss: 1.0851309299468994\n",
            "epoch: 9/10,    batch: 12593/15469    Discriminator_loss: 1.4696203470230103  Generator_loss: 1.0760705471038818\n",
            "epoch: 9/10,    batch: 12594/15469    Discriminator_loss: 1.4520728588104248  Generator_loss: 1.0670576095581055\n",
            "epoch: 9/10,    batch: 12595/15469    Discriminator_loss: 1.427333950996399  Generator_loss: 1.0579829216003418\n",
            "epoch: 9/10,    batch: 12596/15469    Discriminator_loss: 1.4094829559326172  Generator_loss: 1.0489124059677124\n",
            "epoch: 9/10,    batch: 12597/15469    Discriminator_loss: 1.4084594249725342  Generator_loss: 1.0399727821350098\n",
            "epoch: 9/10,    batch: 12598/15469    Discriminator_loss: 1.3709633350372314  Generator_loss: 1.0309360027313232\n",
            "epoch: 9/10,    batch: 12599/15469    Discriminator_loss: 1.3663771152496338  Generator_loss: 1.0220164060592651\n",
            "epoch: 9/10,    batch: 12600/15469    Discriminator_loss: 1.354691743850708  Generator_loss: 1.013108491897583\n",
            "epoch: 9/10,    batch: 12601/15469    Discriminator_loss: 1.3131752014160156  Generator_loss: 1.004115343093872\n",
            "epoch: 9/10,    batch: 12602/15469    Discriminator_loss: 1.2469630241394043  Generator_loss: 0.9945570230484009\n",
            "epoch: 9/10,    batch: 12603/15469    Discriminator_loss: 1.2315998077392578  Generator_loss: 0.9844738245010376\n",
            "epoch: 9/10,    batch: 12604/15469    Discriminator_loss: 1.1274876594543457  Generator_loss: 0.9732564687728882\n",
            "epoch: 9/10,    batch: 12605/15469    Discriminator_loss: 1.0454039573669434  Generator_loss: 0.9610240459442139\n",
            "epoch: 9/10,    batch: 12606/15469    Discriminator_loss: 0.8321877121925354  Generator_loss: 0.947408139705658\n",
            "epoch: 9/10,    batch: 12607/15469    Discriminator_loss: 0.6544092893600464  Generator_loss: 0.9346346259117126\n",
            "epoch: 9/10,    batch: 12608/15469    Discriminator_loss: 0.5121740102767944  Generator_loss: 0.9251217842102051\n",
            "epoch: 9/10,    batch: 12609/15469    Discriminator_loss: 0.5093426704406738  Generator_loss: 0.9186228513717651\n",
            "epoch: 9/10,    batch: 12610/15469    Discriminator_loss: 0.5122292041778564  Generator_loss: 0.9148206114768982\n",
            "epoch: 9/10,    batch: 12611/15469    Discriminator_loss: 0.5139346718788147  Generator_loss: 0.9133541584014893\n",
            "epoch: 9/10,    batch: 12612/15469    Discriminator_loss: 0.5142884254455566  Generator_loss: 0.9139785170555115\n",
            "epoch: 9/10,    batch: 12613/15469    Discriminator_loss: 0.5133129358291626  Generator_loss: 0.9162902235984802\n",
            "epoch: 9/10,    batch: 12614/15469    Discriminator_loss: 0.5112552046775818  Generator_loss: 0.9201524257659912\n",
            "epoch: 9/10,    batch: 12615/15469    Discriminator_loss: 0.5082998871803284  Generator_loss: 0.9252632856369019\n",
            "epoch: 9/10,    batch: 12616/15469    Discriminator_loss: 0.5045518279075623  Generator_loss: 0.9314298033714294\n",
            "epoch: 9/10,    batch: 12617/15469    Discriminator_loss: 0.500241756439209  Generator_loss: 0.9385915398597717\n",
            "epoch: 9/10,    batch: 12618/15469    Discriminator_loss: 0.4954259991645813  Generator_loss: 0.9463741183280945\n",
            "epoch: 9/10,    batch: 12619/15469    Discriminator_loss: 0.49023330211639404  Generator_loss: 0.9548394680023193\n",
            "epoch: 9/10,    batch: 12620/15469    Discriminator_loss: 0.48477423191070557  Generator_loss: 0.9637693166732788\n",
            "epoch: 9/10,    batch: 12621/15469    Discriminator_loss: 0.4790770709514618  Generator_loss: 0.9731827974319458\n",
            "epoch: 9/10,    batch: 12622/15469    Discriminator_loss: 0.47321295738220215  Generator_loss: 0.9828771352767944\n",
            "epoch: 9/10,    batch: 12623/15469    Discriminator_loss: 0.46729686856269836  Generator_loss: 0.9928812384605408\n",
            "epoch: 9/10,    batch: 12624/15469    Discriminator_loss: 0.46133920550346375  Generator_loss: 1.0031020641326904\n",
            "epoch: 9/10,    batch: 12625/15469    Discriminator_loss: 0.45534971356391907  Generator_loss: 1.0135014057159424\n",
            "epoch: 9/10,    batch: 12626/15469    Discriminator_loss: 0.4493892788887024  Generator_loss: 1.0239466428756714\n",
            "epoch: 9/10,    batch: 12627/15469    Discriminator_loss: 0.44346722960472107  Generator_loss: 1.0345423221588135\n",
            "epoch: 9/10,    batch: 12628/15469    Discriminator_loss: 0.43760597705841064  Generator_loss: 1.0451663732528687\n",
            "epoch: 9/10,    batch: 12629/15469    Discriminator_loss: 0.4318285286426544  Generator_loss: 1.0557957887649536\n",
            "epoch: 9/10,    batch: 12630/15469    Discriminator_loss: 0.4261232316493988  Generator_loss: 1.0664535760879517\n",
            "epoch: 9/10,    batch: 12631/15469    Discriminator_loss: 0.42054855823516846  Generator_loss: 1.077056884765625\n",
            "epoch: 9/10,    batch: 12632/15469    Discriminator_loss: 0.415056049823761  Generator_loss: 1.087616205215454\n",
            "epoch: 9/10,    batch: 12633/15469    Discriminator_loss: 0.4097014367580414  Generator_loss: 1.0980925559997559\n",
            "epoch: 9/10,    batch: 12634/15469    Discriminator_loss: 0.4044565260410309  Generator_loss: 1.1085339784622192\n",
            "epoch: 9/10,    batch: 12635/15469    Discriminator_loss: 0.39933332800865173  Generator_loss: 1.1188595294952393\n",
            "epoch: 9/10,    batch: 12636/15469    Discriminator_loss: 0.39433154463768005  Generator_loss: 1.1290937662124634\n",
            "epoch: 9/10,    batch: 12637/15469    Discriminator_loss: 0.3894692063331604  Generator_loss: 1.1392285823822021\n",
            "epoch: 9/10,    batch: 12638/15469    Discriminator_loss: 0.3847163915634155  Generator_loss: 1.1492154598236084\n",
            "epoch: 9/10,    batch: 12639/15469    Discriminator_loss: 0.3801101744174957  Generator_loss: 1.1590604782104492\n",
            "epoch: 9/10,    batch: 12640/15469    Discriminator_loss: 0.3756130039691925  Generator_loss: 1.168770432472229\n",
            "epoch: 9/10,    batch: 12641/15469    Discriminator_loss: 0.37129709124565125  Generator_loss: 1.1783130168914795\n",
            "epoch: 9/10,    batch: 12642/15469    Discriminator_loss: 0.3670767545700073  Generator_loss: 1.187709927558899\n",
            "epoch: 9/10,    batch: 12643/15469    Discriminator_loss: 0.36300647258758545  Generator_loss: 1.1968640089035034\n",
            "epoch: 9/10,    batch: 12644/15469    Discriminator_loss: 0.3590345084667206  Generator_loss: 1.2059314250946045\n",
            "epoch: 9/10,    batch: 12645/15469    Discriminator_loss: 0.35523951053619385  Generator_loss: 1.2146581411361694\n",
            "epoch: 9/10,    batch: 12646/15469    Discriminator_loss: 0.3515743613243103  Generator_loss: 1.2232530117034912\n",
            "epoch: 9/10,    batch: 12647/15469    Discriminator_loss: 0.3480333089828491  Generator_loss: 1.231642723083496\n",
            "epoch: 9/10,    batch: 12648/15469    Discriminator_loss: 0.344634085893631  Generator_loss: 1.239825963973999\n",
            "epoch: 9/10,    batch: 12649/15469    Discriminator_loss: 0.3413461744785309  Generator_loss: 1.247808575630188\n",
            "epoch: 9/10,    batch: 12650/15469    Discriminator_loss: 0.3382062315940857  Generator_loss: 1.2554576396942139\n",
            "epoch: 9/10,    batch: 12651/15469    Discriminator_loss: 0.33516278862953186  Generator_loss: 1.2630051374435425\n",
            "epoch: 9/10,    batch: 12652/15469    Discriminator_loss: 0.33227062225341797  Generator_loss: 1.2702317237854004\n",
            "epoch: 9/10,    batch: 12653/15469    Discriminator_loss: 0.32948872447013855  Generator_loss: 1.2773363590240479\n",
            "epoch: 9/10,    batch: 12654/15469    Discriminator_loss: 0.32681962847709656  Generator_loss: 1.2841174602508545\n",
            "epoch: 9/10,    batch: 12655/15469    Discriminator_loss: 0.32423675060272217  Generator_loss: 1.2907698154449463\n",
            "epoch: 9/10,    batch: 12656/15469    Discriminator_loss: 0.32173988223075867  Generator_loss: 1.2972445487976074\n",
            "epoch: 9/10,    batch: 12657/15469    Discriminator_loss: 0.31934884190559387  Generator_loss: 1.3035778999328613\n",
            "epoch: 9/10,    batch: 12658/15469    Discriminator_loss: 0.31704795360565186  Generator_loss: 1.3097178936004639\n",
            "epoch: 9/10,    batch: 12659/15469    Discriminator_loss: 0.31477785110473633  Generator_loss: 1.3156914710998535\n",
            "epoch: 9/10,    batch: 12660/15469    Discriminator_loss: 0.31259796023368835  Generator_loss: 1.321645975112915\n",
            "epoch: 9/10,    batch: 12661/15469    Discriminator_loss: 0.3104656934738159  Generator_loss: 1.3274646997451782\n",
            "epoch: 9/10,    batch: 12662/15469    Discriminator_loss: 0.3083656132221222  Generator_loss: 1.3332372903823853\n",
            "epoch: 9/10,    batch: 12663/15469    Discriminator_loss: 0.3062954246997833  Generator_loss: 1.338989496231079\n",
            "epoch: 9/10,    batch: 12664/15469    Discriminator_loss: 0.3042720556259155  Generator_loss: 1.3446571826934814\n",
            "epoch: 9/10,    batch: 12665/15469    Discriminator_loss: 0.3022625744342804  Generator_loss: 1.3503668308258057\n",
            "epoch: 9/10,    batch: 12666/15469    Discriminator_loss: 0.3002753257751465  Generator_loss: 1.3560283184051514\n",
            "epoch: 9/10,    batch: 12667/15469    Discriminator_loss: 0.29829034209251404  Generator_loss: 1.361703872680664\n",
            "epoch: 9/10,    batch: 12668/15469    Discriminator_loss: 0.29631897807121277  Generator_loss: 1.3674702644348145\n",
            "epoch: 9/10,    batch: 12669/15469    Discriminator_loss: 0.294342577457428  Generator_loss: 1.3732290267944336\n",
            "epoch: 9/10,    batch: 12670/15469    Discriminator_loss: 0.292379766702652  Generator_loss: 1.3791170120239258\n",
            "epoch: 9/10,    batch: 12671/15469    Discriminator_loss: 0.2903956472873688  Generator_loss: 1.3850011825561523\n",
            "epoch: 9/10,    batch: 12672/15469    Discriminator_loss: 0.28840798139572144  Generator_loss: 1.3909761905670166\n",
            "epoch: 9/10,    batch: 12673/15469    Discriminator_loss: 0.2864059805870056  Generator_loss: 1.3970532417297363\n",
            "epoch: 9/10,    batch: 12674/15469    Discriminator_loss: 0.2843899130821228  Generator_loss: 1.403306007385254\n",
            "epoch: 9/10,    batch: 12675/15469    Discriminator_loss: 0.2823663353919983  Generator_loss: 1.4096252918243408\n",
            "epoch: 9/10,    batch: 12676/15469    Discriminator_loss: 0.28030329942703247  Generator_loss: 1.4161293506622314\n",
            "epoch: 9/10,    batch: 12677/15469    Discriminator_loss: 0.2781846821308136  Generator_loss: 1.4228161573410034\n",
            "epoch: 9/10,    batch: 12678/15469    Discriminator_loss: 0.2760680913925171  Generator_loss: 1.4296150207519531\n",
            "epoch: 9/10,    batch: 12679/15469    Discriminator_loss: 0.2738947570323944  Generator_loss: 1.436650037765503\n",
            "epoch: 9/10,    batch: 12680/15469    Discriminator_loss: 0.2716788947582245  Generator_loss: 1.4439663887023926\n",
            "epoch: 9/10,    batch: 12681/15469    Discriminator_loss: 0.2694110870361328  Generator_loss: 1.4513798952102661\n",
            "epoch: 9/10,    batch: 12682/15469    Discriminator_loss: 0.2671021521091461  Generator_loss: 1.4591844081878662\n",
            "epoch: 9/10,    batch: 12683/15469    Discriminator_loss: 0.26470738649368286  Generator_loss: 1.4672119617462158\n",
            "epoch: 9/10,    batch: 12684/15469    Discriminator_loss: 0.26227375864982605  Generator_loss: 1.4755971431732178\n",
            "epoch: 9/10,    batch: 12685/15469    Discriminator_loss: 0.25977805256843567  Generator_loss: 1.4842926263809204\n",
            "epoch: 9/10,    batch: 12686/15469    Discriminator_loss: 0.2571578919887543  Generator_loss: 1.4934155941009521\n",
            "epoch: 9/10,    batch: 12687/15469    Discriminator_loss: 0.2545059621334076  Generator_loss: 1.5029518604278564\n",
            "epoch: 9/10,    batch: 12688/15469    Discriminator_loss: 0.2517700791358948  Generator_loss: 1.51283597946167\n",
            "epoch: 9/10,    batch: 12689/15469    Discriminator_loss: 0.24889756739139557  Generator_loss: 1.5231871604919434\n",
            "epoch: 9/10,    batch: 12690/15469    Discriminator_loss: 0.24594426155090332  Generator_loss: 1.5339977741241455\n",
            "epoch: 9/10,    batch: 12691/15469    Discriminator_loss: 0.2428482323884964  Generator_loss: 1.545454978942871\n",
            "epoch: 9/10,    batch: 12692/15469    Discriminator_loss: 0.23969677090644836  Generator_loss: 1.5576465129852295\n",
            "epoch: 9/10,    batch: 12693/15469    Discriminator_loss: 0.23636186122894287  Generator_loss: 1.5703424215316772\n",
            "epoch: 9/10,    batch: 12694/15469    Discriminator_loss: 0.23286662995815277  Generator_loss: 1.5839260816574097\n",
            "epoch: 9/10,    batch: 12695/15469    Discriminator_loss: 0.22924143075942993  Generator_loss: 1.5982215404510498\n",
            "epoch: 9/10,    batch: 12696/15469    Discriminator_loss: 0.2255132794380188  Generator_loss: 1.613040566444397\n",
            "epoch: 9/10,    batch: 12697/15469    Discriminator_loss: 0.22166386246681213  Generator_loss: 1.6284732818603516\n",
            "epoch: 9/10,    batch: 12698/15469    Discriminator_loss: 0.2177935391664505  Generator_loss: 1.6443142890930176\n",
            "epoch: 9/10,    batch: 12699/15469    Discriminator_loss: 0.213944673538208  Generator_loss: 1.6601884365081787\n",
            "epoch: 9/10,    batch: 12700/15469    Discriminator_loss: 0.21022602915763855  Generator_loss: 1.6757149696350098\n",
            "epoch: 9/10,    batch: 12701/15469    Discriminator_loss: 0.20670056343078613  Generator_loss: 1.6905560493469238\n",
            "epoch: 9/10,    batch: 12702/15469    Discriminator_loss: 0.203380286693573  Generator_loss: 1.704728603363037\n",
            "epoch: 9/10,    batch: 12703/15469    Discriminator_loss: 0.20030565559864044  Generator_loss: 1.7178703546524048\n",
            "epoch: 9/10,    batch: 12704/15469    Discriminator_loss: 0.197471484541893  Generator_loss: 1.7302467823028564\n",
            "epoch: 9/10,    batch: 12705/15469    Discriminator_loss: 0.19479404389858246  Generator_loss: 1.741963505744934\n",
            "epoch: 9/10,    batch: 12706/15469    Discriminator_loss: 0.19226551055908203  Generator_loss: 1.7532368898391724\n",
            "epoch: 9/10,    batch: 12707/15469    Discriminator_loss: 0.18987108767032623  Generator_loss: 1.764180302619934\n",
            "epoch: 9/10,    batch: 12708/15469    Discriminator_loss: 0.18758343160152435  Generator_loss: 1.7746301889419556\n",
            "epoch: 9/10,    batch: 12709/15469    Discriminator_loss: 0.18539626896381378  Generator_loss: 1.7849282026290894\n",
            "epoch: 9/10,    batch: 12710/15469    Discriminator_loss: 0.1833007037639618  Generator_loss: 1.7949731349945068\n",
            "epoch: 9/10,    batch: 12711/15469    Discriminator_loss: 0.18128298223018646  Generator_loss: 1.8046622276306152\n",
            "epoch: 9/10,    batch: 12712/15469    Discriminator_loss: 0.17935237288475037  Generator_loss: 1.8142765760421753\n",
            "epoch: 9/10,    batch: 12713/15469    Discriminator_loss: 0.17749665677547455  Generator_loss: 1.8233802318572998\n",
            "epoch: 9/10,    batch: 12714/15469    Discriminator_loss: 0.17572695016860962  Generator_loss: 1.8325039148330688\n",
            "epoch: 9/10,    batch: 12715/15469    Discriminator_loss: 0.17403019964694977  Generator_loss: 1.8411253690719604\n",
            "epoch: 9/10,    batch: 12716/15469    Discriminator_loss: 0.1723957359790802  Generator_loss: 1.8496272563934326\n",
            "epoch: 9/10,    batch: 12717/15469    Discriminator_loss: 0.1708160638809204  Generator_loss: 1.857914924621582\n",
            "epoch: 9/10,    batch: 12718/15469    Discriminator_loss: 0.1692868024110794  Generator_loss: 1.8660469055175781\n",
            "epoch: 9/10,    batch: 12719/15469    Discriminator_loss: 0.16778822243213654  Generator_loss: 1.8740313053131104\n",
            "epoch: 9/10,    batch: 12720/15469    Discriminator_loss: 0.16634394228458405  Generator_loss: 1.8818587064743042\n",
            "epoch: 9/10,    batch: 12721/15469    Discriminator_loss: 0.16493472456932068  Generator_loss: 1.8897013664245605\n",
            "epoch: 9/10,    batch: 12722/15469    Discriminator_loss: 0.16355234384536743  Generator_loss: 1.8974201679229736\n",
            "epoch: 9/10,    batch: 12723/15469    Discriminator_loss: 0.1621749997138977  Generator_loss: 1.9052000045776367\n",
            "epoch: 9/10,    batch: 12724/15469    Discriminator_loss: 0.16081984341144562  Generator_loss: 1.9128292798995972\n",
            "epoch: 9/10,    batch: 12725/15469    Discriminator_loss: 0.15948686003684998  Generator_loss: 1.9204734563827515\n",
            "epoch: 9/10,    batch: 12726/15469    Discriminator_loss: 0.15815602242946625  Generator_loss: 1.9281904697418213\n",
            "epoch: 9/10,    batch: 12727/15469    Discriminator_loss: 0.15684111416339874  Generator_loss: 1.9358513355255127\n",
            "epoch: 9/10,    batch: 12728/15469    Discriminator_loss: 0.1555396169424057  Generator_loss: 1.9435170888900757\n",
            "epoch: 9/10,    batch: 12729/15469    Discriminator_loss: 0.1542493849992752  Generator_loss: 1.9512155055999756\n",
            "epoch: 9/10,    batch: 12730/15469    Discriminator_loss: 0.1529724895954132  Generator_loss: 1.958893060684204\n",
            "epoch: 9/10,    batch: 12731/15469    Discriminator_loss: 0.15170827507972717  Generator_loss: 1.9665493965148926\n",
            "epoch: 9/10,    batch: 12732/15469    Discriminator_loss: 0.15045952796936035  Generator_loss: 1.9741798639297485\n",
            "epoch: 9/10,    batch: 12733/15469    Discriminator_loss: 0.1492253839969635  Generator_loss: 1.981804370880127\n",
            "epoch: 9/10,    batch: 12734/15469    Discriminator_loss: 0.14800255000591278  Generator_loss: 1.9894189834594727\n",
            "epoch: 9/10,    batch: 12735/15469    Discriminator_loss: 0.14679425954818726  Generator_loss: 1.9969849586486816\n",
            "epoch: 9/10,    batch: 12736/15469    Discriminator_loss: 0.14560386538505554  Generator_loss: 2.0045735836029053\n",
            "epoch: 9/10,    batch: 12737/15469    Discriminator_loss: 0.1444297730922699  Generator_loss: 2.0120902061462402\n",
            "epoch: 9/10,    batch: 12738/15469    Discriminator_loss: 0.14326848089694977  Generator_loss: 2.0195846557617188\n",
            "epoch: 9/10,    batch: 12739/15469    Discriminator_loss: 0.1421271562576294  Generator_loss: 2.0270724296569824\n",
            "epoch: 9/10,    batch: 12740/15469    Discriminator_loss: 0.14098688960075378  Generator_loss: 2.034519672393799\n",
            "epoch: 9/10,    batch: 12741/15469    Discriminator_loss: 0.13986697793006897  Generator_loss: 2.0419442653656006\n",
            "epoch: 9/10,    batch: 12742/15469    Discriminator_loss: 0.13875174522399902  Generator_loss: 2.0493829250335693\n",
            "epoch: 9/10,    batch: 12743/15469    Discriminator_loss: 0.13765466213226318  Generator_loss: 2.0567917823791504\n",
            "epoch: 9/10,    batch: 12744/15469    Discriminator_loss: 0.13656392693519592  Generator_loss: 2.0641825199127197\n",
            "epoch: 9/10,    batch: 12745/15469    Discriminator_loss: 0.1354806274175644  Generator_loss: 2.071591377258301\n",
            "epoch: 9/10,    batch: 12746/15469    Discriminator_loss: 0.1344134360551834  Generator_loss: 2.0789756774902344\n",
            "epoch: 9/10,    batch: 12747/15469    Discriminator_loss: 0.1333487182855606  Generator_loss: 2.0863661766052246\n",
            "epoch: 9/10,    batch: 12748/15469    Discriminator_loss: 0.13229386508464813  Generator_loss: 2.0937981605529785\n",
            "epoch: 9/10,    batch: 12749/15469    Discriminator_loss: 0.13124816119670868  Generator_loss: 2.1012039184570312\n",
            "epoch: 9/10,    batch: 12750/15469    Discriminator_loss: 0.13021202385425568  Generator_loss: 2.1086156368255615\n",
            "epoch: 9/10,    batch: 12751/15469    Discriminator_loss: 0.12918582558631897  Generator_loss: 2.116030693054199\n",
            "epoch: 9/10,    batch: 12752/15469    Discriminator_loss: 0.1281655877828598  Generator_loss: 2.123443603515625\n",
            "epoch: 9/10,    batch: 12753/15469    Discriminator_loss: 0.12715336680412292  Generator_loss: 2.130887508392334\n",
            "epoch: 9/10,    batch: 12754/15469    Discriminator_loss: 0.12615031003952026  Generator_loss: 2.1383039951324463\n",
            "epoch: 9/10,    batch: 12755/15469    Discriminator_loss: 0.12515677511692047  Generator_loss: 2.1457300186157227\n",
            "epoch: 9/10,    batch: 12756/15469    Discriminator_loss: 0.12416960299015045  Generator_loss: 2.153148889541626\n",
            "epoch: 9/10,    batch: 12757/15469    Discriminator_loss: 0.12319204956293106  Generator_loss: 2.1605944633483887\n",
            "epoch: 9/10,    batch: 12758/15469    Discriminator_loss: 0.12222378700971603  Generator_loss: 2.1680169105529785\n",
            "epoch: 9/10,    batch: 12759/15469    Discriminator_loss: 0.12125947326421738  Generator_loss: 2.175469398498535\n",
            "epoch: 9/10,    batch: 12760/15469    Discriminator_loss: 0.1203043982386589  Generator_loss: 2.1828861236572266\n",
            "epoch: 9/10,    batch: 12761/15469    Discriminator_loss: 0.119366355240345  Generator_loss: 2.1903109550476074\n",
            "epoch: 9/10,    batch: 12762/15469    Discriminator_loss: 0.1184302493929863  Generator_loss: 2.1977267265319824\n",
            "epoch: 9/10,    batch: 12763/15469    Discriminator_loss: 0.11750202625989914  Generator_loss: 2.2051849365234375\n",
            "epoch: 9/10,    batch: 12764/15469    Discriminator_loss: 0.11658758670091629  Generator_loss: 2.2125957012176514\n",
            "epoch: 9/10,    batch: 12765/15469    Discriminator_loss: 0.11567505449056625  Generator_loss: 2.2200069427490234\n",
            "epoch: 9/10,    batch: 12766/15469    Discriminator_loss: 0.11477094888687134  Generator_loss: 2.227412700653076\n",
            "epoch: 9/10,    batch: 12767/15469    Discriminator_loss: 0.11387834697961807  Generator_loss: 2.2348179817199707\n",
            "epoch: 9/10,    batch: 12768/15469    Discriminator_loss: 0.11299014836549759  Generator_loss: 2.2422103881835938\n",
            "epoch: 9/10,    batch: 12769/15469    Discriminator_loss: 0.11211216449737549  Generator_loss: 2.2495718002319336\n",
            "epoch: 9/10,    batch: 12770/15469    Discriminator_loss: 0.1112501323223114  Generator_loss: 2.256974220275879\n",
            "epoch: 9/10,    batch: 12771/15469    Discriminator_loss: 0.11038294434547424  Generator_loss: 2.2643423080444336\n",
            "epoch: 9/10,    batch: 12772/15469    Discriminator_loss: 0.10953894257545471  Generator_loss: 2.271699905395508\n",
            "epoch: 9/10,    batch: 12773/15469    Discriminator_loss: 0.10870110988616943  Generator_loss: 2.2790212631225586\n",
            "epoch: 9/10,    batch: 12774/15469    Discriminator_loss: 0.1078571081161499  Generator_loss: 2.2863693237304688\n",
            "epoch: 9/10,    batch: 12775/15469    Discriminator_loss: 0.10703153908252716  Generator_loss: 2.2936887741088867\n",
            "epoch: 9/10,    batch: 12776/15469    Discriminator_loss: 0.1062416359782219  Generator_loss: 2.300994396209717\n",
            "epoch: 9/10,    batch: 12777/15469    Discriminator_loss: 0.10542821884155273  Generator_loss: 2.308237075805664\n",
            "epoch: 9/10,    batch: 12778/15469    Discriminator_loss: 0.10463391989469528  Generator_loss: 2.3154945373535156\n",
            "epoch: 9/10,    batch: 12779/15469    Discriminator_loss: 0.1915532648563385  Generator_loss: 2.321871757507324\n",
            "epoch: 9/10,    batch: 12780/15469    Discriminator_loss: 2.4966890811920166  Generator_loss: 2.3130815029144287\n",
            "epoch: 9/10,    batch: 12781/15469    Discriminator_loss: 2.488889694213867  Generator_loss: 2.2929301261901855\n",
            "epoch: 9/10,    batch: 12782/15469    Discriminator_loss: 2.4686107635498047  Generator_loss: 2.264482259750366\n",
            "epoch: 9/10,    batch: 12783/15469    Discriminator_loss: 2.445333242416382  Generator_loss: 2.230466365814209\n",
            "epoch: 9/10,    batch: 12784/15469    Discriminator_loss: 2.4117894172668457  Generator_loss: 2.1927409172058105\n",
            "epoch: 9/10,    batch: 12785/15469    Discriminator_loss: 2.366340160369873  Generator_loss: 2.15299654006958\n",
            "epoch: 9/10,    batch: 12786/15469    Discriminator_loss: 2.3325023651123047  Generator_loss: 2.112421751022339\n",
            "epoch: 9/10,    batch: 12787/15469    Discriminator_loss: 2.2959353923797607  Generator_loss: 2.0719356536865234\n",
            "epoch: 9/10,    batch: 12788/15469    Discriminator_loss: 2.2584035396575928  Generator_loss: 2.0318171977996826\n",
            "epoch: 9/10,    batch: 12789/15469    Discriminator_loss: 2.218380928039551  Generator_loss: 1.992727279663086\n",
            "epoch: 9/10,    batch: 12790/15469    Discriminator_loss: 2.184006929397583  Generator_loss: 1.9546369314193726\n",
            "epoch: 9/10,    batch: 12791/15469    Discriminator_loss: 2.1535239219665527  Generator_loss: 1.9178980588912964\n",
            "epoch: 9/10,    batch: 12792/15469    Discriminator_loss: 2.1176650524139404  Generator_loss: 1.882333517074585\n",
            "epoch: 9/10,    batch: 12793/15469    Discriminator_loss: 2.0834298133850098  Generator_loss: 1.8483319282531738\n",
            "epoch: 9/10,    batch: 12794/15469    Discriminator_loss: 2.0552518367767334  Generator_loss: 1.8156912326812744\n",
            "epoch: 9/10,    batch: 12795/15469    Discriminator_loss: 2.031911611557007  Generator_loss: 1.7844583988189697\n",
            "epoch: 9/10,    batch: 12796/15469    Discriminator_loss: 2.0001285076141357  Generator_loss: 1.7546606063842773\n",
            "epoch: 9/10,    batch: 12797/15469    Discriminator_loss: 1.980304479598999  Generator_loss: 1.7261297702789307\n",
            "epoch: 9/10,    batch: 12798/15469    Discriminator_loss: 1.9545986652374268  Generator_loss: 1.6989152431488037\n",
            "epoch: 9/10,    batch: 12799/15469    Discriminator_loss: 1.9367954730987549  Generator_loss: 1.6728546619415283\n",
            "epoch: 9/10,    batch: 12800/15469    Discriminator_loss: 1.917375922203064  Generator_loss: 1.647995948791504\n",
            "epoch: 9/10,    batch: 12801/15469    Discriminator_loss: 1.8958624601364136  Generator_loss: 1.6242209672927856\n",
            "epoch: 9/10,    batch: 12802/15469    Discriminator_loss: 1.3794341087341309  Generator_loss: 1.6028292179107666\n",
            "epoch: 9/10,    batch: 12803/15469    Discriminator_loss: 0.2272939831018448  Generator_loss: 1.5869345664978027\n",
            "epoch: 9/10,    batch: 12804/15469    Discriminator_loss: 0.23078106343746185  Generator_loss: 1.5753612518310547\n",
            "epoch: 9/10,    batch: 12805/15469    Discriminator_loss: 0.23344649374485016  Generator_loss: 1.5672372579574585\n",
            "epoch: 9/10,    batch: 12806/15469    Discriminator_loss: 0.23506194353103638  Generator_loss: 1.5619020462036133\n",
            "epoch: 9/10,    batch: 12807/15469    Discriminator_loss: 0.23613831400871277  Generator_loss: 1.558791160583496\n",
            "epoch: 9/10,    batch: 12808/15469    Discriminator_loss: 0.23670046031475067  Generator_loss: 1.5574980974197388\n",
            "epoch: 9/10,    batch: 12809/15469    Discriminator_loss: 0.23682764172554016  Generator_loss: 1.5576786994934082\n",
            "epoch: 9/10,    batch: 12810/15469    Discriminator_loss: 0.23658882081508636  Generator_loss: 1.5590301752090454\n",
            "epoch: 9/10,    batch: 12811/15469    Discriminator_loss: 0.23608539998531342  Generator_loss: 1.5614054203033447\n",
            "epoch: 9/10,    batch: 12812/15469    Discriminator_loss: 0.7699659466743469  Generator_loss: 1.5628488063812256\n",
            "epoch: 9/10,    batch: 12813/15469    Discriminator_loss: 1.8399029970169067  Generator_loss: 1.5604500770568848\n",
            "epoch: 9/10,    batch: 12814/15469    Discriminator_loss: 1.8431669473648071  Generator_loss: 1.5549185276031494\n",
            "epoch: 9/10,    batch: 12815/15469    Discriminator_loss: 1.8394697904586792  Generator_loss: 1.546943187713623\n",
            "epoch: 9/10,    batch: 12816/15469    Discriminator_loss: 1.8377774953842163  Generator_loss: 1.5371344089508057\n",
            "epoch: 9/10,    batch: 12817/15469    Discriminator_loss: 1.834498405456543  Generator_loss: 1.5257658958435059\n",
            "epoch: 9/10,    batch: 12818/15469    Discriminator_loss: 1.8247942924499512  Generator_loss: 1.5133126974105835\n",
            "epoch: 9/10,    batch: 12819/15469    Discriminator_loss: 1.8391951322555542  Generator_loss: 1.4999816417694092\n",
            "epoch: 9/10,    batch: 12820/15469    Discriminator_loss: 1.8543202877044678  Generator_loss: 1.4859589338302612\n",
            "epoch: 9/10,    batch: 12821/15469    Discriminator_loss: 0.48877936601638794  Generator_loss: 1.4752118587493896\n",
            "epoch: 9/10,    batch: 12822/15469    Discriminator_loss: 0.26116031408309937  Generator_loss: 1.4678348302841187\n",
            "epoch: 9/10,    batch: 12823/15469    Discriminator_loss: 0.26298531889915466  Generator_loss: 1.4630852937698364\n",
            "epoch: 9/10,    batch: 12824/15469    Discriminator_loss: 0.26410138607025146  Generator_loss: 1.4604754447937012\n",
            "epoch: 9/10,    batch: 12825/15469    Discriminator_loss: 0.2645299732685089  Generator_loss: 1.4596364498138428\n",
            "epoch: 9/10,    batch: 12826/15469    Discriminator_loss: 0.2645682990550995  Generator_loss: 1.460218906402588\n",
            "epoch: 9/10,    batch: 12827/15469    Discriminator_loss: 0.2642056941986084  Generator_loss: 1.4619089365005493\n",
            "epoch: 9/10,    batch: 12828/15469    Discriminator_loss: 0.2635396718978882  Generator_loss: 1.4645462036132812\n",
            "epoch: 9/10,    batch: 12829/15469    Discriminator_loss: 0.2626155912876129  Generator_loss: 1.4679548740386963\n",
            "epoch: 9/10,    batch: 12830/15469    Discriminator_loss: 0.2615220546722412  Generator_loss: 1.4719772338867188\n",
            "epoch: 9/10,    batch: 12831/15469    Discriminator_loss: 1.220448613166809  Generator_loss: 1.4735050201416016\n",
            "epoch: 9/10,    batch: 12832/15469    Discriminator_loss: 1.8259035348892212  Generator_loss: 1.4714124202728271\n",
            "epoch: 9/10,    batch: 12833/15469    Discriminator_loss: 1.8154006004333496  Generator_loss: 1.4664393663406372\n",
            "epoch: 9/10,    batch: 12834/15469    Discriminator_loss: 1.8123701810836792  Generator_loss: 1.4591612815856934\n",
            "epoch: 9/10,    batch: 12835/15469    Discriminator_loss: 1.8058409690856934  Generator_loss: 1.450111985206604\n",
            "epoch: 9/10,    batch: 12836/15469    Discriminator_loss: 1.799148678779602  Generator_loss: 1.4397037029266357\n",
            "epoch: 9/10,    batch: 12837/15469    Discriminator_loss: 1.793973684310913  Generator_loss: 1.428225040435791\n",
            "epoch: 9/10,    batch: 12838/15469    Discriminator_loss: 1.8200324773788452  Generator_loss: 1.4157487154006958\n",
            "epoch: 9/10,    batch: 12839/15469    Discriminator_loss: 1.7910974025726318  Generator_loss: 1.402483344078064\n",
            "epoch: 9/10,    batch: 12840/15469    Discriminator_loss: 1.7712725400924683  Generator_loss: 1.388512372970581\n",
            "epoch: 9/10,    batch: 12841/15469    Discriminator_loss: 1.738088607788086  Generator_loss: 1.3743157386779785\n",
            "epoch: 9/10,    batch: 12842/15469    Discriminator_loss: 1.7093737125396729  Generator_loss: 1.3600267171859741\n",
            "epoch: 9/10,    batch: 12843/15469    Discriminator_loss: 1.670497179031372  Generator_loss: 1.3455274105072021\n",
            "epoch: 9/10,    batch: 12844/15469    Discriminator_loss: 1.638408899307251  Generator_loss: 1.3308106660842896\n",
            "epoch: 9/10,    batch: 12845/15469    Discriminator_loss: 1.576185703277588  Generator_loss: 1.3156647682189941\n",
            "epoch: 9/10,    batch: 12846/15469    Discriminator_loss: 1.5676308870315552  Generator_loss: 1.3003334999084473\n",
            "epoch: 9/10,    batch: 12847/15469    Discriminator_loss: 1.3331212997436523  Generator_loss: 1.2830281257629395\n",
            "epoch: 9/10,    batch: 12848/15469    Discriminator_loss: 0.9237356185913086  Generator_loss: 1.263382077217102\n",
            "epoch: 9/10,    batch: 12849/15469    Discriminator_loss: 0.6556435227394104  Generator_loss: 1.242633581161499\n",
            "epoch: 9/10,    batch: 12850/15469    Discriminator_loss: 0.34721770882606506  Generator_loss: 1.223939061164856\n",
            "epoch: 9/10,    batch: 12851/15469    Discriminator_loss: 0.3524992763996124  Generator_loss: 1.2067729234695435\n",
            "epoch: 9/10,    batch: 12852/15469    Discriminator_loss: 0.3598226010799408  Generator_loss: 1.1903800964355469\n",
            "epoch: 9/10,    batch: 12853/15469    Discriminator_loss: 0.3672313988208771  Generator_loss: 1.1746785640716553\n",
            "epoch: 9/10,    batch: 12854/15469    Discriminator_loss: 0.3748299181461334  Generator_loss: 1.1597392559051514\n",
            "epoch: 9/10,    batch: 12855/15469    Discriminator_loss: 0.38265523314476013  Generator_loss: 1.1449209451675415\n",
            "epoch: 9/10,    batch: 12856/15469    Discriminator_loss: 0.3910605311393738  Generator_loss: 1.1308255195617676\n",
            "epoch: 9/10,    batch: 12857/15469    Discriminator_loss: 0.4006315767765045  Generator_loss: 1.1159336566925049\n",
            "epoch: 9/10,    batch: 12858/15469    Discriminator_loss: 0.41181066632270813  Generator_loss: 1.1008172035217285\n",
            "epoch: 9/10,    batch: 12859/15469    Discriminator_loss: 0.4265996217727661  Generator_loss: 1.0841455459594727\n",
            "epoch: 9/10,    batch: 12860/15469    Discriminator_loss: 0.4465012550354004  Generator_loss: 1.0669069290161133\n",
            "epoch: 9/10,    batch: 12861/15469    Discriminator_loss: 0.47283023595809937  Generator_loss: 1.0484561920166016\n",
            "epoch: 9/10,    batch: 12862/15469    Discriminator_loss: 0.5022901296615601  Generator_loss: 1.0310710668563843\n",
            "epoch: 9/10,    batch: 12863/15469    Discriminator_loss: 0.5309085845947266  Generator_loss: 1.0164824724197388\n",
            "epoch: 9/10,    batch: 12864/15469    Discriminator_loss: 0.5535886883735657  Generator_loss: 1.0109691619873047\n",
            "epoch: 9/10,    batch: 12865/15469    Discriminator_loss: 0.5721138715744019  Generator_loss: 1.0217268466949463\n",
            "epoch: 9/10,    batch: 12866/15469    Discriminator_loss: 0.6106172800064087  Generator_loss: 1.0680627822875977\n",
            "epoch: 9/10,    batch: 12867/15469    Discriminator_loss: 0.6941850781440735  Generator_loss: 1.2047436237335205\n",
            "epoch: 9/10,    batch: 12868/15469    Discriminator_loss: 0.47363734245300293  Generator_loss: 1.4002861976623535\n",
            "epoch: 9/10,    batch: 12869/15469    Discriminator_loss: 0.26414164900779724  Generator_loss: 1.5529146194458008\n",
            "epoch: 9/10,    batch: 12870/15469    Discriminator_loss: 0.2155725359916687  Generator_loss: 1.6487740278244019\n",
            "epoch: 9/10,    batch: 12871/15469    Discriminator_loss: 0.2022920548915863  Generator_loss: 1.698552131652832\n",
            "epoch: 9/10,    batch: 12872/15469    Discriminator_loss: 0.24322448670864105  Generator_loss: 1.7202626466751099\n",
            "epoch: 9/10,    batch: 12873/15469    Discriminator_loss: 0.47691670060157776  Generator_loss: 1.7226853370666504\n",
            "epoch: 9/10,    batch: 12874/15469    Discriminator_loss: 0.2408013641834259  Generator_loss: 1.7076952457427979\n",
            "epoch: 9/10,    batch: 12875/15469    Discriminator_loss: 0.2557142972946167  Generator_loss: 1.6800862550735474\n",
            "epoch: 9/10,    batch: 12876/15469    Discriminator_loss: 0.2246772199869156  Generator_loss: 1.6376562118530273\n",
            "epoch: 9/10,    batch: 12877/15469    Discriminator_loss: 0.23213905096054077  Generator_loss: 1.5940721035003662\n",
            "epoch: 9/10,    batch: 12878/15469    Discriminator_loss: 0.23873062431812286  Generator_loss: 1.562598466873169\n",
            "epoch: 9/10,    batch: 12879/15469    Discriminator_loss: 0.24210336804389954  Generator_loss: 1.5481806993484497\n",
            "epoch: 9/10,    batch: 12880/15469    Discriminator_loss: 0.24237826466560364  Generator_loss: 1.5456217527389526\n",
            "epoch: 9/10,    batch: 12881/15469    Discriminator_loss: 0.24112878739833832  Generator_loss: 1.5493462085723877\n",
            "epoch: 9/10,    batch: 12882/15469    Discriminator_loss: 0.23932518064975739  Generator_loss: 1.5552783012390137\n",
            "epoch: 9/10,    batch: 12883/15469    Discriminator_loss: 0.23750631511211395  Generator_loss: 1.561734914779663\n",
            "epoch: 9/10,    batch: 12884/15469    Discriminator_loss: 0.2358628511428833  Generator_loss: 1.5676941871643066\n",
            "epoch: 9/10,    batch: 12885/15469    Discriminator_loss: 0.23447826504707336  Generator_loss: 1.5724146366119385\n",
            "epoch: 9/10,    batch: 12886/15469    Discriminator_loss: 0.23330479860305786  Generator_loss: 1.5766351222991943\n",
            "epoch: 9/10,    batch: 12887/15469    Discriminator_loss: 0.23237182199954987  Generator_loss: 1.580233097076416\n",
            "epoch: 9/10,    batch: 12888/15469    Discriminator_loss: 0.2313845008611679  Generator_loss: 1.5835814476013184\n",
            "epoch: 9/10,    batch: 12889/15469    Discriminator_loss: 0.23050889372825623  Generator_loss: 1.5864496231079102\n",
            "epoch: 9/10,    batch: 12890/15469    Discriminator_loss: 0.22963488101959229  Generator_loss: 1.5900743007659912\n",
            "epoch: 9/10,    batch: 12891/15469    Discriminator_loss: 0.22863814234733582  Generator_loss: 1.5937740802764893\n",
            "epoch: 9/10,    batch: 12892/15469    Discriminator_loss: 0.2275874763727188  Generator_loss: 1.5977025032043457\n",
            "epoch: 9/10,    batch: 12893/15469    Discriminator_loss: 0.22647348046302795  Generator_loss: 1.6021902561187744\n",
            "epoch: 9/10,    batch: 12894/15469    Discriminator_loss: 0.2252432107925415  Generator_loss: 1.6068072319030762\n",
            "epoch: 9/10,    batch: 12895/15469    Discriminator_loss: 0.224016010761261  Generator_loss: 1.6116971969604492\n",
            "epoch: 9/10,    batch: 12896/15469    Discriminator_loss: 0.22268036007881165  Generator_loss: 1.6169536113739014\n",
            "epoch: 9/10,    batch: 12897/15469    Discriminator_loss: 0.22131215035915375  Generator_loss: 1.6221297979354858\n",
            "epoch: 9/10,    batch: 12898/15469    Discriminator_loss: 0.22000068426132202  Generator_loss: 1.627523422241211\n",
            "epoch: 9/10,    batch: 12899/15469    Discriminator_loss: 0.21870079636573792  Generator_loss: 1.632838249206543\n",
            "epoch: 9/10,    batch: 12900/15469    Discriminator_loss: 0.21740911900997162  Generator_loss: 1.6380019187927246\n",
            "epoch: 9/10,    batch: 12901/15469    Discriminator_loss: 0.2161722630262375  Generator_loss: 1.6431012153625488\n",
            "epoch: 9/10,    batch: 12902/15469    Discriminator_loss: 0.21497641503810883  Generator_loss: 1.6480638980865479\n",
            "epoch: 9/10,    batch: 12903/15469    Discriminator_loss: 0.21381479501724243  Generator_loss: 1.6528358459472656\n",
            "epoch: 9/10,    batch: 12904/15469    Discriminator_loss: 0.21271121501922607  Generator_loss: 1.6574656963348389\n",
            "epoch: 9/10,    batch: 12905/15469    Discriminator_loss: 0.211631178855896  Generator_loss: 1.6619341373443604\n",
            "epoch: 9/10,    batch: 12906/15469    Discriminator_loss: 0.2105991393327713  Generator_loss: 1.6663519144058228\n",
            "epoch: 9/10,    batch: 12907/15469    Discriminator_loss: 0.20956236124038696  Generator_loss: 1.6706674098968506\n",
            "epoch: 9/10,    batch: 12908/15469    Discriminator_loss: 0.20855416357517242  Generator_loss: 1.6750092506408691\n",
            "epoch: 9/10,    batch: 12909/15469    Discriminator_loss: 0.20756098628044128  Generator_loss: 1.6792781352996826\n",
            "epoch: 9/10,    batch: 12910/15469    Discriminator_loss: 0.20653729140758514  Generator_loss: 1.6836116313934326\n",
            "epoch: 9/10,    batch: 12911/15469    Discriminator_loss: 0.20553256571292877  Generator_loss: 1.6879537105560303\n",
            "epoch: 9/10,    batch: 12912/15469    Discriminator_loss: 0.20449870824813843  Generator_loss: 1.6924484968185425\n",
            "epoch: 9/10,    batch: 12913/15469    Discriminator_loss: 0.20347505807876587  Generator_loss: 1.6970247030258179\n",
            "epoch: 9/10,    batch: 12914/15469    Discriminator_loss: 0.20240376889705658  Generator_loss: 1.7016572952270508\n",
            "epoch: 9/10,    batch: 12915/15469    Discriminator_loss: 0.20133648812770844  Generator_loss: 1.7064399719238281\n",
            "epoch: 9/10,    batch: 12916/15469    Discriminator_loss: 0.20024710893630981  Generator_loss: 1.7113471031188965\n",
            "epoch: 9/10,    batch: 12917/15469    Discriminator_loss: 0.19913819432258606  Generator_loss: 1.7162874937057495\n",
            "epoch: 9/10,    batch: 12918/15469    Discriminator_loss: 0.19801917672157288  Generator_loss: 1.7214393615722656\n",
            "epoch: 9/10,    batch: 12919/15469    Discriminator_loss: 0.19687798619270325  Generator_loss: 1.7265828847885132\n",
            "epoch: 9/10,    batch: 12920/15469    Discriminator_loss: 0.19572362303733826  Generator_loss: 1.731923222541809\n",
            "epoch: 9/10,    batch: 12921/15469    Discriminator_loss: 0.1945679783821106  Generator_loss: 1.737281322479248\n",
            "epoch: 9/10,    batch: 12922/15469    Discriminator_loss: 0.19341498613357544  Generator_loss: 1.742793321609497\n",
            "epoch: 9/10,    batch: 12923/15469    Discriminator_loss: 0.19218631088733673  Generator_loss: 1.7483341693878174\n",
            "epoch: 9/10,    batch: 12924/15469    Discriminator_loss: 0.19099298119544983  Generator_loss: 1.7539068460464478\n",
            "epoch: 9/10,    batch: 12925/15469    Discriminator_loss: 0.18979144096374512  Generator_loss: 1.7596579790115356\n",
            "epoch: 9/10,    batch: 12926/15469    Discriminator_loss: 0.18858076632022858  Generator_loss: 1.7654802799224854\n",
            "epoch: 9/10,    batch: 12927/15469    Discriminator_loss: 0.18737824261188507  Generator_loss: 1.7713664770126343\n",
            "epoch: 9/10,    batch: 12928/15469    Discriminator_loss: 0.18615537881851196  Generator_loss: 1.7772483825683594\n",
            "epoch: 9/10,    batch: 12929/15469    Discriminator_loss: 0.18493950366973877  Generator_loss: 1.783197045326233\n",
            "epoch: 9/10,    batch: 12930/15469    Discriminator_loss: 0.1837204396724701  Generator_loss: 1.7892411947250366\n",
            "epoch: 9/10,    batch: 12931/15469    Discriminator_loss: 0.18249234557151794  Generator_loss: 1.7953011989593506\n",
            "epoch: 9/10,    batch: 12932/15469    Discriminator_loss: 0.18127962946891785  Generator_loss: 1.8014743328094482\n",
            "epoch: 9/10,    batch: 12933/15469    Discriminator_loss: 0.18005840480327606  Generator_loss: 1.8075804710388184\n",
            "epoch: 9/10,    batch: 12934/15469    Discriminator_loss: 0.17883414030075073  Generator_loss: 1.81382417678833\n",
            "epoch: 9/10,    batch: 12935/15469    Discriminator_loss: 0.1776244193315506  Generator_loss: 1.8200197219848633\n",
            "epoch: 9/10,    batch: 12936/15469    Discriminator_loss: 0.17641115188598633  Generator_loss: 1.8262507915496826\n",
            "epoch: 9/10,    batch: 12937/15469    Discriminator_loss: 0.17521628737449646  Generator_loss: 1.8325469493865967\n",
            "epoch: 9/10,    batch: 12938/15469    Discriminator_loss: 0.17400065064430237  Generator_loss: 1.8388782739639282\n",
            "epoch: 9/10,    batch: 12939/15469    Discriminator_loss: 0.17280608415603638  Generator_loss: 1.8451871871948242\n",
            "epoch: 9/10,    batch: 12940/15469    Discriminator_loss: 0.17161765694618225  Generator_loss: 1.8515617847442627\n",
            "epoch: 9/10,    batch: 12941/15469    Discriminator_loss: 0.1704137921333313  Generator_loss: 1.85793137550354\n",
            "epoch: 9/10,    batch: 12942/15469    Discriminator_loss: 0.1692308485507965  Generator_loss: 1.8643244504928589\n",
            "epoch: 9/10,    batch: 12943/15469    Discriminator_loss: 0.16805967688560486  Generator_loss: 1.8706984519958496\n",
            "epoch: 9/10,    batch: 12944/15469    Discriminator_loss: 0.16687870025634766  Generator_loss: 1.877118468284607\n",
            "epoch: 9/10,    batch: 12945/15469    Discriminator_loss: 0.1657138466835022  Generator_loss: 1.8835885524749756\n",
            "epoch: 9/10,    batch: 12946/15469    Discriminator_loss: 0.16455192863941193  Generator_loss: 1.890060544013977\n",
            "epoch: 9/10,    batch: 12947/15469    Discriminator_loss: 0.16340070962905884  Generator_loss: 1.8965712785720825\n",
            "epoch: 9/10,    batch: 12948/15469    Discriminator_loss: 0.16224516928195953  Generator_loss: 1.903085470199585\n",
            "epoch: 9/10,    batch: 12949/15469    Discriminator_loss: 0.16109749674797058  Generator_loss: 1.9095836877822876\n",
            "epoch: 9/10,    batch: 12950/15469    Discriminator_loss: 0.15995709598064423  Generator_loss: 1.9161412715911865\n",
            "epoch: 9/10,    batch: 12951/15469    Discriminator_loss: 0.15882590413093567  Generator_loss: 1.9227148294448853\n",
            "epoch: 9/10,    batch: 12952/15469    Discriminator_loss: 0.15769532322883606  Generator_loss: 1.9292701482772827\n",
            "epoch: 9/10,    batch: 12953/15469    Discriminator_loss: 0.1565673053264618  Generator_loss: 1.9359101057052612\n",
            "epoch: 9/10,    batch: 12954/15469    Discriminator_loss: 0.15545320510864258  Generator_loss: 1.942518949508667\n",
            "epoch: 9/10,    batch: 12955/15469    Discriminator_loss: 0.15433883666992188  Generator_loss: 1.9491783380508423\n",
            "epoch: 9/10,    batch: 12956/15469    Discriminator_loss: 0.15322861075401306  Generator_loss: 1.9558255672454834\n",
            "epoch: 9/10,    batch: 12957/15469    Discriminator_loss: 0.15213404595851898  Generator_loss: 1.9625107049942017\n",
            "epoch: 9/10,    batch: 12958/15469    Discriminator_loss: 0.1510348618030548  Generator_loss: 1.9691851139068604\n",
            "epoch: 9/10,    batch: 12959/15469    Discriminator_loss: 0.14994290471076965  Generator_loss: 1.9759037494659424\n",
            "epoch: 9/10,    batch: 12960/15469    Discriminator_loss: 0.14886406064033508  Generator_loss: 1.982605218887329\n",
            "epoch: 9/10,    batch: 12961/15469    Discriminator_loss: 0.1477864533662796  Generator_loss: 1.9893882274627686\n",
            "epoch: 9/10,    batch: 12962/15469    Discriminator_loss: 0.1467178910970688  Generator_loss: 1.9961016178131104\n",
            "epoch: 9/10,    batch: 12963/15469    Discriminator_loss: 0.14565348625183105  Generator_loss: 2.002875804901123\n",
            "epoch: 9/10,    batch: 12964/15469    Discriminator_loss: 0.14459264278411865  Generator_loss: 2.009620189666748\n",
            "epoch: 9/10,    batch: 12965/15469    Discriminator_loss: 0.14354974031448364  Generator_loss: 2.0164453983306885\n",
            "epoch: 9/10,    batch: 12966/15469    Discriminator_loss: 0.14250361919403076  Generator_loss: 2.023183822631836\n",
            "epoch: 9/10,    batch: 12967/15469    Discriminator_loss: 0.14147041738033295  Generator_loss: 2.0299806594848633\n",
            "epoch: 9/10,    batch: 12968/15469    Discriminator_loss: 0.14043767750263214  Generator_loss: 2.036783218383789\n",
            "epoch: 9/10,    batch: 12969/15469    Discriminator_loss: 0.13941919803619385  Generator_loss: 2.0435781478881836\n",
            "epoch: 9/10,    batch: 12970/15469    Discriminator_loss: 0.13840153813362122  Generator_loss: 2.0504086017608643\n",
            "epoch: 9/10,    batch: 12971/15469    Discriminator_loss: 0.13739131391048431  Generator_loss: 2.0572197437286377\n",
            "epoch: 9/10,    batch: 12972/15469    Discriminator_loss: 0.13639229536056519  Generator_loss: 2.064030170440674\n",
            "epoch: 9/10,    batch: 12973/15469    Discriminator_loss: 0.1353960782289505  Generator_loss: 2.070876121520996\n",
            "epoch: 9/10,    batch: 12974/15469    Discriminator_loss: 0.13441357016563416  Generator_loss: 2.0777034759521484\n",
            "epoch: 9/10,    batch: 12975/15469    Discriminator_loss: 0.13342872262001038  Generator_loss: 2.0845706462860107\n",
            "epoch: 9/10,    batch: 12976/15469    Discriminator_loss: 0.13245877623558044  Generator_loss: 2.091416120529175\n",
            "epoch: 9/10,    batch: 12977/15469    Discriminator_loss: 0.1314961016178131  Generator_loss: 2.098282814025879\n",
            "epoch: 9/10,    batch: 12978/15469    Discriminator_loss: 0.1305307149887085  Generator_loss: 2.105118989944458\n",
            "epoch: 9/10,    batch: 12979/15469    Discriminator_loss: 0.12957744300365448  Generator_loss: 2.111985921859741\n",
            "epoch: 9/10,    batch: 12980/15469    Discriminator_loss: 0.12863536179065704  Generator_loss: 2.1188735961914062\n",
            "epoch: 9/10,    batch: 12981/15469    Discriminator_loss: 0.1276930570602417  Generator_loss: 2.1257381439208984\n",
            "epoch: 9/10,    batch: 12982/15469    Discriminator_loss: 0.12676304578781128  Generator_loss: 2.13260555267334\n",
            "epoch: 9/10,    batch: 12983/15469    Discriminator_loss: 0.12583181262016296  Generator_loss: 2.1394972801208496\n",
            "epoch: 9/10,    batch: 12984/15469    Discriminator_loss: 0.12491413950920105  Generator_loss: 2.1463592052459717\n",
            "epoch: 9/10,    batch: 12985/15469    Discriminator_loss: 0.12400342524051666  Generator_loss: 2.153257369995117\n",
            "epoch: 9/10,    batch: 12986/15469    Discriminator_loss: 0.12309463322162628  Generator_loss: 2.160177707672119\n",
            "epoch: 9/10,    batch: 12987/15469    Discriminator_loss: 0.12219314277172089  Generator_loss: 2.1670966148376465\n",
            "epoch: 9/10,    batch: 12988/15469    Discriminator_loss: 0.12130064517259598  Generator_loss: 2.173985004425049\n",
            "epoch: 9/10,    batch: 12989/15469    Discriminator_loss: 0.1204119473695755  Generator_loss: 2.180920124053955\n",
            "epoch: 9/10,    batch: 12990/15469    Discriminator_loss: 0.11953310668468475  Generator_loss: 2.1878013610839844\n",
            "epoch: 9/10,    batch: 12991/15469    Discriminator_loss: 0.11865644156932831  Generator_loss: 2.1947479248046875\n",
            "epoch: 9/10,    batch: 12992/15469    Discriminator_loss: 0.1177852675318718  Generator_loss: 2.201657295227051\n",
            "epoch: 9/10,    batch: 12993/15469    Discriminator_loss: 0.11692418158054352  Generator_loss: 2.2086069583892822\n",
            "epoch: 9/10,    batch: 12994/15469    Discriminator_loss: 0.11606753617525101  Generator_loss: 2.215522289276123\n",
            "epoch: 9/10,    batch: 12995/15469    Discriminator_loss: 0.11521821469068527  Generator_loss: 2.2224650382995605\n",
            "epoch: 9/10,    batch: 12996/15469    Discriminator_loss: 0.1143764853477478  Generator_loss: 2.22941255569458\n",
            "epoch: 9/10,    batch: 12997/15469    Discriminator_loss: 0.11353649199008942  Generator_loss: 2.23637318611145\n",
            "epoch: 9/10,    batch: 12998/15469    Discriminator_loss: 0.11270488053560257  Generator_loss: 2.2433042526245117\n",
            "epoch: 9/10,    batch: 12999/15469    Discriminator_loss: 0.11187823861837387  Generator_loss: 2.2502756118774414\n",
            "epoch: 9/10,    batch: 13000/15469    Discriminator_loss: 0.11105988174676895  Generator_loss: 2.257225513458252\n",
            "epoch: 9/10,    batch: 13001/15469    Discriminator_loss: 0.11024288088083267  Generator_loss: 2.2641913890838623\n",
            "epoch: 9/10,    batch: 13002/15469    Discriminator_loss: 0.109440378844738  Generator_loss: 2.2711124420166016\n",
            "epoch: 9/10,    batch: 13003/15469    Discriminator_loss: 0.1086377501487732  Generator_loss: 2.278075933456421\n",
            "epoch: 9/10,    batch: 13004/15469    Discriminator_loss: 0.10784467309713364  Generator_loss: 2.2850589752197266\n",
            "epoch: 9/10,    batch: 13005/15469    Discriminator_loss: 0.1070530116558075  Generator_loss: 2.2920422554016113\n",
            "epoch: 9/10,    batch: 13006/15469    Discriminator_loss: 0.10626670718193054  Generator_loss: 2.2990074157714844\n",
            "epoch: 9/10,    batch: 13007/15469    Discriminator_loss: 0.10548514127731323  Generator_loss: 2.3059744834899902\n",
            "epoch: 9/10,    batch: 13008/15469    Discriminator_loss: 0.10472709685564041  Generator_loss: 2.3129758834838867\n",
            "epoch: 9/10,    batch: 13009/15469    Discriminator_loss: 0.10396433621644974  Generator_loss: 2.3199548721313477\n",
            "epoch: 9/10,    batch: 13010/15469    Discriminator_loss: 0.1032087504863739  Generator_loss: 2.3269333839416504\n",
            "epoch: 9/10,    batch: 13011/15469    Discriminator_loss: 0.10244711488485336  Generator_loss: 2.3339414596557617\n",
            "epoch: 9/10,    batch: 13012/15469    Discriminator_loss: 0.10170214623212814  Generator_loss: 2.34092378616333\n",
            "epoch: 9/10,    batch: 13013/15469    Discriminator_loss: 0.10095711052417755  Generator_loss: 2.347933292388916\n",
            "epoch: 9/10,    batch: 13014/15469    Discriminator_loss: 0.10021352767944336  Generator_loss: 2.354919910430908\n",
            "epoch: 9/10,    batch: 13015/15469    Discriminator_loss: 0.09947899729013443  Generator_loss: 2.361929416656494\n",
            "epoch: 9/10,    batch: 13016/15469    Discriminator_loss: 0.09875418990850449  Generator_loss: 2.368915319442749\n",
            "epoch: 9/10,    batch: 13017/15469    Discriminator_loss: 0.09803638607263565  Generator_loss: 2.375980854034424\n",
            "epoch: 9/10,    batch: 13018/15469    Discriminator_loss: 0.09732042998075485  Generator_loss: 2.3829309940338135\n",
            "epoch: 9/10,    batch: 13019/15469    Discriminator_loss: 0.09660408645868301  Generator_loss: 2.3899786472320557\n",
            "epoch: 9/10,    batch: 13020/15469    Discriminator_loss: 0.0958893895149231  Generator_loss: 2.396963596343994\n",
            "epoch: 9/10,    batch: 13021/15469    Discriminator_loss: 0.09518439322710037  Generator_loss: 2.403999090194702\n",
            "epoch: 9/10,    batch: 13022/15469    Discriminator_loss: 0.094495989382267  Generator_loss: 2.411040782928467\n",
            "epoch: 9/10,    batch: 13023/15469    Discriminator_loss: 0.09379991143941879  Generator_loss: 2.4180357456207275\n",
            "epoch: 9/10,    batch: 13024/15469    Discriminator_loss: 0.09311220794916153  Generator_loss: 2.4250659942626953\n",
            "epoch: 9/10,    batch: 13025/15469    Discriminator_loss: 0.09243221580982208  Generator_loss: 2.432100296020508\n",
            "epoch: 9/10,    batch: 13026/15469    Discriminator_loss: 0.09176158905029297  Generator_loss: 2.4391355514526367\n",
            "epoch: 9/10,    batch: 13027/15469    Discriminator_loss: 0.09108767658472061  Generator_loss: 2.446197748184204\n",
            "epoch: 9/10,    batch: 13028/15469    Discriminator_loss: 0.09041457623243332  Generator_loss: 2.453244686126709\n",
            "epoch: 9/10,    batch: 13029/15469    Discriminator_loss: 0.08975478261709213  Generator_loss: 2.4602794647216797\n",
            "epoch: 9/10,    batch: 13030/15469    Discriminator_loss: 0.08910150825977325  Generator_loss: 2.4673354625701904\n",
            "epoch: 9/10,    batch: 13031/15469    Discriminator_loss: 0.08845837414264679  Generator_loss: 2.474397897720337\n",
            "epoch: 9/10,    batch: 13032/15469    Discriminator_loss: 0.08781656622886658  Generator_loss: 2.481442451477051\n",
            "epoch: 9/10,    batch: 13033/15469    Discriminator_loss: 0.08715897053480148  Generator_loss: 2.488494634628296\n",
            "epoch: 9/10,    batch: 13034/15469    Discriminator_loss: 0.0879584476351738  Generator_loss: 2.4955191612243652\n",
            "epoch: 9/10,    batch: 13035/15469    Discriminator_loss: 0.08620642125606537  Generator_loss: 2.502565860748291\n",
            "epoch: 9/10,    batch: 13036/15469    Discriminator_loss: 0.08572572469711304  Generator_loss: 2.50960111618042\n",
            "epoch: 9/10,    batch: 13037/15469    Discriminator_loss: 0.08536353707313538  Generator_loss: 2.5166516304016113\n",
            "epoch: 9/10,    batch: 13038/15469    Discriminator_loss: 0.08444672077894211  Generator_loss: 2.523681640625\n",
            "epoch: 9/10,    batch: 13039/15469    Discriminator_loss: 0.0846729651093483  Generator_loss: 2.5306882858276367\n",
            "epoch: 9/10,    batch: 13040/15469    Discriminator_loss: 0.08420317620038986  Generator_loss: 2.537708282470703\n",
            "epoch: 9/10,    batch: 13041/15469    Discriminator_loss: 0.08314995467662811  Generator_loss: 2.5446999073028564\n",
            "epoch: 9/10,    batch: 13042/15469    Discriminator_loss: 0.08275561779737473  Generator_loss: 2.551705837249756\n",
            "epoch: 9/10,    batch: 13043/15469    Discriminator_loss: 0.08316253870725632  Generator_loss: 2.558706760406494\n",
            "epoch: 9/10,    batch: 13044/15469    Discriminator_loss: 0.08239428699016571  Generator_loss: 2.5656864643096924\n",
            "epoch: 9/10,    batch: 13045/15469    Discriminator_loss: 0.08312506973743439  Generator_loss: 2.5725927352905273\n",
            "epoch: 9/10,    batch: 13046/15469    Discriminator_loss: 0.08171844482421875  Generator_loss: 2.5795154571533203\n",
            "epoch: 9/10,    batch: 13047/15469    Discriminator_loss: 0.08192037791013718  Generator_loss: 2.586395025253296\n",
            "epoch: 9/10,    batch: 13048/15469    Discriminator_loss: 0.08113601803779602  Generator_loss: 2.593242645263672\n",
            "epoch: 9/10,    batch: 13049/15469    Discriminator_loss: 0.8821272850036621  Generator_loss: 2.593576431274414\n",
            "epoch: 9/10,    batch: 13050/15469    Discriminator_loss: 2.756941795349121  Generator_loss: 2.5746049880981445\n",
            "epoch: 9/10,    batch: 13051/15469    Discriminator_loss: 2.733999490737915  Generator_loss: 2.542506217956543\n",
            "epoch: 9/10,    batch: 13052/15469    Discriminator_loss: 2.708707332611084  Generator_loss: 2.5017080307006836\n",
            "epoch: 9/10,    batch: 13053/15469    Discriminator_loss: 2.668729782104492  Generator_loss: 2.455789566040039\n",
            "epoch: 9/10,    batch: 13054/15469    Discriminator_loss: 2.62788724899292  Generator_loss: 2.407228946685791\n",
            "epoch: 9/10,    batch: 13055/15469    Discriminator_loss: 2.5817196369171143  Generator_loss: 2.3578598499298096\n",
            "epoch: 9/10,    batch: 13056/15469    Discriminator_loss: 2.508200168609619  Generator_loss: 2.3088741302490234\n",
            "epoch: 9/10,    batch: 13057/15469    Discriminator_loss: 2.4727365970611572  Generator_loss: 2.260967254638672\n",
            "epoch: 9/10,    batch: 13058/15469    Discriminator_loss: 2.412950277328491  Generator_loss: 2.214545726776123\n",
            "epoch: 9/10,    batch: 13059/15469    Discriminator_loss: 2.373403549194336  Generator_loss: 2.1698379516601562\n",
            "epoch: 9/10,    batch: 13060/15469    Discriminator_loss: 2.333965539932251  Generator_loss: 2.1268558502197266\n",
            "epoch: 9/10,    batch: 13061/15469    Discriminator_loss: 2.300262451171875  Generator_loss: 2.0856728553771973\n",
            "epoch: 9/10,    batch: 13062/15469    Discriminator_loss: 2.2562549114227295  Generator_loss: 2.046175479888916\n",
            "epoch: 9/10,    batch: 13063/15469    Discriminator_loss: 2.221013069152832  Generator_loss: 2.0084612369537354\n",
            "epoch: 9/10,    batch: 13064/15469    Discriminator_loss: 2.183011054992676  Generator_loss: 1.972355842590332\n",
            "epoch: 9/10,    batch: 13065/15469    Discriminator_loss: 2.150766134262085  Generator_loss: 1.937901496887207\n",
            "epoch: 9/10,    batch: 13066/15469    Discriminator_loss: 2.122549057006836  Generator_loss: 1.9048815965652466\n",
            "epoch: 9/10,    batch: 13067/15469    Discriminator_loss: 2.086833953857422  Generator_loss: 1.8734328746795654\n",
            "epoch: 9/10,    batch: 13068/15469    Discriminator_loss: 2.0640947818756104  Generator_loss: 1.8432539701461792\n",
            "epoch: 9/10,    batch: 13069/15469    Discriminator_loss: 2.039052963256836  Generator_loss: 1.8144272565841675\n",
            "epoch: 9/10,    batch: 13070/15469    Discriminator_loss: 2.0139389038085938  Generator_loss: 1.7868268489837646\n",
            "epoch: 9/10,    batch: 13071/15469    Discriminator_loss: 1.98984956741333  Generator_loss: 1.760388970375061\n",
            "epoch: 9/10,    batch: 13072/15469    Discriminator_loss: 1.9698529243469238  Generator_loss: 1.7350246906280518\n",
            "epoch: 9/10,    batch: 13073/15469    Discriminator_loss: 1.944821834564209  Generator_loss: 1.7106349468231201\n",
            "epoch: 9/10,    batch: 13074/15469    Discriminator_loss: 1.9257452487945557  Generator_loss: 1.687239408493042\n",
            "epoch: 9/10,    batch: 13075/15469    Discriminator_loss: 1.9037812948226929  Generator_loss: 1.6647180318832397\n",
            "epoch: 9/10,    batch: 13076/15469    Discriminator_loss: 0.9934136867523193  Generator_loss: 1.645343542098999\n",
            "epoch: 9/10,    batch: 13077/15469    Discriminator_loss: 0.21640731394290924  Generator_loss: 1.6308789253234863\n",
            "epoch: 9/10,    batch: 13078/15469    Discriminator_loss: 0.21942465007305145  Generator_loss: 1.6203964948654175\n",
            "epoch: 9/10,    batch: 13079/15469    Discriminator_loss: 0.221692755818367  Generator_loss: 1.6130177974700928\n",
            "epoch: 9/10,    batch: 13080/15469    Discriminator_loss: 0.22307941317558289  Generator_loss: 1.6081591844558716\n",
            "epoch: 9/10,    batch: 13081/15469    Discriminator_loss: 0.22401362657546997  Generator_loss: 1.6054174900054932\n",
            "epoch: 9/10,    batch: 13082/15469    Discriminator_loss: 0.22448942065238953  Generator_loss: 1.6043062210083008\n",
            "epoch: 9/10,    batch: 13083/15469    Discriminator_loss: 0.22457502782344818  Generator_loss: 1.6045424938201904\n",
            "epoch: 9/10,    batch: 13084/15469    Discriminator_loss: 0.22436146438121796  Generator_loss: 1.6058764457702637\n",
            "epoch: 9/10,    batch: 13085/15469    Discriminator_loss: 0.22390605509281158  Generator_loss: 1.6080923080444336\n",
            "epoch: 9/10,    batch: 13086/15469    Discriminator_loss: 1.1618068218231201  Generator_loss: 1.608196496963501\n",
            "epoch: 9/10,    batch: 13087/15469    Discriminator_loss: 1.8485276699066162  Generator_loss: 1.604766607284546\n",
            "epoch: 9/10,    batch: 13088/15469    Discriminator_loss: 1.8476101160049438  Generator_loss: 1.5984708070755005\n",
            "epoch: 9/10,    batch: 13089/15469    Discriminator_loss: 1.8428220748901367  Generator_loss: 1.5899362564086914\n",
            "epoch: 9/10,    batch: 13090/15469    Discriminator_loss: 1.8425960540771484  Generator_loss: 1.5796246528625488\n",
            "epoch: 9/10,    batch: 13091/15469    Discriminator_loss: 0.5914971828460693  Generator_loss: 1.571241855621338\n",
            "epoch: 9/10,    batch: 13092/15469    Discriminator_loss: 0.23389405012130737  Generator_loss: 1.565614938735962\n",
            "epoch: 9/10,    batch: 13093/15469    Discriminator_loss: 0.23507685959339142  Generator_loss: 1.5622285604476929\n",
            "epoch: 9/10,    batch: 13094/15469    Discriminator_loss: 0.23576471209526062  Generator_loss: 1.5606257915496826\n",
            "epoch: 9/10,    batch: 13095/15469    Discriminator_loss: 0.2359214574098587  Generator_loss: 1.5604825019836426\n",
            "epoch: 9/10,    batch: 13096/15469    Discriminator_loss: 0.23579968512058258  Generator_loss: 1.5615124702453613\n",
            "epoch: 9/10,    batch: 13097/15469    Discriminator_loss: 0.23538894951343536  Generator_loss: 1.5635056495666504\n",
            "epoch: 9/10,    batch: 13098/15469    Discriminator_loss: 0.23473821580410004  Generator_loss: 1.5662907361984253\n",
            "epoch: 9/10,    batch: 13099/15469    Discriminator_loss: 0.2339136153459549  Generator_loss: 1.569700837135315\n",
            "epoch: 9/10,    batch: 13100/15469    Discriminator_loss: 0.23316560685634613  Generator_loss: 1.5736186504364014\n",
            "epoch: 9/10,    batch: 13101/15469    Discriminator_loss: 1.155409812927246  Generator_loss: 1.5751994848251343\n",
            "epoch: 9/10,    batch: 13102/15469    Discriminator_loss: 1.8784123659133911  Generator_loss: 1.5728139877319336\n",
            "epoch: 9/10,    batch: 13103/15469    Discriminator_loss: 1.8768911361694336  Generator_loss: 1.5672943592071533\n",
            "epoch: 9/10,    batch: 13104/15469    Discriminator_loss: 1.8681344985961914  Generator_loss: 1.5592641830444336\n",
            "epoch: 9/10,    batch: 13105/15469    Discriminator_loss: 1.869791030883789  Generator_loss: 1.5492615699768066\n",
            "epoch: 9/10,    batch: 13106/15469    Discriminator_loss: 1.892034888267517  Generator_loss: 1.537656545639038\n",
            "epoch: 9/10,    batch: 13107/15469    Discriminator_loss: 1.860213279724121  Generator_loss: 1.5248420238494873\n",
            "epoch: 9/10,    batch: 13108/15469    Discriminator_loss: 1.860140323638916  Generator_loss: 1.5100116729736328\n",
            "epoch: 9/10,    batch: 13109/15469    Discriminator_loss: 1.7849183082580566  Generator_loss: 1.4934356212615967\n",
            "epoch: 9/10,    batch: 13110/15469    Discriminator_loss: 1.034109115600586  Generator_loss: 1.4726201295852661\n",
            "epoch: 9/10,    batch: 13111/15469    Discriminator_loss: 0.47382214665412903  Generator_loss: 1.4533536434173584\n",
            "epoch: 9/10,    batch: 13112/15469    Discriminator_loss: 0.2805880010128021  Generator_loss: 1.438510775566101\n",
            "epoch: 9/10,    batch: 13113/15469    Discriminator_loss: 0.27288585901260376  Generator_loss: 1.4275872707366943\n",
            "epoch: 9/10,    batch: 13114/15469    Discriminator_loss: 0.2758050560951233  Generator_loss: 1.4197964668273926\n",
            "epoch: 9/10,    batch: 13115/15469    Discriminator_loss: 0.27785682678222656  Generator_loss: 1.4145653247833252\n",
            "epoch: 9/10,    batch: 13116/15469    Discriminator_loss: 0.27920952439308167  Generator_loss: 1.4113861322402954\n",
            "epoch: 9/10,    batch: 13117/15469    Discriminator_loss: 0.2799447774887085  Generator_loss: 1.4098999500274658\n",
            "epoch: 9/10,    batch: 13118/15469    Discriminator_loss: 0.2802038788795471  Generator_loss: 1.409735918045044\n",
            "epoch: 9/10,    batch: 13119/15469    Discriminator_loss: 0.28008097410202026  Generator_loss: 1.410789132118225\n",
            "epoch: 9/10,    batch: 13120/15469    Discriminator_loss: 0.2796168029308319  Generator_loss: 1.412567377090454\n",
            "epoch: 9/10,    batch: 13121/15469    Discriminator_loss: 0.27891358733177185  Generator_loss: 1.4151809215545654\n",
            "epoch: 9/10,    batch: 13122/15469    Discriminator_loss: 0.2779960036277771  Generator_loss: 1.4183564186096191\n",
            "epoch: 9/10,    batch: 13123/15469    Discriminator_loss: 0.27689385414123535  Generator_loss: 1.4219787120819092\n",
            "epoch: 9/10,    batch: 13124/15469    Discriminator_loss: 0.27569153904914856  Generator_loss: 1.4259587526321411\n",
            "epoch: 9/10,    batch: 13125/15469    Discriminator_loss: 0.2743586599826813  Generator_loss: 1.4302968978881836\n",
            "epoch: 9/10,    batch: 13126/15469    Discriminator_loss: 0.27297306060791016  Generator_loss: 1.4348642826080322\n",
            "epoch: 9/10,    batch: 13127/15469    Discriminator_loss: 0.271521657705307  Generator_loss: 1.4396727085113525\n",
            "epoch: 9/10,    batch: 13128/15469    Discriminator_loss: 0.2699991464614868  Generator_loss: 1.444626808166504\n",
            "epoch: 9/10,    batch: 13129/15469    Discriminator_loss: 0.2684611976146698  Generator_loss: 1.4496994018554688\n",
            "epoch: 9/10,    batch: 13130/15469    Discriminator_loss: 0.2668881416320801  Generator_loss: 1.45486581325531\n",
            "epoch: 9/10,    batch: 13131/15469    Discriminator_loss: 0.26530173420906067  Generator_loss: 1.460082769393921\n",
            "epoch: 9/10,    batch: 13132/15469    Discriminator_loss: 0.2636915147304535  Generator_loss: 1.465444803237915\n",
            "epoch: 9/10,    batch: 13133/15469    Discriminator_loss: 0.262077271938324  Generator_loss: 1.4708027839660645\n",
            "epoch: 9/10,    batch: 13134/15469    Discriminator_loss: 0.2604653537273407  Generator_loss: 1.4762769937515259\n",
            "epoch: 9/10,    batch: 13135/15469    Discriminator_loss: 0.25885462760925293  Generator_loss: 1.4817380905151367\n",
            "epoch: 9/10,    batch: 13136/15469    Discriminator_loss: 0.2572301924228668  Generator_loss: 1.487234115600586\n",
            "epoch: 9/10,    batch: 13137/15469    Discriminator_loss: 0.255617618560791  Generator_loss: 1.4927656650543213\n",
            "epoch: 9/10,    batch: 13138/15469    Discriminator_loss: 0.25400301814079285  Generator_loss: 1.4983618259429932\n",
            "epoch: 9/10,    batch: 13139/15469    Discriminator_loss: 0.2524012327194214  Generator_loss: 1.503928542137146\n",
            "epoch: 9/10,    batch: 13140/15469    Discriminator_loss: 0.25080606341362  Generator_loss: 1.509507179260254\n",
            "epoch: 9/10,    batch: 13141/15469    Discriminator_loss: 0.24920322000980377  Generator_loss: 1.515136480331421\n",
            "epoch: 9/10,    batch: 13142/15469    Discriminator_loss: 0.2476271092891693  Generator_loss: 1.520756721496582\n",
            "epoch: 9/10,    batch: 13143/15469    Discriminator_loss: 0.24604777991771698  Generator_loss: 1.5264228582382202\n",
            "epoch: 9/10,    batch: 13144/15469    Discriminator_loss: 0.2444804310798645  Generator_loss: 1.5320852994918823\n",
            "epoch: 9/10,    batch: 13145/15469    Discriminator_loss: 0.2429078221321106  Generator_loss: 1.5377604961395264\n",
            "epoch: 9/10,    batch: 13146/15469    Discriminator_loss: 0.24134834110736847  Generator_loss: 1.5434129238128662\n",
            "epoch: 9/10,    batch: 13147/15469    Discriminator_loss: 0.2398013323545456  Generator_loss: 1.5491242408752441\n",
            "epoch: 9/10,    batch: 13148/15469    Discriminator_loss: 0.2382652312517166  Generator_loss: 1.5548560619354248\n",
            "epoch: 9/10,    batch: 13149/15469    Discriminator_loss: 0.23672783374786377  Generator_loss: 1.560526728630066\n",
            "epoch: 9/10,    batch: 13150/15469    Discriminator_loss: 0.2352040708065033  Generator_loss: 1.5663172006607056\n",
            "epoch: 9/10,    batch: 13151/15469    Discriminator_loss: 0.23368708789348602  Generator_loss: 1.572070598602295\n",
            "epoch: 9/10,    batch: 13152/15469    Discriminator_loss: 0.23217205703258514  Generator_loss: 1.5778257846832275\n",
            "epoch: 9/10,    batch: 13153/15469    Discriminator_loss: 0.2306673377752304  Generator_loss: 1.5836224555969238\n",
            "epoch: 9/10,    batch: 13154/15469    Discriminator_loss: 0.22916892170906067  Generator_loss: 1.5894376039505005\n",
            "epoch: 9/10,    batch: 13155/15469    Discriminator_loss: 0.22767551243305206  Generator_loss: 1.5952627658843994\n",
            "epoch: 9/10,    batch: 13156/15469    Discriminator_loss: 0.22619080543518066  Generator_loss: 1.601095199584961\n",
            "epoch: 9/10,    batch: 13157/15469    Discriminator_loss: 0.2247086614370346  Generator_loss: 1.606975793838501\n",
            "epoch: 9/10,    batch: 13158/15469    Discriminator_loss: 0.22323264181613922  Generator_loss: 1.6128501892089844\n",
            "epoch: 9/10,    batch: 13159/15469    Discriminator_loss: 0.22175957262516022  Generator_loss: 1.6187458038330078\n",
            "epoch: 9/10,    batch: 13160/15469    Discriminator_loss: 0.22030141949653625  Generator_loss: 1.6246699094772339\n",
            "epoch: 9/10,    batch: 13161/15469    Discriminator_loss: 0.21884389221668243  Generator_loss: 1.6306428909301758\n",
            "epoch: 9/10,    batch: 13162/15469    Discriminator_loss: 0.2173866480588913  Generator_loss: 1.636614203453064\n",
            "epoch: 9/10,    batch: 13163/15469    Discriminator_loss: 0.2159356325864792  Generator_loss: 1.6425858736038208\n",
            "epoch: 9/10,    batch: 13164/15469    Discriminator_loss: 0.21448734402656555  Generator_loss: 1.6486632823944092\n",
            "epoch: 9/10,    batch: 13165/15469    Discriminator_loss: 0.2130548059940338  Generator_loss: 1.654691219329834\n",
            "epoch: 9/10,    batch: 13166/15469    Discriminator_loss: 0.2116176337003708  Generator_loss: 1.6607699394226074\n",
            "epoch: 9/10,    batch: 13167/15469    Discriminator_loss: 0.2101929932832718  Generator_loss: 1.6668920516967773\n",
            "epoch: 9/10,    batch: 13168/15469    Discriminator_loss: 0.20876489579677582  Generator_loss: 1.673017978668213\n",
            "epoch: 9/10,    batch: 13169/15469    Discriminator_loss: 0.20734138786792755  Generator_loss: 1.6791739463806152\n",
            "epoch: 9/10,    batch: 13170/15469    Discriminator_loss: 0.20592255890369415  Generator_loss: 1.6853458881378174\n",
            "epoch: 9/10,    batch: 13171/15469    Discriminator_loss: 0.20451806485652924  Generator_loss: 1.6915602684020996\n",
            "epoch: 9/10,    batch: 13172/15469    Discriminator_loss: 0.2031063735485077  Generator_loss: 1.6978017091751099\n",
            "epoch: 9/10,    batch: 13173/15469    Discriminator_loss: 0.20170043408870697  Generator_loss: 1.704045057296753\n",
            "epoch: 9/10,    batch: 13174/15469    Discriminator_loss: 0.20030565559864044  Generator_loss: 1.7103495597839355\n",
            "epoch: 9/10,    batch: 13175/15469    Discriminator_loss: 0.19891509413719177  Generator_loss: 1.7166967391967773\n",
            "epoch: 9/10,    batch: 13176/15469    Discriminator_loss: 0.19753211736679077  Generator_loss: 1.7229915857315063\n",
            "epoch: 9/10,    batch: 13177/15469    Discriminator_loss: 0.19614091515541077  Generator_loss: 1.729392409324646\n",
            "epoch: 9/10,    batch: 13178/15469    Discriminator_loss: 0.19477088749408722  Generator_loss: 1.735769271850586\n",
            "epoch: 9/10,    batch: 13179/15469    Discriminator_loss: 0.19338536262512207  Generator_loss: 1.742180585861206\n",
            "epoch: 9/10,    batch: 13180/15469    Discriminator_loss: 0.19202366471290588  Generator_loss: 1.7486591339111328\n",
            "epoch: 9/10,    batch: 13181/15469    Discriminator_loss: 0.19065918028354645  Generator_loss: 1.7551007270812988\n",
            "epoch: 9/10,    batch: 13182/15469    Discriminator_loss: 0.18929822742938995  Generator_loss: 1.761622428894043\n",
            "epoch: 9/10,    batch: 13183/15469    Discriminator_loss: 0.18794310092926025  Generator_loss: 1.7681562900543213\n",
            "epoch: 9/10,    batch: 13184/15469    Discriminator_loss: 0.18660183250904083  Generator_loss: 1.7746942043304443\n",
            "epoch: 9/10,    batch: 13185/15469    Discriminator_loss: 0.18525299429893494  Generator_loss: 1.7812718152999878\n",
            "epoch: 9/10,    batch: 13186/15469    Discriminator_loss: 0.18392059206962585  Generator_loss: 1.7879035472869873\n",
            "epoch: 9/10,    batch: 13187/15469    Discriminator_loss: 0.18259111046791077  Generator_loss: 1.7945502996444702\n",
            "epoch: 9/10,    batch: 13188/15469    Discriminator_loss: 0.1812649518251419  Generator_loss: 1.801181674003601\n",
            "epoch: 9/10,    batch: 13189/15469    Discriminator_loss: 0.1799442023038864  Generator_loss: 1.8079034090042114\n",
            "epoch: 9/10,    batch: 13190/15469    Discriminator_loss: 0.17862065136432648  Generator_loss: 1.8146313428878784\n",
            "epoch: 9/10,    batch: 13191/15469    Discriminator_loss: 0.17732129991054535  Generator_loss: 1.8213179111480713\n",
            "epoch: 9/10,    batch: 13192/15469    Discriminator_loss: 0.1760069578886032  Generator_loss: 1.8280489444732666\n",
            "epoch: 9/10,    batch: 13193/15469    Discriminator_loss: 0.17471686005592346  Generator_loss: 1.8348181247711182\n",
            "epoch: 9/10,    batch: 13194/15469    Discriminator_loss: 0.17342375218868256  Generator_loss: 1.8416732549667358\n",
            "epoch: 9/10,    batch: 13195/15469    Discriminator_loss: 0.17213305830955505  Generator_loss: 1.8484978675842285\n",
            "epoch: 9/10,    batch: 13196/15469    Discriminator_loss: 0.17086036503314972  Generator_loss: 1.8553597927093506\n",
            "epoch: 9/10,    batch: 13197/15469    Discriminator_loss: 0.1696465164422989  Generator_loss: 1.8621797561645508\n",
            "epoch: 9/10,    batch: 13198/15469    Discriminator_loss: 0.16836710274219513  Generator_loss: 1.8690996170043945\n",
            "epoch: 9/10,    batch: 13199/15469    Discriminator_loss: 0.16714052855968475  Generator_loss: 1.87601637840271\n",
            "epoch: 9/10,    batch: 13200/15469    Discriminator_loss: 0.1657976657152176  Generator_loss: 1.8829066753387451\n",
            "epoch: 9/10,    batch: 13201/15469    Discriminator_loss: 0.16455955803394318  Generator_loss: 1.8899043798446655\n",
            "epoch: 9/10,    batch: 13202/15469    Discriminator_loss: 0.1633155196905136  Generator_loss: 1.896932601928711\n",
            "epoch: 9/10,    batch: 13203/15469    Discriminator_loss: 0.16209177672863007  Generator_loss: 1.9038689136505127\n",
            "epoch: 9/10,    batch: 13204/15469    Discriminator_loss: 0.16087448596954346  Generator_loss: 1.9108881950378418\n",
            "epoch: 9/10,    batch: 13205/15469    Discriminator_loss: 0.15963760018348694  Generator_loss: 1.9179234504699707\n",
            "epoch: 9/10,    batch: 13206/15469    Discriminator_loss: 0.1584143042564392  Generator_loss: 1.9249666929244995\n",
            "epoch: 9/10,    batch: 13207/15469    Discriminator_loss: 0.1572035849094391  Generator_loss: 1.93203604221344\n",
            "epoch: 9/10,    batch: 13208/15469    Discriminator_loss: 0.15600407123565674  Generator_loss: 1.9391460418701172\n",
            "epoch: 9/10,    batch: 13209/15469    Discriminator_loss: 0.15481752157211304  Generator_loss: 1.9462089538574219\n",
            "epoch: 9/10,    batch: 13210/15469    Discriminator_loss: 0.15362417697906494  Generator_loss: 1.9533464908599854\n",
            "epoch: 9/10,    batch: 13211/15469    Discriminator_loss: 0.15245211124420166  Generator_loss: 1.9604800939559937\n",
            "epoch: 9/10,    batch: 13212/15469    Discriminator_loss: 0.15127763152122498  Generator_loss: 1.9676182270050049\n",
            "epoch: 9/10,    batch: 13213/15469    Discriminator_loss: 0.15012043714523315  Generator_loss: 1.9747655391693115\n",
            "epoch: 9/10,    batch: 13214/15469    Discriminator_loss: 0.14896367490291595  Generator_loss: 1.9819300174713135\n",
            "epoch: 9/10,    batch: 13215/15469    Discriminator_loss: 0.1478165239095688  Generator_loss: 1.9891343116760254\n",
            "epoch: 9/10,    batch: 13216/15469    Discriminator_loss: 0.14667874574661255  Generator_loss: 1.9963152408599854\n",
            "epoch: 9/10,    batch: 13217/15469    Discriminator_loss: 0.14554759860038757  Generator_loss: 2.0034964084625244\n",
            "epoch: 9/10,    batch: 13218/15469    Discriminator_loss: 0.14441633224487305  Generator_loss: 2.01070499420166\n",
            "epoch: 9/10,    batch: 13219/15469    Discriminator_loss: 0.14330419898033142  Generator_loss: 2.017925262451172\n",
            "epoch: 9/10,    batch: 13220/15469    Discriminator_loss: 0.14219722151756287  Generator_loss: 2.0251424312591553\n",
            "epoch: 9/10,    batch: 13221/15469    Discriminator_loss: 0.14109425246715546  Generator_loss: 2.032386302947998\n",
            "epoch: 9/10,    batch: 13222/15469    Discriminator_loss: 0.1400052309036255  Generator_loss: 2.039630174636841\n",
            "epoch: 9/10,    batch: 13223/15469    Discriminator_loss: 0.1389251947402954  Generator_loss: 2.046846866607666\n",
            "epoch: 9/10,    batch: 13224/15469    Discriminator_loss: 0.13784563541412354  Generator_loss: 2.0541133880615234\n",
            "epoch: 9/10,    batch: 13225/15469    Discriminator_loss: 0.13678058981895447  Generator_loss: 2.061368942260742\n",
            "epoch: 9/10,    batch: 13226/15469    Discriminator_loss: 0.13572077453136444  Generator_loss: 2.0686116218566895\n",
            "epoch: 9/10,    batch: 13227/15469    Discriminator_loss: 0.13467159867286682  Generator_loss: 2.075866460800171\n",
            "epoch: 9/10,    batch: 13228/15469    Discriminator_loss: 0.13363146781921387  Generator_loss: 2.083115339279175\n",
            "epoch: 9/10,    batch: 13229/15469    Discriminator_loss: 0.13259902596473694  Generator_loss: 2.090363025665283\n",
            "epoch: 9/10,    batch: 13230/15469    Discriminator_loss: 0.13157504796981812  Generator_loss: 2.0976314544677734\n",
            "epoch: 9/10,    batch: 13231/15469    Discriminator_loss: 0.1305578351020813  Generator_loss: 2.10488224029541\n",
            "epoch: 9/10,    batch: 13232/15469    Discriminator_loss: 0.1295527070760727  Generator_loss: 2.11212158203125\n",
            "epoch: 9/10,    batch: 13233/15469    Discriminator_loss: 0.1285538524389267  Generator_loss: 2.119372844696045\n",
            "epoch: 9/10,    batch: 13234/15469    Discriminator_loss: 0.12756451964378357  Generator_loss: 2.126615047454834\n",
            "epoch: 9/10,    batch: 13235/15469    Discriminator_loss: 0.1265832781791687  Generator_loss: 2.1338584423065186\n",
            "epoch: 9/10,    batch: 13236/15469    Discriminator_loss: 0.12561066448688507  Generator_loss: 2.1410958766937256\n",
            "epoch: 9/10,    batch: 13237/15469    Discriminator_loss: 0.12464724481105804  Generator_loss: 2.148308515548706\n",
            "epoch: 9/10,    batch: 13238/15469    Discriminator_loss: 0.12369272112846375  Generator_loss: 2.155534267425537\n",
            "epoch: 9/10,    batch: 13239/15469    Discriminator_loss: 0.12274728715419769  Generator_loss: 2.162743091583252\n",
            "epoch: 9/10,    batch: 13240/15469    Discriminator_loss: 0.12181069701910019  Generator_loss: 2.1699464321136475\n",
            "epoch: 9/10,    batch: 13241/15469    Discriminator_loss: 0.12088128924369812  Generator_loss: 2.177124500274658\n",
            "epoch: 9/10,    batch: 13242/15469    Discriminator_loss: 0.11996276676654816  Generator_loss: 2.184330463409424\n",
            "epoch: 9/10,    batch: 13243/15469    Discriminator_loss: 0.11904928088188171  Generator_loss: 2.191500663757324\n",
            "epoch: 9/10,    batch: 13244/15469    Discriminator_loss: 0.11814944446086884  Generator_loss: 2.1986634731292725\n",
            "epoch: 9/10,    batch: 13245/15469    Discriminator_loss: 0.11725496500730515  Generator_loss: 2.2058119773864746\n",
            "epoch: 9/10,    batch: 13246/15469    Discriminator_loss: 0.11636777222156525  Generator_loss: 2.2129671573638916\n",
            "epoch: 9/10,    batch: 13247/15469    Discriminator_loss: 0.11549092829227448  Generator_loss: 2.2200927734375\n",
            "epoch: 9/10,    batch: 13248/15469    Discriminator_loss: 0.11462123692035675  Generator_loss: 2.2271952629089355\n",
            "epoch: 9/10,    batch: 13249/15469    Discriminator_loss: 0.11376134306192398  Generator_loss: 2.234299659729004\n",
            "epoch: 9/10,    batch: 13250/15469    Discriminator_loss: 0.11290908604860306  Generator_loss: 2.2414462566375732\n",
            "epoch: 9/10,    batch: 13251/15469    Discriminator_loss: 0.11206534504890442  Generator_loss: 2.2485156059265137\n",
            "epoch: 9/10,    batch: 13252/15469    Discriminator_loss: 0.11122617870569229  Generator_loss: 2.2555813789367676\n",
            "epoch: 9/10,    batch: 13253/15469    Discriminator_loss: 0.11040079593658447  Generator_loss: 2.2626569271087646\n",
            "epoch: 9/10,    batch: 13254/15469    Discriminator_loss: 0.10958222299814224  Generator_loss: 2.269683361053467\n",
            "epoch: 9/10,    batch: 13255/15469    Discriminator_loss: 0.10876752436161041  Generator_loss: 2.276731014251709\n",
            "epoch: 9/10,    batch: 13256/15469    Discriminator_loss: 0.10796148329973221  Generator_loss: 2.2837414741516113\n",
            "epoch: 9/10,    batch: 13257/15469    Discriminator_loss: 0.10716460645198822  Generator_loss: 2.2907464504241943\n",
            "epoch: 9/10,    batch: 13258/15469    Discriminator_loss: 0.10637731850147247  Generator_loss: 2.297797679901123\n",
            "epoch: 9/10,    batch: 13259/15469    Discriminator_loss: 0.105591781437397  Generator_loss: 2.3047661781311035\n",
            "epoch: 9/10,    batch: 13260/15469    Discriminator_loss: 0.10481766611337662  Generator_loss: 2.311732769012451\n",
            "epoch: 9/10,    batch: 13261/15469    Discriminator_loss: 0.10404961556196213  Generator_loss: 2.318718433380127\n",
            "epoch: 9/10,    batch: 13262/15469    Discriminator_loss: 0.10329059511423111  Generator_loss: 2.3256750106811523\n",
            "epoch: 9/10,    batch: 13263/15469    Discriminator_loss: 0.10253380239009857  Generator_loss: 2.332665205001831\n",
            "epoch: 9/10,    batch: 13264/15469    Discriminator_loss: 0.1017899140715599  Generator_loss: 2.3395581245422363\n",
            "epoch: 9/10,    batch: 13265/15469    Discriminator_loss: 0.10104583948850632  Generator_loss: 2.346475601196289\n",
            "epoch: 9/10,    batch: 13266/15469    Discriminator_loss: 0.1003149151802063  Generator_loss: 2.3534114360809326\n",
            "epoch: 9/10,    batch: 13267/15469    Discriminator_loss: 0.0995919406414032  Generator_loss: 2.3603010177612305\n",
            "epoch: 9/10,    batch: 13268/15469    Discriminator_loss: 0.09887215495109558  Generator_loss: 2.3672053813934326\n",
            "epoch: 9/10,    batch: 13269/15469    Discriminator_loss: 0.09815523773431778  Generator_loss: 2.3740956783294678\n",
            "epoch: 9/10,    batch: 13270/15469    Discriminator_loss: 0.0974510982632637  Generator_loss: 2.3809971809387207\n",
            "epoch: 9/10,    batch: 13271/15469    Discriminator_loss: 0.0967491865158081  Generator_loss: 2.387843132019043\n",
            "epoch: 9/10,    batch: 13272/15469    Discriminator_loss: 0.09605568647384644  Generator_loss: 2.3946585655212402\n",
            "epoch: 9/10,    batch: 13273/15469    Discriminator_loss: 0.09536541253328323  Generator_loss: 2.401535749435425\n",
            "epoch: 9/10,    batch: 13274/15469    Discriminator_loss: 0.09468484669923782  Generator_loss: 2.4083690643310547\n",
            "epoch: 9/10,    batch: 13275/15469    Discriminator_loss: 0.09401068836450577  Generator_loss: 2.4151837825775146\n",
            "epoch: 9/10,    batch: 13276/15469    Discriminator_loss: 0.09334313869476318  Generator_loss: 2.42204213142395\n",
            "epoch: 9/10,    batch: 13277/15469    Discriminator_loss: 0.09268008917570114  Generator_loss: 2.4287638664245605\n",
            "epoch: 9/10,    batch: 13278/15469    Discriminator_loss: 0.09202326834201813  Generator_loss: 2.435558795928955\n",
            "epoch: 9/10,    batch: 13279/15469    Discriminator_loss: 0.09137159585952759  Generator_loss: 2.442326068878174\n",
            "epoch: 9/10,    batch: 13280/15469    Discriminator_loss: 0.09072361886501312  Generator_loss: 2.4491376876831055\n",
            "epoch: 9/10,    batch: 13281/15469    Discriminator_loss: 0.09008881449699402  Generator_loss: 2.455864191055298\n",
            "epoch: 9/10,    batch: 13282/15469    Discriminator_loss: 0.08945561945438385  Generator_loss: 2.46258544921875\n",
            "epoch: 9/10,    batch: 13283/15469    Discriminator_loss: 0.08882320672273636  Generator_loss: 2.4693169593811035\n",
            "epoch: 9/10,    batch: 13284/15469    Discriminator_loss: 0.08820172399282455  Generator_loss: 2.476012706756592\n",
            "epoch: 9/10,    batch: 13285/15469    Discriminator_loss: 0.08758829534053802  Generator_loss: 2.4826788902282715\n",
            "epoch: 9/10,    batch: 13286/15469    Discriminator_loss: 0.08697937428951263  Generator_loss: 2.489400863647461\n",
            "epoch: 9/10,    batch: 13287/15469    Discriminator_loss: 0.08637960255146027  Generator_loss: 2.49603533744812\n",
            "epoch: 9/10,    batch: 13288/15469    Discriminator_loss: 0.08577991276979446  Generator_loss: 2.5026936531066895\n",
            "epoch: 9/10,    batch: 13289/15469    Discriminator_loss: 0.08518868684768677  Generator_loss: 2.509305477142334\n",
            "epoch: 9/10,    batch: 13290/15469    Discriminator_loss: 0.08459945768117905  Generator_loss: 2.5159270763397217\n",
            "epoch: 9/10,    batch: 13291/15469    Discriminator_loss: 0.08402270823717117  Generator_loss: 2.5225472450256348\n",
            "epoch: 9/10,    batch: 13292/15469    Discriminator_loss: 0.08344531059265137  Generator_loss: 2.529055118560791\n",
            "epoch: 9/10,    batch: 13293/15469    Discriminator_loss: 0.08287414163351059  Generator_loss: 2.5356197357177734\n",
            "epoch: 9/10,    batch: 13294/15469    Discriminator_loss: 0.08231274783611298  Generator_loss: 2.5421526432037354\n",
            "epoch: 9/10,    batch: 13295/15469    Discriminator_loss: 0.08175656199455261  Generator_loss: 2.548671245574951\n",
            "epoch: 9/10,    batch: 13296/15469    Discriminator_loss: 0.08120691776275635  Generator_loss: 2.55517315864563\n",
            "epoch: 9/10,    batch: 13297/15469    Discriminator_loss: 0.08065466582775116  Generator_loss: 2.5616085529327393\n",
            "epoch: 9/10,    batch: 13298/15469    Discriminator_loss: 0.08011879026889801  Generator_loss: 2.5680432319641113\n",
            "epoch: 9/10,    batch: 13299/15469    Discriminator_loss: 0.07958780974149704  Generator_loss: 2.57444167137146\n",
            "epoch: 9/10,    batch: 13300/15469    Discriminator_loss: 0.07905852049589157  Generator_loss: 2.580862522125244\n",
            "epoch: 9/10,    batch: 13301/15469    Discriminator_loss: 0.07853500545024872  Generator_loss: 2.587203025817871\n",
            "epoch: 9/10,    batch: 13302/15469    Discriminator_loss: 0.07801859825849533  Generator_loss: 2.593543529510498\n",
            "epoch: 9/10,    batch: 13303/15469    Discriminator_loss: 0.07750682532787323  Generator_loss: 2.5997958183288574\n",
            "epoch: 9/10,    batch: 13304/15469    Discriminator_loss: 0.07699992507696152  Generator_loss: 2.606131076812744\n",
            "epoch: 9/10,    batch: 13305/15469    Discriminator_loss: 0.07649961858987808  Generator_loss: 2.6123409271240234\n",
            "epoch: 9/10,    batch: 13306/15469    Discriminator_loss: 0.07600723206996918  Generator_loss: 2.618666648864746\n",
            "epoch: 9/10,    batch: 13307/15469    Discriminator_loss: 0.0755191445350647  Generator_loss: 2.6248016357421875\n",
            "epoch: 9/10,    batch: 13308/15469    Discriminator_loss: 0.07503460347652435  Generator_loss: 2.6310200691223145\n",
            "epoch: 9/10,    batch: 13309/15469    Discriminator_loss: 0.07455497980117798  Generator_loss: 2.637157440185547\n",
            "epoch: 9/10,    batch: 13310/15469    Discriminator_loss: 0.07408034801483154  Generator_loss: 2.6432700157165527\n",
            "epoch: 9/10,    batch: 13311/15469    Discriminator_loss: 0.07361489534378052  Generator_loss: 2.649360179901123\n",
            "epoch: 9/10,    batch: 13312/15469    Discriminator_loss: 0.07314981520175934  Generator_loss: 2.6553921699523926\n",
            "epoch: 9/10,    batch: 13313/15469    Discriminator_loss: 0.07269555330276489  Generator_loss: 2.6614537239074707\n",
            "epoch: 9/10,    batch: 13314/15469    Discriminator_loss: 0.07224196940660477  Generator_loss: 2.6674399375915527\n",
            "epoch: 9/10,    batch: 13315/15469    Discriminator_loss: 0.07179742306470871  Generator_loss: 2.673417091369629\n",
            "epoch: 9/10,    batch: 13316/15469    Discriminator_loss: 0.07135625928640366  Generator_loss: 2.6793158054351807\n",
            "epoch: 9/10,    batch: 13317/15469    Discriminator_loss: 0.07092059403657913  Generator_loss: 2.685251235961914\n",
            "epoch: 9/10,    batch: 13318/15469    Discriminator_loss: 0.0704881027340889  Generator_loss: 2.691124677658081\n",
            "epoch: 9/10,    batch: 13319/15469    Discriminator_loss: 0.0700608342885971  Generator_loss: 2.696989059448242\n",
            "epoch: 9/10,    batch: 13320/15469    Discriminator_loss: 0.06963731348514557  Generator_loss: 2.7027344703674316\n",
            "epoch: 9/10,    batch: 13321/15469    Discriminator_loss: 0.06922239810228348  Generator_loss: 2.708563804626465\n",
            "epoch: 9/10,    batch: 13322/15469    Discriminator_loss: 0.06881003081798553  Generator_loss: 2.714353322982788\n",
            "epoch: 9/10,    batch: 13323/15469    Discriminator_loss: 0.06840285658836365  Generator_loss: 2.720043182373047\n",
            "epoch: 9/10,    batch: 13324/15469    Discriminator_loss: 0.06800281256437302  Generator_loss: 2.72576904296875\n",
            "epoch: 9/10,    batch: 13325/15469    Discriminator_loss: 0.06760456413030624  Generator_loss: 2.731433391571045\n",
            "epoch: 9/10,    batch: 13326/15469    Discriminator_loss: 0.06720968335866928  Generator_loss: 2.737064838409424\n",
            "epoch: 9/10,    batch: 13327/15469    Discriminator_loss: 0.06682081520557404  Generator_loss: 2.7426483631134033\n",
            "epoch: 9/10,    batch: 13328/15469    Discriminator_loss: 0.06643540412187576  Generator_loss: 2.7482495307922363\n",
            "epoch: 9/10,    batch: 13329/15469    Discriminator_loss: 0.06605187058448792  Generator_loss: 2.7538094520568848\n",
            "epoch: 9/10,    batch: 13330/15469    Discriminator_loss: 0.0656762346625328  Generator_loss: 2.7592997550964355\n",
            "epoch: 9/10,    batch: 13331/15469    Discriminator_loss: 0.06530122458934784  Generator_loss: 2.7648048400878906\n",
            "epoch: 9/10,    batch: 13332/15469    Discriminator_loss: 0.06493406742811203  Generator_loss: 2.7703018188476562\n",
            "epoch: 9/10,    batch: 13333/15469    Discriminator_loss: 0.06457094103097916  Generator_loss: 2.775754928588867\n",
            "epoch: 9/10,    batch: 13334/15469    Discriminator_loss: 0.06420900672674179  Generator_loss: 2.7811429500579834\n",
            "epoch: 9/10,    batch: 13335/15469    Discriminator_loss: 0.06385234743356705  Generator_loss: 2.786569356918335\n",
            "epoch: 9/10,    batch: 13336/15469    Discriminator_loss: 0.06349875777959824  Generator_loss: 2.7919363975524902\n",
            "epoch: 9/10,    batch: 13337/15469    Discriminator_loss: 0.06314855813980103  Generator_loss: 2.79727840423584\n",
            "epoch: 9/10,    batch: 13338/15469    Discriminator_loss: 0.06280197948217392  Generator_loss: 2.8026437759399414\n",
            "epoch: 9/10,    batch: 13339/15469    Discriminator_loss: 0.062457375228405  Generator_loss: 2.8079190254211426\n",
            "epoch: 9/10,    batch: 13340/15469    Discriminator_loss: 0.06211784854531288  Generator_loss: 2.8132288455963135\n",
            "epoch: 9/10,    batch: 13341/15469    Discriminator_loss: 0.06177925691008568  Generator_loss: 2.8184666633605957\n",
            "epoch: 9/10,    batch: 13342/15469    Discriminator_loss: 0.061445608735084534  Generator_loss: 2.823737144470215\n",
            "epoch: 9/10,    batch: 13343/15469    Discriminator_loss: 0.06111333146691322  Generator_loss: 2.828991413116455\n",
            "epoch: 9/10,    batch: 13344/15469    Discriminator_loss: 0.06078484654426575  Generator_loss: 2.8342270851135254\n",
            "epoch: 9/10,    batch: 13345/15469    Discriminator_loss: 0.06045881658792496  Generator_loss: 2.8394455909729004\n",
            "epoch: 9/10,    batch: 13346/15469    Discriminator_loss: 0.06013519689440727  Generator_loss: 2.8445940017700195\n",
            "epoch: 9/10,    batch: 13347/15469    Discriminator_loss: 0.05981419235467911  Generator_loss: 2.8498082160949707\n",
            "epoch: 9/10,    batch: 13348/15469    Discriminator_loss: 0.059496957808732986  Generator_loss: 2.854968547821045\n",
            "epoch: 9/10,    batch: 13349/15469    Discriminator_loss: 0.059180986136198044  Generator_loss: 2.860125780105591\n",
            "epoch: 9/10,    batch: 13350/15469    Discriminator_loss: 0.05886926129460335  Generator_loss: 2.8652753829956055\n",
            "epoch: 9/10,    batch: 13351/15469    Discriminator_loss: 0.05855855345726013  Generator_loss: 2.8703906536102295\n",
            "epoch: 9/10,    batch: 13352/15469    Discriminator_loss: 0.058252494782209396  Generator_loss: 2.875502824783325\n",
            "epoch: 9/10,    batch: 13353/15469    Discriminator_loss: 0.05794679746031761  Generator_loss: 2.8805952072143555\n",
            "epoch: 9/10,    batch: 13354/15469    Discriminator_loss: 0.057644713670015335  Generator_loss: 2.8856780529022217\n",
            "epoch: 9/10,    batch: 13355/15469    Discriminator_loss: 0.05734443664550781  Generator_loss: 2.8907456398010254\n",
            "epoch: 9/10,    batch: 13356/15469    Discriminator_loss: 0.057049497961997986  Generator_loss: 2.8957479000091553\n",
            "epoch: 9/10,    batch: 13357/15469    Discriminator_loss: 0.0567534863948822  Generator_loss: 2.9007794857025146\n",
            "epoch: 9/10,    batch: 13358/15469    Discriminator_loss: 0.05670047178864479  Generator_loss: 2.9057812690734863\n",
            "epoch: 9/10,    batch: 13359/15469    Discriminator_loss: 0.056405115872621536  Generator_loss: 2.9107913970947266\n",
            "epoch: 9/10,    batch: 13360/15469    Discriminator_loss: 0.05613956227898598  Generator_loss: 2.915740489959717\n",
            "epoch: 9/10,    batch: 13361/15469    Discriminator_loss: 0.05560311675071716  Generator_loss: 2.9206864833831787\n",
            "epoch: 9/10,    batch: 13362/15469    Discriminator_loss: 0.05533714219927788  Generator_loss: 2.92561411857605\n",
            "epoch: 9/10,    batch: 13363/15469    Discriminator_loss: 0.05505572259426117  Generator_loss: 2.930532932281494\n",
            "epoch: 9/10,    batch: 13364/15469    Discriminator_loss: 0.05477400869131088  Generator_loss: 2.935424327850342\n",
            "epoch: 9/10,    batch: 13365/15469    Discriminator_loss: 0.054492346942424774  Generator_loss: 2.9402968883514404\n",
            "epoch: 9/10,    batch: 13366/15469    Discriminator_loss: 0.05422098562121391  Generator_loss: 2.945161819458008\n",
            "epoch: 9/10,    batch: 13367/15469    Discriminator_loss: 1.2771291732788086  Generator_loss: 2.941704034805298\n",
            "epoch: 9/10,    batch: 13368/15469    Discriminator_loss: 3.2090210914611816  Generator_loss: 2.9196529388427734\n",
            "epoch: 9/10,    batch: 13369/15469    Discriminator_loss: 3.177351713180542  Generator_loss: 2.884951114654541\n",
            "epoch: 9/10,    batch: 13370/15469    Discriminator_loss: 3.1443498134613037  Generator_loss: 2.8420259952545166\n",
            "epoch: 9/10,    batch: 13371/15469    Discriminator_loss: 3.0725057125091553  Generator_loss: 2.793840169906616\n",
            "epoch: 9/10,    batch: 13372/15469    Discriminator_loss: 3.027601957321167  Generator_loss: 2.7426233291625977\n",
            "epoch: 9/10,    batch: 13373/15469    Discriminator_loss: 2.9482083320617676  Generator_loss: 2.689868450164795\n",
            "epoch: 9/10,    batch: 13374/15469    Discriminator_loss: 2.865097761154175  Generator_loss: 2.636533260345459\n",
            "epoch: 9/10,    batch: 13375/15469    Discriminator_loss: 2.7940731048583984  Generator_loss: 2.583214521408081\n",
            "epoch: 9/10,    batch: 13376/15469    Discriminator_loss: 2.7249298095703125  Generator_loss: 2.5305447578430176\n",
            "epoch: 9/10,    batch: 13377/15469    Discriminator_loss: 2.6541202068328857  Generator_loss: 2.4787840843200684\n",
            "epoch: 9/10,    batch: 13378/15469    Discriminator_loss: 2.607619524002075  Generator_loss: 2.428133726119995\n",
            "epoch: 9/10,    batch: 13379/15469    Discriminator_loss: 2.537773370742798  Generator_loss: 2.3785314559936523\n",
            "epoch: 9/10,    batch: 13380/15469    Discriminator_loss: 2.489471912384033  Generator_loss: 2.3301103115081787\n",
            "epoch: 9/10,    batch: 13381/15469    Discriminator_loss: 2.4294826984405518  Generator_loss: 2.282886266708374\n",
            "epoch: 9/10,    batch: 13382/15469    Discriminator_loss: 2.377798557281494  Generator_loss: 2.236823081970215\n",
            "epoch: 9/10,    batch: 13383/15469    Discriminator_loss: 2.3364763259887695  Generator_loss: 2.191936492919922\n",
            "epoch: 9/10,    batch: 13384/15469    Discriminator_loss: 2.2885758876800537  Generator_loss: 2.1482138633728027\n",
            "epoch: 9/10,    batch: 13385/15469    Discriminator_loss: 2.24090313911438  Generator_loss: 2.10579514503479\n",
            "epoch: 9/10,    batch: 13386/15469    Discriminator_loss: 2.198667526245117  Generator_loss: 2.064417839050293\n",
            "epoch: 9/10,    batch: 13387/15469    Discriminator_loss: 2.1446382999420166  Generator_loss: 2.023861885070801\n",
            "epoch: 9/10,    batch: 13388/15469    Discriminator_loss: 2.11879563331604  Generator_loss: 1.9844567775726318\n",
            "epoch: 9/10,    batch: 13389/15469    Discriminator_loss: 2.0703649520874023  Generator_loss: 1.9459129571914673\n",
            "epoch: 9/10,    batch: 13390/15469    Discriminator_loss: 2.0321121215820312  Generator_loss: 1.9084181785583496\n",
            "epoch: 9/10,    batch: 13391/15469    Discriminator_loss: 1.9866727590560913  Generator_loss: 1.8716895580291748\n",
            "epoch: 9/10,    batch: 13392/15469    Discriminator_loss: 1.841437816619873  Generator_loss: 1.8327807188034058\n",
            "epoch: 9/10,    batch: 13393/15469    Discriminator_loss: 1.8687188625335693  Generator_loss: 1.7932159900665283\n",
            "epoch: 9/10,    batch: 13394/15469    Discriminator_loss: 1.4773006439208984  Generator_loss: 1.7534761428833008\n",
            "epoch: 9/10,    batch: 13395/15469    Discriminator_loss: 1.2230513095855713  Generator_loss: 1.7132818698883057\n",
            "epoch: 9/10,    batch: 13396/15469    Discriminator_loss: 0.8899675607681274  Generator_loss: 1.6710222959518433\n",
            "epoch: 9/10,    batch: 13397/15469    Discriminator_loss: 0.9264501333236694  Generator_loss: 1.6204702854156494\n",
            "epoch: 9/10,    batch: 13398/15469    Discriminator_loss: 0.2854843735694885  Generator_loss: 1.5363248586654663\n",
            "epoch: 9/10,    batch: 13399/15469    Discriminator_loss: 0.3143749535083771  Generator_loss: 1.1600149869918823\n",
            "epoch: 9/10,    batch: 13400/15469    Discriminator_loss: 7.9419732093811035  Generator_loss: 0.025119785219430923\n",
            "epoch: 9/10,    batch: 13401/15469    Discriminator_loss: 6.583528995513916  Generator_loss: 0.18072441220283508\n",
            "epoch: 9/10,    batch: 13402/15469    Discriminator_loss: 3.6090242862701416  Generator_loss: 0.6546328067779541\n",
            "epoch: 9/10,    batch: 13403/15469    Discriminator_loss: 1.2484232187271118  Generator_loss: 0.9919509887695312\n",
            "epoch: 9/10,    batch: 13404/15469    Discriminator_loss: 0.6113740801811218  Generator_loss: 1.1419754028320312\n",
            "epoch: 9/10,    batch: 13405/15469    Discriminator_loss: 0.4525279700756073  Generator_loss: 1.1737655401229858\n",
            "epoch: 9/10,    batch: 13406/15469    Discriminator_loss: 0.3835933804512024  Generator_loss: 1.206337571144104\n",
            "epoch: 9/10,    batch: 13407/15469    Discriminator_loss: 0.34421852231025696  Generator_loss: 1.2610225677490234\n",
            "epoch: 9/10,    batch: 13408/15469    Discriminator_loss: 0.3277321755886078  Generator_loss: 1.3097078800201416\n",
            "epoch: 9/10,    batch: 13409/15469    Discriminator_loss: 0.3181580901145935  Generator_loss: 1.364884853363037\n",
            "epoch: 9/10,    batch: 13410/15469    Discriminator_loss: 0.2930157482624054  Generator_loss: 1.4603066444396973\n",
            "epoch: 9/10,    batch: 13411/15469    Discriminator_loss: 0.2536582350730896  Generator_loss: 1.5960625410079956\n",
            "epoch: 9/10,    batch: 13412/15469    Discriminator_loss: 0.39628392457962036  Generator_loss: 1.7413479089736938\n",
            "epoch: 9/10,    batch: 13413/15469    Discriminator_loss: 1.8534300327301025  Generator_loss: 1.8500547409057617\n",
            "epoch: 9/10,    batch: 13414/15469    Discriminator_loss: 1.779613971710205  Generator_loss: 1.901294469833374\n",
            "epoch: 9/10,    batch: 13415/15469    Discriminator_loss: 1.8441942930221558  Generator_loss: 1.9117045402526855\n",
            "epoch: 9/10,    batch: 13416/15469    Discriminator_loss: 1.8408026695251465  Generator_loss: 1.8883426189422607\n",
            "epoch: 9/10,    batch: 13417/15469    Discriminator_loss: 1.8287532329559326  Generator_loss: 1.8301944732666016\n",
            "epoch: 9/10,    batch: 13418/15469    Discriminator_loss: 1.7938201427459717  Generator_loss: 1.7365498542785645\n",
            "epoch: 9/10,    batch: 13419/15469    Discriminator_loss: 1.7064943313598633  Generator_loss: 1.6402333974838257\n",
            "epoch: 9/10,    batch: 13420/15469    Discriminator_loss: 1.5513267517089844  Generator_loss: 1.5319771766662598\n",
            "epoch: 9/10,    batch: 13421/15469    Discriminator_loss: 1.576147198677063  Generator_loss: 1.4315136671066284\n",
            "epoch: 9/10,    batch: 13422/15469    Discriminator_loss: 1.6608312129974365  Generator_loss: 1.3386924266815186\n",
            "epoch: 9/10,    batch: 13423/15469    Discriminator_loss: 1.8357207775115967  Generator_loss: 1.252122163772583\n",
            "epoch: 9/10,    batch: 13424/15469    Discriminator_loss: 1.7226030826568604  Generator_loss: 1.1752963066101074\n",
            "epoch: 9/10,    batch: 13425/15469    Discriminator_loss: 1.6820989847183228  Generator_loss: 1.0920305252075195\n",
            "epoch: 9/10,    batch: 13426/15469    Discriminator_loss: 1.763533592224121  Generator_loss: 1.0063848495483398\n",
            "epoch: 9/10,    batch: 13427/15469    Discriminator_loss: 1.8084826469421387  Generator_loss: 0.931153416633606\n",
            "epoch: 9/10,    batch: 13428/15469    Discriminator_loss: 1.8770707845687866  Generator_loss: 0.8825949430465698\n",
            "epoch: 9/10,    batch: 13429/15469    Discriminator_loss: 1.8856382369995117  Generator_loss: 0.8997336030006409\n",
            "epoch: 9/10,    batch: 13430/15469    Discriminator_loss: 1.9230130910873413  Generator_loss: 1.032511591911316\n",
            "epoch: 9/10,    batch: 13431/15469    Discriminator_loss: 1.7291812896728516  Generator_loss: 1.2876566648483276\n",
            "epoch: 9/10,    batch: 13432/15469    Discriminator_loss: 1.5165555477142334  Generator_loss: 1.6176328659057617\n",
            "epoch: 9/10,    batch: 13433/15469    Discriminator_loss: 1.4376434087753296  Generator_loss: 1.939312219619751\n",
            "epoch: 9/10,    batch: 13434/15469    Discriminator_loss: 1.3403993844985962  Generator_loss: 2.167311191558838\n",
            "epoch: 9/10,    batch: 13435/15469    Discriminator_loss: 1.274526596069336  Generator_loss: 2.2321293354034424\n",
            "epoch: 9/10,    batch: 13436/15469    Discriminator_loss: 1.2453638315200806  Generator_loss: 2.1760218143463135\n",
            "epoch: 9/10,    batch: 13437/15469    Discriminator_loss: 1.2847027778625488  Generator_loss: 2.0532684326171875\n",
            "epoch: 9/10,    batch: 13438/15469    Discriminator_loss: 1.2356675863265991  Generator_loss: 1.9406123161315918\n",
            "epoch: 9/10,    batch: 13439/15469    Discriminator_loss: 1.234984040260315  Generator_loss: 1.8625543117523193\n",
            "epoch: 9/10,    batch: 13440/15469    Discriminator_loss: 1.2020628452301025  Generator_loss: 1.8105659484863281\n",
            "epoch: 9/10,    batch: 13441/15469    Discriminator_loss: 1.1878057718276978  Generator_loss: 1.7702667713165283\n",
            "epoch: 9/10,    batch: 13442/15469    Discriminator_loss: 1.1150429248809814  Generator_loss: 1.7348613739013672\n",
            "epoch: 9/10,    batch: 13443/15469    Discriminator_loss: 1.044740915298462  Generator_loss: 1.7022687196731567\n",
            "epoch: 9/10,    batch: 13444/15469    Discriminator_loss: 0.9787535667419434  Generator_loss: 1.6722915172576904\n",
            "epoch: 9/10,    batch: 13445/15469    Discriminator_loss: 0.9017245769500732  Generator_loss: 1.6451573371887207\n",
            "epoch: 9/10,    batch: 13446/15469    Discriminator_loss: 0.8020912408828735  Generator_loss: 1.6206281185150146\n",
            "epoch: 9/10,    batch: 13447/15469    Discriminator_loss: 0.697779655456543  Generator_loss: 1.5983572006225586\n",
            "epoch: 9/10,    batch: 13448/15469    Discriminator_loss: 0.5671056509017944  Generator_loss: 1.5791242122650146\n",
            "epoch: 9/10,    batch: 13449/15469    Discriminator_loss: 0.39651262760162354  Generator_loss: 1.5627235174179077\n",
            "epoch: 9/10,    batch: 13450/15469    Discriminator_loss: 0.3135283291339874  Generator_loss: 1.5504493713378906\n",
            "epoch: 9/10,    batch: 13451/15469    Discriminator_loss: 0.2888815402984619  Generator_loss: 1.5420193672180176\n",
            "epoch: 9/10,    batch: 13452/15469    Discriminator_loss: 0.2944169342517853  Generator_loss: 1.5364487171173096\n",
            "epoch: 9/10,    batch: 13453/15469    Discriminator_loss: 0.2592807412147522  Generator_loss: 1.5345070362091064\n",
            "epoch: 9/10,    batch: 13454/15469    Discriminator_loss: 0.2569591999053955  Generator_loss: 1.5358197689056396\n",
            "epoch: 9/10,    batch: 13455/15469    Discriminator_loss: 0.2527363896369934  Generator_loss: 1.5401586294174194\n",
            "epoch: 9/10,    batch: 13456/15469    Discriminator_loss: 0.2498803287744522  Generator_loss: 1.5477335453033447\n",
            "epoch: 9/10,    batch: 13457/15469    Discriminator_loss: 0.24707560241222382  Generator_loss: 1.5574946403503418\n",
            "epoch: 9/10,    batch: 13458/15469    Discriminator_loss: 0.24329057335853577  Generator_loss: 1.5696485042572021\n",
            "epoch: 9/10,    batch: 13459/15469    Discriminator_loss: 0.23942719399929047  Generator_loss: 1.584101915359497\n",
            "epoch: 9/10,    batch: 13460/15469    Discriminator_loss: 0.23527303338050842  Generator_loss: 1.600598692893982\n",
            "epoch: 9/10,    batch: 13461/15469    Discriminator_loss: 0.2305421084165573  Generator_loss: 1.6183750629425049\n",
            "epoch: 9/10,    batch: 13462/15469    Discriminator_loss: 0.22558030486106873  Generator_loss: 1.6384621858596802\n",
            "epoch: 9/10,    batch: 13463/15469    Discriminator_loss: 0.22038134932518005  Generator_loss: 1.6597062349319458\n",
            "epoch: 9/10,    batch: 13464/15469    Discriminator_loss: 0.2148628532886505  Generator_loss: 1.6829358339309692\n",
            "epoch: 9/10,    batch: 13465/15469    Discriminator_loss: 0.20930072665214539  Generator_loss: 1.7068665027618408\n",
            "epoch: 9/10,    batch: 13466/15469    Discriminator_loss: 0.2038373500108719  Generator_loss: 1.7322230339050293\n",
            "epoch: 9/10,    batch: 13467/15469    Discriminator_loss: 0.19809648394584656  Generator_loss: 1.7576758861541748\n",
            "epoch: 9/10,    batch: 13468/15469    Discriminator_loss: 0.19258633255958557  Generator_loss: 1.7832558155059814\n",
            "epoch: 9/10,    batch: 13469/15469    Discriminator_loss: 0.1872713714838028  Generator_loss: 1.8088327646255493\n",
            "epoch: 9/10,    batch: 13470/15469    Discriminator_loss: 0.18233995139598846  Generator_loss: 1.8334195613861084\n",
            "epoch: 9/10,    batch: 13471/15469    Discriminator_loss: 0.17763932049274445  Generator_loss: 1.8570488691329956\n",
            "epoch: 9/10,    batch: 13472/15469    Discriminator_loss: 0.17318107187747955  Generator_loss: 1.8786249160766602\n",
            "epoch: 9/10,    batch: 13473/15469    Discriminator_loss: 0.16900122165679932  Generator_loss: 1.8986003398895264\n",
            "epoch: 9/10,    batch: 13474/15469    Discriminator_loss: 0.16573666036128998  Generator_loss: 1.9158105850219727\n",
            "epoch: 9/10,    batch: 13475/15469    Discriminator_loss: 0.16295978426933289  Generator_loss: 1.9312398433685303\n",
            "epoch: 9/10,    batch: 13476/15469    Discriminator_loss: 0.16067704558372498  Generator_loss: 1.944101095199585\n",
            "epoch: 9/10,    batch: 13477/15469    Discriminator_loss: 0.1588171273469925  Generator_loss: 1.954280138015747\n",
            "epoch: 9/10,    batch: 13478/15469    Discriminator_loss: 0.15745912492275238  Generator_loss: 1.9630744457244873\n",
            "epoch: 9/10,    batch: 13479/15469    Discriminator_loss: 0.15628944337368011  Generator_loss: 1.9696029424667358\n",
            "epoch: 9/10,    batch: 13480/15469    Discriminator_loss: 0.15552613139152527  Generator_loss: 1.9745908975601196\n",
            "epoch: 9/10,    batch: 13481/15469    Discriminator_loss: 0.15493108332157135  Generator_loss: 1.9782465696334839\n",
            "epoch: 9/10,    batch: 13482/15469    Discriminator_loss: 0.15432670712471008  Generator_loss: 1.9819543361663818\n",
            "epoch: 9/10,    batch: 13483/15469    Discriminator_loss: 0.1536978781223297  Generator_loss: 1.9865789413452148\n",
            "epoch: 9/10,    batch: 13484/15469    Discriminator_loss: 0.1526871621608734  Generator_loss: 1.9926042556762695\n",
            "epoch: 9/10,    batch: 13485/15469    Discriminator_loss: 0.15127694606781006  Generator_loss: 2.001509189605713\n",
            "epoch: 9/10,    batch: 13486/15469    Discriminator_loss: 0.14912867546081543  Generator_loss: 2.0142343044281006\n",
            "epoch: 9/10,    batch: 13487/15469    Discriminator_loss: 0.1467420905828476  Generator_loss: 2.0284180641174316\n",
            "epoch: 9/10,    batch: 13488/15469    Discriminator_loss: 0.14417845010757446  Generator_loss: 2.0448825359344482\n",
            "epoch: 9/10,    batch: 13489/15469    Discriminator_loss: 0.14133337140083313  Generator_loss: 2.0620102882385254\n",
            "epoch: 9/10,    batch: 13490/15469    Discriminator_loss: 0.13877233862876892  Generator_loss: 2.079491138458252\n",
            "epoch: 9/10,    batch: 13491/15469    Discriminator_loss: 0.13620692491531372  Generator_loss: 2.0962984561920166\n",
            "epoch: 9/10,    batch: 13492/15469    Discriminator_loss: 0.1339171677827835  Generator_loss: 2.111696481704712\n",
            "epoch: 9/10,    batch: 13493/15469    Discriminator_loss: 0.1316002607345581  Generator_loss: 2.126667022705078\n",
            "epoch: 9/10,    batch: 13494/15469    Discriminator_loss: 0.1293635070323944  Generator_loss: 2.142119884490967\n",
            "epoch: 9/10,    batch: 13495/15469    Discriminator_loss: 0.12700049579143524  Generator_loss: 2.1582279205322266\n",
            "epoch: 9/10,    batch: 13496/15469    Discriminator_loss: 0.12463125586509705  Generator_loss: 2.175393581390381\n",
            "epoch: 9/10,    batch: 13497/15469    Discriminator_loss: 0.12225300818681717  Generator_loss: 2.19279408454895\n",
            "epoch: 9/10,    batch: 13498/15469    Discriminator_loss: 0.11981067806482315  Generator_loss: 2.2108161449432373\n",
            "epoch: 9/10,    batch: 13499/15469    Discriminator_loss: 0.11743602901697159  Generator_loss: 2.2285172939300537\n",
            "epoch: 9/10,    batch: 13500/15469    Discriminator_loss: 0.11510311812162399  Generator_loss: 2.246006488800049\n",
            "epoch: 9/10,    batch: 13501/15469    Discriminator_loss: 0.11286824196577072  Generator_loss: 2.2636218070983887\n",
            "epoch: 9/10,    batch: 13502/15469    Discriminator_loss: 0.11072041839361191  Generator_loss: 2.280153512954712\n",
            "epoch: 9/10,    batch: 13503/15469    Discriminator_loss: 0.10872143507003784  Generator_loss: 2.2965426445007324\n",
            "epoch: 9/10,    batch: 13504/15469    Discriminator_loss: 0.10686659067869186  Generator_loss: 2.311579704284668\n",
            "epoch: 9/10,    batch: 13505/15469    Discriminator_loss: 0.10516536235809326  Generator_loss: 2.32564640045166\n",
            "epoch: 9/10,    batch: 13506/15469    Discriminator_loss: 0.10363601893186569  Generator_loss: 2.338761568069458\n",
            "epoch: 9/10,    batch: 13507/15469    Discriminator_loss: 0.10225357115268707  Generator_loss: 2.3502614498138428\n",
            "epoch: 9/10,    batch: 13508/15469    Discriminator_loss: 0.10099788010120392  Generator_loss: 2.3608713150024414\n",
            "epoch: 9/10,    batch: 13509/15469    Discriminator_loss: 0.09996325522661209  Generator_loss: 2.3699169158935547\n",
            "epoch: 9/10,    batch: 13510/15469    Discriminator_loss: 0.099040687084198  Generator_loss: 2.377870559692383\n",
            "epoch: 9/10,    batch: 13511/15469    Discriminator_loss: 0.09824341535568237  Generator_loss: 2.3846793174743652\n",
            "epoch: 9/10,    batch: 13512/15469    Discriminator_loss: 0.09761032462120056  Generator_loss: 2.3899993896484375\n",
            "epoch: 9/10,    batch: 13513/15469    Discriminator_loss: 0.09701012074947357  Generator_loss: 2.3949437141418457\n",
            "epoch: 9/10,    batch: 13514/15469    Discriminator_loss: 0.09656421840190887  Generator_loss: 2.3988430500030518\n",
            "epoch: 9/10,    batch: 13515/15469    Discriminator_loss: 0.09617100656032562  Generator_loss: 2.4025306701660156\n",
            "epoch: 9/10,    batch: 13516/15469    Discriminator_loss: 0.09581057727336884  Generator_loss: 2.405149459838867\n",
            "epoch: 9/10,    batch: 13517/15469    Discriminator_loss: 0.0955517366528511  Generator_loss: 2.407960891723633\n",
            "epoch: 9/10,    batch: 13518/15469    Discriminator_loss: 0.09523670375347137  Generator_loss: 2.4103596210479736\n",
            "epoch: 9/10,    batch: 13519/15469    Discriminator_loss: 0.09497292339801788  Generator_loss: 2.4126062393188477\n",
            "epoch: 9/10,    batch: 13520/15469    Discriminator_loss: 0.09473846107721329  Generator_loss: 2.4149417877197266\n",
            "epoch: 9/10,    batch: 13521/15469    Discriminator_loss: 0.09446433186531067  Generator_loss: 2.4174909591674805\n",
            "epoch: 9/10,    batch: 13522/15469    Discriminator_loss: 0.09418575465679169  Generator_loss: 2.4201464653015137\n",
            "epoch: 9/10,    batch: 13523/15469    Discriminator_loss: 0.09391432255506516  Generator_loss: 2.4228873252868652\n",
            "epoch: 9/10,    batch: 13524/15469    Discriminator_loss: 0.09361230581998825  Generator_loss: 2.425541877746582\n",
            "epoch: 9/10,    batch: 13525/15469    Discriminator_loss: 0.0933300033211708  Generator_loss: 2.4287161827087402\n",
            "epoch: 9/10,    batch: 13526/15469    Discriminator_loss: 0.09298411011695862  Generator_loss: 2.4317750930786133\n",
            "epoch: 9/10,    batch: 13527/15469    Discriminator_loss: 0.09264176338911057  Generator_loss: 2.435218572616577\n",
            "epoch: 9/10,    batch: 13528/15469    Discriminator_loss: 0.09230685979127884  Generator_loss: 2.438687801361084\n",
            "epoch: 9/10,    batch: 13529/15469    Discriminator_loss: 0.0919676274061203  Generator_loss: 2.442538261413574\n",
            "epoch: 9/10,    batch: 13530/15469    Discriminator_loss: 0.09161818772554398  Generator_loss: 2.44631028175354\n",
            "epoch: 9/10,    batch: 13531/15469    Discriminator_loss: 0.09123847633600235  Generator_loss: 2.4499430656433105\n",
            "epoch: 9/10,    batch: 13532/15469    Discriminator_loss: 0.09085982292890549  Generator_loss: 2.453930139541626\n",
            "epoch: 9/10,    batch: 13533/15469    Discriminator_loss: 0.09049106389284134  Generator_loss: 2.457714080810547\n",
            "epoch: 9/10,    batch: 13534/15469    Discriminator_loss: 0.09011765569448471  Generator_loss: 2.4617013931274414\n",
            "epoch: 9/10,    batch: 13535/15469    Discriminator_loss: 0.08973310142755508  Generator_loss: 2.4659457206726074\n",
            "epoch: 9/10,    batch: 13536/15469    Discriminator_loss: 0.08934482932090759  Generator_loss: 2.47023344039917\n",
            "epoch: 9/10,    batch: 13537/15469    Discriminator_loss: 0.08894165605306625  Generator_loss: 2.474475145339966\n",
            "epoch: 9/10,    batch: 13538/15469    Discriminator_loss: 0.08854121714830399  Generator_loss: 2.4789199829101562\n",
            "epoch: 9/10,    batch: 13539/15469    Discriminator_loss: 0.08811727166175842  Generator_loss: 2.483586311340332\n",
            "epoch: 9/10,    batch: 13540/15469    Discriminator_loss: 0.08769337832927704  Generator_loss: 2.488030433654785\n",
            "epoch: 9/10,    batch: 13541/15469    Discriminator_loss: 0.08725498616695404  Generator_loss: 2.4929187297821045\n",
            "epoch: 9/10,    batch: 13542/15469    Discriminator_loss: 0.08680170774459839  Generator_loss: 2.4979419708251953\n",
            "epoch: 9/10,    batch: 13543/15469    Discriminator_loss: 0.08633524179458618  Generator_loss: 2.502967357635498\n",
            "epoch: 9/10,    batch: 13544/15469    Discriminator_loss: 0.0858619287610054  Generator_loss: 2.508371591567993\n",
            "epoch: 9/10,    batch: 13545/15469    Discriminator_loss: 0.08537475764751434  Generator_loss: 2.5135884284973145\n",
            "epoch: 9/10,    batch: 13546/15469    Discriminator_loss: 0.08489002287387848  Generator_loss: 2.5192599296569824\n",
            "epoch: 9/10,    batch: 13547/15469    Discriminator_loss: 0.08438677340745926  Generator_loss: 2.5249075889587402\n",
            "epoch: 9/10,    batch: 13548/15469    Discriminator_loss: 0.08388396352529526  Generator_loss: 2.5306506156921387\n",
            "epoch: 9/10,    batch: 13549/15469    Discriminator_loss: 0.0833711102604866  Generator_loss: 2.536648750305176\n",
            "epoch: 9/10,    batch: 13550/15469    Discriminator_loss: 0.08284418284893036  Generator_loss: 2.5426416397094727\n",
            "epoch: 9/10,    batch: 13551/15469    Discriminator_loss: 0.0823163166642189  Generator_loss: 2.5488991737365723\n",
            "epoch: 9/10,    batch: 13552/15469    Discriminator_loss: 0.08178627490997314  Generator_loss: 2.555142402648926\n",
            "epoch: 9/10,    batch: 13553/15469    Discriminator_loss: 0.08125219494104385  Generator_loss: 2.561478614807129\n",
            "epoch: 9/10,    batch: 13554/15469    Discriminator_loss: 0.08070642501115799  Generator_loss: 2.5678024291992188\n",
            "epoch: 9/10,    batch: 13555/15469    Discriminator_loss: 0.08016510307788849  Generator_loss: 2.574214220046997\n",
            "epoch: 9/10,    batch: 13556/15469    Discriminator_loss: 0.07962672412395477  Generator_loss: 2.5809450149536133\n",
            "epoch: 9/10,    batch: 13557/15469    Discriminator_loss: 0.07906929403543472  Generator_loss: 2.5875189304351807\n",
            "epoch: 9/10,    batch: 13558/15469    Discriminator_loss: 0.07852619886398315  Generator_loss: 2.594174385070801\n",
            "epoch: 9/10,    batch: 13559/15469    Discriminator_loss: 0.07797612249851227  Generator_loss: 2.600971221923828\n",
            "epoch: 9/10,    batch: 13560/15469    Discriminator_loss: 0.07749371230602264  Generator_loss: 2.607869863510132\n",
            "epoch: 9/10,    batch: 13561/15469    Discriminator_loss: 0.07698162645101547  Generator_loss: 2.614804267883301\n",
            "epoch: 9/10,    batch: 13562/15469    Discriminator_loss: 0.07642153650522232  Generator_loss: 2.621816635131836\n",
            "epoch: 9/10,    batch: 13563/15469    Discriminator_loss: 0.0758933424949646  Generator_loss: 2.628934383392334\n",
            "epoch: 9/10,    batch: 13564/15469    Discriminator_loss: 0.07534816116094589  Generator_loss: 2.6360108852386475\n",
            "epoch: 9/10,    batch: 13565/15469    Discriminator_loss: 0.07477454096078873  Generator_loss: 2.6432642936706543\n",
            "epoch: 9/10,    batch: 13566/15469    Discriminator_loss: 0.07419033348560333  Generator_loss: 2.6505026817321777\n",
            "epoch: 9/10,    batch: 13567/15469    Discriminator_loss: 0.07363862544298172  Generator_loss: 2.6579325199127197\n",
            "epoch: 9/10,    batch: 13568/15469    Discriminator_loss: 0.07310942560434341  Generator_loss: 2.665306568145752\n",
            "epoch: 9/10,    batch: 13569/15469    Discriminator_loss: 0.07251813262701035  Generator_loss: 2.67287015914917\n",
            "epoch: 9/10,    batch: 13570/15469    Discriminator_loss: 0.07198109477758408  Generator_loss: 2.680473804473877\n",
            "epoch: 9/10,    batch: 13571/15469    Discriminator_loss: 0.07137317210435867  Generator_loss: 2.6881814002990723\n",
            "epoch: 9/10,    batch: 13572/15469    Discriminator_loss: 0.07083576172590256  Generator_loss: 2.695920705795288\n",
            "epoch: 9/10,    batch: 13573/15469    Discriminator_loss: 0.07028424739837646  Generator_loss: 2.7037124633789062\n",
            "epoch: 9/10,    batch: 13574/15469    Discriminator_loss: 0.06975826621055603  Generator_loss: 2.7116451263427734\n",
            "epoch: 9/10,    batch: 13575/15469    Discriminator_loss: 0.06920045614242554  Generator_loss: 2.719557285308838\n",
            "epoch: 9/10,    batch: 13576/15469    Discriminator_loss: 0.06861522793769836  Generator_loss: 2.7277560234069824\n",
            "epoch: 9/10,    batch: 13577/15469    Discriminator_loss: 0.0679074078798294  Generator_loss: 2.735884189605713\n",
            "epoch: 9/10,    batch: 13578/15469    Discriminator_loss: 0.06723525375127792  Generator_loss: 2.7441134452819824\n",
            "epoch: 9/10,    batch: 13579/15469    Discriminator_loss: 0.06665461510419846  Generator_loss: 2.7523980140686035\n",
            "epoch: 9/10,    batch: 13580/15469    Discriminator_loss: 0.06607685983181  Generator_loss: 2.7608962059020996\n",
            "epoch: 9/10,    batch: 13581/15469    Discriminator_loss: 0.06549381464719772  Generator_loss: 2.7695584297180176\n",
            "epoch: 9/10,    batch: 13582/15469    Discriminator_loss: 0.06491030752658844  Generator_loss: 2.778132677078247\n",
            "epoch: 9/10,    batch: 13583/15469    Discriminator_loss: 0.06432589888572693  Generator_loss: 2.7868974208831787\n",
            "epoch: 9/10,    batch: 13584/15469    Discriminator_loss: 0.0637434795498848  Generator_loss: 2.795732021331787\n",
            "epoch: 9/10,    batch: 13585/15469    Discriminator_loss: 0.06315319240093231  Generator_loss: 2.8047807216644287\n",
            "epoch: 9/10,    batch: 13586/15469    Discriminator_loss: 0.06255938857793808  Generator_loss: 2.8139052391052246\n",
            "epoch: 9/10,    batch: 13587/15469    Discriminator_loss: 0.0619676299393177  Generator_loss: 2.8231239318847656\n",
            "epoch: 9/10,    batch: 13588/15469    Discriminator_loss: 0.061371803283691406  Generator_loss: 2.8324477672576904\n",
            "epoch: 9/10,    batch: 13589/15469    Discriminator_loss: 0.06077107787132263  Generator_loss: 2.8419318199157715\n",
            "epoch: 9/10,    batch: 13590/15469    Discriminator_loss: 0.06017007678747177  Generator_loss: 2.8515090942382812\n",
            "epoch: 9/10,    batch: 13591/15469    Discriminator_loss: 0.05957958102226257  Generator_loss: 2.86129093170166\n",
            "epoch: 9/10,    batch: 13592/15469    Discriminator_loss: 0.05897097662091255  Generator_loss: 2.871187925338745\n",
            "epoch: 9/10,    batch: 13593/15469    Discriminator_loss: 0.0583769865334034  Generator_loss: 2.8810391426086426\n",
            "epoch: 9/10,    batch: 13594/15469    Discriminator_loss: 0.057770777493715286  Generator_loss: 2.8911385536193848\n",
            "epoch: 9/10,    batch: 13595/15469    Discriminator_loss: 0.05716925859451294  Generator_loss: 2.9013688564300537\n",
            "epoch: 9/10,    batch: 13596/15469    Discriminator_loss: 0.056565795093774796  Generator_loss: 2.911552906036377\n",
            "epoch: 9/10,    batch: 13597/15469    Discriminator_loss: 0.055963464081287384  Generator_loss: 2.92195987701416\n",
            "epoch: 9/10,    batch: 13598/15469    Discriminator_loss: 0.05536644160747528  Generator_loss: 2.932490587234497\n",
            "epoch: 9/10,    batch: 13599/15469    Discriminator_loss: 0.05476315692067146  Generator_loss: 2.943021774291992\n",
            "epoch: 9/10,    batch: 13600/15469    Discriminator_loss: 0.05416493117809296  Generator_loss: 2.953690528869629\n",
            "epoch: 9/10,    batch: 13601/15469    Discriminator_loss: 0.05356935039162636  Generator_loss: 2.9644107818603516\n",
            "epoch: 9/10,    batch: 13602/15469    Discriminator_loss: 0.052974455058574677  Generator_loss: 2.975281000137329\n",
            "epoch: 9/10,    batch: 13603/15469    Discriminator_loss: 0.052386436611413956  Generator_loss: 2.9861936569213867\n",
            "epoch: 9/10,    batch: 13604/15469    Discriminator_loss: 0.0517931804060936  Generator_loss: 2.997124195098877\n",
            "epoch: 9/10,    batch: 13605/15469    Discriminator_loss: 0.0512109249830246  Generator_loss: 3.008168935775757\n",
            "epoch: 9/10,    batch: 13606/15469    Discriminator_loss: 0.0506250336766243  Generator_loss: 3.0192856788635254\n",
            "epoch: 9/10,    batch: 13607/15469    Discriminator_loss: 0.050049878656864166  Generator_loss: 3.0304853916168213\n",
            "epoch: 9/10,    batch: 13608/15469    Discriminator_loss: 0.049473412334918976  Generator_loss: 3.041710376739502\n",
            "epoch: 9/10,    batch: 13609/15469    Discriminator_loss: 0.0489058792591095  Generator_loss: 3.0529422760009766\n",
            "epoch: 9/10,    batch: 13610/15469    Discriminator_loss: 0.04834187775850296  Generator_loss: 3.064230442047119\n",
            "epoch: 9/10,    batch: 13611/15469    Discriminator_loss: 0.04777998849749565  Generator_loss: 3.075559616088867\n",
            "epoch: 9/10,    batch: 13612/15469    Discriminator_loss: 0.04722827300429344  Generator_loss: 3.086904525756836\n",
            "epoch: 9/10,    batch: 13613/15469    Discriminator_loss: 0.04667738080024719  Generator_loss: 3.098208427429199\n",
            "epoch: 9/10,    batch: 13614/15469    Discriminator_loss: 0.04613737761974335  Generator_loss: 3.1096277236938477\n",
            "epoch: 9/10,    batch: 13615/15469    Discriminator_loss: 0.04559910669922829  Generator_loss: 3.1210248470306396\n",
            "epoch: 9/10,    batch: 13616/15469    Discriminator_loss: 0.045069754123687744  Generator_loss: 3.1323909759521484\n",
            "epoch: 9/10,    batch: 13617/15469    Discriminator_loss: 0.044545914977788925  Generator_loss: 3.143759250640869\n",
            "epoch: 9/10,    batch: 13618/15469    Discriminator_loss: 0.044028058648109436  Generator_loss: 3.155151128768921\n",
            "epoch: 9/10,    batch: 13619/15469    Discriminator_loss: 0.043518368154764175  Generator_loss: 3.1664960384368896\n",
            "epoch: 9/10,    batch: 13620/15469    Discriminator_loss: 0.043027229607105255  Generator_loss: 3.1778132915496826\n",
            "epoch: 9/10,    batch: 13621/15469    Discriminator_loss: 0.042543184012174606  Generator_loss: 3.1891555786132812\n",
            "epoch: 9/10,    batch: 13622/15469    Discriminator_loss: 0.04205106198787689  Generator_loss: 3.2003941535949707\n",
            "epoch: 9/10,    batch: 13623/15469    Discriminator_loss: 0.04351957142353058  Generator_loss: 3.2110371589660645\n",
            "epoch: 9/10,    batch: 13624/15469    Discriminator_loss: 0.04111999645829201  Generator_loss: 3.2217419147491455\n",
            "epoch: 9/10,    batch: 13625/15469    Discriminator_loss: 0.040738023817539215  Generator_loss: 3.232457160949707\n",
            "epoch: 9/10,    batch: 13626/15469    Discriminator_loss: 0.04036427661776543  Generator_loss: 3.2432808876037598\n",
            "epoch: 9/10,    batch: 13627/15469    Discriminator_loss: 0.03978551924228668  Generator_loss: 3.2540688514709473\n",
            "epoch: 9/10,    batch: 13628/15469    Discriminator_loss: 0.039331305772066116  Generator_loss: 3.2648701667785645\n",
            "epoch: 9/10,    batch: 13629/15469    Discriminator_loss: 0.038899850100278854  Generator_loss: 3.2756543159484863\n",
            "epoch: 9/10,    batch: 13630/15469    Discriminator_loss: 0.038472823798656464  Generator_loss: 3.2863821983337402\n",
            "epoch: 9/10,    batch: 13631/15469    Discriminator_loss: 0.03805561736226082  Generator_loss: 3.2971367835998535\n",
            "epoch: 9/10,    batch: 13632/15469    Discriminator_loss: 0.037641603499650955  Generator_loss: 3.307783842086792\n",
            "epoch: 9/10,    batch: 13633/15469    Discriminator_loss: 0.03723040595650673  Generator_loss: 3.3185019493103027\n",
            "epoch: 9/10,    batch: 13634/15469    Discriminator_loss: 0.03688376396894455  Generator_loss: 3.3289942741394043\n",
            "epoch: 9/10,    batch: 13635/15469    Discriminator_loss: 0.03651367872953415  Generator_loss: 3.339488983154297\n",
            "epoch: 9/10,    batch: 13636/15469    Discriminator_loss: 0.03610815852880478  Generator_loss: 3.349976062774658\n",
            "epoch: 9/10,    batch: 13637/15469    Discriminator_loss: 0.035764578729867935  Generator_loss: 3.360325336456299\n",
            "epoch: 9/10,    batch: 13638/15469    Discriminator_loss: 0.03541693463921547  Generator_loss: 3.3706469535827637\n",
            "epoch: 9/10,    batch: 13639/15469    Discriminator_loss: 0.03516511246562004  Generator_loss: 3.38086199760437\n",
            "epoch: 9/10,    batch: 13640/15469    Discriminator_loss: 0.034640535712242126  Generator_loss: 3.3909807205200195\n",
            "epoch: 9/10,    batch: 13641/15469    Discriminator_loss: 0.03425784409046173  Generator_loss: 3.401031970977783\n",
            "epoch: 9/10,    batch: 13642/15469    Discriminator_loss: 0.03390324115753174  Generator_loss: 3.411060333251953\n",
            "epoch: 9/10,    batch: 13643/15469    Discriminator_loss: 0.033551350235939026  Generator_loss: 3.420914649963379\n",
            "epoch: 9/10,    batch: 13644/15469    Discriminator_loss: 0.03318179398775101  Generator_loss: 3.430774450302124\n",
            "epoch: 9/10,    batch: 13645/15469    Discriminator_loss: 0.032850246876478195  Generator_loss: 3.4405479431152344\n",
            "epoch: 9/10,    batch: 13646/15469    Discriminator_loss: 0.03252821043133736  Generator_loss: 3.4502358436584473\n",
            "epoch: 9/10,    batch: 13647/15469    Discriminator_loss: 0.032207388430833817  Generator_loss: 3.4598896503448486\n",
            "epoch: 9/10,    batch: 13648/15469    Discriminator_loss: 0.03189794719219208  Generator_loss: 3.469419479370117\n",
            "epoch: 9/10,    batch: 13649/15469    Discriminator_loss: 0.03158997371792793  Generator_loss: 3.478884220123291\n",
            "epoch: 9/10,    batch: 13650/15469    Discriminator_loss: 0.03128863126039505  Generator_loss: 3.4882936477661133\n",
            "epoch: 9/10,    batch: 13651/15469    Discriminator_loss: 0.030992431566119194  Generator_loss: 3.4975838661193848\n",
            "epoch: 9/10,    batch: 13652/15469    Discriminator_loss: 0.030700210481882095  Generator_loss: 3.506864070892334\n",
            "epoch: 9/10,    batch: 13653/15469    Discriminator_loss: 0.0304146409034729  Generator_loss: 3.5159623622894287\n",
            "epoch: 9/10,    batch: 13654/15469    Discriminator_loss: 0.030133845284581184  Generator_loss: 3.525045394897461\n",
            "epoch: 9/10,    batch: 13655/15469    Discriminator_loss: 0.029858402907848358  Generator_loss: 3.5341105461120605\n",
            "epoch: 9/10,    batch: 13656/15469    Discriminator_loss: 0.02958485297858715  Generator_loss: 3.5429816246032715\n",
            "epoch: 9/10,    batch: 13657/15469    Discriminator_loss: 0.029319651424884796  Generator_loss: 3.551868200302124\n",
            "epoch: 9/10,    batch: 13658/15469    Discriminator_loss: 0.02905825525522232  Generator_loss: 3.5606207847595215\n",
            "epoch: 9/10,    batch: 13659/15469    Discriminator_loss: 0.028799230232834816  Generator_loss: 3.569350481033325\n",
            "epoch: 9/10,    batch: 13660/15469    Discriminator_loss: 0.028548501431941986  Generator_loss: 3.5779976844787598\n",
            "epoch: 9/10,    batch: 13661/15469    Discriminator_loss: 0.028299245983362198  Generator_loss: 3.5865421295166016\n",
            "epoch: 9/10,    batch: 13662/15469    Discriminator_loss: 0.02805520035326481  Generator_loss: 3.5950541496276855\n",
            "epoch: 9/10,    batch: 13663/15469    Discriminator_loss: 0.027816280722618103  Generator_loss: 3.6035232543945312\n",
            "epoch: 9/10,    batch: 13664/15469    Discriminator_loss: 0.027578722685575485  Generator_loss: 3.611849069595337\n",
            "epoch: 9/10,    batch: 13665/15469    Discriminator_loss: 0.027347445487976074  Generator_loss: 3.6201183795928955\n",
            "epoch: 9/10,    batch: 13666/15469    Discriminator_loss: 0.027118485420942307  Generator_loss: 3.6283278465270996\n",
            "epoch: 9/10,    batch: 13667/15469    Discriminator_loss: 0.02689371444284916  Generator_loss: 3.636517286300659\n",
            "epoch: 9/10,    batch: 13668/15469    Discriminator_loss: 0.026674611493945122  Generator_loss: 3.6445651054382324\n",
            "epoch: 9/10,    batch: 13669/15469    Discriminator_loss: 0.0264559555798769  Generator_loss: 3.6526153087615967\n",
            "epoch: 9/10,    batch: 13670/15469    Discriminator_loss: 0.02624231018126011  Generator_loss: 3.6605544090270996\n",
            "epoch: 9/10,    batch: 13671/15469    Discriminator_loss: 0.026032723486423492  Generator_loss: 3.6684813499450684\n",
            "epoch: 9/10,    batch: 13672/15469    Discriminator_loss: 0.025826213881373405  Generator_loss: 3.6763031482696533\n",
            "epoch: 9/10,    batch: 13673/15469    Discriminator_loss: 0.0256221741437912  Generator_loss: 3.6840813159942627\n",
            "epoch: 9/10,    batch: 13674/15469    Discriminator_loss: 0.025420023128390312  Generator_loss: 3.69179630279541\n",
            "epoch: 9/10,    batch: 13675/15469    Discriminator_loss: 0.025222381576895714  Generator_loss: 3.699467897415161\n",
            "epoch: 9/10,    batch: 13676/15469    Discriminator_loss: 0.025027675554156303  Generator_loss: 3.7070703506469727\n",
            "epoch: 9/10,    batch: 13677/15469    Discriminator_loss: 0.02483733370900154  Generator_loss: 3.7146360874176025\n",
            "epoch: 9/10,    batch: 13678/15469    Discriminator_loss: 0.024647805839776993  Generator_loss: 3.7221624851226807\n",
            "epoch: 9/10,    batch: 13679/15469    Discriminator_loss: 0.024460460990667343  Generator_loss: 3.7296323776245117\n",
            "epoch: 9/10,    batch: 13680/15469    Discriminator_loss: 0.024277804419398308  Generator_loss: 3.737046480178833\n",
            "epoch: 9/10,    batch: 13681/15469    Discriminator_loss: 0.024099992588162422  Generator_loss: 3.744429349899292\n",
            "epoch: 9/10,    batch: 13682/15469    Discriminator_loss: 0.023920688778162003  Generator_loss: 3.751716136932373\n",
            "epoch: 9/10,    batch: 13683/15469    Discriminator_loss: 0.023745102807879448  Generator_loss: 3.758957862854004\n",
            "epoch: 9/10,    batch: 13684/15469    Discriminator_loss: 0.023572219535708427  Generator_loss: 3.766209125518799\n",
            "epoch: 9/10,    batch: 13685/15469    Discriminator_loss: 0.023404935374855995  Generator_loss: 3.7733473777770996\n",
            "epoch: 9/10,    batch: 13686/15469    Discriminator_loss: 0.023239383473992348  Generator_loss: 3.7805192470550537\n",
            "epoch: 9/10,    batch: 13687/15469    Discriminator_loss: 0.023075992241501808  Generator_loss: 3.7875559329986572\n",
            "epoch: 9/10,    batch: 13688/15469    Discriminator_loss: 0.02290794998407364  Generator_loss: 3.794616937637329\n",
            "epoch: 9/10,    batch: 13689/15469    Discriminator_loss: 1.9914757013320923  Generator_loss: 3.777087450027466\n",
            "epoch: 9/10,    batch: 13690/15469    Discriminator_loss: 4.493395805358887  Generator_loss: 3.724299907684326\n",
            "epoch: 9/10,    batch: 13691/15469    Discriminator_loss: 4.3665289878845215  Generator_loss: 3.6507041454315186\n",
            "epoch: 9/10,    batch: 13692/15469    Discriminator_loss: 4.2393293380737305  Generator_loss: 3.565812110900879\n",
            "epoch: 9/10,    batch: 13693/15469    Discriminator_loss: 4.0982184410095215  Generator_loss: 3.4748287200927734\n",
            "epoch: 9/10,    batch: 13694/15469    Discriminator_loss: 3.957960605621338  Generator_loss: 3.381901502609253\n",
            "epoch: 9/10,    batch: 13695/15469    Discriminator_loss: 3.820871114730835  Generator_loss: 3.289703845977783\n",
            "epoch: 9/10,    batch: 13696/15469    Discriminator_loss: 3.690096855163574  Generator_loss: 3.1995956897735596\n",
            "epoch: 9/10,    batch: 13697/15469    Discriminator_loss: 3.5550553798675537  Generator_loss: 3.11220645904541\n",
            "epoch: 9/10,    batch: 13698/15469    Discriminator_loss: 3.430746078491211  Generator_loss: 3.027806282043457\n",
            "epoch: 9/10,    batch: 13699/15469    Discriminator_loss: 3.313106060028076  Generator_loss: 2.946643352508545\n",
            "epoch: 9/10,    batch: 13700/15469    Discriminator_loss: 3.20172119140625  Generator_loss: 2.8689517974853516\n",
            "epoch: 9/10,    batch: 13701/15469    Discriminator_loss: 3.1051018238067627  Generator_loss: 2.79484486579895\n",
            "epoch: 9/10,    batch: 13702/15469    Discriminator_loss: 2.9996354579925537  Generator_loss: 2.7239766120910645\n",
            "epoch: 9/10,    batch: 13703/15469    Discriminator_loss: 2.9027817249298096  Generator_loss: 2.6562118530273438\n",
            "epoch: 9/10,    batch: 13704/15469    Discriminator_loss: 2.808260440826416  Generator_loss: 2.591546058654785\n",
            "epoch: 9/10,    batch: 13705/15469    Discriminator_loss: 2.715785026550293  Generator_loss: 2.530188798904419\n",
            "epoch: 9/10,    batch: 13706/15469    Discriminator_loss: 2.639329671859741  Generator_loss: 2.4714479446411133\n",
            "epoch: 9/10,    batch: 13707/15469    Discriminator_loss: 2.5600695610046387  Generator_loss: 2.415529251098633\n",
            "epoch: 9/10,    batch: 13708/15469    Discriminator_loss: 2.481774091720581  Generator_loss: 2.362417221069336\n",
            "epoch: 9/10,    batch: 13709/15469    Discriminator_loss: 2.4148097038269043  Generator_loss: 2.3116698265075684\n",
            "epoch: 9/10,    batch: 13710/15469    Discriminator_loss: 2.3756608963012695  Generator_loss: 2.2635767459869385\n",
            "epoch: 9/10,    batch: 13711/15469    Discriminator_loss: 2.287761926651001  Generator_loss: 2.2173266410827637\n",
            "epoch: 9/10,    batch: 13712/15469    Discriminator_loss: 2.232882261276245  Generator_loss: 2.1735639572143555\n",
            "epoch: 9/10,    batch: 13713/15469    Discriminator_loss: 2.162076473236084  Generator_loss: 2.1316423416137695\n",
            "epoch: 9/10,    batch: 13714/15469    Discriminator_loss: 2.1032235622406006  Generator_loss: 2.0921545028686523\n",
            "epoch: 9/10,    batch: 13715/15469    Discriminator_loss: 2.046588659286499  Generator_loss: 2.0541927814483643\n",
            "epoch: 9/10,    batch: 13716/15469    Discriminator_loss: 2.0091423988342285  Generator_loss: 2.0180466175079346\n",
            "epoch: 9/10,    batch: 13717/15469    Discriminator_loss: 1.945518970489502  Generator_loss: 1.9835479259490967\n",
            "epoch: 9/10,    batch: 13718/15469    Discriminator_loss: 1.8920915126800537  Generator_loss: 1.9502978324890137\n",
            "epoch: 9/10,    batch: 13719/15469    Discriminator_loss: 1.8304214477539062  Generator_loss: 1.9187605381011963\n",
            "epoch: 9/10,    batch: 13720/15469    Discriminator_loss: 1.7854492664337158  Generator_loss: 1.8885759115219116\n",
            "epoch: 9/10,    batch: 13721/15469    Discriminator_loss: 1.702138900756836  Generator_loss: 1.859490156173706\n",
            "epoch: 9/10,    batch: 13722/15469    Discriminator_loss: 1.6647846698760986  Generator_loss: 1.8311231136322021\n",
            "epoch: 9/10,    batch: 13723/15469    Discriminator_loss: 1.6021549701690674  Generator_loss: 1.8036603927612305\n",
            "epoch: 9/10,    batch: 13724/15469    Discriminator_loss: 1.532882809638977  Generator_loss: 1.7767620086669922\n",
            "epoch: 9/10,    batch: 13725/15469    Discriminator_loss: 1.3975356817245483  Generator_loss: 1.74965238571167\n",
            "epoch: 9/10,    batch: 13726/15469    Discriminator_loss: 1.3521424531936646  Generator_loss: 1.7225096225738525\n",
            "epoch: 9/10,    batch: 13727/15469    Discriminator_loss: 1.160543441772461  Generator_loss: 1.6955320835113525\n",
            "epoch: 9/10,    batch: 13728/15469    Discriminator_loss: 0.9571564793586731  Generator_loss: 1.6693930625915527\n",
            "epoch: 9/10,    batch: 13729/15469    Discriminator_loss: 0.2377404123544693  Generator_loss: 1.6493163108825684\n",
            "epoch: 9/10,    batch: 13730/15469    Discriminator_loss: 0.21628090739250183  Generator_loss: 1.6351879835128784\n",
            "epoch: 9/10,    batch: 13731/15469    Discriminator_loss: 0.2191515564918518  Generator_loss: 1.6253397464752197\n",
            "epoch: 9/10,    batch: 13732/15469    Discriminator_loss: 0.2212262749671936  Generator_loss: 1.619707703590393\n",
            "epoch: 9/10,    batch: 13733/15469    Discriminator_loss: 0.22213159501552582  Generator_loss: 1.6169929504394531\n",
            "epoch: 9/10,    batch: 13734/15469    Discriminator_loss: 0.22251632809638977  Generator_loss: 1.617011308670044\n",
            "epoch: 9/10,    batch: 13735/15469    Discriminator_loss: 0.2222207486629486  Generator_loss: 1.6195012331008911\n",
            "epoch: 9/10,    batch: 13736/15469    Discriminator_loss: 0.22133223712444305  Generator_loss: 1.6237266063690186\n",
            "epoch: 9/10,    batch: 13737/15469    Discriminator_loss: 0.22002531588077545  Generator_loss: 1.629723072052002\n",
            "epoch: 9/10,    batch: 13738/15469    Discriminator_loss: 0.22881437838077545  Generator_loss: 1.6369048357009888\n",
            "epoch: 9/10,    batch: 13739/15469    Discriminator_loss: 0.2207135707139969  Generator_loss: 1.6454057693481445\n",
            "epoch: 9/10,    batch: 13740/15469    Discriminator_loss: 0.21524661779403687  Generator_loss: 1.6543935537338257\n",
            "epoch: 9/10,    batch: 13741/15469    Discriminator_loss: 0.21211759746074677  Generator_loss: 1.6642086505889893\n",
            "epoch: 9/10,    batch: 13742/15469    Discriminator_loss: 0.20969663560390472  Generator_loss: 1.6747136116027832\n",
            "epoch: 9/10,    batch: 13743/15469    Discriminator_loss: 0.20731183886528015  Generator_loss: 1.685384750366211\n",
            "epoch: 9/10,    batch: 13744/15469    Discriminator_loss: 0.20488131046295166  Generator_loss: 1.696287751197815\n",
            "epoch: 9/10,    batch: 13745/15469    Discriminator_loss: 0.2024681568145752  Generator_loss: 1.7068803310394287\n",
            "epoch: 9/10,    batch: 13746/15469    Discriminator_loss: 0.2001192271709442  Generator_loss: 1.7177194356918335\n",
            "epoch: 9/10,    batch: 13747/15469    Discriminator_loss: 0.1978248953819275  Generator_loss: 1.7280752658843994\n",
            "epoch: 9/10,    batch: 13748/15469    Discriminator_loss: 0.19560852646827698  Generator_loss: 1.738114595413208\n",
            "epoch: 9/10,    batch: 13749/15469    Discriminator_loss: 0.19362178444862366  Generator_loss: 1.7474948167800903\n",
            "epoch: 9/10,    batch: 13750/15469    Discriminator_loss: 0.19176001846790314  Generator_loss: 1.756182074546814\n",
            "epoch: 9/10,    batch: 13751/15469    Discriminator_loss: 0.18995791673660278  Generator_loss: 1.764219045639038\n",
            "epoch: 9/10,    batch: 13752/15469    Discriminator_loss: 0.1884535253047943  Generator_loss: 1.7715775966644287\n",
            "epoch: 9/10,    batch: 13753/15469    Discriminator_loss: 0.18704766035079956  Generator_loss: 1.7781747579574585\n",
            "epoch: 9/10,    batch: 13754/15469    Discriminator_loss: 0.18587949872016907  Generator_loss: 1.7841274738311768\n",
            "epoch: 9/10,    batch: 13755/15469    Discriminator_loss: 0.18484221398830414  Generator_loss: 1.7889704704284668\n",
            "epoch: 9/10,    batch: 13756/15469    Discriminator_loss: 0.18400606513023376  Generator_loss: 1.7927443981170654\n",
            "epoch: 9/10,    batch: 13757/15469    Discriminator_loss: 0.1832483410835266  Generator_loss: 1.7963634729385376\n",
            "epoch: 9/10,    batch: 13758/15469    Discriminator_loss: 0.18292419612407684  Generator_loss: 1.7979260683059692\n",
            "epoch: 9/10,    batch: 13759/15469    Discriminator_loss: 0.18261979520320892  Generator_loss: 1.7989165782928467\n",
            "epoch: 9/10,    batch: 13760/15469    Discriminator_loss: 0.18273776769638062  Generator_loss: 1.79915189743042\n",
            "epoch: 9/10,    batch: 13761/15469    Discriminator_loss: 0.1829487383365631  Generator_loss: 1.798471212387085\n",
            "epoch: 9/10,    batch: 13762/15469    Discriminator_loss: 0.18306982517242432  Generator_loss: 1.7959657907485962\n",
            "epoch: 9/10,    batch: 13763/15469    Discriminator_loss: 0.18384413421154022  Generator_loss: 1.7936654090881348\n",
            "epoch: 9/10,    batch: 13764/15469    Discriminator_loss: 0.1844492256641388  Generator_loss: 1.7900381088256836\n",
            "epoch: 9/10,    batch: 13765/15469    Discriminator_loss: 0.18531163036823273  Generator_loss: 1.7829678058624268\n",
            "epoch: 9/10,    batch: 13766/15469    Discriminator_loss: 0.18683740496635437  Generator_loss: 1.7797164916992188\n",
            "epoch: 9/10,    batch: 13767/15469    Discriminator_loss: 0.1869928538799286  Generator_loss: 1.7748841047286987\n",
            "epoch: 9/10,    batch: 13768/15469    Discriminator_loss: 0.18778640031814575  Generator_loss: 1.772235631942749\n",
            "epoch: 9/10,    batch: 13769/15469    Discriminator_loss: 0.1890089511871338  Generator_loss: 1.7720298767089844\n",
            "epoch: 9/10,    batch: 13770/15469    Discriminator_loss: 0.18719200789928436  Generator_loss: 1.778204321861267\n",
            "epoch: 9/10,    batch: 13771/15469    Discriminator_loss: 0.1870480626821518  Generator_loss: 1.778307318687439\n",
            "epoch: 9/10,    batch: 13772/15469    Discriminator_loss: 0.18611450493335724  Generator_loss: 1.7777471542358398\n",
            "epoch: 9/10,    batch: 13773/15469    Discriminator_loss: 0.1850409209728241  Generator_loss: 1.7907124757766724\n",
            "epoch: 9/10,    batch: 13774/15469    Discriminator_loss: 0.1845124214887619  Generator_loss: 1.7921065092086792\n",
            "epoch: 9/10,    batch: 13775/15469    Discriminator_loss: 0.18250450491905212  Generator_loss: 1.8009750843048096\n",
            "epoch: 9/10,    batch: 13776/15469    Discriminator_loss: 0.1797846257686615  Generator_loss: 1.8136155605316162\n",
            "epoch: 9/10,    batch: 13777/15469    Discriminator_loss: 0.1785181760787964  Generator_loss: 1.817542552947998\n",
            "epoch: 9/10,    batch: 13778/15469    Discriminator_loss: 0.17683224380016327  Generator_loss: 1.8290668725967407\n",
            "epoch: 9/10,    batch: 13779/15469    Discriminator_loss: 0.17574109137058258  Generator_loss: 1.8342807292938232\n",
            "epoch: 9/10,    batch: 13780/15469    Discriminator_loss: 0.17400576174259186  Generator_loss: 1.8468986749649048\n",
            "epoch: 9/10,    batch: 13781/15469    Discriminator_loss: 0.17230816185474396  Generator_loss: 1.8547910451889038\n",
            "epoch: 9/10,    batch: 13782/15469    Discriminator_loss: 0.17153914272785187  Generator_loss: 1.8585821390151978\n",
            "epoch: 9/10,    batch: 13783/15469    Discriminator_loss: 0.16927920281887054  Generator_loss: 1.8695058822631836\n",
            "epoch: 9/10,    batch: 13784/15469    Discriminator_loss: 0.16831400990486145  Generator_loss: 1.872612714767456\n",
            "epoch: 9/10,    batch: 13785/15469    Discriminator_loss: 0.16827885806560516  Generator_loss: 1.880138874053955\n",
            "epoch: 9/10,    batch: 13786/15469    Discriminator_loss: 0.16646529734134674  Generator_loss: 1.8872265815734863\n",
            "epoch: 9/10,    batch: 13787/15469    Discriminator_loss: 0.16573143005371094  Generator_loss: 1.8933730125427246\n",
            "epoch: 9/10,    batch: 13788/15469    Discriminator_loss: 0.16483917832374573  Generator_loss: 1.8940675258636475\n",
            "epoch: 9/10,    batch: 13789/15469    Discriminator_loss: 0.162331685423851  Generator_loss: 1.9042373895645142\n",
            "epoch: 9/10,    batch: 13790/15469    Discriminator_loss: 0.1626541018486023  Generator_loss: 1.9081964492797852\n",
            "epoch: 9/10,    batch: 13791/15469    Discriminator_loss: 0.16221751272678375  Generator_loss: 1.907145380973816\n",
            "epoch: 9/10,    batch: 13792/15469    Discriminator_loss: 0.16231998801231384  Generator_loss: 1.9091037511825562\n",
            "epoch: 9/10,    batch: 13793/15469    Discriminator_loss: 0.16169589757919312  Generator_loss: 1.9102811813354492\n",
            "epoch: 9/10,    batch: 13794/15469    Discriminator_loss: 0.16147665679454803  Generator_loss: 1.9145389795303345\n",
            "epoch: 9/10,    batch: 13795/15469    Discriminator_loss: 0.1614072173833847  Generator_loss: 1.9127051830291748\n",
            "epoch: 9/10,    batch: 13796/15469    Discriminator_loss: 0.1617727428674698  Generator_loss: 1.9119277000427246\n",
            "epoch: 9/10,    batch: 13797/15469    Discriminator_loss: 0.1623343825340271  Generator_loss: 1.9101711511611938\n",
            "epoch: 9/10,    batch: 13798/15469    Discriminator_loss: 0.16276764869689941  Generator_loss: 1.9066020250320435\n",
            "epoch: 9/10,    batch: 13799/15469    Discriminator_loss: 0.16364037990570068  Generator_loss: 1.9017456769943237\n",
            "epoch: 9/10,    batch: 13800/15469    Discriminator_loss: 0.16520972549915314  Generator_loss: 1.8943990468978882\n",
            "epoch: 9/10,    batch: 13801/15469    Discriminator_loss: 0.16731691360473633  Generator_loss: 1.8838772773742676\n",
            "epoch: 9/10,    batch: 13802/15469    Discriminator_loss: 0.1697716861963272  Generator_loss: 1.8720991611480713\n",
            "epoch: 9/10,    batch: 13803/15469    Discriminator_loss: 0.17298808693885803  Generator_loss: 1.8564488887786865\n",
            "epoch: 9/10,    batch: 13804/15469    Discriminator_loss: 0.17776435613632202  Generator_loss: 1.8351079225540161\n",
            "epoch: 9/10,    batch: 13805/15469    Discriminator_loss: 0.1851917803287506  Generator_loss: 1.801680564880371\n",
            "epoch: 9/10,    batch: 13806/15469    Discriminator_loss: 0.19611315429210663  Generator_loss: 1.7541767358779907\n",
            "epoch: 9/10,    batch: 13807/15469    Discriminator_loss: 0.20990729331970215  Generator_loss: 1.6948503255844116\n",
            "epoch: 9/10,    batch: 13808/15469    Discriminator_loss: 0.22276708483695984  Generator_loss: 1.6425600051879883\n",
            "epoch: 9/10,    batch: 13809/15469    Discriminator_loss: 0.23600046336650848  Generator_loss: 1.598102331161499\n",
            "epoch: 9/10,    batch: 13810/15469    Discriminator_loss: 0.25706255435943604  Generator_loss: 1.5466152429580688\n",
            "epoch: 9/10,    batch: 13811/15469    Discriminator_loss: 0.34177735447883606  Generator_loss: 1.3964968919754028\n",
            "epoch: 9/10,    batch: 13812/15469    Discriminator_loss: 0.7356120944023132  Generator_loss: 1.4440643787384033\n",
            "epoch: 9/10,    batch: 13813/15469    Discriminator_loss: 0.4914622902870178  Generator_loss: 2.276232957839966\n",
            "epoch: 9/10,    batch: 13814/15469    Discriminator_loss: 0.1075006052851677  Generator_loss: 2.7288858890533447\n",
            "epoch: 9/10,    batch: 13815/15469    Discriminator_loss: 0.0967828705906868  Generator_loss: 2.4769115447998047\n",
            "epoch: 9/10,    batch: 13816/15469    Discriminator_loss: 0.09009459614753723  Generator_loss: 2.4872488975524902\n",
            "epoch: 9/10,    batch: 13817/15469    Discriminator_loss: 0.0856395736336708  Generator_loss: 2.523017406463623\n",
            "epoch: 9/10,    batch: 13818/15469    Discriminator_loss: 0.0811062604188919  Generator_loss: 2.56845760345459\n",
            "epoch: 9/10,    batch: 13819/15469    Discriminator_loss: 0.07787129282951355  Generator_loss: 2.602529764175415\n",
            "epoch: 9/10,    batch: 13820/15469    Discriminator_loss: 0.07581140100955963  Generator_loss: 2.629746913909912\n",
            "epoch: 9/10,    batch: 13821/15469    Discriminator_loss: 0.07408660650253296  Generator_loss: 2.649932384490967\n",
            "epoch: 9/10,    batch: 13822/15469    Discriminator_loss: 0.07322362065315247  Generator_loss: 2.6564998626708984\n",
            "epoch: 9/10,    batch: 13823/15469    Discriminator_loss: 0.07290545850992203  Generator_loss: 2.6596879959106445\n",
            "epoch: 9/10,    batch: 13824/15469    Discriminator_loss: 0.07281535118818283  Generator_loss: 2.6653804779052734\n",
            "epoch: 9/10,    batch: 13825/15469    Discriminator_loss: 0.0729573592543602  Generator_loss: 2.660857915878296\n",
            "epoch: 9/10,    batch: 13826/15469    Discriminator_loss: 0.07299719005823135  Generator_loss: 2.65962815284729\n",
            "epoch: 9/10,    batch: 13827/15469    Discriminator_loss: 0.07302538305521011  Generator_loss: 2.6589765548706055\n",
            "epoch: 9/10,    batch: 13828/15469    Discriminator_loss: 0.07303830981254578  Generator_loss: 2.658355712890625\n",
            "epoch: 9/10,    batch: 13829/15469    Discriminator_loss: 0.07310006022453308  Generator_loss: 2.6575489044189453\n",
            "epoch: 9/10,    batch: 13830/15469    Discriminator_loss: 0.07303373515605927  Generator_loss: 2.6579744815826416\n",
            "epoch: 9/10,    batch: 13831/15469    Discriminator_loss: 0.0729900449514389  Generator_loss: 2.658446788787842\n",
            "epoch: 9/10,    batch: 13832/15469    Discriminator_loss: 0.07293929159641266  Generator_loss: 2.659046173095703\n",
            "epoch: 9/10,    batch: 13833/15469    Discriminator_loss: 0.07289217412471771  Generator_loss: 2.6598033905029297\n",
            "epoch: 9/10,    batch: 13834/15469    Discriminator_loss: 0.07287058234214783  Generator_loss: 2.659626007080078\n",
            "epoch: 9/10,    batch: 13835/15469    Discriminator_loss: 0.07286714017391205  Generator_loss: 2.6594388484954834\n",
            "epoch: 9/10,    batch: 13836/15469    Discriminator_loss: 0.0729348212480545  Generator_loss: 2.6584036350250244\n",
            "epoch: 9/10,    batch: 13837/15469    Discriminator_loss: 0.07304710149765015  Generator_loss: 2.6575422286987305\n",
            "epoch: 9/10,    batch: 13838/15469    Discriminator_loss: 0.07317718863487244  Generator_loss: 2.655531883239746\n",
            "epoch: 9/10,    batch: 13839/15469    Discriminator_loss: 0.07333307713270187  Generator_loss: 2.6532373428344727\n",
            "epoch: 9/10,    batch: 13840/15469    Discriminator_loss: 0.07356847077608109  Generator_loss: 2.6506173610687256\n",
            "epoch: 9/10,    batch: 13841/15469    Discriminator_loss: 0.07378563284873962  Generator_loss: 2.648186445236206\n",
            "epoch: 9/10,    batch: 13842/15469    Discriminator_loss: 0.07400944083929062  Generator_loss: 2.6451313495635986\n",
            "epoch: 9/10,    batch: 13843/15469    Discriminator_loss: 0.07422976940870285  Generator_loss: 2.641911029815674\n",
            "epoch: 9/10,    batch: 13844/15469    Discriminator_loss: 0.07447504252195358  Generator_loss: 2.638650894165039\n",
            "epoch: 9/10,    batch: 13845/15469    Discriminator_loss: 0.07469642907381058  Generator_loss: 2.635878801345825\n",
            "epoch: 9/10,    batch: 13846/15469    Discriminator_loss: 0.07487791776657104  Generator_loss: 2.6335113048553467\n",
            "epoch: 9/10,    batch: 13847/15469    Discriminator_loss: 0.07504896819591522  Generator_loss: 2.6310157775878906\n",
            "epoch: 9/10,    batch: 13848/15469    Discriminator_loss: 0.07520202547311783  Generator_loss: 2.6294748783111572\n",
            "epoch: 9/10,    batch: 13849/15469    Discriminator_loss: 0.07529564201831818  Generator_loss: 2.628091812133789\n",
            "epoch: 9/10,    batch: 13850/15469    Discriminator_loss: 0.07535932958126068  Generator_loss: 2.6270620822906494\n",
            "epoch: 9/10,    batch: 13851/15469    Discriminator_loss: 0.07540105283260345  Generator_loss: 2.626962661743164\n",
            "epoch: 9/10,    batch: 13852/15469    Discriminator_loss: 0.07537644356489182  Generator_loss: 2.6270222663879395\n",
            "epoch: 9/10,    batch: 13853/15469    Discriminator_loss: 0.07534268498420715  Generator_loss: 2.627709150314331\n",
            "epoch: 9/10,    batch: 13854/15469    Discriminator_loss: 0.0752435028553009  Generator_loss: 2.6288909912109375\n",
            "epoch: 9/10,    batch: 13855/15469    Discriminator_loss: 0.07512989640235901  Generator_loss: 2.629958152770996\n",
            "epoch: 9/10,    batch: 13856/15469    Discriminator_loss: 0.07499387115240097  Generator_loss: 2.6321074962615967\n",
            "epoch: 9/10,    batch: 13857/15469    Discriminator_loss: 0.07483863830566406  Generator_loss: 2.634248971939087\n",
            "epoch: 9/10,    batch: 13858/15469    Discriminator_loss: 0.07461924850940704  Generator_loss: 2.63667631149292\n",
            "epoch: 9/10,    batch: 13859/15469    Discriminator_loss: 0.07441624253988266  Generator_loss: 2.639524459838867\n",
            "epoch: 9/10,    batch: 13860/15469    Discriminator_loss: 0.07417651265859604  Generator_loss: 2.642378807067871\n",
            "epoch: 9/10,    batch: 13861/15469    Discriminator_loss: 0.07397173345088959  Generator_loss: 2.645407199859619\n",
            "epoch: 9/10,    batch: 13862/15469    Discriminator_loss: 0.07373560965061188  Generator_loss: 2.6485939025878906\n",
            "epoch: 9/10,    batch: 13863/15469    Discriminator_loss: 0.07350066304206848  Generator_loss: 2.651656150817871\n",
            "epoch: 9/10,    batch: 13864/15469    Discriminator_loss: 0.07325614988803864  Generator_loss: 2.6547646522521973\n",
            "epoch: 9/10,    batch: 13865/15469    Discriminator_loss: 0.07302065193653107  Generator_loss: 2.6580028533935547\n",
            "epoch: 9/10,    batch: 13866/15469    Discriminator_loss: 0.07276450097560883  Generator_loss: 2.6613988876342773\n",
            "epoch: 9/10,    batch: 13867/15469    Discriminator_loss: 0.0725225880742073  Generator_loss: 2.664684534072876\n",
            "epoch: 9/10,    batch: 13868/15469    Discriminator_loss: 0.07226025313138962  Generator_loss: 2.668095588684082\n",
            "epoch: 9/10,    batch: 13869/15469    Discriminator_loss: 0.07198905199766159  Generator_loss: 2.671614170074463\n",
            "epoch: 9/10,    batch: 13870/15469    Discriminator_loss: 0.07172909379005432  Generator_loss: 2.6752381324768066\n",
            "epoch: 9/10,    batch: 13871/15469    Discriminator_loss: 0.07144249230623245  Generator_loss: 2.6789205074310303\n",
            "epoch: 9/10,    batch: 13872/15469    Discriminator_loss: 0.07116002589464188  Generator_loss: 2.682844877243042\n",
            "epoch: 9/10,    batch: 13873/15469    Discriminator_loss: 0.07086225599050522  Generator_loss: 2.6868577003479004\n",
            "epoch: 9/10,    batch: 13874/15469    Discriminator_loss: 0.07055815309286118  Generator_loss: 2.691028594970703\n",
            "epoch: 9/10,    batch: 13875/15469    Discriminator_loss: 0.07025650888681412  Generator_loss: 2.6953482627868652\n",
            "epoch: 9/10,    batch: 13876/15469    Discriminator_loss: 0.0699283704161644  Generator_loss: 2.699554443359375\n",
            "epoch: 9/10,    batch: 13877/15469    Discriminator_loss: 0.06961847841739655  Generator_loss: 2.7040300369262695\n",
            "epoch: 9/10,    batch: 13878/15469    Discriminator_loss: 0.06930126994848251  Generator_loss: 2.708505630493164\n",
            "epoch: 9/10,    batch: 13879/15469    Discriminator_loss: 0.06896954029798508  Generator_loss: 2.713118314743042\n",
            "epoch: 9/10,    batch: 13880/15469    Discriminator_loss: 0.068643718957901  Generator_loss: 2.717787265777588\n",
            "epoch: 9/10,    batch: 13881/15469    Discriminator_loss: 0.06830023229122162  Generator_loss: 2.722318172454834\n",
            "epoch: 9/10,    batch: 13882/15469    Discriminator_loss: 0.06796789169311523  Generator_loss: 2.727055549621582\n",
            "epoch: 9/10,    batch: 13883/15469    Discriminator_loss: 0.06763406097888947  Generator_loss: 2.731799602508545\n",
            "epoch: 9/10,    batch: 13884/15469    Discriminator_loss: 0.06730152666568756  Generator_loss: 2.7366786003112793\n",
            "epoch: 9/10,    batch: 13885/15469    Discriminator_loss: 0.0669659972190857  Generator_loss: 2.7416040897369385\n",
            "epoch: 9/10,    batch: 13886/15469    Discriminator_loss: 0.06663887202739716  Generator_loss: 2.746488094329834\n",
            "epoch: 9/10,    batch: 13887/15469    Discriminator_loss: 0.06628625094890594  Generator_loss: 2.751278877258301\n",
            "epoch: 9/10,    batch: 13888/15469    Discriminator_loss: 0.06595754623413086  Generator_loss: 2.756392002105713\n",
            "epoch: 9/10,    batch: 13889/15469    Discriminator_loss: 0.06561359018087387  Generator_loss: 2.761390447616577\n",
            "epoch: 9/10,    batch: 13890/15469    Discriminator_loss: 0.0652698203921318  Generator_loss: 2.7664148807525635\n",
            "epoch: 9/10,    batch: 13891/15469    Discriminator_loss: 0.06491955369710922  Generator_loss: 2.771388530731201\n",
            "epoch: 9/10,    batch: 13892/15469    Discriminator_loss: 0.06458684056997299  Generator_loss: 2.776675224304199\n",
            "epoch: 9/10,    batch: 13893/15469    Discriminator_loss: 0.06423535197973251  Generator_loss: 2.7818374633789062\n",
            "epoch: 9/10,    batch: 13894/15469    Discriminator_loss: 0.06388971954584122  Generator_loss: 2.7869739532470703\n",
            "epoch: 9/10,    batch: 13895/15469    Discriminator_loss: 0.06356624513864517  Generator_loss: 2.7923684120178223\n",
            "epoch: 9/10,    batch: 13896/15469    Discriminator_loss: 0.06323833763599396  Generator_loss: 2.797616958618164\n",
            "epoch: 9/10,    batch: 13897/15469    Discriminator_loss: 0.06285305321216583  Generator_loss: 2.802896022796631\n",
            "epoch: 9/10,    batch: 13898/15469    Discriminator_loss: 0.0625077560544014  Generator_loss: 2.8084239959716797\n",
            "epoch: 9/10,    batch: 13899/15469    Discriminator_loss: 0.062162499874830246  Generator_loss: 2.8136706352233887\n",
            "epoch: 9/10,    batch: 13900/15469    Discriminator_loss: 0.061818141490221024  Generator_loss: 2.819103956222534\n",
            "epoch: 9/10,    batch: 13901/15469    Discriminator_loss: 0.061470914632081985  Generator_loss: 2.824498176574707\n",
            "epoch: 9/10,    batch: 13902/15469    Discriminator_loss: 0.06113352254033089  Generator_loss: 2.829944610595703\n",
            "epoch: 9/10,    batch: 13903/15469    Discriminator_loss: 0.060787543654441833  Generator_loss: 2.835291624069214\n",
            "epoch: 9/10,    batch: 13904/15469    Discriminator_loss: 0.0604490227997303  Generator_loss: 2.8408665657043457\n",
            "epoch: 9/10,    batch: 13905/15469    Discriminator_loss: 0.06010182946920395  Generator_loss: 2.8463144302368164\n",
            "epoch: 9/10,    batch: 13906/15469    Discriminator_loss: 0.05976194888353348  Generator_loss: 2.8517913818359375\n",
            "epoch: 9/10,    batch: 13907/15469    Discriminator_loss: 0.059434425085783005  Generator_loss: 2.857356071472168\n",
            "epoch: 9/10,    batch: 13908/15469    Discriminator_loss: 0.05909058079123497  Generator_loss: 2.863100528717041\n",
            "epoch: 9/10,    batch: 13909/15469    Discriminator_loss: 0.05875210464000702  Generator_loss: 2.8683505058288574\n",
            "epoch: 9/10,    batch: 13910/15469    Discriminator_loss: 0.058412712067365646  Generator_loss: 2.8739285469055176\n",
            "epoch: 9/10,    batch: 13911/15469    Discriminator_loss: 0.058080967515707016  Generator_loss: 2.8794403076171875\n",
            "epoch: 9/10,    batch: 13912/15469    Discriminator_loss: 0.05774501711130142  Generator_loss: 2.885002613067627\n",
            "epoch: 9/10,    batch: 13913/15469    Discriminator_loss: 0.05742444843053818  Generator_loss: 2.890665292739868\n",
            "epoch: 9/10,    batch: 13914/15469    Discriminator_loss: 0.057096704840660095  Generator_loss: 2.896385669708252\n",
            "epoch: 9/10,    batch: 13915/15469    Discriminator_loss: 0.056771621108055115  Generator_loss: 2.9018778800964355\n",
            "epoch: 9/10,    batch: 13916/15469    Discriminator_loss: 0.056448183953762054  Generator_loss: 2.907349109649658\n",
            "epoch: 9/10,    batch: 13917/15469    Discriminator_loss: 0.05612218379974365  Generator_loss: 2.9129538536071777\n",
            "epoch: 9/10,    batch: 13918/15469    Discriminator_loss: 0.05578906089067459  Generator_loss: 2.9183883666992188\n",
            "epoch: 9/10,    batch: 13919/15469    Discriminator_loss: 0.05548403412103653  Generator_loss: 2.9241080284118652\n",
            "epoch: 9/10,    batch: 13920/15469    Discriminator_loss: 0.05516159534454346  Generator_loss: 2.9297308921813965\n",
            "epoch: 9/10,    batch: 13921/15469    Discriminator_loss: 0.054840002208948135  Generator_loss: 2.9353349208831787\n",
            "epoch: 9/10,    batch: 13922/15469    Discriminator_loss: 0.05452173948287964  Generator_loss: 2.9410808086395264\n",
            "epoch: 9/10,    batch: 13923/15469    Discriminator_loss: 0.05420079082250595  Generator_loss: 2.946770668029785\n",
            "epoch: 9/10,    batch: 13924/15469    Discriminator_loss: 0.05388069897890091  Generator_loss: 2.952441692352295\n",
            "epoch: 9/10,    batch: 13925/15469    Discriminator_loss: 0.05357206612825394  Generator_loss: 2.9582126140594482\n",
            "epoch: 9/10,    batch: 13926/15469    Discriminator_loss: 0.05325512960553169  Generator_loss: 2.963937997817993\n",
            "epoch: 9/10,    batch: 13927/15469    Discriminator_loss: 0.05294704809784889  Generator_loss: 2.9695382118225098\n",
            "epoch: 9/10,    batch: 13928/15469    Discriminator_loss: 0.052637819200754166  Generator_loss: 2.975339889526367\n",
            "epoch: 9/10,    batch: 13929/15469    Discriminator_loss: 0.052330512553453445  Generator_loss: 2.981027126312256\n",
            "epoch: 9/10,    batch: 13930/15469    Discriminator_loss: 0.052021417766809464  Generator_loss: 2.986715793609619\n",
            "epoch: 9/10,    batch: 13931/15469    Discriminator_loss: 0.05172039940953255  Generator_loss: 2.9923713207244873\n",
            "epoch: 9/10,    batch: 13932/15469    Discriminator_loss: 0.05141451954841614  Generator_loss: 2.998094320297241\n",
            "epoch: 9/10,    batch: 13933/15469    Discriminator_loss: 0.05111809819936752  Generator_loss: 3.0038509368896484\n",
            "epoch: 9/10,    batch: 13934/15469    Discriminator_loss: 0.0508187934756279  Generator_loss: 3.0095856189727783\n",
            "epoch: 9/10,    batch: 13935/15469    Discriminator_loss: 0.05051465332508087  Generator_loss: 3.0153274536132812\n",
            "epoch: 9/10,    batch: 13936/15469    Discriminator_loss: 0.05021435394883156  Generator_loss: 3.021071195602417\n",
            "epoch: 9/10,    batch: 13937/15469    Discriminator_loss: 0.04992053285241127  Generator_loss: 3.026886224746704\n",
            "epoch: 9/10,    batch: 13938/15469    Discriminator_loss: 0.04962592571973801  Generator_loss: 3.032721757888794\n",
            "epoch: 9/10,    batch: 13939/15469    Discriminator_loss: 0.04933258146047592  Generator_loss: 3.0384833812713623\n",
            "epoch: 9/10,    batch: 13940/15469    Discriminator_loss: 0.04904338717460632  Generator_loss: 3.0442142486572266\n",
            "epoch: 9/10,    batch: 13941/15469    Discriminator_loss: 0.048753995448350906  Generator_loss: 3.049983024597168\n",
            "epoch: 9/10,    batch: 13942/15469    Discriminator_loss: 0.048468366265296936  Generator_loss: 3.0557875633239746\n",
            "epoch: 9/10,    batch: 13943/15469    Discriminator_loss: 0.048179320991039276  Generator_loss: 3.0614962577819824\n",
            "epoch: 9/10,    batch: 13944/15469    Discriminator_loss: 0.04789690300822258  Generator_loss: 3.06731915473938\n",
            "epoch: 9/10,    batch: 13945/15469    Discriminator_loss: 0.047615211457014084  Generator_loss: 3.0731382369995117\n",
            "epoch: 9/10,    batch: 13946/15469    Discriminator_loss: 0.047333456575870514  Generator_loss: 3.078826665878296\n",
            "epoch: 9/10,    batch: 13947/15469    Discriminator_loss: 0.04705403372645378  Generator_loss: 3.0846877098083496\n",
            "epoch: 9/10,    batch: 13948/15469    Discriminator_loss: 0.04677773267030716  Generator_loss: 3.090531349182129\n",
            "epoch: 9/10,    batch: 13949/15469    Discriminator_loss: 0.046498507261276245  Generator_loss: 3.096228837966919\n",
            "epoch: 9/10,    batch: 13950/15469    Discriminator_loss: 0.04622756689786911  Generator_loss: 3.102081775665283\n",
            "epoch: 9/10,    batch: 13951/15469    Discriminator_loss: 0.045952972024679184  Generator_loss: 3.107876777648926\n",
            "epoch: 9/10,    batch: 13952/15469    Discriminator_loss: 0.0456811785697937  Generator_loss: 3.1136393547058105\n",
            "epoch: 9/10,    batch: 13953/15469    Discriminator_loss: 0.04541526734828949  Generator_loss: 3.1193737983703613\n",
            "epoch: 9/10,    batch: 13954/15469    Discriminator_loss: 0.04514569416642189  Generator_loss: 3.1250839233398438\n",
            "epoch: 9/10,    batch: 13955/15469    Discriminator_loss: 0.04488353431224823  Generator_loss: 3.1308648586273193\n",
            "epoch: 9/10,    batch: 13956/15469    Discriminator_loss: 0.04462337866425514  Generator_loss: 3.136672019958496\n",
            "epoch: 9/10,    batch: 13957/15469    Discriminator_loss: 0.04436032474040985  Generator_loss: 3.1424124240875244\n",
            "epoch: 9/10,    batch: 13958/15469    Discriminator_loss: 0.04410865902900696  Generator_loss: 3.1482017040252686\n",
            "epoch: 9/10,    batch: 13959/15469    Discriminator_loss: 0.04384792223572731  Generator_loss: 3.154006004333496\n",
            "epoch: 9/10,    batch: 13960/15469    Discriminator_loss: 0.06429057568311691  Generator_loss: 3.1578001976013184\n",
            "epoch: 9/10,    batch: 13961/15469    Discriminator_loss: 3.598858594894409  Generator_loss: 3.1309216022491455\n",
            "epoch: 9/10,    batch: 13962/15469    Discriminator_loss: 3.5544023513793945  Generator_loss: 3.0828990936279297\n",
            "epoch: 9/10,    batch: 13963/15469    Discriminator_loss: 3.5005178451538086  Generator_loss: 3.0213215351104736\n",
            "epoch: 9/10,    batch: 13964/15469    Discriminator_loss: 3.4110472202301025  Generator_loss: 2.951813220977783\n",
            "epoch: 9/10,    batch: 13965/15469    Discriminator_loss: 3.3173274993896484  Generator_loss: 2.8777217864990234\n",
            "epoch: 9/10,    batch: 13966/15469    Discriminator_loss: 3.2220582962036133  Generator_loss: 2.8022403717041016\n",
            "epoch: 9/10,    batch: 13967/15469    Discriminator_loss: 3.1211049556732178  Generator_loss: 2.7268729209899902\n",
            "epoch: 9/10,    batch: 13968/15469    Discriminator_loss: 3.040630340576172  Generator_loss: 2.652975082397461\n",
            "epoch: 9/10,    batch: 13969/15469    Discriminator_loss: 2.937868356704712  Generator_loss: 2.580770492553711\n",
            "epoch: 9/10,    batch: 13970/15469    Discriminator_loss: 2.8519749641418457  Generator_loss: 2.5112736225128174\n",
            "epoch: 9/10,    batch: 13971/15469    Discriminator_loss: 2.7807557582855225  Generator_loss: 2.4441258907318115\n",
            "epoch: 9/10,    batch: 13972/15469    Discriminator_loss: 2.695136308670044  Generator_loss: 2.3797364234924316\n",
            "epoch: 9/10,    batch: 13973/15469    Discriminator_loss: 2.6281962394714355  Generator_loss: 2.3181071281433105\n",
            "epoch: 9/10,    batch: 13974/15469    Discriminator_loss: 2.5573315620422363  Generator_loss: 2.25905179977417\n",
            "epoch: 9/10,    batch: 13975/15469    Discriminator_loss: 2.5026559829711914  Generator_loss: 2.202394485473633\n",
            "epoch: 9/10,    batch: 13976/15469    Discriminator_loss: 2.442816972732544  Generator_loss: 2.1480679512023926\n",
            "epoch: 9/10,    batch: 13977/15469    Discriminator_loss: 2.3765437602996826  Generator_loss: 2.09598445892334\n",
            "epoch: 9/10,    batch: 13978/15469    Discriminator_loss: 2.3328843116760254  Generator_loss: 2.0458827018737793\n",
            "epoch: 9/10,    batch: 13979/15469    Discriminator_loss: 2.270138740539551  Generator_loss: 1.9978597164154053\n",
            "epoch: 9/10,    batch: 13980/15469    Discriminator_loss: 2.2202625274658203  Generator_loss: 1.951823115348816\n",
            "epoch: 9/10,    batch: 13981/15469    Discriminator_loss: 2.179476261138916  Generator_loss: 1.9072265625\n",
            "epoch: 9/10,    batch: 13982/15469    Discriminator_loss: 2.13403582572937  Generator_loss: 1.8643444776535034\n",
            "epoch: 9/10,    batch: 13983/15469    Discriminator_loss: 2.0938291549682617  Generator_loss: 1.8227825164794922\n",
            "epoch: 9/10,    batch: 13984/15469    Discriminator_loss: 2.058901786804199  Generator_loss: 1.7823337316513062\n",
            "epoch: 9/10,    batch: 13985/15469    Discriminator_loss: 2.0147879123687744  Generator_loss: 1.7431697845458984\n",
            "epoch: 9/10,    batch: 13986/15469    Discriminator_loss: 1.9803268909454346  Generator_loss: 1.7053892612457275\n",
            "epoch: 9/10,    batch: 13987/15469    Discriminator_loss: 1.9444611072540283  Generator_loss: 1.6683201789855957\n",
            "epoch: 9/10,    batch: 13988/15469    Discriminator_loss: 1.9137294292449951  Generator_loss: 1.6325335502624512\n",
            "epoch: 9/10,    batch: 13989/15469    Discriminator_loss: 1.8836344480514526  Generator_loss: 1.5975632667541504\n",
            "epoch: 9/10,    batch: 13990/15469    Discriminator_loss: 1.8464659452438354  Generator_loss: 1.5637117624282837\n",
            "epoch: 9/10,    batch: 13991/15469    Discriminator_loss: 1.8115314245224  Generator_loss: 1.530556559562683\n",
            "epoch: 9/10,    batch: 13992/15469    Discriminator_loss: 1.790966272354126  Generator_loss: 1.4982001781463623\n",
            "epoch: 9/10,    batch: 13993/15469    Discriminator_loss: 1.7587964534759521  Generator_loss: 1.4665319919586182\n",
            "epoch: 9/10,    batch: 13994/15469    Discriminator_loss: 1.7229487895965576  Generator_loss: 1.435387372970581\n",
            "epoch: 9/10,    batch: 13995/15469    Discriminator_loss: 1.6886727809906006  Generator_loss: 1.4051406383514404\n",
            "epoch: 9/10,    batch: 13996/15469    Discriminator_loss: 1.6717324256896973  Generator_loss: 1.3756184577941895\n",
            "epoch: 9/10,    batch: 13997/15469    Discriminator_loss: 1.6349732875823975  Generator_loss: 1.3464748859405518\n",
            "epoch: 9/10,    batch: 13998/15469    Discriminator_loss: 1.5940595865249634  Generator_loss: 1.3178826570510864\n",
            "epoch: 9/10,    batch: 13999/15469    Discriminator_loss: 1.5642980337142944  Generator_loss: 1.2900086641311646\n",
            "epoch: 9/10,    batch: 14000/15469    Discriminator_loss: 1.5360774993896484  Generator_loss: 1.2622876167297363\n",
            "epoch: 9/10,    batch: 14001/15469    Discriminator_loss: 1.4772217273712158  Generator_loss: 1.2352395057678223\n",
            "epoch: 9/10,    batch: 14002/15469    Discriminator_loss: 1.2720986604690552  Generator_loss: 1.2101292610168457\n",
            "epoch: 9/10,    batch: 14003/15469    Discriminator_loss: 0.3600057363510132  Generator_loss: 1.19258713722229\n",
            "epoch: 9/10,    batch: 14004/15469    Discriminator_loss: 0.366303414106369  Generator_loss: 1.181431770324707\n",
            "epoch: 9/10,    batch: 14005/15469    Discriminator_loss: 0.3700752258300781  Generator_loss: 1.175707221031189\n",
            "epoch: 9/10,    batch: 14006/15469    Discriminator_loss: 0.3716251850128174  Generator_loss: 1.1743546724319458\n",
            "epoch: 9/10,    batch: 14007/15469    Discriminator_loss: 0.37129583954811096  Generator_loss: 1.1766774654388428\n",
            "epoch: 9/10,    batch: 14008/15469    Discriminator_loss: 0.3694874346256256  Generator_loss: 1.1821210384368896\n",
            "epoch: 9/10,    batch: 14009/15469    Discriminator_loss: 0.366404265165329  Generator_loss: 1.190211296081543\n",
            "epoch: 9/10,    batch: 14010/15469    Discriminator_loss: 0.3623029887676239  Generator_loss: 1.2005202770233154\n",
            "epoch: 9/10,    batch: 14011/15469    Discriminator_loss: 0.35739439725875854  Generator_loss: 1.2127668857574463\n",
            "epoch: 9/10,    batch: 14012/15469    Discriminator_loss: 0.35188615322113037  Generator_loss: 1.2264615297317505\n",
            "epoch: 9/10,    batch: 14013/15469    Discriminator_loss: 0.3458845913410187  Generator_loss: 1.2413402795791626\n",
            "epoch: 9/10,    batch: 14014/15469    Discriminator_loss: 0.33959484100341797  Generator_loss: 1.2571450471878052\n",
            "epoch: 9/10,    batch: 14015/15469    Discriminator_loss: 0.33312299847602844  Generator_loss: 1.273806095123291\n",
            "epoch: 9/10,    batch: 14016/15469    Discriminator_loss: 0.32655295729637146  Generator_loss: 1.2908726930618286\n",
            "epoch: 9/10,    batch: 14017/15469    Discriminator_loss: 0.31991347670555115  Generator_loss: 1.3084717988967896\n",
            "epoch: 9/10,    batch: 14018/15469    Discriminator_loss: 0.31335538625717163  Generator_loss: 1.3260483741760254\n",
            "epoch: 9/10,    batch: 14019/15469    Discriminator_loss: 0.3068930506706238  Generator_loss: 1.3439557552337646\n",
            "epoch: 9/10,    batch: 14020/15469    Discriminator_loss: 0.3006005883216858  Generator_loss: 1.3616323471069336\n",
            "epoch: 9/10,    batch: 14021/15469    Discriminator_loss: 0.29447245597839355  Generator_loss: 1.379380464553833\n",
            "epoch: 9/10,    batch: 14022/15469    Discriminator_loss: 0.2885094881057739  Generator_loss: 1.3967586755752563\n",
            "epoch: 9/10,    batch: 14023/15469    Discriminator_loss: 0.28278446197509766  Generator_loss: 1.414082646369934\n",
            "epoch: 9/10,    batch: 14024/15469    Discriminator_loss: 0.2772166132926941  Generator_loss: 1.4310307502746582\n",
            "epoch: 9/10,    batch: 14025/15469    Discriminator_loss: 0.2718673348426819  Generator_loss: 1.4479634761810303\n",
            "epoch: 9/10,    batch: 14026/15469    Discriminator_loss: 0.26671603322029114  Generator_loss: 1.4644181728363037\n",
            "epoch: 9/10,    batch: 14027/15469    Discriminator_loss: 0.2617470920085907  Generator_loss: 1.4806181192398071\n",
            "epoch: 9/10,    batch: 14028/15469    Discriminator_loss: 0.2569699287414551  Generator_loss: 1.4964901208877563\n",
            "epoch: 9/10,    batch: 14029/15469    Discriminator_loss: 0.25237518548965454  Generator_loss: 1.512212872505188\n",
            "epoch: 9/10,    batch: 14030/15469    Discriminator_loss: 0.24794620275497437  Generator_loss: 1.5274531841278076\n",
            "epoch: 9/10,    batch: 14031/15469    Discriminator_loss: 0.24369487166404724  Generator_loss: 1.5426450967788696\n",
            "epoch: 9/10,    batch: 14032/15469    Discriminator_loss: 0.23960265517234802  Generator_loss: 1.557548999786377\n",
            "epoch: 9/10,    batch: 14033/15469    Discriminator_loss: 0.2356230616569519  Generator_loss: 1.5720772743225098\n",
            "epoch: 9/10,    batch: 14034/15469    Discriminator_loss: 0.23182088136672974  Generator_loss: 1.5864031314849854\n",
            "epoch: 9/10,    batch: 14035/15469    Discriminator_loss: 0.22815071046352386  Generator_loss: 1.6005245447158813\n",
            "epoch: 9/10,    batch: 14036/15469    Discriminator_loss: 0.2245796024799347  Generator_loss: 1.614327073097229\n",
            "epoch: 9/10,    batch: 14037/15469    Discriminator_loss: 0.22112412750720978  Generator_loss: 1.6279634237289429\n",
            "epoch: 9/10,    batch: 14038/15469    Discriminator_loss: 0.2177952527999878  Generator_loss: 1.6412968635559082\n",
            "epoch: 9/10,    batch: 14039/15469    Discriminator_loss: 0.21459951996803284  Generator_loss: 1.6544607877731323\n",
            "epoch: 9/10,    batch: 14040/15469    Discriminator_loss: 0.2114812731742859  Generator_loss: 1.6674649715423584\n",
            "epoch: 9/10,    batch: 14041/15469    Discriminator_loss: 0.2084522247314453  Generator_loss: 1.6802524328231812\n",
            "epoch: 9/10,    batch: 14042/15469    Discriminator_loss: 0.2055453062057495  Generator_loss: 1.6927731037139893\n",
            "epoch: 9/10,    batch: 14043/15469    Discriminator_loss: 0.20268023014068604  Generator_loss: 1.705321192741394\n",
            "epoch: 9/10,    batch: 14044/15469    Discriminator_loss: 0.19990859925746918  Generator_loss: 1.7175633907318115\n",
            "epoch: 9/10,    batch: 14045/15469    Discriminator_loss: 0.19723519682884216  Generator_loss: 1.7296792268753052\n",
            "epoch: 9/10,    batch: 14046/15469    Discriminator_loss: 0.1946028470993042  Generator_loss: 1.7416176795959473\n",
            "epoch: 9/10,    batch: 14047/15469    Discriminator_loss: 0.1920631229877472  Generator_loss: 1.7534462213516235\n",
            "epoch: 9/10,    batch: 14048/15469    Discriminator_loss: 0.18957354128360748  Generator_loss: 1.765114665031433\n",
            "epoch: 9/10,    batch: 14049/15469    Discriminator_loss: 0.18715836107730865  Generator_loss: 1.7766854763031006\n",
            "epoch: 9/10,    batch: 14050/15469    Discriminator_loss: 0.18480786681175232  Generator_loss: 1.7880581617355347\n",
            "epoch: 9/10,    batch: 14051/15469    Discriminator_loss: 0.18251031637191772  Generator_loss: 1.7993850708007812\n",
            "epoch: 9/10,    batch: 14052/15469    Discriminator_loss: 0.1802714467048645  Generator_loss: 1.8104883432388306\n",
            "epoch: 9/10,    batch: 14053/15469    Discriminator_loss: 0.17808972299098969  Generator_loss: 1.8215782642364502\n",
            "epoch: 9/10,    batch: 14054/15469    Discriminator_loss: 0.17595122754573822  Generator_loss: 1.832480549812317\n",
            "epoch: 9/10,    batch: 14055/15469    Discriminator_loss: 0.17386215925216675  Generator_loss: 1.8432997465133667\n",
            "epoch: 9/10,    batch: 14056/15469    Discriminator_loss: 0.1718219816684723  Generator_loss: 1.8540222644805908\n",
            "epoch: 9/10,    batch: 14057/15469    Discriminator_loss: 0.169819638133049  Generator_loss: 1.8646142482757568\n",
            "epoch: 9/10,    batch: 14058/15469    Discriminator_loss: 0.16786253452301025  Generator_loss: 1.8751505613327026\n",
            "epoch: 9/10,    batch: 14059/15469    Discriminator_loss: 0.16596587002277374  Generator_loss: 1.8855047225952148\n",
            "epoch: 9/10,    batch: 14060/15469    Discriminator_loss: 0.16410242021083832  Generator_loss: 1.8958375453948975\n",
            "epoch: 9/10,    batch: 14061/15469    Discriminator_loss: 0.16226758062839508  Generator_loss: 1.906128168106079\n",
            "epoch: 9/10,    batch: 14062/15469    Discriminator_loss: 0.1604703962802887  Generator_loss: 1.916278600692749\n",
            "epoch: 9/10,    batch: 14063/15469    Discriminator_loss: 0.15871597826480865  Generator_loss: 1.9263371229171753\n",
            "epoch: 9/10,    batch: 14064/15469    Discriminator_loss: 0.1569761484861374  Generator_loss: 1.9364187717437744\n",
            "epoch: 9/10,    batch: 14065/15469    Discriminator_loss: 0.15529672801494598  Generator_loss: 1.946345329284668\n",
            "epoch: 9/10,    batch: 14066/15469    Discriminator_loss: 0.15364140272140503  Generator_loss: 1.956152081489563\n",
            "epoch: 9/10,    batch: 14067/15469    Discriminator_loss: 0.1520017385482788  Generator_loss: 1.965981364250183\n",
            "epoch: 9/10,    batch: 14068/15469    Discriminator_loss: 0.15041139721870422  Generator_loss: 1.975637674331665\n",
            "epoch: 9/10,    batch: 14069/15469    Discriminator_loss: 0.1488322615623474  Generator_loss: 1.9852771759033203\n",
            "epoch: 9/10,    batch: 14070/15469    Discriminator_loss: 0.1472979038953781  Generator_loss: 1.994903802871704\n",
            "epoch: 9/10,    batch: 14071/15469    Discriminator_loss: 0.14579147100448608  Generator_loss: 2.0044407844543457\n",
            "epoch: 9/10,    batch: 14072/15469    Discriminator_loss: 0.14429694414138794  Generator_loss: 2.013904571533203\n",
            "epoch: 9/10,    batch: 14073/15469    Discriminator_loss: 0.14284361898899078  Generator_loss: 2.023251533508301\n",
            "epoch: 9/10,    batch: 14074/15469    Discriminator_loss: 0.14141008257865906  Generator_loss: 2.0325088500976562\n",
            "epoch: 9/10,    batch: 14075/15469    Discriminator_loss: 0.1400074064731598  Generator_loss: 2.041797637939453\n",
            "epoch: 9/10,    batch: 14076/15469    Discriminator_loss: 0.13862159848213196  Generator_loss: 2.05100154876709\n",
            "epoch: 9/10,    batch: 14077/15469    Discriminator_loss: 0.13725975155830383  Generator_loss: 2.060119390487671\n",
            "epoch: 9/10,    batch: 14078/15469    Discriminator_loss: 0.13592909276485443  Generator_loss: 2.0692245960235596\n",
            "epoch: 9/10,    batch: 14079/15469    Discriminator_loss: 0.13461236655712128  Generator_loss: 2.078212261199951\n",
            "epoch: 9/10,    batch: 14080/15469    Discriminator_loss: 0.13331496715545654  Generator_loss: 2.0872304439544678\n",
            "epoch: 9/10,    batch: 14081/15469    Discriminator_loss: 0.13205452263355255  Generator_loss: 2.0961546897888184\n",
            "epoch: 9/10,    batch: 14082/15469    Discriminator_loss: 0.13079984486103058  Generator_loss: 2.1049857139587402\n",
            "epoch: 9/10,    batch: 14083/15469    Discriminator_loss: 0.12957611680030823  Generator_loss: 2.113769292831421\n",
            "epoch: 9/10,    batch: 14084/15469    Discriminator_loss: 0.12835705280303955  Generator_loss: 2.1225473880767822\n",
            "epoch: 9/10,    batch: 14085/15469    Discriminator_loss: 0.12716732919216156  Generator_loss: 2.1311774253845215\n",
            "epoch: 9/10,    batch: 14086/15469    Discriminator_loss: 0.12600548565387726  Generator_loss: 2.1398253440856934\n",
            "epoch: 9/10,    batch: 14087/15469    Discriminator_loss: 0.12485858798027039  Generator_loss: 2.148362159729004\n",
            "epoch: 9/10,    batch: 14088/15469    Discriminator_loss: 0.1237240880727768  Generator_loss: 2.156879425048828\n",
            "epoch: 9/10,    batch: 14089/15469    Discriminator_loss: 0.12260714918375015  Generator_loss: 2.1653738021850586\n",
            "epoch: 9/10,    batch: 14090/15469    Discriminator_loss: 0.12150311470031738  Generator_loss: 2.1737918853759766\n",
            "epoch: 9/10,    batch: 14091/15469    Discriminator_loss: 0.12042789906263351  Generator_loss: 2.18206787109375\n",
            "epoch: 9/10,    batch: 14092/15469    Discriminator_loss: 0.11937563866376877  Generator_loss: 2.1903624534606934\n",
            "epoch: 9/10,    batch: 14093/15469    Discriminator_loss: 0.11832953989505768  Generator_loss: 2.1985909938812256\n",
            "epoch: 9/10,    batch: 14094/15469    Discriminator_loss: 0.11730008572340012  Generator_loss: 2.2067575454711914\n",
            "epoch: 9/10,    batch: 14095/15469    Discriminator_loss: 0.11629093438386917  Generator_loss: 2.2148900032043457\n",
            "epoch: 9/10,    batch: 14096/15469    Discriminator_loss: 0.11529234051704407  Generator_loss: 2.2229084968566895\n",
            "epoch: 9/10,    batch: 14097/15469    Discriminator_loss: 0.11431785672903061  Generator_loss: 2.230940818786621\n",
            "epoch: 9/10,    batch: 14098/15469    Discriminator_loss: 0.11336415261030197  Generator_loss: 2.2389092445373535\n",
            "epoch: 9/10,    batch: 14099/15469    Discriminator_loss: 0.11241818219423294  Generator_loss: 2.246735095977783\n",
            "epoch: 9/10,    batch: 14100/15469    Discriminator_loss: 0.11148843914270401  Generator_loss: 2.2545039653778076\n",
            "epoch: 9/10,    batch: 14101/15469    Discriminator_loss: 0.11057605594396591  Generator_loss: 2.2622947692871094\n",
            "epoch: 9/10,    batch: 14102/15469    Discriminator_loss: 0.10967649519443512  Generator_loss: 2.2699217796325684\n",
            "epoch: 9/10,    batch: 14103/15469    Discriminator_loss: 0.10879708081483841  Generator_loss: 2.277618646621704\n",
            "epoch: 9/10,    batch: 14104/15469    Discriminator_loss: 0.10792601108551025  Generator_loss: 2.285172462463379\n",
            "epoch: 9/10,    batch: 14105/15469    Discriminator_loss: 0.10708115249872208  Generator_loss: 2.292576551437378\n",
            "epoch: 9/10,    batch: 14106/15469    Discriminator_loss: 0.10624127089977264  Generator_loss: 2.299994707107544\n",
            "epoch: 9/10,    batch: 14107/15469    Discriminator_loss: 0.1054176539182663  Generator_loss: 2.307255268096924\n",
            "epoch: 9/10,    batch: 14108/15469    Discriminator_loss: 0.10461548715829849  Generator_loss: 2.314513683319092\n",
            "epoch: 9/10,    batch: 14109/15469    Discriminator_loss: 0.10382673144340515  Generator_loss: 2.321669340133667\n",
            "epoch: 9/10,    batch: 14110/15469    Discriminator_loss: 0.10305213928222656  Generator_loss: 2.328784704208374\n",
            "epoch: 9/10,    batch: 14111/15469    Discriminator_loss: 0.10229339450597763  Generator_loss: 2.3357231616973877\n",
            "epoch: 9/10,    batch: 14112/15469    Discriminator_loss: 0.10154885053634644  Generator_loss: 2.3426313400268555\n",
            "epoch: 9/10,    batch: 14113/15469    Discriminator_loss: 0.10083310306072235  Generator_loss: 2.3494338989257812\n",
            "epoch: 9/10,    batch: 14114/15469    Discriminator_loss: 0.10011498630046844  Generator_loss: 2.3561277389526367\n",
            "epoch: 9/10,    batch: 14115/15469    Discriminator_loss: 0.09941814839839935  Generator_loss: 2.3627090454101562\n",
            "epoch: 9/10,    batch: 14116/15469    Discriminator_loss: 0.09873780608177185  Generator_loss: 2.369250774383545\n",
            "epoch: 9/10,    batch: 14117/15469    Discriminator_loss: 0.09806116670370102  Generator_loss: 2.375675678253174\n",
            "epoch: 9/10,    batch: 14118/15469    Discriminator_loss: 2.065657615661621  Generator_loss: 2.3707633018493652\n",
            "epoch: 9/10,    batch: 14119/15469    Discriminator_loss: 2.429550886154175  Generator_loss: 2.3549046516418457\n",
            "epoch: 9/10,    batch: 14120/15469    Discriminator_loss: 0.9536049962043762  Generator_loss: 2.338170051574707\n",
            "epoch: 9/10,    batch: 14121/15469    Discriminator_loss: 0.1023569330573082  Generator_loss: 2.325967788696289\n",
            "epoch: 9/10,    batch: 14122/15469    Discriminator_loss: 0.10347601771354675  Generator_loss: 2.317288875579834\n",
            "epoch: 9/10,    batch: 14123/15469    Discriminator_loss: 0.10425522923469543  Generator_loss: 2.311542510986328\n",
            "epoch: 9/10,    batch: 14124/15469    Discriminator_loss: 0.10475679486989975  Generator_loss: 2.3079946041107178\n",
            "epoch: 9/10,    batch: 14125/15469    Discriminator_loss: 0.10503546893596649  Generator_loss: 2.306411027908325\n",
            "epoch: 9/10,    batch: 14126/15469    Discriminator_loss: 0.105130136013031  Generator_loss: 2.3062310218811035\n",
            "epoch: 9/10,    batch: 14127/15469    Discriminator_loss: 0.10508081316947937  Generator_loss: 2.30726957321167\n",
            "epoch: 9/10,    batch: 14128/15469    Discriminator_loss: 0.10490552335977554  Generator_loss: 2.3092620372772217\n",
            "epoch: 9/10,    batch: 14129/15469    Discriminator_loss: 0.10464444011449814  Generator_loss: 2.3120250701904297\n",
            "epoch: 9/10,    batch: 14130/15469    Discriminator_loss: 0.10430371761322021  Generator_loss: 2.3153908252716064\n",
            "epoch: 9/10,    batch: 14131/15469    Discriminator_loss: 0.10389944165945053  Generator_loss: 2.3193180561065674\n",
            "epoch: 9/10,    batch: 14132/15469    Discriminator_loss: 0.10345547646284103  Generator_loss: 2.323549270629883\n",
            "epoch: 9/10,    batch: 14133/15469    Discriminator_loss: 0.10297203063964844  Generator_loss: 2.3281195163726807\n",
            "epoch: 9/10,    batch: 14134/15469    Discriminator_loss: 0.10246910154819489  Generator_loss: 2.3329031467437744\n",
            "epoch: 9/10,    batch: 14135/15469    Discriminator_loss: 0.10194186121225357  Generator_loss: 2.337921380996704\n",
            "epoch: 9/10,    batch: 14136/15469    Discriminator_loss: 0.10139606148004532  Generator_loss: 2.3430426120758057\n",
            "epoch: 9/10,    batch: 14137/15469    Discriminator_loss: 0.1008424386382103  Generator_loss: 2.348287582397461\n",
            "epoch: 9/10,    batch: 14138/15469    Discriminator_loss: 0.1002870574593544  Generator_loss: 2.353626251220703\n",
            "epoch: 9/10,    batch: 14139/15469    Discriminator_loss: 0.09971986711025238  Generator_loss: 2.3590102195739746\n",
            "epoch: 9/10,    batch: 14140/15469    Discriminator_loss: 0.0991540402173996  Generator_loss: 2.3644211292266846\n",
            "epoch: 9/10,    batch: 14141/15469    Discriminator_loss: 0.09858979284763336  Generator_loss: 2.369896411895752\n",
            "epoch: 9/10,    batch: 14142/15469    Discriminator_loss: 0.09802672266960144  Generator_loss: 2.375344753265381\n",
            "epoch: 9/10,    batch: 14143/15469    Discriminator_loss: 0.09746014326810837  Generator_loss: 2.3808326721191406\n",
            "epoch: 9/10,    batch: 14144/15469    Discriminator_loss: 0.09690961241722107  Generator_loss: 2.3863067626953125\n",
            "epoch: 9/10,    batch: 14145/15469    Discriminator_loss: 0.09634867310523987  Generator_loss: 2.391753673553467\n",
            "epoch: 9/10,    batch: 14146/15469    Discriminator_loss: 0.09580428898334503  Generator_loss: 2.3972110748291016\n",
            "epoch: 9/10,    batch: 14147/15469    Discriminator_loss: 0.09525727480649948  Generator_loss: 2.402663230895996\n",
            "epoch: 9/10,    batch: 14148/15469    Discriminator_loss: 0.0947190374135971  Generator_loss: 2.408021926879883\n",
            "epoch: 9/10,    batch: 14149/15469    Discriminator_loss: 0.09418430179357529  Generator_loss: 2.4134328365325928\n",
            "epoch: 9/10,    batch: 14150/15469    Discriminator_loss: 0.0936540737748146  Generator_loss: 2.4188337326049805\n",
            "epoch: 9/10,    batch: 14151/15469    Discriminator_loss: 0.09312862157821655  Generator_loss: 2.4241533279418945\n",
            "epoch: 9/10,    batch: 14152/15469    Discriminator_loss: 0.09260901808738708  Generator_loss: 2.429497718811035\n",
            "epoch: 9/10,    batch: 14153/15469    Discriminator_loss: 0.09209226071834564  Generator_loss: 2.4348599910736084\n",
            "epoch: 9/10,    batch: 14154/15469    Discriminator_loss: 0.09158399701118469  Generator_loss: 2.4401092529296875\n",
            "epoch: 9/10,    batch: 14155/15469    Discriminator_loss: 0.09107609838247299  Generator_loss: 2.4453935623168945\n",
            "epoch: 9/10,    batch: 14156/15469    Discriminator_loss: 0.09057663381099701  Generator_loss: 2.450636386871338\n",
            "epoch: 9/10,    batch: 14157/15469    Discriminator_loss: 0.09008301794528961  Generator_loss: 2.455878973007202\n",
            "epoch: 9/10,    batch: 14158/15469    Discriminator_loss: 0.08958670496940613  Generator_loss: 2.4611291885375977\n",
            "epoch: 9/10,    batch: 14159/15469    Discriminator_loss: 0.08909790217876434  Generator_loss: 2.466369152069092\n",
            "epoch: 9/10,    batch: 14160/15469    Discriminator_loss: 0.08861363679170609  Generator_loss: 2.4715523719787598\n",
            "epoch: 9/10,    batch: 14161/15469    Discriminator_loss: 0.08813752233982086  Generator_loss: 2.4767343997955322\n",
            "epoch: 9/10,    batch: 14162/15469    Discriminator_loss: 0.0876871645450592  Generator_loss: 2.4819350242614746\n",
            "epoch: 9/10,    batch: 14163/15469    Discriminator_loss: 0.08720207214355469  Generator_loss: 2.487126350402832\n",
            "epoch: 9/10,    batch: 14164/15469    Discriminator_loss: 0.08671195805072784  Generator_loss: 2.492274761199951\n",
            "epoch: 9/10,    batch: 14165/15469    Discriminator_loss: 0.0862395316362381  Generator_loss: 2.497513771057129\n",
            "epoch: 9/10,    batch: 14166/15469    Discriminator_loss: 0.08577670902013779  Generator_loss: 2.5026187896728516\n",
            "epoch: 9/10,    batch: 14167/15469    Discriminator_loss: 0.08531521260738373  Generator_loss: 2.5077872276306152\n",
            "epoch: 9/10,    batch: 14168/15469    Discriminator_loss: 0.0848558098077774  Generator_loss: 2.512932777404785\n",
            "epoch: 9/10,    batch: 14169/15469    Discriminator_loss: 0.08440006524324417  Generator_loss: 2.5181126594543457\n",
            "epoch: 9/10,    batch: 14170/15469    Discriminator_loss: 0.08394875377416611  Generator_loss: 2.5232536792755127\n",
            "epoch: 9/10,    batch: 14171/15469    Discriminator_loss: 0.08349353820085526  Generator_loss: 2.5284183025360107\n",
            "epoch: 9/10,    batch: 14172/15469    Discriminator_loss: 0.08304982632398605  Generator_loss: 2.5335776805877686\n",
            "epoch: 9/10,    batch: 14173/15469    Discriminator_loss: 0.0826009139418602  Generator_loss: 2.538745164871216\n",
            "epoch: 9/10,    batch: 14174/15469    Discriminator_loss: 0.08215601742267609  Generator_loss: 2.543891429901123\n",
            "epoch: 9/10,    batch: 14175/15469    Discriminator_loss: 0.08171948790550232  Generator_loss: 2.549060344696045\n",
            "epoch: 9/10,    batch: 14176/15469    Discriminator_loss: 0.0812787339091301  Generator_loss: 2.5542502403259277\n",
            "epoch: 9/10,    batch: 14177/15469    Discriminator_loss: 0.08083811402320862  Generator_loss: 2.5594136714935303\n",
            "epoch: 9/10,    batch: 14178/15469    Discriminator_loss: 0.08040343225002289  Generator_loss: 2.564570903778076\n",
            "epoch: 9/10,    batch: 14179/15469    Discriminator_loss: 0.07997282594442368  Generator_loss: 2.5697455406188965\n",
            "epoch: 9/10,    batch: 14180/15469    Discriminator_loss: 0.07954321801662445  Generator_loss: 2.574920654296875\n",
            "epoch: 9/10,    batch: 14181/15469    Discriminator_loss: 0.07911300659179688  Generator_loss: 2.5800890922546387\n",
            "epoch: 9/10,    batch: 14182/15469    Discriminator_loss: 0.07868625223636627  Generator_loss: 2.585284948348999\n",
            "epoch: 9/10,    batch: 14183/15469    Discriminator_loss: 0.07826423645019531  Generator_loss: 2.590475559234619\n",
            "epoch: 9/10,    batch: 14184/15469    Discriminator_loss: 0.07783963531255722  Generator_loss: 2.5957024097442627\n",
            "epoch: 9/10,    batch: 14185/15469    Discriminator_loss: 0.07742061465978622  Generator_loss: 2.6008830070495605\n",
            "epoch: 9/10,    batch: 14186/15469    Discriminator_loss: 0.07700207829475403  Generator_loss: 2.6060562133789062\n",
            "epoch: 9/10,    batch: 14187/15469    Discriminator_loss: 0.07658576965332031  Generator_loss: 2.6113061904907227\n",
            "epoch: 9/10,    batch: 14188/15469    Discriminator_loss: 0.07616987824440002  Generator_loss: 2.616487741470337\n",
            "epoch: 9/10,    batch: 14189/15469    Discriminator_loss: 0.07575862109661102  Generator_loss: 2.6217541694641113\n",
            "epoch: 9/10,    batch: 14190/15469    Discriminator_loss: 0.07534871250391006  Generator_loss: 2.626943588256836\n",
            "epoch: 9/10,    batch: 14191/15469    Discriminator_loss: 0.07494150102138519  Generator_loss: 2.6321797370910645\n",
            "epoch: 9/10,    batch: 14192/15469    Discriminator_loss: 0.0745355486869812  Generator_loss: 2.637396812438965\n",
            "epoch: 9/10,    batch: 14193/15469    Discriminator_loss: 0.07412856817245483  Generator_loss: 2.6426727771759033\n",
            "epoch: 9/10,    batch: 14194/15469    Discriminator_loss: 0.07373024523258209  Generator_loss: 2.647876262664795\n",
            "epoch: 9/10,    batch: 14195/15469    Discriminator_loss: 0.07332781702280045  Generator_loss: 2.653141498565674\n",
            "epoch: 9/10,    batch: 14196/15469    Discriminator_loss: 0.07293359935283661  Generator_loss: 2.6583406925201416\n",
            "epoch: 9/10,    batch: 14197/15469    Discriminator_loss: 0.07253346592187881  Generator_loss: 2.663646697998047\n",
            "epoch: 9/10,    batch: 14198/15469    Discriminator_loss: 0.07214149087667465  Generator_loss: 2.6688640117645264\n",
            "epoch: 9/10,    batch: 14199/15469    Discriminator_loss: 0.07174775004386902  Generator_loss: 2.6741228103637695\n",
            "epoch: 9/10,    batch: 14200/15469    Discriminator_loss: 0.07135787606239319  Generator_loss: 2.6793642044067383\n",
            "epoch: 9/10,    batch: 14201/15469    Discriminator_loss: 0.07096906751394272  Generator_loss: 2.684682846069336\n",
            "epoch: 9/10,    batch: 14202/15469    Discriminator_loss: 0.07058562338352203  Generator_loss: 2.689905881881714\n",
            "epoch: 9/10,    batch: 14203/15469    Discriminator_loss: 0.07020101696252823  Generator_loss: 2.6951303482055664\n",
            "epoch: 9/10,    batch: 14204/15469    Discriminator_loss: 0.06981809437274933  Generator_loss: 2.7003684043884277\n",
            "epoch: 9/10,    batch: 14205/15469    Discriminator_loss: 0.06943926960229874  Generator_loss: 2.705671548843384\n",
            "epoch: 9/10,    batch: 14206/15469    Discriminator_loss: 0.06905965507030487  Generator_loss: 2.7109475135803223\n",
            "epoch: 9/10,    batch: 14207/15469    Discriminator_loss: 0.06868533045053482  Generator_loss: 2.716174602508545\n",
            "epoch: 9/10,    batch: 14208/15469    Discriminator_loss: 0.06831047683954239  Generator_loss: 2.7214913368225098\n",
            "epoch: 9/10,    batch: 14209/15469    Discriminator_loss: 0.06793975830078125  Generator_loss: 2.726677894592285\n",
            "epoch: 9/10,    batch: 14210/15469    Discriminator_loss: 0.0675727128982544  Generator_loss: 2.7319681644439697\n",
            "epoch: 9/10,    batch: 14211/15469    Discriminator_loss: 0.06720501184463501  Generator_loss: 2.7372288703918457\n",
            "epoch: 9/10,    batch: 14212/15469    Discriminator_loss: 0.06684037297964096  Generator_loss: 2.742509126663208\n",
            "epoch: 9/10,    batch: 14213/15469    Discriminator_loss: 0.06647734344005585  Generator_loss: 2.7477641105651855\n",
            "epoch: 9/10,    batch: 14214/15469    Discriminator_loss: 0.06611663848161697  Generator_loss: 2.752987861633301\n",
            "epoch: 9/10,    batch: 14215/15469    Discriminator_loss: 0.0657561719417572  Generator_loss: 2.758216142654419\n",
            "epoch: 9/10,    batch: 14216/15469    Discriminator_loss: 0.0654059499502182  Generator_loss: 2.763404369354248\n",
            "epoch: 9/10,    batch: 14217/15469    Discriminator_loss: 0.06505168229341507  Generator_loss: 2.7686705589294434\n",
            "epoch: 9/10,    batch: 14218/15469    Discriminator_loss: 0.06470531225204468  Generator_loss: 2.773864984512329\n",
            "epoch: 9/10,    batch: 14219/15469    Discriminator_loss: 0.06435844302177429  Generator_loss: 2.7790470123291016\n",
            "epoch: 9/10,    batch: 14220/15469    Discriminator_loss: 0.06401102989912033  Generator_loss: 2.784304141998291\n",
            "epoch: 9/10,    batch: 14221/15469    Discriminator_loss: 0.06366918236017227  Generator_loss: 2.7894673347473145\n",
            "epoch: 9/10,    batch: 14222/15469    Discriminator_loss: 0.0633307546377182  Generator_loss: 2.794651508331299\n",
            "epoch: 9/10,    batch: 14223/15469    Discriminator_loss: 0.06298958510160446  Generator_loss: 2.7998218536376953\n",
            "epoch: 9/10,    batch: 14224/15469    Discriminator_loss: 0.06265787780284882  Generator_loss: 2.80501651763916\n",
            "epoch: 9/10,    batch: 14225/15469    Discriminator_loss: 0.06232365965843201  Generator_loss: 2.8101308345794678\n",
            "epoch: 9/10,    batch: 14226/15469    Discriminator_loss: 0.06199381873011589  Generator_loss: 2.815323829650879\n",
            "epoch: 9/10,    batch: 14227/15469    Discriminator_loss: 0.061668045818805695  Generator_loss: 2.820392608642578\n",
            "epoch: 9/10,    batch: 14228/15469    Discriminator_loss: 0.06134489178657532  Generator_loss: 2.8255820274353027\n",
            "epoch: 9/10,    batch: 14229/15469    Discriminator_loss: 0.061020638793706894  Generator_loss: 2.830660104751587\n",
            "epoch: 9/10,    batch: 14230/15469    Discriminator_loss: 0.060703095048666  Generator_loss: 2.8356940746307373\n",
            "epoch: 9/10,    batch: 14231/15469    Discriminator_loss: 0.060384538024663925  Generator_loss: 2.840806722640991\n",
            "epoch: 9/10,    batch: 14232/15469    Discriminator_loss: 0.060538094490766525  Generator_loss: 2.845858573913574\n",
            "epoch: 9/10,    batch: 14233/15469    Discriminator_loss: 3.01554274559021  Generator_loss: 2.8314874172210693\n",
            "epoch: 9/10,    batch: 14234/15469    Discriminator_loss: 3.0001819133758545  Generator_loss: 2.8021340370178223\n",
            "epoch: 9/10,    batch: 14235/15469    Discriminator_loss: 2.952746868133545  Generator_loss: 2.762176752090454\n",
            "epoch: 9/10,    batch: 14236/15469    Discriminator_loss: 2.9194626808166504  Generator_loss: 2.714834690093994\n",
            "epoch: 9/10,    batch: 14237/15469    Discriminator_loss: 2.8713502883911133  Generator_loss: 2.662649631500244\n",
            "epoch: 9/10,    batch: 14238/15469    Discriminator_loss: 2.805992603302002  Generator_loss: 2.6075069904327393\n",
            "epoch: 9/10,    batch: 14239/15469    Discriminator_loss: 2.7441253662109375  Generator_loss: 2.5509984493255615\n",
            "epoch: 9/10,    batch: 14240/15469    Discriminator_loss: 2.682964324951172  Generator_loss: 2.494253158569336\n",
            "epoch: 9/10,    batch: 14241/15469    Discriminator_loss: 2.6372365951538086  Generator_loss: 2.437607765197754\n",
            "epoch: 9/10,    batch: 14242/15469    Discriminator_loss: 2.578131675720215  Generator_loss: 2.381671190261841\n",
            "epoch: 9/10,    batch: 14243/15469    Discriminator_loss: 2.506063461303711  Generator_loss: 2.327012062072754\n",
            "epoch: 9/10,    batch: 14244/15469    Discriminator_loss: 2.44344425201416  Generator_loss: 2.273721218109131\n",
            "epoch: 9/10,    batch: 14245/15469    Discriminator_loss: 2.386310577392578  Generator_loss: 2.221881866455078\n",
            "epoch: 9/10,    batch: 14246/15469    Discriminator_loss: 2.329326629638672  Generator_loss: 2.17159104347229\n",
            "epoch: 9/10,    batch: 14247/15469    Discriminator_loss: 2.272448778152466  Generator_loss: 2.1228110790252686\n",
            "epoch: 9/10,    batch: 14248/15469    Discriminator_loss: 2.2110695838928223  Generator_loss: 2.075648784637451\n",
            "epoch: 9/10,    batch: 14249/15469    Discriminator_loss: 2.1666667461395264  Generator_loss: 2.0299665927886963\n",
            "epoch: 9/10,    batch: 14250/15469    Discriminator_loss: 2.118826150894165  Generator_loss: 1.9857670068740845\n",
            "epoch: 9/10,    batch: 14251/15469    Discriminator_loss: 2.0756452083587646  Generator_loss: 1.9428486824035645\n",
            "epoch: 9/10,    batch: 14252/15469    Discriminator_loss: 2.017235279083252  Generator_loss: 1.9011045694351196\n",
            "epoch: 9/10,    batch: 14253/15469    Discriminator_loss: 1.9594517946243286  Generator_loss: 1.860347032546997\n",
            "epoch: 9/10,    batch: 14254/15469    Discriminator_loss: 1.9064563512802124  Generator_loss: 1.8205722570419312\n",
            "epoch: 9/10,    batch: 14255/15469    Discriminator_loss: 1.8677465915679932  Generator_loss: 1.781623363494873\n",
            "epoch: 9/10,    batch: 14256/15469    Discriminator_loss: 1.7961935997009277  Generator_loss: 1.7432730197906494\n",
            "epoch: 9/10,    batch: 14257/15469    Discriminator_loss: 1.741227626800537  Generator_loss: 1.7050807476043701\n",
            "epoch: 9/10,    batch: 14258/15469    Discriminator_loss: 1.6628426313400269  Generator_loss: 1.667001724243164\n",
            "epoch: 9/10,    batch: 14259/15469    Discriminator_loss: 1.5922961235046387  Generator_loss: 1.6276895999908447\n",
            "epoch: 9/10,    batch: 14260/15469    Discriminator_loss: 1.5328818559646606  Generator_loss: 1.5870894193649292\n",
            "epoch: 9/10,    batch: 14261/15469    Discriminator_loss: 1.4640532732009888  Generator_loss: 1.5423014163970947\n",
            "epoch: 9/10,    batch: 14262/15469    Discriminator_loss: 1.3951127529144287  Generator_loss: 1.492475152015686\n",
            "epoch: 9/10,    batch: 14263/15469    Discriminator_loss: 1.3312394618988037  Generator_loss: 1.4352772235870361\n",
            "epoch: 9/10,    batch: 14264/15469    Discriminator_loss: 1.2677570581436157  Generator_loss: 1.3671419620513916\n",
            "epoch: 9/10,    batch: 14265/15469    Discriminator_loss: 1.2435963153839111  Generator_loss: 1.2894833087921143\n",
            "epoch: 9/10,    batch: 14266/15469    Discriminator_loss: 1.1828148365020752  Generator_loss: 1.202347993850708\n",
            "epoch: 9/10,    batch: 14267/15469    Discriminator_loss: 1.1419094800949097  Generator_loss: 1.1126091480255127\n",
            "epoch: 9/10,    batch: 14268/15469    Discriminator_loss: 1.1320769786834717  Generator_loss: 1.02341628074646\n",
            "epoch: 9/10,    batch: 14269/15469    Discriminator_loss: 1.0956528186798096  Generator_loss: 0.9394413828849792\n",
            "epoch: 9/10,    batch: 14270/15469    Discriminator_loss: 1.0340461730957031  Generator_loss: 0.8654203414916992\n",
            "epoch: 9/10,    batch: 14271/15469    Discriminator_loss: 1.0797569751739502  Generator_loss: 0.8030532598495483\n",
            "epoch: 9/10,    batch: 14272/15469    Discriminator_loss: 0.6329976916313171  Generator_loss: 0.7630003690719604\n",
            "epoch: 9/10,    batch: 14273/15469    Discriminator_loss: 0.6530935764312744  Generator_loss: 0.7398733496665955\n",
            "epoch: 9/10,    batch: 14274/15469    Discriminator_loss: 0.6686543822288513  Generator_loss: 0.7301104068756104\n",
            "epoch: 9/10,    batch: 14275/15469    Discriminator_loss: 0.6779294610023499  Generator_loss: 0.7273818850517273\n",
            "epoch: 9/10,    batch: 14276/15469    Discriminator_loss: 0.6850989460945129  Generator_loss: 0.7268422245979309\n",
            "epoch: 9/10,    batch: 14277/15469    Discriminator_loss: 0.6891279816627502  Generator_loss: 0.729624330997467\n",
            "epoch: 9/10,    batch: 14278/15469    Discriminator_loss: 0.6850919127464294  Generator_loss: 0.7382714152336121\n",
            "epoch: 9/10,    batch: 14279/15469    Discriminator_loss: 0.6693934798240662  Generator_loss: 0.7617537975311279\n",
            "epoch: 9/10,    batch: 14280/15469    Discriminator_loss: 0.6374791264533997  Generator_loss: 0.8043400049209595\n",
            "epoch: 9/10,    batch: 14281/15469    Discriminator_loss: 0.5921852588653564  Generator_loss: 0.863705039024353\n",
            "epoch: 9/10,    batch: 14282/15469    Discriminator_loss: 0.5401743054389954  Generator_loss: 0.9328947067260742\n",
            "epoch: 9/10,    batch: 14283/15469    Discriminator_loss: 0.4901466369628906  Generator_loss: 1.0048203468322754\n",
            "epoch: 9/10,    batch: 14284/15469    Discriminator_loss: 0.4467550218105316  Generator_loss: 1.0741326808929443\n",
            "epoch: 9/10,    batch: 14285/15469    Discriminator_loss: 0.40931951999664307  Generator_loss: 1.1407092809677124\n",
            "epoch: 9/10,    batch: 14286/15469    Discriminator_loss: 0.3774946331977844  Generator_loss: 1.2049864530563354\n",
            "epoch: 9/10,    batch: 14287/15469    Discriminator_loss: 0.3502795398235321  Generator_loss: 1.2654001712799072\n",
            "epoch: 9/10,    batch: 14288/15469    Discriminator_loss: 0.32626548409461975  Generator_loss: 1.32192063331604\n",
            "epoch: 9/10,    batch: 14289/15469    Discriminator_loss: 0.30492621660232544  Generator_loss: 1.3768508434295654\n",
            "epoch: 9/10,    batch: 14290/15469    Discriminator_loss: 0.28609463572502136  Generator_loss: 1.428736686706543\n",
            "epoch: 9/10,    batch: 14291/15469    Discriminator_loss: 0.26949936151504517  Generator_loss: 1.4777257442474365\n",
            "epoch: 9/10,    batch: 14292/15469    Discriminator_loss: 0.25489822030067444  Generator_loss: 1.5237547159194946\n",
            "epoch: 9/10,    batch: 14293/15469    Discriminator_loss: 0.2420283854007721  Generator_loss: 1.5669937133789062\n",
            "epoch: 9/10,    batch: 14294/15469    Discriminator_loss: 0.23069989681243896  Generator_loss: 1.607120394706726\n",
            "epoch: 9/10,    batch: 14295/15469    Discriminator_loss: 0.22071626782417297  Generator_loss: 1.6446434259414673\n",
            "epoch: 9/10,    batch: 14296/15469    Discriminator_loss: 0.2118118703365326  Generator_loss: 1.679790735244751\n",
            "epoch: 9/10,    batch: 14297/15469    Discriminator_loss: 0.2038024365901947  Generator_loss: 1.7126425504684448\n",
            "epoch: 9/10,    batch: 14298/15469    Discriminator_loss: 0.19662466645240784  Generator_loss: 1.7436357736587524\n",
            "epoch: 9/10,    batch: 14299/15469    Discriminator_loss: 0.19015412032604218  Generator_loss: 1.7725776433944702\n",
            "epoch: 9/10,    batch: 14300/15469    Discriminator_loss: 0.18432870507240295  Generator_loss: 1.7996189594268799\n",
            "epoch: 9/10,    batch: 14301/15469    Discriminator_loss: 0.17906904220581055  Generator_loss: 1.8248538970947266\n",
            "epoch: 9/10,    batch: 14302/15469    Discriminator_loss: 0.17433780431747437  Generator_loss: 1.8482695817947388\n",
            "epoch: 9/10,    batch: 14303/15469    Discriminator_loss: 0.17010614275932312  Generator_loss: 1.8698439598083496\n",
            "epoch: 9/10,    batch: 14304/15469    Discriminator_loss: 0.16632142663002014  Generator_loss: 1.8901667594909668\n",
            "epoch: 9/10,    batch: 14305/15469    Discriminator_loss: 0.1628413200378418  Generator_loss: 1.908437728881836\n",
            "epoch: 9/10,    batch: 14306/15469    Discriminator_loss: 0.15971432626247406  Generator_loss: 1.9256041049957275\n",
            "epoch: 9/10,    batch: 14307/15469    Discriminator_loss: 0.15684768557548523  Generator_loss: 1.9415650367736816\n",
            "epoch: 9/10,    batch: 14308/15469    Discriminator_loss: 0.15423062443733215  Generator_loss: 1.9567991495132446\n",
            "epoch: 9/10,    batch: 14309/15469    Discriminator_loss: 0.15177491307258606  Generator_loss: 1.9710452556610107\n",
            "epoch: 9/10,    batch: 14310/15469    Discriminator_loss: 0.14948807656764984  Generator_loss: 1.9846076965332031\n",
            "epoch: 9/10,    batch: 14311/15469    Discriminator_loss: 0.14731135964393616  Generator_loss: 1.9977930784225464\n",
            "epoch: 9/10,    batch: 14312/15469    Discriminator_loss: 0.14524830877780914  Generator_loss: 2.010308265686035\n",
            "epoch: 9/10,    batch: 14313/15469    Discriminator_loss: 0.14330729842185974  Generator_loss: 2.022581100463867\n",
            "epoch: 9/10,    batch: 14314/15469    Discriminator_loss: 0.14143171906471252  Generator_loss: 2.034337282180786\n",
            "epoch: 9/10,    batch: 14315/15469    Discriminator_loss: 0.13967034220695496  Generator_loss: 2.045806646347046\n",
            "epoch: 9/10,    batch: 14316/15469    Discriminator_loss: 0.13797426223754883  Generator_loss: 2.0569515228271484\n",
            "epoch: 9/10,    batch: 14317/15469    Discriminator_loss: 0.13634265959262848  Generator_loss: 2.067829132080078\n",
            "epoch: 9/10,    batch: 14318/15469    Discriminator_loss: 0.13478215038776398  Generator_loss: 2.078216075897217\n",
            "epoch: 9/10,    batch: 14319/15469    Discriminator_loss: 0.13330015540122986  Generator_loss: 2.0884602069854736\n",
            "epoch: 9/10,    batch: 14320/15469    Discriminator_loss: 0.13184866309165955  Generator_loss: 2.098391532897949\n",
            "epoch: 9/10,    batch: 14321/15469    Discriminator_loss: 0.13047945499420166  Generator_loss: 2.1080589294433594\n",
            "epoch: 9/10,    batch: 14322/15469    Discriminator_loss: 0.12914404273033142  Generator_loss: 2.1175544261932373\n",
            "epoch: 9/10,    batch: 14323/15469    Discriminator_loss: 0.12786632776260376  Generator_loss: 2.1267995834350586\n",
            "epoch: 9/10,    batch: 14324/15469    Discriminator_loss: 0.1266288310289383  Generator_loss: 2.135805606842041\n",
            "epoch: 9/10,    batch: 14325/15469    Discriminator_loss: 0.12542036175727844  Generator_loss: 2.144596576690674\n",
            "epoch: 9/10,    batch: 14326/15469    Discriminator_loss: 0.12425326555967331  Generator_loss: 2.153353691101074\n",
            "epoch: 9/10,    batch: 14327/15469    Discriminator_loss: 0.12311019748449326  Generator_loss: 2.1619105339050293\n",
            "epoch: 9/10,    batch: 14328/15469    Discriminator_loss: 0.12199459969997406  Generator_loss: 2.1703171730041504\n",
            "epoch: 9/10,    batch: 14329/15469    Discriminator_loss: 0.12091158330440521  Generator_loss: 2.1786398887634277\n",
            "epoch: 9/10,    batch: 14330/15469    Discriminator_loss: 0.11985617876052856  Generator_loss: 2.1867589950561523\n",
            "epoch: 9/10,    batch: 14331/15469    Discriminator_loss: 0.11881620436906815  Generator_loss: 2.194978713989258\n",
            "epoch: 9/10,    batch: 14332/15469    Discriminator_loss: 0.11779072880744934  Generator_loss: 2.202983856201172\n",
            "epoch: 9/10,    batch: 14333/15469    Discriminator_loss: 0.11679640412330627  Generator_loss: 2.2109103202819824\n",
            "epoch: 9/10,    batch: 14334/15469    Discriminator_loss: 0.11582446843385696  Generator_loss: 2.2187869548797607\n",
            "epoch: 9/10,    batch: 14335/15469    Discriminator_loss: 0.11485843360424042  Generator_loss: 2.226696014404297\n",
            "epoch: 9/10,    batch: 14336/15469    Discriminator_loss: 0.11389894783496857  Generator_loss: 2.2345287799835205\n",
            "epoch: 9/10,    batch: 14337/15469    Discriminator_loss: 0.11297662556171417  Generator_loss: 2.242131233215332\n",
            "epoch: 9/10,    batch: 14338/15469    Discriminator_loss: 0.1120324656367302  Generator_loss: 2.249879837036133\n",
            "epoch: 9/10,    batch: 14339/15469    Discriminator_loss: 0.11113733053207397  Generator_loss: 2.257607936859131\n",
            "epoch: 9/10,    batch: 14340/15469    Discriminator_loss: 0.11024340987205505  Generator_loss: 2.2651567459106445\n",
            "epoch: 9/10,    batch: 14341/15469    Discriminator_loss: 0.10936134308576584  Generator_loss: 2.272770404815674\n",
            "epoch: 9/10,    batch: 14342/15469    Discriminator_loss: 0.10850252211093903  Generator_loss: 2.2802228927612305\n",
            "epoch: 9/10,    batch: 14343/15469    Discriminator_loss: 0.9263455271720886  Generator_loss: 2.2831294536590576\n",
            "epoch: 9/10,    batch: 14344/15469    Discriminator_loss: 2.472022533416748  Generator_loss: 2.274437427520752\n",
            "epoch: 9/10,    batch: 14345/15469    Discriminator_loss: 2.4557254314422607  Generator_loss: 2.256793975830078\n",
            "epoch: 9/10,    batch: 14346/15469    Discriminator_loss: 2.438034772872925  Generator_loss: 2.232405424118042\n",
            "epoch: 9/10,    batch: 14347/15469    Discriminator_loss: 2.3959736824035645  Generator_loss: 2.203075408935547\n",
            "epoch: 9/10,    batch: 14348/15469    Discriminator_loss: 2.3564693927764893  Generator_loss: 2.1700403690338135\n",
            "epoch: 9/10,    batch: 14349/15469    Discriminator_loss: 2.3256752490997314  Generator_loss: 2.1344687938690186\n",
            "epoch: 9/10,    batch: 14350/15469    Discriminator_loss: 2.29417085647583  Generator_loss: 2.097683906555176\n",
            "epoch: 9/10,    batch: 14351/15469    Discriminator_loss: 2.2406115531921387  Generator_loss: 2.0597052574157715\n",
            "epoch: 9/10,    batch: 14352/15469    Discriminator_loss: 2.193535804748535  Generator_loss: 2.0210111141204834\n",
            "epoch: 9/10,    batch: 14353/15469    Discriminator_loss: 0.4643043279647827  Generator_loss: 1.9901673793792725\n",
            "epoch: 9/10,    batch: 14354/15469    Discriminator_loss: 0.1495240479707718  Generator_loss: 1.966565728187561\n",
            "epoch: 9/10,    batch: 14355/15469    Discriminator_loss: 0.15258516371250153  Generator_loss: 1.9488964080810547\n",
            "epoch: 9/10,    batch: 14356/15469    Discriminator_loss: 0.15510952472686768  Generator_loss: 1.9361958503723145\n",
            "epoch: 9/10,    batch: 14357/15469    Discriminator_loss: 0.15690991282463074  Generator_loss: 1.9267058372497559\n",
            "epoch: 9/10,    batch: 14358/15469    Discriminator_loss: 1.40132737159729  Generator_loss: 1.9149532318115234\n",
            "epoch: 9/10,    batch: 14359/15469    Discriminator_loss: 2.203780174255371  Generator_loss: 1.8967769145965576\n",
            "epoch: 9/10,    batch: 14360/15469    Discriminator_loss: 2.1618847846984863  Generator_loss: 1.8738954067230225\n",
            "epoch: 9/10,    batch: 14361/15469    Discriminator_loss: 2.0964250564575195  Generator_loss: 1.8477129936218262\n",
            "epoch: 9/10,    batch: 14362/15469    Discriminator_loss: 2.1653668880462646  Generator_loss: 1.8181567192077637\n",
            "epoch: 9/10,    batch: 14363/15469    Discriminator_loss: 2.1235508918762207  Generator_loss: 1.7863670587539673\n",
            "epoch: 9/10,    batch: 14364/15469    Discriminator_loss: 2.056086540222168  Generator_loss: 1.753955602645874\n",
            "epoch: 9/10,    batch: 14365/15469    Discriminator_loss: 2.0205633640289307  Generator_loss: 1.721137285232544\n",
            "epoch: 9/10,    batch: 14366/15469    Discriminator_loss: 1.9604581594467163  Generator_loss: 1.6887071132659912\n",
            "epoch: 9/10,    batch: 14367/15469    Discriminator_loss: 1.9327740669250488  Generator_loss: 1.6567890644073486\n",
            "epoch: 9/10,    batch: 14368/15469    Discriminator_loss: 1.883555293083191  Generator_loss: 1.6256846189498901\n",
            "epoch: 9/10,    batch: 14369/15469    Discriminator_loss: 1.8494033813476562  Generator_loss: 1.5953110456466675\n",
            "epoch: 9/10,    batch: 14370/15469    Discriminator_loss: 1.8496692180633545  Generator_loss: 1.5653579235076904\n",
            "epoch: 9/10,    batch: 14371/15469    Discriminator_loss: 1.7994985580444336  Generator_loss: 1.5363112688064575\n",
            "epoch: 9/10,    batch: 14372/15469    Discriminator_loss: 1.7483093738555908  Generator_loss: 1.5078439712524414\n",
            "epoch: 9/10,    batch: 14373/15469    Discriminator_loss: 1.7178122997283936  Generator_loss: 1.4801037311553955\n",
            "epoch: 9/10,    batch: 14374/15469    Discriminator_loss: 1.6846126317977905  Generator_loss: 1.4527614116668701\n",
            "epoch: 9/10,    batch: 14375/15469    Discriminator_loss: 1.6445934772491455  Generator_loss: 1.426501989364624\n",
            "epoch: 9/10,    batch: 14376/15469    Discriminator_loss: 1.621572732925415  Generator_loss: 1.400085687637329\n",
            "epoch: 9/10,    batch: 14377/15469    Discriminator_loss: 1.5654969215393066  Generator_loss: 1.3733025789260864\n",
            "epoch: 9/10,    batch: 14378/15469    Discriminator_loss: 1.5201915502548218  Generator_loss: 1.3470993041992188\n",
            "epoch: 9/10,    batch: 14379/15469    Discriminator_loss: 1.4562426805496216  Generator_loss: 1.3200724124908447\n",
            "epoch: 9/10,    batch: 14380/15469    Discriminator_loss: 1.4156861305236816  Generator_loss: 1.2935644388198853\n",
            "epoch: 9/10,    batch: 14381/15469    Discriminator_loss: 1.3376916646957397  Generator_loss: 1.2655551433563232\n",
            "epoch: 9/10,    batch: 14382/15469    Discriminator_loss: 1.2648396492004395  Generator_loss: 1.237311840057373\n",
            "epoch: 9/10,    batch: 14383/15469    Discriminator_loss: 0.9187151789665222  Generator_loss: 1.2041497230529785\n",
            "epoch: 9/10,    batch: 14384/15469    Discriminator_loss: 0.7504680156707764  Generator_loss: 1.1693916320800781\n",
            "epoch: 9/10,    batch: 14385/15469    Discriminator_loss: 0.4080929756164551  Generator_loss: 1.1391956806182861\n",
            "epoch: 9/10,    batch: 14386/15469    Discriminator_loss: 0.4005958139896393  Generator_loss: 1.112471342086792\n",
            "epoch: 9/10,    batch: 14387/15469    Discriminator_loss: 0.40809139609336853  Generator_loss: 1.0906789302825928\n",
            "epoch: 9/10,    batch: 14388/15469    Discriminator_loss: 0.4186553657054901  Generator_loss: 1.0712999105453491\n",
            "epoch: 9/10,    batch: 14389/15469    Discriminator_loss: 0.4293408989906311  Generator_loss: 1.0529156923294067\n",
            "epoch: 9/10,    batch: 14390/15469    Discriminator_loss: 0.44011861085891724  Generator_loss: 1.0359477996826172\n",
            "epoch: 9/10,    batch: 14391/15469    Discriminator_loss: 0.4516298174858093  Generator_loss: 1.0187259912490845\n",
            "epoch: 9/10,    batch: 14392/15469    Discriminator_loss: 0.46427303552627563  Generator_loss: 0.9985650181770325\n",
            "epoch: 9/10,    batch: 14393/15469    Discriminator_loss: 0.48044416308403015  Generator_loss: 0.9754683375358582\n",
            "epoch: 9/10,    batch: 14394/15469    Discriminator_loss: 0.500913143157959  Generator_loss: 0.9492160081863403\n",
            "epoch: 9/10,    batch: 14395/15469    Discriminator_loss: 0.5243837833404541  Generator_loss: 0.9168251752853394\n",
            "epoch: 9/10,    batch: 14396/15469    Discriminator_loss: 0.5513988733291626  Generator_loss: 0.883476734161377\n",
            "epoch: 9/10,    batch: 14397/15469    Discriminator_loss: 0.5817462205886841  Generator_loss: 0.8508660793304443\n",
            "epoch: 9/10,    batch: 14398/15469    Discriminator_loss: 0.6167038083076477  Generator_loss: 0.8223650455474854\n",
            "epoch: 9/10,    batch: 14399/15469    Discriminator_loss: 0.6667613983154297  Generator_loss: 0.7980737686157227\n",
            "epoch: 9/10,    batch: 14400/15469    Discriminator_loss: 0.7413784265518188  Generator_loss: 0.797815203666687\n",
            "epoch: 9/10,    batch: 14401/15469    Discriminator_loss: 0.7959064841270447  Generator_loss: 0.8680136203765869\n",
            "epoch: 9/10,    batch: 14402/15469    Discriminator_loss: 0.6769073605537415  Generator_loss: 1.0260783433914185\n",
            "epoch: 9/10,    batch: 14403/15469    Discriminator_loss: 0.4660927355289459  Generator_loss: 1.237820029258728\n",
            "epoch: 9/10,    batch: 14404/15469    Discriminator_loss: 0.3274822533130646  Generator_loss: 1.4368809461593628\n",
            "epoch: 9/10,    batch: 14405/15469    Discriminator_loss: 0.27196410298347473  Generator_loss: 1.5608584880828857\n",
            "epoch: 9/10,    batch: 14406/15469    Discriminator_loss: 0.4707266092300415  Generator_loss: 1.5063023567199707\n",
            "epoch: 9/10,    batch: 14407/15469    Discriminator_loss: 0.4529280662536621  Generator_loss: 1.4145183563232422\n",
            "epoch: 9/10,    batch: 14408/15469    Discriminator_loss: 0.30130845308303833  Generator_loss: 1.3720266819000244\n",
            "epoch: 9/10,    batch: 14409/15469    Discriminator_loss: 0.30186036229133606  Generator_loss: 1.3567442893981934\n",
            "epoch: 9/10,    batch: 14410/15469    Discriminator_loss: 0.3059725761413574  Generator_loss: 1.34779953956604\n",
            "epoch: 9/10,    batch: 14411/15469    Discriminator_loss: 0.31036925315856934  Generator_loss: 1.3391252756118774\n",
            "epoch: 9/10,    batch: 14412/15469    Discriminator_loss: 0.3144342005252838  Generator_loss: 1.329483985900879\n",
            "epoch: 9/10,    batch: 14413/15469    Discriminator_loss: 0.3184533417224884  Generator_loss: 1.3213396072387695\n",
            "epoch: 9/10,    batch: 14414/15469    Discriminator_loss: 0.32184335589408875  Generator_loss: 1.3148761987686157\n",
            "epoch: 9/10,    batch: 14415/15469    Discriminator_loss: 0.32320713996887207  Generator_loss: 1.3134046792984009\n",
            "epoch: 9/10,    batch: 14416/15469    Discriminator_loss: 0.32274729013442993  Generator_loss: 1.3207054138183594\n",
            "epoch: 9/10,    batch: 14417/15469    Discriminator_loss: 0.31862980127334595  Generator_loss: 1.3382296562194824\n",
            "epoch: 9/10,    batch: 14418/15469    Discriminator_loss: 0.30885446071624756  Generator_loss: 1.3729819059371948\n",
            "epoch: 9/10,    batch: 14419/15469    Discriminator_loss: 0.29404714703559875  Generator_loss: 1.427048683166504\n",
            "epoch: 9/10,    batch: 14420/15469    Discriminator_loss: 0.2742735743522644  Generator_loss: 1.5005416870117188\n",
            "epoch: 9/10,    batch: 14421/15469    Discriminator_loss: 0.2508229613304138  Generator_loss: 1.5909521579742432\n",
            "epoch: 9/10,    batch: 14422/15469    Discriminator_loss: 0.2263394296169281  Generator_loss: 1.684304118156433\n",
            "epoch: 9/10,    batch: 14423/15469    Discriminator_loss: 0.20431943237781525  Generator_loss: 1.7622103691101074\n",
            "epoch: 9/10,    batch: 14424/15469    Discriminator_loss: 0.19121518731117249  Generator_loss: 1.8046674728393555\n",
            "epoch: 9/10,    batch: 14425/15469    Discriminator_loss: 0.1874372363090515  Generator_loss: 1.8086700439453125\n",
            "epoch: 9/10,    batch: 14426/15469    Discriminator_loss: 0.18785487115383148  Generator_loss: 1.7961901426315308\n",
            "epoch: 9/10,    batch: 14427/15469    Discriminator_loss: 0.18804782629013062  Generator_loss: 1.7889052629470825\n",
            "epoch: 9/10,    batch: 14428/15469    Discriminator_loss: 0.1869349479675293  Generator_loss: 1.7889913320541382\n",
            "epoch: 9/10,    batch: 14429/15469    Discriminator_loss: 0.18499994277954102  Generator_loss: 1.7948780059814453\n",
            "epoch: 9/10,    batch: 14430/15469    Discriminator_loss: 0.1827392876148224  Generator_loss: 1.8038042783737183\n",
            "epoch: 9/10,    batch: 14431/15469    Discriminator_loss: 0.18056948482990265  Generator_loss: 1.8128085136413574\n",
            "epoch: 9/10,    batch: 14432/15469    Discriminator_loss: 0.17857323586940765  Generator_loss: 1.8217175006866455\n",
            "epoch: 9/10,    batch: 14433/15469    Discriminator_loss: 0.1769322007894516  Generator_loss: 1.8292386531829834\n",
            "epoch: 9/10,    batch: 14434/15469    Discriminator_loss: 0.17544886469841003  Generator_loss: 1.8360862731933594\n",
            "epoch: 9/10,    batch: 14435/15469    Discriminator_loss: 0.17423667013645172  Generator_loss: 1.8417431116104126\n",
            "epoch: 9/10,    batch: 14436/15469    Discriminator_loss: 0.1731289178133011  Generator_loss: 1.8468366861343384\n",
            "epoch: 9/10,    batch: 14437/15469    Discriminator_loss: 0.17219415307044983  Generator_loss: 1.851463794708252\n",
            "epoch: 9/10,    batch: 14438/15469    Discriminator_loss: 0.17134934663772583  Generator_loss: 1.855527400970459\n",
            "epoch: 9/10,    batch: 14439/15469    Discriminator_loss: 0.17055842280387878  Generator_loss: 1.8596974611282349\n",
            "epoch: 9/10,    batch: 14440/15469    Discriminator_loss: 0.16975916922092438  Generator_loss: 1.8636808395385742\n",
            "epoch: 9/10,    batch: 14441/15469    Discriminator_loss: 0.16893260180950165  Generator_loss: 1.867923617362976\n",
            "epoch: 9/10,    batch: 14442/15469    Discriminator_loss: 0.16810911893844604  Generator_loss: 1.872146725654602\n",
            "epoch: 9/10,    batch: 14443/15469    Discriminator_loss: 0.16725794970989227  Generator_loss: 1.8766133785247803\n",
            "epoch: 9/10,    batch: 14444/15469    Discriminator_loss: 0.16637635231018066  Generator_loss: 1.8814364671707153\n",
            "epoch: 9/10,    batch: 14445/15469    Discriminator_loss: 0.16547581553459167  Generator_loss: 1.8863627910614014\n",
            "epoch: 9/10,    batch: 14446/15469    Discriminator_loss: 0.1645154058933258  Generator_loss: 1.891521692276001\n",
            "epoch: 9/10,    batch: 14447/15469    Discriminator_loss: 0.16354835033416748  Generator_loss: 1.896881103515625\n",
            "epoch: 9/10,    batch: 14448/15469    Discriminator_loss: 0.16255509853363037  Generator_loss: 1.902397871017456\n",
            "epoch: 9/10,    batch: 14449/15469    Discriminator_loss: 0.16155532002449036  Generator_loss: 1.9079780578613281\n",
            "epoch: 9/10,    batch: 14450/15469    Discriminator_loss: 0.16051539778709412  Generator_loss: 1.9139117002487183\n",
            "epoch: 9/10,    batch: 14451/15469    Discriminator_loss: 0.1594889909029007  Generator_loss: 1.9197691679000854\n",
            "epoch: 9/10,    batch: 14452/15469    Discriminator_loss: 0.15845288336277008  Generator_loss: 1.9257466793060303\n",
            "epoch: 9/10,    batch: 14453/15469    Discriminator_loss: 0.15741434693336487  Generator_loss: 1.9317803382873535\n",
            "epoch: 9/10,    batch: 14454/15469    Discriminator_loss: 0.15637382864952087  Generator_loss: 1.9378652572631836\n",
            "epoch: 9/10,    batch: 14455/15469    Discriminator_loss: 0.15533854067325592  Generator_loss: 1.9439760446548462\n",
            "epoch: 9/10,    batch: 14456/15469    Discriminator_loss: 0.15431049466133118  Generator_loss: 1.9501044750213623\n",
            "epoch: 9/10,    batch: 14457/15469    Discriminator_loss: 0.15328601002693176  Generator_loss: 1.9562127590179443\n",
            "epoch: 9/10,    batch: 14458/15469    Discriminator_loss: 0.15225805342197418  Generator_loss: 1.9623241424560547\n",
            "epoch: 9/10,    batch: 14459/15469    Discriminator_loss: 0.15125787258148193  Generator_loss: 1.9684529304504395\n",
            "epoch: 9/10,    batch: 14460/15469    Discriminator_loss: 0.15024933218955994  Generator_loss: 1.9745290279388428\n",
            "epoch: 9/10,    batch: 14461/15469    Discriminator_loss: 0.1492641419172287  Generator_loss: 1.980677843093872\n",
            "epoch: 9/10,    batch: 14462/15469    Discriminator_loss: 0.1482701599597931  Generator_loss: 1.9868227243423462\n",
            "epoch: 9/10,    batch: 14463/15469    Discriminator_loss: 0.14729443192481995  Generator_loss: 1.9929053783416748\n",
            "epoch: 9/10,    batch: 14464/15469    Discriminator_loss: 0.1463308036327362  Generator_loss: 1.9990384578704834\n",
            "epoch: 9/10,    batch: 14465/15469    Discriminator_loss: 0.1453583687543869  Generator_loss: 2.0051746368408203\n",
            "epoch: 9/10,    batch: 14466/15469    Discriminator_loss: 0.1444138139486313  Generator_loss: 2.0112111568450928\n",
            "epoch: 9/10,    batch: 14467/15469    Discriminator_loss: 0.14346709847450256  Generator_loss: 2.017305850982666\n",
            "epoch: 9/10,    batch: 14468/15469    Discriminator_loss: 0.14252978563308716  Generator_loss: 2.0234100818634033\n",
            "epoch: 9/10,    batch: 14469/15469    Discriminator_loss: 1.9424262046813965  Generator_loss: 2.0226688385009766\n",
            "epoch: 9/10,    batch: 14470/15469    Discriminator_loss: 2.1217236518859863  Generator_loss: 2.015768051147461\n",
            "epoch: 9/10,    batch: 14471/15469    Discriminator_loss: 2.1134767532348633  Generator_loss: 2.003870964050293\n",
            "epoch: 9/10,    batch: 14472/15469    Discriminator_loss: 2.1021249294281006  Generator_loss: 1.988245964050293\n",
            "epoch: 9/10,    batch: 14473/15469    Discriminator_loss: 2.080634832382202  Generator_loss: 1.9696414470672607\n",
            "epoch: 9/10,    batch: 14474/15469    Discriminator_loss: 2.0625288486480713  Generator_loss: 1.9489469528198242\n",
            "epoch: 9/10,    batch: 14475/15469    Discriminator_loss: 2.0426650047302246  Generator_loss: 1.9266531467437744\n",
            "epoch: 9/10,    batch: 14476/15469    Discriminator_loss: 2.0191380977630615  Generator_loss: 1.9033753871917725\n",
            "epoch: 9/10,    batch: 14477/15469    Discriminator_loss: 1.9954036474227905  Generator_loss: 1.8793658018112183\n",
            "epoch: 9/10,    batch: 14478/15469    Discriminator_loss: 1.969995141029358  Generator_loss: 1.8548860549926758\n",
            "epoch: 9/10,    batch: 14479/15469    Discriminator_loss: 1.9504786729812622  Generator_loss: 1.8302080631256104\n",
            "epoch: 9/10,    batch: 14480/15469    Discriminator_loss: 1.9274489879608154  Generator_loss: 1.8055886030197144\n",
            "epoch: 9/10,    batch: 14481/15469    Discriminator_loss: 1.907174825668335  Generator_loss: 1.7809324264526367\n",
            "epoch: 9/10,    batch: 14482/15469    Discriminator_loss: 1.8893049955368042  Generator_loss: 1.7566664218902588\n",
            "epoch: 9/10,    batch: 14483/15469    Discriminator_loss: 1.8657481670379639  Generator_loss: 1.732483148574829\n",
            "epoch: 9/10,    batch: 14484/15469    Discriminator_loss: 1.8479578495025635  Generator_loss: 1.7085567712783813\n",
            "epoch: 9/10,    batch: 14485/15469    Discriminator_loss: 1.8300025463104248  Generator_loss: 1.6850037574768066\n",
            "epoch: 9/10,    batch: 14486/15469    Discriminator_loss: 1.8098721504211426  Generator_loss: 1.6618813276290894\n",
            "epoch: 9/10,    batch: 14487/15469    Discriminator_loss: 1.7938106060028076  Generator_loss: 1.639060139656067\n",
            "epoch: 9/10,    batch: 14488/15469    Discriminator_loss: 0.47081083059310913  Generator_loss: 1.6210074424743652\n",
            "epoch: 9/10,    batch: 14489/15469    Discriminator_loss: 0.22276675701141357  Generator_loss: 1.6076431274414062\n",
            "epoch: 9/10,    batch: 14490/15469    Discriminator_loss: 0.22538870573043823  Generator_loss: 1.5980651378631592\n",
            "epoch: 9/10,    batch: 14491/15469    Discriminator_loss: 0.227395161986351  Generator_loss: 1.5915910005569458\n",
            "epoch: 9/10,    batch: 14492/15469    Discriminator_loss: 0.22867999970912933  Generator_loss: 1.5875753164291382\n",
            "epoch: 9/10,    batch: 14493/15469    Discriminator_loss: 1.1574983596801758  Generator_loss: 1.5820802450180054\n",
            "epoch: 9/10,    batch: 14494/15469    Discriminator_loss: 1.7573996782302856  Generator_loss: 1.5733535289764404\n",
            "epoch: 9/10,    batch: 14495/15469    Discriminator_loss: 1.7532501220703125  Generator_loss: 1.5619401931762695\n",
            "epoch: 9/10,    batch: 14496/15469    Discriminator_loss: 1.73750638961792  Generator_loss: 1.548084020614624\n",
            "epoch: 9/10,    batch: 14497/15469    Discriminator_loss: 1.7603217363357544  Generator_loss: 1.5315970182418823\n",
            "epoch: 9/10,    batch: 14498/15469    Discriminator_loss: 1.760730266571045  Generator_loss: 1.5131309032440186\n",
            "epoch: 9/10,    batch: 14499/15469    Discriminator_loss: 1.7383843660354614  Generator_loss: 1.4933711290359497\n",
            "epoch: 9/10,    batch: 14500/15469    Discriminator_loss: 1.7260266542434692  Generator_loss: 1.4725514650344849\n",
            "epoch: 9/10,    batch: 14501/15469    Discriminator_loss: 1.7154669761657715  Generator_loss: 1.4510995149612427\n",
            "epoch: 9/10,    batch: 14502/15469    Discriminator_loss: 1.6949856281280518  Generator_loss: 1.429349422454834\n",
            "epoch: 9/10,    batch: 14503/15469    Discriminator_loss: 1.682098627090454  Generator_loss: 1.4074757099151611\n",
            "epoch: 9/10,    batch: 14504/15469    Discriminator_loss: 1.6694822311401367  Generator_loss: 1.3856735229492188\n",
            "epoch: 9/10,    batch: 14505/15469    Discriminator_loss: 1.6574747562408447  Generator_loss: 1.3639590740203857\n",
            "epoch: 9/10,    batch: 14506/15469    Discriminator_loss: 1.6394309997558594  Generator_loss: 1.342490792274475\n",
            "epoch: 9/10,    batch: 14507/15469    Discriminator_loss: 1.6286380290985107  Generator_loss: 1.3213436603546143\n",
            "epoch: 9/10,    batch: 14508/15469    Discriminator_loss: 1.6144973039627075  Generator_loss: 1.3005688190460205\n",
            "epoch: 9/10,    batch: 14509/15469    Discriminator_loss: 1.5964360237121582  Generator_loss: 1.2800899744033813\n",
            "epoch: 9/10,    batch: 14510/15469    Discriminator_loss: 1.6070733070373535  Generator_loss: 1.2597877979278564\n",
            "epoch: 9/10,    batch: 14511/15469    Discriminator_loss: 1.5897108316421509  Generator_loss: 1.2397239208221436\n",
            "epoch: 9/10,    batch: 14512/15469    Discriminator_loss: 1.5783145427703857  Generator_loss: 1.2199846506118774\n",
            "epoch: 9/10,    batch: 14513/15469    Discriminator_loss: 1.5889058113098145  Generator_loss: 1.2003861665725708\n",
            "epoch: 9/10,    batch: 14514/15469    Discriminator_loss: 1.5468086004257202  Generator_loss: 1.181077003479004\n",
            "epoch: 9/10,    batch: 14515/15469    Discriminator_loss: 1.5430744886398315  Generator_loss: 1.161940097808838\n",
            "epoch: 9/10,    batch: 14516/15469    Discriminator_loss: 1.5460443496704102  Generator_loss: 1.142967700958252\n",
            "epoch: 9/10,    batch: 14517/15469    Discriminator_loss: 1.533286690711975  Generator_loss: 1.124251365661621\n",
            "epoch: 9/10,    batch: 14518/15469    Discriminator_loss: 1.512775182723999  Generator_loss: 1.1056513786315918\n",
            "epoch: 9/10,    batch: 14519/15469    Discriminator_loss: 1.5220365524291992  Generator_loss: 1.087263822555542\n",
            "epoch: 9/10,    batch: 14520/15469    Discriminator_loss: 1.5154540538787842  Generator_loss: 1.0689806938171387\n",
            "epoch: 9/10,    batch: 14521/15469    Discriminator_loss: 1.5157512426376343  Generator_loss: 1.0505133867263794\n",
            "epoch: 9/10,    batch: 14522/15469    Discriminator_loss: 1.5121469497680664  Generator_loss: 1.031954050064087\n",
            "epoch: 9/10,    batch: 14523/15469    Discriminator_loss: 1.4748196601867676  Generator_loss: 1.0132346153259277\n",
            "epoch: 9/10,    batch: 14524/15469    Discriminator_loss: 1.4735584259033203  Generator_loss: 0.9933634400367737\n",
            "epoch: 9/10,    batch: 14525/15469    Discriminator_loss: 1.3854492902755737  Generator_loss: 0.9728455543518066\n",
            "epoch: 9/10,    batch: 14526/15469    Discriminator_loss: 1.2362245321273804  Generator_loss: 0.95130455493927\n",
            "epoch: 9/10,    batch: 14527/15469    Discriminator_loss: 1.1349501609802246  Generator_loss: 0.9279810190200806\n",
            "epoch: 9/10,    batch: 14528/15469    Discriminator_loss: 0.8017793893814087  Generator_loss: 0.9048771858215332\n",
            "epoch: 9/10,    batch: 14529/15469    Discriminator_loss: 0.5383710861206055  Generator_loss: 0.8870790004730225\n",
            "epoch: 9/10,    batch: 14530/15469    Discriminator_loss: 0.5395342707633972  Generator_loss: 0.8745560050010681\n",
            "epoch: 9/10,    batch: 14531/15469    Discriminator_loss: 0.5466251373291016  Generator_loss: 0.8658156991004944\n",
            "epoch: 9/10,    batch: 14532/15469    Discriminator_loss: 0.5512475967407227  Generator_loss: 0.8608521223068237\n",
            "epoch: 9/10,    batch: 14533/15469    Discriminator_loss: 0.553878664970398  Generator_loss: 0.8589577674865723\n",
            "epoch: 9/10,    batch: 14534/15469    Discriminator_loss: 0.5543612837791443  Generator_loss: 0.8590244650840759\n",
            "epoch: 9/10,    batch: 14535/15469    Discriminator_loss: 0.5536245107650757  Generator_loss: 0.8615705966949463\n",
            "epoch: 9/10,    batch: 14536/15469    Discriminator_loss: 0.5512359738349915  Generator_loss: 0.8653619885444641\n",
            "epoch: 9/10,    batch: 14537/15469    Discriminator_loss: 0.5478639006614685  Generator_loss: 0.8707202076911926\n",
            "epoch: 9/10,    batch: 14538/15469    Discriminator_loss: 0.5431054830551147  Generator_loss: 0.8775432705879211\n",
            "epoch: 9/10,    batch: 14539/15469    Discriminator_loss: 0.5381027460098267  Generator_loss: 0.8852748870849609\n",
            "epoch: 9/10,    batch: 14540/15469    Discriminator_loss: 0.5324776768684387  Generator_loss: 0.8935944437980652\n",
            "epoch: 9/10,    batch: 14541/15469    Discriminator_loss: 0.5263658165931702  Generator_loss: 0.9030109643936157\n",
            "epoch: 9/10,    batch: 14542/15469    Discriminator_loss: 0.5197990536689758  Generator_loss: 0.912482500076294\n",
            "epoch: 9/10,    batch: 14543/15469    Discriminator_loss: 0.5127012729644775  Generator_loss: 0.9228243827819824\n",
            "epoch: 9/10,    batch: 14544/15469    Discriminator_loss: 0.5059075951576233  Generator_loss: 0.9336059093475342\n",
            "epoch: 9/10,    batch: 14545/15469    Discriminator_loss: 0.49899744987487793  Generator_loss: 0.9444577693939209\n",
            "epoch: 9/10,    batch: 14546/15469    Discriminator_loss: 0.4921295940876007  Generator_loss: 0.9559242725372314\n",
            "epoch: 9/10,    batch: 14547/15469    Discriminator_loss: 0.4847825765609741  Generator_loss: 0.9673054218292236\n",
            "epoch: 9/10,    batch: 14548/15469    Discriminator_loss: 0.477500855922699  Generator_loss: 0.9782754182815552\n",
            "epoch: 9/10,    batch: 14549/15469    Discriminator_loss: 0.470573753118515  Generator_loss: 0.9895280599594116\n",
            "epoch: 9/10,    batch: 14550/15469    Discriminator_loss: 0.46383336186408997  Generator_loss: 1.000664234161377\n",
            "epoch: 9/10,    batch: 14551/15469    Discriminator_loss: 0.4574187099933624  Generator_loss: 1.0116382837295532\n",
            "epoch: 9/10,    batch: 14552/15469    Discriminator_loss: 0.45096832513809204  Generator_loss: 1.02273428440094\n",
            "epoch: 9/10,    batch: 14553/15469    Discriminator_loss: 0.44517791271209717  Generator_loss: 1.0334253311157227\n",
            "epoch: 9/10,    batch: 14554/15469    Discriminator_loss: 0.43921899795532227  Generator_loss: 1.0445077419281006\n",
            "epoch: 9/10,    batch: 14555/15469    Discriminator_loss: 0.4334917366504669  Generator_loss: 1.054366946220398\n",
            "epoch: 9/10,    batch: 14556/15469    Discriminator_loss: 0.4280772805213928  Generator_loss: 1.0646555423736572\n",
            "epoch: 9/10,    batch: 14557/15469    Discriminator_loss: 0.4229336678981781  Generator_loss: 1.0743072032928467\n",
            "epoch: 9/10,    batch: 14558/15469    Discriminator_loss: 0.4179545044898987  Generator_loss: 1.0841593742370605\n",
            "epoch: 9/10,    batch: 14559/15469    Discriminator_loss: 0.413028746843338  Generator_loss: 1.09379243850708\n",
            "epoch: 9/10,    batch: 14560/15469    Discriminator_loss: 0.4081129729747772  Generator_loss: 1.1030305624008179\n",
            "epoch: 9/10,    batch: 14561/15469    Discriminator_loss: 0.40356671810150146  Generator_loss: 1.112478256225586\n",
            "epoch: 9/10,    batch: 14562/15469    Discriminator_loss: 0.3988991975784302  Generator_loss: 1.1221575736999512\n",
            "epoch: 9/10,    batch: 14563/15469    Discriminator_loss: 0.39421308040618896  Generator_loss: 1.1318204402923584\n",
            "epoch: 9/10,    batch: 14564/15469    Discriminator_loss: 0.3896867334842682  Generator_loss: 1.1414141654968262\n",
            "epoch: 9/10,    batch: 14565/15469    Discriminator_loss: 0.38513216376304626  Generator_loss: 1.1511874198913574\n",
            "epoch: 9/10,    batch: 14566/15469    Discriminator_loss: 0.3804298937320709  Generator_loss: 1.1613798141479492\n",
            "epoch: 9/10,    batch: 14567/15469    Discriminator_loss: 0.3758005201816559  Generator_loss: 1.171450138092041\n",
            "epoch: 9/10,    batch: 14568/15469    Discriminator_loss: 0.3712691068649292  Generator_loss: 1.1817805767059326\n",
            "epoch: 9/10,    batch: 14569/15469    Discriminator_loss: 0.3666893541812897  Generator_loss: 1.1920585632324219\n",
            "epoch: 9/10,    batch: 14570/15469    Discriminator_loss: 0.36233460903167725  Generator_loss: 1.2022583484649658\n",
            "epoch: 9/10,    batch: 14571/15469    Discriminator_loss: 0.3581516742706299  Generator_loss: 1.2123966217041016\n",
            "epoch: 9/10,    batch: 14572/15469    Discriminator_loss: 0.35411226749420166  Generator_loss: 1.2219096422195435\n",
            "epoch: 9/10,    batch: 14573/15469    Discriminator_loss: 0.3502139449119568  Generator_loss: 1.231360673904419\n",
            "epoch: 9/10,    batch: 14574/15469    Discriminator_loss: 0.34673845767974854  Generator_loss: 1.240411639213562\n",
            "epoch: 9/10,    batch: 14575/15469    Discriminator_loss: 0.34337174892425537  Generator_loss: 1.2486672401428223\n",
            "epoch: 9/10,    batch: 14576/15469    Discriminator_loss: 0.34038007259368896  Generator_loss: 1.2566198110580444\n",
            "epoch: 9/10,    batch: 14577/15469    Discriminator_loss: 0.3377642333507538  Generator_loss: 1.263681173324585\n",
            "epoch: 9/10,    batch: 14578/15469    Discriminator_loss: 0.3355773389339447  Generator_loss: 1.2703604698181152\n",
            "epoch: 9/10,    batch: 14579/15469    Discriminator_loss: 0.3339107632637024  Generator_loss: 1.2761199474334717\n",
            "epoch: 9/10,    batch: 14580/15469    Discriminator_loss: 0.33280089497566223  Generator_loss: 1.2808903455734253\n",
            "epoch: 9/10,    batch: 14581/15469    Discriminator_loss: 0.33305469155311584  Generator_loss: 1.2837212085723877\n",
            "epoch: 9/10,    batch: 14582/15469    Discriminator_loss: 0.3353908061981201  Generator_loss: 1.284172773361206\n",
            "epoch: 9/10,    batch: 14583/15469    Discriminator_loss: 0.34370288252830505  Generator_loss: 1.2807620763778687\n",
            "epoch: 9/10,    batch: 14584/15469    Discriminator_loss: 0.371573805809021  Generator_loss: 1.2767398357391357\n",
            "epoch: 9/10,    batch: 14585/15469    Discriminator_loss: 0.43780645728111267  Generator_loss: 1.325322151184082\n",
            "epoch: 9/10,    batch: 14586/15469    Discriminator_loss: 0.39201849699020386  Generator_loss: 1.4973037242889404\n",
            "epoch: 9/10,    batch: 14587/15469    Discriminator_loss: 0.24405629932880402  Generator_loss: 1.734576940536499\n",
            "epoch: 9/10,    batch: 14588/15469    Discriminator_loss: 0.16987691819667816  Generator_loss: 1.9319473505020142\n",
            "epoch: 9/10,    batch: 14589/15469    Discriminator_loss: 0.14517734944820404  Generator_loss: 2.0375349521636963\n",
            "epoch: 9/10,    batch: 14590/15469    Discriminator_loss: 0.1396431177854538  Generator_loss: 2.0683329105377197\n",
            "epoch: 9/10,    batch: 14591/15469    Discriminator_loss: 0.14079998433589935  Generator_loss: 2.0588183403015137\n",
            "epoch: 9/10,    batch: 14592/15469    Discriminator_loss: 0.14467039704322815  Generator_loss: 2.031550645828247\n",
            "epoch: 9/10,    batch: 14593/15469    Discriminator_loss: 0.14928896725177765  Generator_loss: 1.9994091987609863\n",
            "epoch: 9/10,    batch: 14594/15469    Discriminator_loss: 0.15358348190784454  Generator_loss: 1.9691723585128784\n",
            "epoch: 9/10,    batch: 14595/15469    Discriminator_loss: 0.1573026478290558  Generator_loss: 1.9432194232940674\n",
            "epoch: 9/10,    batch: 14596/15469    Discriminator_loss: 0.16055212914943695  Generator_loss: 1.9224872589111328\n",
            "epoch: 9/10,    batch: 14597/15469    Discriminator_loss: 0.16328664124011993  Generator_loss: 1.9048027992248535\n",
            "epoch: 9/10,    batch: 14598/15469    Discriminator_loss: 0.1654413342475891  Generator_loss: 1.8912670612335205\n",
            "epoch: 9/10,    batch: 14599/15469    Discriminator_loss: 0.16708853840827942  Generator_loss: 1.8807860612869263\n",
            "epoch: 9/10,    batch: 14600/15469    Discriminator_loss: 0.16833946108818054  Generator_loss: 1.873035192489624\n",
            "epoch: 9/10,    batch: 14601/15469    Discriminator_loss: 0.16927355527877808  Generator_loss: 1.8674590587615967\n",
            "epoch: 9/10,    batch: 14602/15469    Discriminator_loss: 0.1698940545320511  Generator_loss: 1.8633408546447754\n",
            "epoch: 9/10,    batch: 14603/15469    Discriminator_loss: 0.17018544673919678  Generator_loss: 1.8612456321716309\n",
            "epoch: 9/10,    batch: 14604/15469    Discriminator_loss: 1.4772807359695435  Generator_loss: 1.8554672002792358\n",
            "epoch: 9/10,    batch: 14605/15469    Discriminator_loss: 1.9228790998458862  Generator_loss: 1.8452835083007812\n",
            "epoch: 9/10,    batch: 14606/15469    Discriminator_loss: 1.917397141456604  Generator_loss: 1.8316140174865723\n",
            "epoch: 9/10,    batch: 14607/15469    Discriminator_loss: 1.9063607454299927  Generator_loss: 1.815373420715332\n",
            "epoch: 9/10,    batch: 14608/15469    Discriminator_loss: 1.9073176383972168  Generator_loss: 1.7972761392593384\n",
            "epoch: 9/10,    batch: 14609/15469    Discriminator_loss: 1.8844223022460938  Generator_loss: 1.7779183387756348\n",
            "epoch: 9/10,    batch: 14610/15469    Discriminator_loss: 1.8671493530273438  Generator_loss: 1.757224678993225\n",
            "epoch: 9/10,    batch: 14611/15469    Discriminator_loss: 1.855888843536377  Generator_loss: 1.736109733581543\n",
            "epoch: 9/10,    batch: 14612/15469    Discriminator_loss: 1.8381983041763306  Generator_loss: 1.7147438526153564\n",
            "epoch: 9/10,    batch: 14613/15469    Discriminator_loss: 1.8277860879898071  Generator_loss: 1.6931638717651367\n",
            "epoch: 9/10,    batch: 14614/15469    Discriminator_loss: 1.8132258653640747  Generator_loss: 1.6711676120758057\n",
            "epoch: 9/10,    batch: 14615/15469    Discriminator_loss: 1.7991728782653809  Generator_loss: 1.6497471332550049\n",
            "epoch: 9/10,    batch: 14616/15469    Discriminator_loss: 1.7845858335494995  Generator_loss: 1.6282907724380493\n",
            "epoch: 9/10,    batch: 14617/15469    Discriminator_loss: 1.7705227136611938  Generator_loss: 1.6068878173828125\n",
            "epoch: 9/10,    batch: 14618/15469    Discriminator_loss: 1.758176326751709  Generator_loss: 1.5862455368041992\n",
            "epoch: 9/10,    batch: 14619/15469    Discriminator_loss: 1.7473828792572021  Generator_loss: 1.5658053159713745\n",
            "epoch: 9/10,    batch: 14620/15469    Discriminator_loss: 1.736846685409546  Generator_loss: 1.5461020469665527\n",
            "epoch: 9/10,    batch: 14621/15469    Discriminator_loss: 1.7318378686904907  Generator_loss: 1.5264172554016113\n",
            "epoch: 9/10,    batch: 14622/15469    Discriminator_loss: 1.7178586721420288  Generator_loss: 1.507370948791504\n",
            "epoch: 9/10,    batch: 14623/15469    Discriminator_loss: 1.701551079750061  Generator_loss: 1.4889650344848633\n",
            "epoch: 9/10,    batch: 14624/15469    Discriminator_loss: 1.6939964294433594  Generator_loss: 1.4707752466201782\n",
            "epoch: 9/10,    batch: 14625/15469    Discriminator_loss: 0.3998572826385498  Generator_loss: 1.4569146633148193\n",
            "epoch: 9/10,    batch: 14626/15469    Discriminator_loss: 0.26780760288238525  Generator_loss: 1.4465794563293457\n",
            "epoch: 9/10,    batch: 14627/15469    Discriminator_loss: 0.2702140510082245  Generator_loss: 1.4394147396087646\n",
            "epoch: 9/10,    batch: 14628/15469    Discriminator_loss: 0.2720493674278259  Generator_loss: 1.4347625970840454\n",
            "epoch: 9/10,    batch: 14629/15469    Discriminator_loss: 0.27312570810317993  Generator_loss: 1.4320013523101807\n",
            "epoch: 9/10,    batch: 14630/15469    Discriminator_loss: 1.2684764862060547  Generator_loss: 1.4279985427856445\n",
            "epoch: 9/10,    batch: 14631/15469    Discriminator_loss: 1.6780134439468384  Generator_loss: 1.4217684268951416\n",
            "epoch: 9/10,    batch: 14632/15469    Discriminator_loss: 1.6860295534133911  Generator_loss: 1.4135265350341797\n",
            "epoch: 9/10,    batch: 14633/15469    Discriminator_loss: 1.6863338947296143  Generator_loss: 1.403586983680725\n",
            "epoch: 9/10,    batch: 14634/15469    Discriminator_loss: 1.8153963088989258  Generator_loss: 1.3909136056900024\n",
            "epoch: 9/10,    batch: 14635/15469    Discriminator_loss: 1.7502453327178955  Generator_loss: 1.3768253326416016\n",
            "epoch: 9/10,    batch: 14636/15469    Discriminator_loss: 1.7073408365249634  Generator_loss: 1.3619881868362427\n",
            "epoch: 9/10,    batch: 14637/15469    Discriminator_loss: 1.6873949766159058  Generator_loss: 1.346816062927246\n",
            "epoch: 9/10,    batch: 14638/15469    Discriminator_loss: 1.6785831451416016  Generator_loss: 1.3315074443817139\n",
            "epoch: 9/10,    batch: 14639/15469    Discriminator_loss: 1.6700576543807983  Generator_loss: 1.3162646293640137\n",
            "epoch: 9/10,    batch: 14640/15469    Discriminator_loss: 1.6636625528335571  Generator_loss: 1.301168441772461\n",
            "epoch: 9/10,    batch: 14641/15469    Discriminator_loss: 1.6536695957183838  Generator_loss: 1.2862603664398193\n",
            "epoch: 9/10,    batch: 14642/15469    Discriminator_loss: 1.6437047719955444  Generator_loss: 1.2716803550720215\n",
            "epoch: 9/10,    batch: 14643/15469    Discriminator_loss: 1.640972375869751  Generator_loss: 1.2573809623718262\n",
            "epoch: 9/10,    batch: 14644/15469    Discriminator_loss: 1.6397645473480225  Generator_loss: 1.24336576461792\n",
            "epoch: 9/10,    batch: 14645/15469    Discriminator_loss: 1.6327531337738037  Generator_loss: 1.2295982837677002\n",
            "epoch: 9/10,    batch: 14646/15469    Discriminator_loss: 1.6292219161987305  Generator_loss: 1.2160706520080566\n",
            "epoch: 9/10,    batch: 14647/15469    Discriminator_loss: 1.6148056983947754  Generator_loss: 1.2029244899749756\n",
            "epoch: 9/10,    batch: 14648/15469    Discriminator_loss: 1.6111538410186768  Generator_loss: 1.1900649070739746\n",
            "epoch: 9/10,    batch: 14649/15469    Discriminator_loss: 1.6074554920196533  Generator_loss: 1.177469253540039\n",
            "epoch: 9/10,    batch: 14650/15469    Discriminator_loss: 1.6102871894836426  Generator_loss: 1.1651275157928467\n",
            "epoch: 9/10,    batch: 14651/15469    Discriminator_loss: 1.6099483966827393  Generator_loss: 1.1529780626296997\n",
            "epoch: 9/10,    batch: 14652/15469    Discriminator_loss: 1.6083190441131592  Generator_loss: 1.141010046005249\n",
            "epoch: 9/10,    batch: 14653/15469    Discriminator_loss: 1.606574535369873  Generator_loss: 1.1291816234588623\n",
            "epoch: 9/10,    batch: 14654/15469    Discriminator_loss: 1.590975284576416  Generator_loss: 1.1176528930664062\n",
            "epoch: 9/10,    batch: 14655/15469    Discriminator_loss: 1.5992238521575928  Generator_loss: 1.1062616109848022\n",
            "epoch: 9/10,    batch: 14656/15469    Discriminator_loss: 1.5814476013183594  Generator_loss: 1.0951937437057495\n",
            "epoch: 9/10,    batch: 14657/15469    Discriminator_loss: 1.6110587120056152  Generator_loss: 1.0841801166534424\n",
            "epoch: 9/10,    batch: 14658/15469    Discriminator_loss: 1.591902732849121  Generator_loss: 1.0734272003173828\n",
            "epoch: 9/10,    batch: 14659/15469    Discriminator_loss: 1.5626375675201416  Generator_loss: 1.0629432201385498\n",
            "epoch: 9/10,    batch: 14660/15469    Discriminator_loss: 1.5773367881774902  Generator_loss: 1.0526808500289917\n",
            "epoch: 9/10,    batch: 14661/15469    Discriminator_loss: 1.587867259979248  Generator_loss: 1.042464017868042\n",
            "epoch: 9/10,    batch: 14662/15469    Discriminator_loss: 1.581899881362915  Generator_loss: 1.0324373245239258\n",
            "epoch: 9/10,    batch: 14663/15469    Discriminator_loss: 1.593492865562439  Generator_loss: 1.0225043296813965\n",
            "epoch: 9/10,    batch: 14664/15469    Discriminator_loss: 1.5710208415985107  Generator_loss: 1.0126450061798096\n",
            "epoch: 9/10,    batch: 14665/15469    Discriminator_loss: 1.5957114696502686  Generator_loss: 1.002975583076477\n",
            "epoch: 9/10,    batch: 14666/15469    Discriminator_loss: 1.5984327793121338  Generator_loss: 0.993242621421814\n",
            "epoch: 9/10,    batch: 14667/15469    Discriminator_loss: 1.5564121007919312  Generator_loss: 0.98362135887146\n",
            "epoch: 9/10,    batch: 14668/15469    Discriminator_loss: 1.4933784008026123  Generator_loss: 0.9743547439575195\n",
            "epoch: 9/10,    batch: 14669/15469    Discriminator_loss: 1.485126256942749  Generator_loss: 0.9652457237243652\n",
            "epoch: 9/10,    batch: 14670/15469    Discriminator_loss: 1.3905508518218994  Generator_loss: 0.9549036026000977\n",
            "epoch: 9/10,    batch: 14671/15469    Discriminator_loss: 1.3184716701507568  Generator_loss: 0.9433571100234985\n",
            "epoch: 9/10,    batch: 14672/15469    Discriminator_loss: 0.85503089427948  Generator_loss: 0.931035041809082\n",
            "epoch: 9/10,    batch: 14673/15469    Discriminator_loss: 0.5474507212638855  Generator_loss: 0.9212448596954346\n",
            "epoch: 9/10,    batch: 14674/15469    Discriminator_loss: 0.5122832655906677  Generator_loss: 0.9145934581756592\n",
            "epoch: 9/10,    batch: 14675/15469    Discriminator_loss: 0.515523374080658  Generator_loss: 0.9107242822647095\n",
            "epoch: 9/10,    batch: 14676/15469    Discriminator_loss: 0.5176902413368225  Generator_loss: 0.9089329242706299\n",
            "epoch: 9/10,    batch: 14677/15469    Discriminator_loss: 0.5171262621879578  Generator_loss: 0.9091675877571106\n",
            "epoch: 9/10,    batch: 14678/15469    Discriminator_loss: 0.5164041519165039  Generator_loss: 0.9108551144599915\n",
            "epoch: 9/10,    batch: 14679/15469    Discriminator_loss: 0.5147299766540527  Generator_loss: 0.9138190746307373\n",
            "epoch: 9/10,    batch: 14680/15469    Discriminator_loss: 0.5123616456985474  Generator_loss: 0.9177649021148682\n",
            "epoch: 9/10,    batch: 14681/15469    Discriminator_loss: 0.5092269778251648  Generator_loss: 0.9227738976478577\n",
            "epoch: 9/10,    batch: 14682/15469    Discriminator_loss: 0.5057371258735657  Generator_loss: 0.9284396171569824\n",
            "epoch: 9/10,    batch: 14683/15469    Discriminator_loss: 0.5016382932662964  Generator_loss: 0.9347636699676514\n",
            "epoch: 9/10,    batch: 14684/15469    Discriminator_loss: 0.4972377419471741  Generator_loss: 0.9414643049240112\n",
            "epoch: 9/10,    batch: 14685/15469    Discriminator_loss: 0.4927997887134552  Generator_loss: 0.9485127925872803\n",
            "epoch: 9/10,    batch: 14686/15469    Discriminator_loss: 0.4881375730037689  Generator_loss: 0.9559179544448853\n",
            "epoch: 9/10,    batch: 14687/15469    Discriminator_loss: 0.4834398329257965  Generator_loss: 0.9636362195014954\n",
            "epoch: 9/10,    batch: 14688/15469    Discriminator_loss: 0.4786566495895386  Generator_loss: 0.9714686870574951\n",
            "epoch: 9/10,    batch: 14689/15469    Discriminator_loss: 0.47381657361984253  Generator_loss: 0.9794940948486328\n",
            "epoch: 9/10,    batch: 14690/15469    Discriminator_loss: 0.46897679567337036  Generator_loss: 0.9875272512435913\n",
            "epoch: 9/10,    batch: 14691/15469    Discriminator_loss: 0.46411246061325073  Generator_loss: 0.9957578182220459\n",
            "epoch: 9/10,    batch: 14692/15469    Discriminator_loss: 0.45933467149734497  Generator_loss: 1.0039386749267578\n",
            "epoch: 9/10,    batch: 14693/15469    Discriminator_loss: 0.4545571804046631  Generator_loss: 1.0121634006500244\n",
            "epoch: 9/10,    batch: 14694/15469    Discriminator_loss: 0.44981420040130615  Generator_loss: 1.0203735828399658\n",
            "epoch: 9/10,    batch: 14695/15469    Discriminator_loss: 0.44516485929489136  Generator_loss: 1.0287237167358398\n",
            "epoch: 9/10,    batch: 14696/15469    Discriminator_loss: 0.4405447244644165  Generator_loss: 1.037000298500061\n",
            "epoch: 9/10,    batch: 14697/15469    Discriminator_loss: 0.43601030111312866  Generator_loss: 1.0452903509140015\n",
            "epoch: 9/10,    batch: 14698/15469    Discriminator_loss: 0.43148690462112427  Generator_loss: 1.0534305572509766\n",
            "epoch: 9/10,    batch: 14699/15469    Discriminator_loss: 0.4271492660045624  Generator_loss: 1.0617324113845825\n",
            "epoch: 9/10,    batch: 14700/15469    Discriminator_loss: 0.4227588474750519  Generator_loss: 1.0698150396347046\n",
            "epoch: 9/10,    batch: 14701/15469    Discriminator_loss: 0.41851165890693665  Generator_loss: 1.0780298709869385\n",
            "epoch: 9/10,    batch: 14702/15469    Discriminator_loss: 0.4143085479736328  Generator_loss: 1.0861103534698486\n",
            "epoch: 9/10,    batch: 14703/15469    Discriminator_loss: 0.41024050116539  Generator_loss: 1.094132423400879\n",
            "epoch: 9/10,    batch: 14704/15469    Discriminator_loss: 0.4061622619628906  Generator_loss: 1.1022144556045532\n",
            "epoch: 9/10,    batch: 14705/15469    Discriminator_loss: 0.4022054970264435  Generator_loss: 1.1101040840148926\n",
            "epoch: 9/10,    batch: 14706/15469    Discriminator_loss: 0.3982781171798706  Generator_loss: 1.1180415153503418\n",
            "epoch: 9/10,    batch: 14707/15469    Discriminator_loss: 0.394437700510025  Generator_loss: 1.1260228157043457\n",
            "epoch: 9/10,    batch: 14708/15469    Discriminator_loss: 0.3906647562980652  Generator_loss: 1.1337589025497437\n",
            "epoch: 9/10,    batch: 14709/15469    Discriminator_loss: 0.38695648312568665  Generator_loss: 1.1415822505950928\n",
            "epoch: 9/10,    batch: 14710/15469    Discriminator_loss: 0.38326045870780945  Generator_loss: 1.1493446826934814\n",
            "epoch: 9/10,    batch: 14711/15469    Discriminator_loss: 0.37970098853111267  Generator_loss: 1.1570379734039307\n",
            "epoch: 9/10,    batch: 14712/15469    Discriminator_loss: 0.3761807978153229  Generator_loss: 1.1648313999176025\n",
            "epoch: 9/10,    batch: 14713/15469    Discriminator_loss: 0.372687965631485  Generator_loss: 1.172410488128662\n",
            "epoch: 9/10,    batch: 14714/15469    Discriminator_loss: 0.3692588806152344  Generator_loss: 1.1800521612167358\n",
            "epoch: 9/10,    batch: 14715/15469    Discriminator_loss: 0.3658694326877594  Generator_loss: 1.1875633001327515\n",
            "epoch: 9/10,    batch: 14716/15469    Discriminator_loss: 0.3625096082687378  Generator_loss: 1.1951960325241089\n",
            "epoch: 9/10,    batch: 14717/15469    Discriminator_loss: 0.3592555820941925  Generator_loss: 1.202713966369629\n",
            "epoch: 9/10,    batch: 14718/15469    Discriminator_loss: 0.3560298681259155  Generator_loss: 1.2103290557861328\n",
            "epoch: 9/10,    batch: 14719/15469    Discriminator_loss: 0.352818101644516  Generator_loss: 1.2177329063415527\n",
            "epoch: 9/10,    batch: 14720/15469    Discriminator_loss: 0.34970492124557495  Generator_loss: 1.225290298461914\n",
            "epoch: 9/10,    batch: 14721/15469    Discriminator_loss: 0.3466070890426636  Generator_loss: 1.232619285583496\n",
            "epoch: 9/10,    batch: 14722/15469    Discriminator_loss: 0.3434903025627136  Generator_loss: 1.2401463985443115\n",
            "epoch: 9/10,    batch: 14723/15469    Discriminator_loss: 0.34047845005989075  Generator_loss: 1.2475645542144775\n",
            "epoch: 9/10,    batch: 14724/15469    Discriminator_loss: 0.3374849259853363  Generator_loss: 1.2549762725830078\n",
            "epoch: 9/10,    batch: 14725/15469    Discriminator_loss: 0.33453628420829773  Generator_loss: 1.2624247074127197\n",
            "epoch: 9/10,    batch: 14726/15469    Discriminator_loss: 0.3316040635108948  Generator_loss: 1.2697540521621704\n",
            "epoch: 9/10,    batch: 14727/15469    Discriminator_loss: 0.32871437072753906  Generator_loss: 1.2770955562591553\n",
            "epoch: 9/10,    batch: 14728/15469    Discriminator_loss: 0.3258514702320099  Generator_loss: 1.2845005989074707\n",
            "epoch: 9/10,    batch: 14729/15469    Discriminator_loss: 0.3230486214160919  Generator_loss: 1.2917767763137817\n",
            "epoch: 9/10,    batch: 14730/15469    Discriminator_loss: 0.32029929757118225  Generator_loss: 1.299115538597107\n",
            "epoch: 9/10,    batch: 14731/15469    Discriminator_loss: 0.3175193667411804  Generator_loss: 1.3064122200012207\n",
            "epoch: 9/10,    batch: 14732/15469    Discriminator_loss: 0.31479689478874207  Generator_loss: 1.3137753009796143\n",
            "epoch: 9/10,    batch: 14733/15469    Discriminator_loss: 0.3121189475059509  Generator_loss: 1.3211119174957275\n",
            "epoch: 9/10,    batch: 14734/15469    Discriminator_loss: 0.30945077538490295  Generator_loss: 1.3284060955047607\n",
            "epoch: 9/10,    batch: 14735/15469    Discriminator_loss: 0.30680862069129944  Generator_loss: 1.3356249332427979\n",
            "epoch: 9/10,    batch: 14736/15469    Discriminator_loss: 0.3041892945766449  Generator_loss: 1.3429962396621704\n",
            "epoch: 9/10,    batch: 14737/15469    Discriminator_loss: 0.3016233444213867  Generator_loss: 1.350217580795288\n",
            "epoch: 9/10,    batch: 14738/15469    Discriminator_loss: 0.2990821897983551  Generator_loss: 1.3574823141098022\n",
            "epoch: 9/10,    batch: 14739/15469    Discriminator_loss: 0.6184285879135132  Generator_loss: 1.3638746738433838\n",
            "epoch: 9/10,    batch: 14740/15469    Discriminator_loss: 1.7648735046386719  Generator_loss: 1.3663125038146973\n",
            "epoch: 9/10,    batch: 14741/15469    Discriminator_loss: 1.7656396627426147  Generator_loss: 1.3657162189483643\n",
            "epoch: 9/10,    batch: 14742/15469    Discriminator_loss: 1.7600089311599731  Generator_loss: 1.3625731468200684\n",
            "epoch: 9/10,    batch: 14743/15469    Discriminator_loss: 1.7654225826263428  Generator_loss: 1.3574326038360596\n",
            "epoch: 9/10,    batch: 14744/15469    Discriminator_loss: 1.7551265954971313  Generator_loss: 1.3506462574005127\n",
            "epoch: 9/10,    batch: 14745/15469    Discriminator_loss: 1.741279125213623  Generator_loss: 1.3428008556365967\n",
            "epoch: 9/10,    batch: 14746/15469    Discriminator_loss: 1.7260102033615112  Generator_loss: 1.334096908569336\n",
            "epoch: 9/10,    batch: 14747/15469    Discriminator_loss: 1.7094403505325317  Generator_loss: 1.324815034866333\n",
            "epoch: 9/10,    batch: 14748/15469    Discriminator_loss: 1.6966753005981445  Generator_loss: 1.315079689025879\n",
            "epoch: 9/10,    batch: 14749/15469    Discriminator_loss: 1.678606629371643  Generator_loss: 1.3050222396850586\n",
            "epoch: 9/10,    batch: 14750/15469    Discriminator_loss: 1.6699442863464355  Generator_loss: 1.2948209047317505\n",
            "epoch: 9/10,    batch: 14751/15469    Discriminator_loss: 1.651292085647583  Generator_loss: 1.2845441102981567\n",
            "epoch: 9/10,    batch: 14752/15469    Discriminator_loss: 1.6414852142333984  Generator_loss: 1.2741966247558594\n",
            "epoch: 9/10,    batch: 14753/15469    Discriminator_loss: 1.6254665851593018  Generator_loss: 1.2637792825698853\n",
            "epoch: 9/10,    batch: 14754/15469    Discriminator_loss: 1.6058778762817383  Generator_loss: 1.2533917427062988\n",
            "epoch: 9/10,    batch: 14755/15469    Discriminator_loss: 1.590802550315857  Generator_loss: 1.2430086135864258\n",
            "epoch: 9/10,    batch: 14756/15469    Discriminator_loss: 1.572700023651123  Generator_loss: 1.2326841354370117\n",
            "epoch: 9/10,    batch: 14757/15469    Discriminator_loss: 1.5533655881881714  Generator_loss: 1.2223551273345947\n",
            "epoch: 9/10,    batch: 14758/15469    Discriminator_loss: 1.535851001739502  Generator_loss: 1.2118557691574097\n",
            "epoch: 9/10,    batch: 14759/15469    Discriminator_loss: 1.5150063037872314  Generator_loss: 1.2011933326721191\n",
            "epoch: 9/10,    batch: 14760/15469    Discriminator_loss: 0.4236416816711426  Generator_loss: 1.1934988498687744\n",
            "epoch: 9/10,    batch: 14761/15469    Discriminator_loss: 0.3634827733039856  Generator_loss: 1.1882660388946533\n",
            "epoch: 9/10,    batch: 14762/15469    Discriminator_loss: 0.3647940158843994  Generator_loss: 1.1852625608444214\n",
            "epoch: 9/10,    batch: 14763/15469    Discriminator_loss: 0.3656750023365021  Generator_loss: 1.1840198040008545\n",
            "epoch: 9/10,    batch: 14764/15469    Discriminator_loss: 0.36581772565841675  Generator_loss: 1.1845239400863647\n",
            "epoch: 9/10,    batch: 14765/15469    Discriminator_loss: 1.1605143547058105  Generator_loss: 1.1833480596542358\n",
            "epoch: 9/10,    batch: 14766/15469    Discriminator_loss: 1.4706251621246338  Generator_loss: 1.1795988082885742\n",
            "epoch: 9/10,    batch: 14767/15469    Discriminator_loss: 1.4563815593719482  Generator_loss: 1.1734764575958252\n",
            "epoch: 9/10,    batch: 14768/15469    Discriminator_loss: 1.5240349769592285  Generator_loss: 1.164517879486084\n",
            "epoch: 9/10,    batch: 14769/15469    Discriminator_loss: 1.616452932357788  Generator_loss: 1.1546847820281982\n",
            "epoch: 9/10,    batch: 14770/15469    Discriminator_loss: 1.5960307121276855  Generator_loss: 1.1437209844589233\n",
            "epoch: 9/10,    batch: 14771/15469    Discriminator_loss: 1.5689148902893066  Generator_loss: 1.1311980485916138\n",
            "epoch: 9/10,    batch: 14772/15469    Discriminator_loss: 1.545255184173584  Generator_loss: 1.117526650428772\n",
            "epoch: 9/10,    batch: 14773/15469    Discriminator_loss: 1.5309529304504395  Generator_loss: 1.102052927017212\n",
            "epoch: 9/10,    batch: 14774/15469    Discriminator_loss: 1.5166202783584595  Generator_loss: 1.084904432296753\n",
            "epoch: 9/10,    batch: 14775/15469    Discriminator_loss: 1.4749250411987305  Generator_loss: 1.0656137466430664\n",
            "epoch: 9/10,    batch: 14776/15469    Discriminator_loss: 1.4759876728057861  Generator_loss: 1.0447511672973633\n",
            "epoch: 9/10,    batch: 14777/15469    Discriminator_loss: 1.4509694576263428  Generator_loss: 1.0226008892059326\n",
            "epoch: 9/10,    batch: 14778/15469    Discriminator_loss: 1.4604072570800781  Generator_loss: 0.9995827078819275\n",
            "epoch: 9/10,    batch: 14779/15469    Discriminator_loss: 1.4543890953063965  Generator_loss: 0.9768539667129517\n",
            "epoch: 9/10,    batch: 14780/15469    Discriminator_loss: 1.396573781967163  Generator_loss: 0.9535011053085327\n",
            "epoch: 9/10,    batch: 14781/15469    Discriminator_loss: 1.4239788055419922  Generator_loss: 0.9306284189224243\n",
            "epoch: 9/10,    batch: 14782/15469    Discriminator_loss: 1.4188024997711182  Generator_loss: 0.9082192182540894\n",
            "epoch: 9/10,    batch: 14783/15469    Discriminator_loss: 1.3872817754745483  Generator_loss: 0.8860628604888916\n",
            "epoch: 9/10,    batch: 14784/15469    Discriminator_loss: 1.3782823085784912  Generator_loss: 0.8636621832847595\n",
            "epoch: 9/10,    batch: 14785/15469    Discriminator_loss: 1.3459274768829346  Generator_loss: 0.840706467628479\n",
            "epoch: 9/10,    batch: 14786/15469    Discriminator_loss: 1.3352010250091553  Generator_loss: 0.8170390129089355\n",
            "epoch: 9/10,    batch: 14787/15469    Discriminator_loss: 1.0421119928359985  Generator_loss: 0.7935227155685425\n",
            "epoch: 9/10,    batch: 14788/15469    Discriminator_loss: 0.9986968636512756  Generator_loss: 0.7703527212142944\n",
            "epoch: 9/10,    batch: 14789/15469    Discriminator_loss: 0.8539167046546936  Generator_loss: 0.7494869232177734\n",
            "epoch: 9/10,    batch: 14790/15469    Discriminator_loss: 0.8529226183891296  Generator_loss: 0.7234673500061035\n",
            "epoch: 9/10,    batch: 14791/15469    Discriminator_loss: 0.7926899194717407  Generator_loss: 0.6885818243026733\n",
            "epoch: 9/10,    batch: 14792/15469    Discriminator_loss: 0.7637450695037842  Generator_loss: 0.6514796018600464\n",
            "epoch: 9/10,    batch: 14793/15469    Discriminator_loss: 0.7952784299850464  Generator_loss: 0.6164923906326294\n",
            "epoch: 9/10,    batch: 14794/15469    Discriminator_loss: 0.8490086197853088  Generator_loss: 0.5838267803192139\n",
            "epoch: 9/10,    batch: 14795/15469    Discriminator_loss: 0.8957163095474243  Generator_loss: 0.5991232395172119\n",
            "epoch: 9/10,    batch: 14796/15469    Discriminator_loss: 0.8503097891807556  Generator_loss: 0.6560623645782471\n",
            "epoch: 9/10,    batch: 14797/15469    Discriminator_loss: 0.7313534617424011  Generator_loss: 0.7201743125915527\n",
            "epoch: 9/10,    batch: 14798/15469    Discriminator_loss: 0.6638858914375305  Generator_loss: 0.7852081060409546\n",
            "epoch: 9/10,    batch: 14799/15469    Discriminator_loss: 0.6029693484306335  Generator_loss: 0.8531730771064758\n",
            "epoch: 9/10,    batch: 14800/15469    Discriminator_loss: 0.5482426881790161  Generator_loss: 0.9298185110092163\n",
            "epoch: 9/10,    batch: 14801/15469    Discriminator_loss: 0.5018339157104492  Generator_loss: 1.015290379524231\n",
            "epoch: 9/10,    batch: 14802/15469    Discriminator_loss: 0.4446530044078827  Generator_loss: 1.1100555658340454\n",
            "epoch: 9/10,    batch: 14803/15469    Discriminator_loss: 0.4237512946128845  Generator_loss: 1.2147104740142822\n",
            "epoch: 9/10,    batch: 14804/15469    Discriminator_loss: 0.3475680351257324  Generator_loss: 1.3056493997573853\n",
            "epoch: 9/10,    batch: 14805/15469    Discriminator_loss: 0.3214700520038605  Generator_loss: 1.3605625629425049\n",
            "epoch: 9/10,    batch: 14806/15469    Discriminator_loss: 0.30033615231513977  Generator_loss: 1.389068603515625\n",
            "epoch: 9/10,    batch: 14807/15469    Discriminator_loss: 0.2909785211086273  Generator_loss: 1.4046112298965454\n",
            "epoch: 9/10,    batch: 14808/15469    Discriminator_loss: 0.28575995564460754  Generator_loss: 1.4171030521392822\n",
            "epoch: 9/10,    batch: 14809/15469    Discriminator_loss: 0.28086185455322266  Generator_loss: 1.4275975227355957\n",
            "epoch: 9/10,    batch: 14810/15469    Discriminator_loss: 0.2770163118839264  Generator_loss: 1.436706304550171\n",
            "epoch: 9/10,    batch: 14811/15469    Discriminator_loss: 0.2735811769962311  Generator_loss: 1.4461456537246704\n",
            "epoch: 9/10,    batch: 14812/15469    Discriminator_loss: 0.27059200406074524  Generator_loss: 1.4539388418197632\n",
            "epoch: 9/10,    batch: 14813/15469    Discriminator_loss: 0.2687433063983917  Generator_loss: 1.4614232778549194\n",
            "epoch: 9/10,    batch: 14814/15469    Discriminator_loss: 0.2659502923488617  Generator_loss: 1.4676403999328613\n",
            "epoch: 9/10,    batch: 14815/15469    Discriminator_loss: 0.2643781006336212  Generator_loss: 1.4727596044540405\n",
            "epoch: 9/10,    batch: 14816/15469    Discriminator_loss: 0.26241517066955566  Generator_loss: 1.4776597023010254\n",
            "epoch: 9/10,    batch: 14817/15469    Discriminator_loss: 0.2609669864177704  Generator_loss: 1.4817062616348267\n",
            "epoch: 9/10,    batch: 14818/15469    Discriminator_loss: 0.25971582531929016  Generator_loss: 1.4855740070343018\n",
            "epoch: 9/10,    batch: 14819/15469    Discriminator_loss: 0.25829386711120605  Generator_loss: 1.488756537437439\n",
            "epoch: 9/10,    batch: 14820/15469    Discriminator_loss: 0.25704118609428406  Generator_loss: 1.4917073249816895\n",
            "epoch: 9/10,    batch: 14821/15469    Discriminator_loss: 0.25612422823905945  Generator_loss: 1.494888424873352\n",
            "epoch: 9/10,    batch: 14822/15469    Discriminator_loss: 0.2551560699939728  Generator_loss: 1.4978524446487427\n",
            "epoch: 9/10,    batch: 14823/15469    Discriminator_loss: 0.25420433282852173  Generator_loss: 1.5010485649108887\n",
            "epoch: 9/10,    batch: 14824/15469    Discriminator_loss: 0.25318247079849243  Generator_loss: 1.5044524669647217\n",
            "epoch: 9/10,    batch: 14825/15469    Discriminator_loss: 0.2521512508392334  Generator_loss: 1.5079069137573242\n",
            "epoch: 9/10,    batch: 14826/15469    Discriminator_loss: 0.2510110139846802  Generator_loss: 1.5116863250732422\n",
            "epoch: 9/10,    batch: 14827/15469    Discriminator_loss: 0.24978527426719666  Generator_loss: 1.5157577991485596\n",
            "epoch: 9/10,    batch: 14828/15469    Discriminator_loss: 0.24848651885986328  Generator_loss: 1.5204102993011475\n",
            "epoch: 9/10,    batch: 14829/15469    Discriminator_loss: 0.2471058964729309  Generator_loss: 1.5252137184143066\n",
            "epoch: 9/10,    batch: 14830/15469    Discriminator_loss: 0.24566343426704407  Generator_loss: 1.5305249691009521\n",
            "epoch: 9/10,    batch: 14831/15469    Discriminator_loss: 0.24410676956176758  Generator_loss: 1.5358819961547852\n",
            "epoch: 9/10,    batch: 14832/15469    Discriminator_loss: 0.24250394105911255  Generator_loss: 1.5416083335876465\n",
            "epoch: 9/10,    batch: 14833/15469    Discriminator_loss: 0.24085459113121033  Generator_loss: 1.5476651191711426\n",
            "epoch: 9/10,    batch: 14834/15469    Discriminator_loss: 0.23916274309158325  Generator_loss: 1.5539093017578125\n",
            "epoch: 9/10,    batch: 14835/15469    Discriminator_loss: 0.2374231219291687  Generator_loss: 1.5603042840957642\n",
            "epoch: 9/10,    batch: 14836/15469    Discriminator_loss: 0.23562316596508026  Generator_loss: 1.5669682025909424\n",
            "epoch: 9/10,    batch: 14837/15469    Discriminator_loss: 0.23385462164878845  Generator_loss: 1.5736806392669678\n",
            "epoch: 9/10,    batch: 14838/15469    Discriminator_loss: 0.2320559322834015  Generator_loss: 1.5805788040161133\n",
            "epoch: 9/10,    batch: 14839/15469    Discriminator_loss: 0.23025639355182648  Generator_loss: 1.5874427556991577\n",
            "epoch: 9/10,    batch: 14840/15469    Discriminator_loss: 0.22844673693180084  Generator_loss: 1.594388723373413\n",
            "epoch: 9/10,    batch: 14841/15469    Discriminator_loss: 0.22667303681373596  Generator_loss: 1.601435661315918\n",
            "epoch: 9/10,    batch: 14842/15469    Discriminator_loss: 0.22488833963871002  Generator_loss: 1.6084449291229248\n",
            "epoch: 9/10,    batch: 14843/15469    Discriminator_loss: 0.2231273651123047  Generator_loss: 1.6153730154037476\n",
            "epoch: 9/10,    batch: 14844/15469    Discriminator_loss: 0.2213890552520752  Generator_loss: 1.622440218925476\n",
            "epoch: 9/10,    batch: 14845/15469    Discriminator_loss: 0.21965526044368744  Generator_loss: 1.6294794082641602\n",
            "epoch: 9/10,    batch: 14846/15469    Discriminator_loss: 0.21796004474163055  Generator_loss: 1.6363873481750488\n",
            "epoch: 9/10,    batch: 14847/15469    Discriminator_loss: 0.2162642925977707  Generator_loss: 1.6433699131011963\n",
            "epoch: 9/10,    batch: 14848/15469    Discriminator_loss: 0.21459262073040009  Generator_loss: 1.6502842903137207\n",
            "epoch: 9/10,    batch: 14849/15469    Discriminator_loss: 0.2129543125629425  Generator_loss: 1.657196283340454\n",
            "epoch: 9/10,    batch: 14850/15469    Discriminator_loss: 0.21132415533065796  Generator_loss: 1.6640219688415527\n",
            "epoch: 9/10,    batch: 14851/15469    Discriminator_loss: 0.20969632267951965  Generator_loss: 1.6709256172180176\n",
            "epoch: 9/10,    batch: 14852/15469    Discriminator_loss: 0.20810356736183167  Generator_loss: 1.6778451204299927\n",
            "epoch: 9/10,    batch: 14853/15469    Discriminator_loss: 0.2065224051475525  Generator_loss: 1.6847662925720215\n",
            "epoch: 9/10,    batch: 14854/15469    Discriminator_loss: 0.2049478143453598  Generator_loss: 1.6915584802627563\n",
            "epoch: 9/10,    batch: 14855/15469    Discriminator_loss: 0.20338115096092224  Generator_loss: 1.6983749866485596\n",
            "epoch: 9/10,    batch: 14856/15469    Discriminator_loss: 0.20183685421943665  Generator_loss: 1.7053813934326172\n",
            "epoch: 9/10,    batch: 14857/15469    Discriminator_loss: 0.20030534267425537  Generator_loss: 1.7121676206588745\n",
            "epoch: 9/10,    batch: 14858/15469    Discriminator_loss: 0.19877828657627106  Generator_loss: 1.7190543413162231\n",
            "epoch: 9/10,    batch: 14859/15469    Discriminator_loss: 0.19726817309856415  Generator_loss: 1.7258951663970947\n",
            "epoch: 9/10,    batch: 14860/15469    Discriminator_loss: 0.19577957689762115  Generator_loss: 1.73276686668396\n",
            "epoch: 9/10,    batch: 14861/15469    Discriminator_loss: 0.19429132342338562  Generator_loss: 1.7397513389587402\n",
            "epoch: 9/10,    batch: 14862/15469    Discriminator_loss: 0.19280092418193817  Generator_loss: 1.74664306640625\n",
            "epoch: 9/10,    batch: 14863/15469    Discriminator_loss: 0.1913377195596695  Generator_loss: 1.7535607814788818\n",
            "epoch: 9/10,    batch: 14864/15469    Discriminator_loss: 0.1898927390575409  Generator_loss: 1.7604265213012695\n",
            "epoch: 9/10,    batch: 14865/15469    Discriminator_loss: 0.1884375363588333  Generator_loss: 1.7672737836837769\n",
            "epoch: 9/10,    batch: 14866/15469    Discriminator_loss: 0.1870282143354416  Generator_loss: 1.7742643356323242\n",
            "epoch: 9/10,    batch: 14867/15469    Discriminator_loss: 0.18559317290782928  Generator_loss: 1.781078815460205\n",
            "epoch: 9/10,    batch: 14868/15469    Discriminator_loss: 0.18421651422977448  Generator_loss: 1.7879807949066162\n",
            "epoch: 9/10,    batch: 14869/15469    Discriminator_loss: 0.18282419443130493  Generator_loss: 1.794835090637207\n",
            "epoch: 9/10,    batch: 14870/15469    Discriminator_loss: 0.18142955005168915  Generator_loss: 1.8016891479492188\n",
            "epoch: 9/10,    batch: 14871/15469    Discriminator_loss: 0.18006695806980133  Generator_loss: 1.8084979057312012\n",
            "epoch: 9/10,    batch: 14872/15469    Discriminator_loss: 0.17872677743434906  Generator_loss: 1.8153878450393677\n",
            "epoch: 9/10,    batch: 14873/15469    Discriminator_loss: 0.17738747596740723  Generator_loss: 1.822270393371582\n",
            "epoch: 9/10,    batch: 14874/15469    Discriminator_loss: 0.17607998847961426  Generator_loss: 1.829039454460144\n",
            "epoch: 9/10,    batch: 14875/15469    Discriminator_loss: 0.1747709959745407  Generator_loss: 1.8357588052749634\n",
            "epoch: 9/10,    batch: 14876/15469    Discriminator_loss: 1.878995656967163  Generator_loss: 1.8329176902770996\n",
            "epoch: 9/10,    batch: 14877/15469    Discriminator_loss: 1.860329270362854  Generator_loss: 1.8234580755233765\n",
            "epoch: 9/10,    batch: 14878/15469    Discriminator_loss: 1.816117763519287  Generator_loss: 1.8093762397766113\n",
            "epoch: 9/10,    batch: 14879/15469    Discriminator_loss: 1.7704912424087524  Generator_loss: 1.7923686504364014\n",
            "epoch: 9/10,    batch: 14880/15469    Discriminator_loss: 1.735849142074585  Generator_loss: 1.773510456085205\n",
            "epoch: 9/10,    batch: 14881/15469    Discriminator_loss: 1.673730492591858  Generator_loss: 1.753821611404419\n",
            "epoch: 9/10,    batch: 14882/15469    Discriminator_loss: 1.6251822710037231  Generator_loss: 1.73347806930542\n",
            "epoch: 9/10,    batch: 14883/15469    Discriminator_loss: 1.5729236602783203  Generator_loss: 1.7130811214447021\n",
            "epoch: 9/10,    batch: 14884/15469    Discriminator_loss: 1.5375810861587524  Generator_loss: 1.6924126148223877\n",
            "epoch: 9/10,    batch: 14885/15469    Discriminator_loss: 1.4961857795715332  Generator_loss: 1.6721093654632568\n",
            "epoch: 9/10,    batch: 14886/15469    Discriminator_loss: 1.4606077671051025  Generator_loss: 1.652311086654663\n",
            "epoch: 9/10,    batch: 14887/15469    Discriminator_loss: 1.439222812652588  Generator_loss: 1.632636308670044\n",
            "epoch: 9/10,    batch: 14888/15469    Discriminator_loss: 1.4077444076538086  Generator_loss: 1.6130311489105225\n",
            "epoch: 9/10,    batch: 14889/15469    Discriminator_loss: 1.380228042602539  Generator_loss: 1.5939819812774658\n",
            "epoch: 9/10,    batch: 14890/15469    Discriminator_loss: 1.368154525756836  Generator_loss: 1.5751100778579712\n",
            "epoch: 9/10,    batch: 14891/15469    Discriminator_loss: 1.3504106998443604  Generator_loss: 1.5561647415161133\n",
            "epoch: 9/10,    batch: 14892/15469    Discriminator_loss: 1.325920581817627  Generator_loss: 1.5375099182128906\n",
            "epoch: 9/10,    batch: 14893/15469    Discriminator_loss: 1.289171814918518  Generator_loss: 1.5185905694961548\n",
            "epoch: 9/10,    batch: 14894/15469    Discriminator_loss: 1.2838411331176758  Generator_loss: 1.49991774559021\n",
            "epoch: 9/10,    batch: 14895/15469    Discriminator_loss: 1.2797263860702515  Generator_loss: 1.4806854724884033\n",
            "epoch: 9/10,    batch: 14896/15469    Discriminator_loss: 1.2171655893325806  Generator_loss: 1.4605295658111572\n",
            "epoch: 9/10,    batch: 14897/15469    Discriminator_loss: 0.2707566022872925  Generator_loss: 1.442928433418274\n",
            "epoch: 9/10,    batch: 14898/15469    Discriminator_loss: 0.2747876048088074  Generator_loss: 1.4274928569793701\n",
            "epoch: 9/10,    batch: 14899/15469    Discriminator_loss: 0.2795323431491852  Generator_loss: 1.413100004196167\n",
            "epoch: 9/10,    batch: 14900/15469    Discriminator_loss: 0.2842879593372345  Generator_loss: 1.4003026485443115\n",
            "epoch: 9/10,    batch: 14901/15469    Discriminator_loss: 0.2885934114456177  Generator_loss: 1.389223575592041\n",
            "epoch: 9/10,    batch: 14902/15469    Discriminator_loss: 1.1630949974060059  Generator_loss: 1.3736835718154907\n",
            "epoch: 9/10,    batch: 14903/15469    Discriminator_loss: 1.3199024200439453  Generator_loss: 1.3529558181762695\n",
            "epoch: 9/10,    batch: 14904/15469    Discriminator_loss: 1.315314769744873  Generator_loss: 1.3280713558197021\n",
            "epoch: 9/10,    batch: 14905/15469    Discriminator_loss: 1.481658935546875  Generator_loss: 1.289448618888855\n",
            "epoch: 9/10,    batch: 14906/15469    Discriminator_loss: 1.8406485319137573  Generator_loss: 1.241173505783081\n",
            "epoch: 9/10,    batch: 14907/15469    Discriminator_loss: 1.8081547021865845  Generator_loss: 1.18619704246521\n",
            "epoch: 9/10,    batch: 14908/15469    Discriminator_loss: 1.7492260932922363  Generator_loss: 1.1274049282073975\n",
            "epoch: 9/10,    batch: 14909/15469    Discriminator_loss: 1.6453194618225098  Generator_loss: 1.0698697566986084\n",
            "epoch: 9/10,    batch: 14910/15469    Discriminator_loss: 1.617645263671875  Generator_loss: 1.0194813013076782\n",
            "epoch: 9/10,    batch: 14911/15469    Discriminator_loss: 1.5752959251403809  Generator_loss: 0.978155255317688\n",
            "epoch: 9/10,    batch: 14912/15469    Discriminator_loss: 1.5415799617767334  Generator_loss: 0.9453595876693726\n",
            "epoch: 9/10,    batch: 14913/15469    Discriminator_loss: 1.51710045337677  Generator_loss: 0.9194265604019165\n",
            "epoch: 9/10,    batch: 14914/15469    Discriminator_loss: 1.503197431564331  Generator_loss: 0.8990222215652466\n",
            "epoch: 9/10,    batch: 14915/15469    Discriminator_loss: 1.4595831632614136  Generator_loss: 0.8826090097427368\n",
            "epoch: 9/10,    batch: 14916/15469    Discriminator_loss: 1.4237849712371826  Generator_loss: 0.8691321015357971\n",
            "epoch: 9/10,    batch: 14917/15469    Discriminator_loss: 1.3625723123550415  Generator_loss: 0.857745885848999\n",
            "epoch: 9/10,    batch: 14918/15469    Discriminator_loss: 1.3693032264709473  Generator_loss: 0.8479695320129395\n",
            "epoch: 9/10,    batch: 14919/15469    Discriminator_loss: 1.3570172786712646  Generator_loss: 0.8395076394081116\n",
            "epoch: 9/10,    batch: 14920/15469    Discriminator_loss: 1.3414757251739502  Generator_loss: 0.8319186568260193\n",
            "epoch: 9/10,    batch: 14921/15469    Discriminator_loss: 1.3483843803405762  Generator_loss: 0.8251700401306152\n",
            "epoch: 9/10,    batch: 14922/15469    Discriminator_loss: 1.3326213359832764  Generator_loss: 0.8190423846244812\n",
            "epoch: 9/10,    batch: 14923/15469    Discriminator_loss: 1.3472919464111328  Generator_loss: 0.8135287761688232\n",
            "epoch: 9/10,    batch: 14924/15469    Discriminator_loss: 1.309109091758728  Generator_loss: 0.8084772825241089\n",
            "epoch: 9/10,    batch: 14925/15469    Discriminator_loss: 1.3038921356201172  Generator_loss: 0.8041030764579773\n",
            "epoch: 9/10,    batch: 14926/15469    Discriminator_loss: 1.305831789970398  Generator_loss: 0.8001335859298706\n",
            "epoch: 9/10,    batch: 14927/15469    Discriminator_loss: 1.294368028640747  Generator_loss: 0.7962667942047119\n",
            "epoch: 9/10,    batch: 14928/15469    Discriminator_loss: 1.3054801225662231  Generator_loss: 0.792604386806488\n",
            "epoch: 9/10,    batch: 14929/15469    Discriminator_loss: 1.2843496799468994  Generator_loss: 0.7889260649681091\n",
            "epoch: 9/10,    batch: 14930/15469    Discriminator_loss: 1.2453365325927734  Generator_loss: 0.7850639820098877\n",
            "epoch: 9/10,    batch: 14931/15469    Discriminator_loss: 1.2677133083343506  Generator_loss: 0.7814045548439026\n",
            "epoch: 9/10,    batch: 14932/15469    Discriminator_loss: 1.2084383964538574  Generator_loss: 0.7775892019271851\n",
            "epoch: 9/10,    batch: 14933/15469    Discriminator_loss: 1.1757807731628418  Generator_loss: 0.7732705473899841\n",
            "epoch: 9/10,    batch: 14934/15469    Discriminator_loss: 0.832531750202179  Generator_loss: 0.7700724601745605\n",
            "epoch: 9/10,    batch: 14935/15469    Discriminator_loss: 0.6347520351409912  Generator_loss: 0.7692532539367676\n",
            "epoch: 9/10,    batch: 14936/15469    Discriminator_loss: 0.6585782766342163  Generator_loss: 0.7700996994972229\n",
            "epoch: 9/10,    batch: 14937/15469    Discriminator_loss: 0.6624994277954102  Generator_loss: 0.7721216678619385\n",
            "epoch: 9/10,    batch: 14938/15469    Discriminator_loss: 0.6242180466651917  Generator_loss: 0.7758048176765442\n",
            "epoch: 9/10,    batch: 14939/15469    Discriminator_loss: 0.6177085638046265  Generator_loss: 0.7805013060569763\n",
            "epoch: 9/10,    batch: 14940/15469    Discriminator_loss: 0.6137194037437439  Generator_loss: 0.7860817313194275\n",
            "epoch: 9/10,    batch: 14941/15469    Discriminator_loss: 0.6073228120803833  Generator_loss: 0.7926598787307739\n",
            "epoch: 9/10,    batch: 14942/15469    Discriminator_loss: 0.6016291975975037  Generator_loss: 0.799654483795166\n",
            "epoch: 9/10,    batch: 14943/15469    Discriminator_loss: 0.5956153869628906  Generator_loss: 0.8072877526283264\n",
            "epoch: 9/10,    batch: 14944/15469    Discriminator_loss: 0.5892933011054993  Generator_loss: 0.8151131868362427\n",
            "epoch: 9/10,    batch: 14945/15469    Discriminator_loss: 0.5828023552894592  Generator_loss: 0.8234260082244873\n",
            "epoch: 9/10,    batch: 14946/15469    Discriminator_loss: 0.576156735420227  Generator_loss: 0.8317984342575073\n",
            "epoch: 9/10,    batch: 14947/15469    Discriminator_loss: 0.5696989893913269  Generator_loss: 0.8403590321540833\n",
            "epoch: 9/10,    batch: 14948/15469    Discriminator_loss: 0.563115656375885  Generator_loss: 0.8490609526634216\n",
            "epoch: 9/10,    batch: 14949/15469    Discriminator_loss: 0.5565273761749268  Generator_loss: 0.857864499092102\n",
            "epoch: 9/10,    batch: 14950/15469    Discriminator_loss: 0.5499351024627686  Generator_loss: 0.8665729761123657\n",
            "epoch: 9/10,    batch: 14951/15469    Discriminator_loss: 0.543668806552887  Generator_loss: 0.875424861907959\n",
            "epoch: 9/10,    batch: 14952/15469    Discriminator_loss: 0.5373477339744568  Generator_loss: 0.884235143661499\n",
            "epoch: 9/10,    batch: 14953/15469    Discriminator_loss: 0.530974268913269  Generator_loss: 0.8930888175964355\n",
            "epoch: 9/10,    batch: 14954/15469    Discriminator_loss: 0.5245175957679749  Generator_loss: 0.901835560798645\n",
            "epoch: 9/10,    batch: 14955/15469    Discriminator_loss: 0.5184887051582336  Generator_loss: 0.910681426525116\n",
            "epoch: 9/10,    batch: 14956/15469    Discriminator_loss: 0.5125795602798462  Generator_loss: 0.9193811416625977\n",
            "epoch: 9/10,    batch: 14957/15469    Discriminator_loss: 0.5067881345748901  Generator_loss: 0.9280866384506226\n",
            "epoch: 9/10,    batch: 14958/15469    Discriminator_loss: 0.5010904669761658  Generator_loss: 0.9367827773094177\n",
            "epoch: 9/10,    batch: 14959/15469    Discriminator_loss: 0.49551743268966675  Generator_loss: 0.9454200267791748\n",
            "epoch: 9/10,    batch: 14960/15469    Discriminator_loss: 0.49000248312950134  Generator_loss: 0.9540171027183533\n",
            "epoch: 9/10,    batch: 14961/15469    Discriminator_loss: 0.48461610078811646  Generator_loss: 0.962567150592804\n",
            "epoch: 9/10,    batch: 14962/15469    Discriminator_loss: 0.47932255268096924  Generator_loss: 0.9711018800735474\n",
            "epoch: 9/10,    batch: 14963/15469    Discriminator_loss: 0.4741332530975342  Generator_loss: 0.9796380400657654\n",
            "epoch: 9/10,    batch: 14964/15469    Discriminator_loss: 0.46899592876434326  Generator_loss: 0.9880518317222595\n",
            "epoch: 9/10,    batch: 14965/15469    Discriminator_loss: 0.4639955759048462  Generator_loss: 0.9964879751205444\n",
            "epoch: 9/10,    batch: 14966/15469    Discriminator_loss: 0.45906710624694824  Generator_loss: 1.0048785209655762\n",
            "epoch: 9/10,    batch: 14967/15469    Discriminator_loss: 0.45424166321754456  Generator_loss: 1.0132410526275635\n",
            "epoch: 9/10,    batch: 14968/15469    Discriminator_loss: 0.44944894313812256  Generator_loss: 1.0215193033218384\n",
            "epoch: 9/10,    batch: 14969/15469    Discriminator_loss: 0.44478052854537964  Generator_loss: 1.0298128128051758\n",
            "epoch: 9/10,    batch: 14970/15469    Discriminator_loss: 0.44015592336654663  Generator_loss: 1.0380771160125732\n",
            "epoch: 9/10,    batch: 14971/15469    Discriminator_loss: 0.4356502592563629  Generator_loss: 1.0462870597839355\n",
            "epoch: 9/10,    batch: 14972/15469    Discriminator_loss: 0.43119800090789795  Generator_loss: 1.0544908046722412\n",
            "epoch: 9/10,    batch: 14973/15469    Discriminator_loss: 0.42681607604026794  Generator_loss: 1.0626507997512817\n",
            "epoch: 9/10,    batch: 14974/15469    Discriminator_loss: 0.4225095212459564  Generator_loss: 1.0708093643188477\n",
            "epoch: 9/10,    batch: 14975/15469    Discriminator_loss: 0.4182663857936859  Generator_loss: 1.0789291858673096\n",
            "epoch: 9/10,    batch: 14976/15469    Discriminator_loss: 0.41408610343933105  Generator_loss: 1.0869812965393066\n",
            "epoch: 9/10,    batch: 14977/15469    Discriminator_loss: 0.4099860191345215  Generator_loss: 1.095080018043518\n",
            "epoch: 9/10,    batch: 14978/15469    Discriminator_loss: 0.4059262275695801  Generator_loss: 1.1031029224395752\n",
            "epoch: 9/10,    batch: 14979/15469    Discriminator_loss: 0.40194448828697205  Generator_loss: 1.1111353635787964\n",
            "epoch: 9/10,    batch: 14980/15469    Discriminator_loss: 0.39800822734832764  Generator_loss: 1.1191234588623047\n",
            "epoch: 9/10,    batch: 14981/15469    Discriminator_loss: 0.39413967728614807  Generator_loss: 1.1270701885223389\n",
            "epoch: 9/10,    batch: 14982/15469    Discriminator_loss: 0.3903313875198364  Generator_loss: 1.1349937915802002\n",
            "epoch: 9/10,    batch: 14983/15469    Discriminator_loss: 0.3865421712398529  Generator_loss: 1.1429674625396729\n",
            "epoch: 9/10,    batch: 14984/15469    Discriminator_loss: 0.3828308582305908  Generator_loss: 1.150895595550537\n",
            "epoch: 9/10,    batch: 14985/15469    Discriminator_loss: 0.37915506958961487  Generator_loss: 1.158765435218811\n",
            "epoch: 9/10,    batch: 14986/15469    Discriminator_loss: 0.3755560517311096  Generator_loss: 1.166709065437317\n",
            "epoch: 9/10,    batch: 14987/15469    Discriminator_loss: 0.37198156118392944  Generator_loss: 1.1745250225067139\n",
            "epoch: 9/10,    batch: 14988/15469    Discriminator_loss: 0.3684484362602234  Generator_loss: 1.1824314594268799\n",
            "epoch: 9/10,    batch: 14989/15469    Discriminator_loss: 0.36497849225997925  Generator_loss: 1.1902494430541992\n",
            "epoch: 9/10,    batch: 14990/15469    Discriminator_loss: 0.36156389117240906  Generator_loss: 1.1981322765350342\n",
            "epoch: 9/10,    batch: 14991/15469    Discriminator_loss: 0.3581453561782837  Generator_loss: 1.2059803009033203\n",
            "epoch: 9/10,    batch: 14992/15469    Discriminator_loss: 0.35479095578193665  Generator_loss: 1.2138361930847168\n",
            "epoch: 9/10,    batch: 14993/15469    Discriminator_loss: 0.3514734208583832  Generator_loss: 1.2217458486557007\n",
            "epoch: 9/10,    batch: 14994/15469    Discriminator_loss: 0.348178505897522  Generator_loss: 1.229490041732788\n",
            "epoch: 9/10,    batch: 14995/15469    Discriminator_loss: 0.3449331521987915  Generator_loss: 1.2373580932617188\n",
            "epoch: 9/10,    batch: 14996/15469    Discriminator_loss: 0.34172433614730835  Generator_loss: 1.245267391204834\n",
            "epoch: 9/10,    batch: 14997/15469    Discriminator_loss: 0.3385436236858368  Generator_loss: 1.253086805343628\n",
            "epoch: 9/10,    batch: 14998/15469    Discriminator_loss: 0.33538419008255005  Generator_loss: 1.260968804359436\n",
            "epoch: 9/10,    batch: 14999/15469    Discriminator_loss: 0.33226266503334045  Generator_loss: 1.2689024209976196\n",
            "epoch: 9/10,    batch: 15000/15469    Discriminator_loss: 0.32915908098220825  Generator_loss: 1.276799201965332\n",
            "epoch: 9/10,    batch: 15001/15469    Discriminator_loss: 0.32609477639198303  Generator_loss: 1.2846949100494385\n",
            "epoch: 9/10,    batch: 15002/15469    Discriminator_loss: 0.32304081320762634  Generator_loss: 1.2927145957946777\n",
            "epoch: 9/10,    batch: 15003/15469    Discriminator_loss: 0.3200071454048157  Generator_loss: 1.3006854057312012\n",
            "epoch: 9/10,    batch: 15004/15469    Discriminator_loss: 0.3170173168182373  Generator_loss: 1.3086990118026733\n",
            "epoch: 9/10,    batch: 15005/15469    Discriminator_loss: 0.3140427768230438  Generator_loss: 1.316711187362671\n",
            "epoch: 9/10,    batch: 15006/15469    Discriminator_loss: 0.31109359860420227  Generator_loss: 1.324796199798584\n",
            "epoch: 9/10,    batch: 15007/15469    Discriminator_loss: 0.3081614375114441  Generator_loss: 1.3328739404678345\n",
            "epoch: 9/10,    batch: 15008/15469    Discriminator_loss: 0.3052433431148529  Generator_loss: 1.3410210609436035\n",
            "epoch: 9/10,    batch: 15009/15469    Discriminator_loss: 0.3023434281349182  Generator_loss: 1.3492345809936523\n",
            "epoch: 9/10,    batch: 15010/15469    Discriminator_loss: 1.3161258697509766  Generator_loss: 1.3542351722717285\n",
            "epoch: 9/10,    batch: 15011/15469    Discriminator_loss: 1.7227606773376465  Generator_loss: 1.355589509010315\n",
            "epoch: 9/10,    batch: 15012/15469    Discriminator_loss: 1.7273647785186768  Generator_loss: 1.353901982307434\n",
            "epoch: 9/10,    batch: 15013/15469    Discriminator_loss: 1.7252371311187744  Generator_loss: 1.3498570919036865\n",
            "epoch: 9/10,    batch: 15014/15469    Discriminator_loss: 1.717748761177063  Generator_loss: 1.3439311981201172\n",
            "epoch: 9/10,    batch: 15015/15469    Discriminator_loss: 1.7108845710754395  Generator_loss: 1.3366649150848389\n",
            "epoch: 9/10,    batch: 15016/15469    Discriminator_loss: 1.7041910886764526  Generator_loss: 1.3283963203430176\n",
            "epoch: 9/10,    batch: 15017/15469    Discriminator_loss: 1.6966041326522827  Generator_loss: 1.3193788528442383\n",
            "epoch: 9/10,    batch: 15018/15469    Discriminator_loss: 1.6931748390197754  Generator_loss: 1.309887409210205\n",
            "epoch: 9/10,    batch: 15019/15469    Discriminator_loss: 1.686537742614746  Generator_loss: 1.300100326538086\n",
            "epoch: 9/10,    batch: 15020/15469    Discriminator_loss: 1.6788851022720337  Generator_loss: 1.2902295589447021\n",
            "epoch: 9/10,    batch: 15021/15469    Discriminator_loss: 1.6729152202606201  Generator_loss: 1.2803634405136108\n",
            "epoch: 9/10,    batch: 15022/15469    Discriminator_loss: 1.6582859754562378  Generator_loss: 1.270437479019165\n",
            "epoch: 9/10,    batch: 15023/15469    Discriminator_loss: 1.6580171585083008  Generator_loss: 1.2605409622192383\n",
            "epoch: 9/10,    batch: 15024/15469    Discriminator_loss: 1.6446270942687988  Generator_loss: 1.2507867813110352\n",
            "epoch: 9/10,    batch: 15025/15469    Discriminator_loss: 1.6365493535995483  Generator_loss: 1.241153597831726\n",
            "epoch: 9/10,    batch: 15026/15469    Discriminator_loss: 1.6281085014343262  Generator_loss: 1.2316237688064575\n",
            "epoch: 9/10,    batch: 15027/15469    Discriminator_loss: 1.6223870515823364  Generator_loss: 1.222277283668518\n",
            "epoch: 9/10,    batch: 15028/15469    Discriminator_loss: 1.616037130355835  Generator_loss: 1.213066577911377\n",
            "epoch: 9/10,    batch: 15029/15469    Discriminator_loss: 1.610006332397461  Generator_loss: 1.2041079998016357\n",
            "epoch: 9/10,    batch: 15030/15469    Discriminator_loss: 0.474068284034729  Generator_loss: 1.198211431503296\n",
            "epoch: 9/10,    batch: 15031/15469    Discriminator_loss: 0.36105287075042725  Generator_loss: 1.195101261138916\n",
            "epoch: 9/10,    batch: 15032/15469    Discriminator_loss: 0.3612942397594452  Generator_loss: 1.1941967010498047\n",
            "epoch: 9/10,    batch: 15033/15469    Discriminator_loss: 0.3612290024757385  Generator_loss: 1.1951549053192139\n",
            "epoch: 9/10,    batch: 15034/15469    Discriminator_loss: 0.360453724861145  Generator_loss: 1.197585105895996\n",
            "epoch: 9/10,    batch: 15035/15469    Discriminator_loss: 1.2218700647354126  Generator_loss: 1.1988885402679443\n",
            "epoch: 9/10,    batch: 15036/15469    Discriminator_loss: 1.6080381870269775  Generator_loss: 1.1983587741851807\n",
            "epoch: 9/10,    batch: 15037/15469    Discriminator_loss: 1.6071276664733887  Generator_loss: 1.1962082386016846\n",
            "epoch: 9/10,    batch: 15038/15469    Discriminator_loss: 1.5754952430725098  Generator_loss: 1.193000078201294\n",
            "epoch: 9/10,    batch: 15039/15469    Discriminator_loss: 1.5524119138717651  Generator_loss: 1.1887787580490112\n",
            "epoch: 9/10,    batch: 15040/15469    Discriminator_loss: 1.540053129196167  Generator_loss: 1.183766484260559\n",
            "epoch: 9/10,    batch: 15041/15469    Discriminator_loss: 1.5458321571350098  Generator_loss: 1.1782760620117188\n",
            "epoch: 9/10,    batch: 15042/15469    Discriminator_loss: 1.5263837575912476  Generator_loss: 1.1721572875976562\n",
            "epoch: 9/10,    batch: 15043/15469    Discriminator_loss: 1.521441102027893  Generator_loss: 1.16567862033844\n",
            "epoch: 9/10,    batch: 15044/15469    Discriminator_loss: 1.4570746421813965  Generator_loss: 1.158531665802002\n",
            "epoch: 9/10,    batch: 15045/15469    Discriminator_loss: 1.521956205368042  Generator_loss: 1.15132737159729\n",
            "epoch: 9/10,    batch: 15046/15469    Discriminator_loss: 1.507867693901062  Generator_loss: 1.1440614461898804\n",
            "epoch: 9/10,    batch: 15047/15469    Discriminator_loss: 1.4916741847991943  Generator_loss: 1.1367524862289429\n",
            "epoch: 9/10,    batch: 15048/15469    Discriminator_loss: 1.4699128866195679  Generator_loss: 1.1291743516921997\n",
            "epoch: 9/10,    batch: 15049/15469    Discriminator_loss: 1.4436218738555908  Generator_loss: 1.1214622259140015\n",
            "epoch: 9/10,    batch: 15050/15469    Discriminator_loss: 1.4273542165756226  Generator_loss: 1.113324522972107\n",
            "epoch: 9/10,    batch: 15051/15469    Discriminator_loss: 1.4476604461669922  Generator_loss: 1.1050798892974854\n",
            "epoch: 9/10,    batch: 15052/15469    Discriminator_loss: 1.2073009014129639  Generator_loss: 1.0951491594314575\n",
            "epoch: 9/10,    batch: 15053/15469    Discriminator_loss: 0.6739488840103149  Generator_loss: 1.0864429473876953\n",
            "epoch: 9/10,    batch: 15054/15469    Discriminator_loss: 1.0945268869400024  Generator_loss: 1.0770283937454224\n",
            "epoch: 9/10,    batch: 15055/15469    Discriminator_loss: 1.109543800354004  Generator_loss: 1.063459038734436\n",
            "epoch: 9/10,    batch: 15056/15469    Discriminator_loss: 0.6844384670257568  Generator_loss: 1.0460376739501953\n",
            "epoch: 9/10,    batch: 15057/15469    Discriminator_loss: 0.4450918734073639  Generator_loss: 1.0286253690719604\n",
            "epoch: 9/10,    batch: 15058/15469    Discriminator_loss: 0.44953179359436035  Generator_loss: 1.0112357139587402\n",
            "epoch: 9/10,    batch: 15059/15469    Discriminator_loss: 0.4603494703769684  Generator_loss: 0.9958668947219849\n",
            "epoch: 9/10,    batch: 15060/15469    Discriminator_loss: 0.47039568424224854  Generator_loss: 0.9821248054504395\n",
            "epoch: 9/10,    batch: 15061/15469    Discriminator_loss: 0.4794395864009857  Generator_loss: 0.9724102020263672\n",
            "epoch: 9/10,    batch: 15062/15469    Discriminator_loss: 0.4814142882823944  Generator_loss: 0.9756368398666382\n",
            "epoch: 9/10,    batch: 15063/15469    Discriminator_loss: 0.46433204412460327  Generator_loss: 1.0273844003677368\n",
            "epoch: 9/10,    batch: 15064/15469    Discriminator_loss: 0.44790542125701904  Generator_loss: 1.0740227699279785\n",
            "epoch: 9/10,    batch: 15065/15469    Discriminator_loss: 0.4407755434513092  Generator_loss: 1.0937793254852295\n",
            "epoch: 9/10,    batch: 15066/15469    Discriminator_loss: 0.437881201505661  Generator_loss: 1.100986123085022\n",
            "epoch: 9/10,    batch: 15067/15469    Discriminator_loss: 0.4377741813659668  Generator_loss: 1.0897771120071411\n",
            "epoch: 9/10,    batch: 15068/15469    Discriminator_loss: 0.4327530860900879  Generator_loss: 1.0730156898498535\n",
            "epoch: 9/10,    batch: 15069/15469    Discriminator_loss: 0.4201659560203552  Generator_loss: 1.0843291282653809\n",
            "epoch: 9/10,    batch: 15070/15469    Discriminator_loss: 0.4070984423160553  Generator_loss: 1.107046127319336\n",
            "epoch: 9/10,    batch: 15071/15469    Discriminator_loss: 0.3956213891506195  Generator_loss: 1.1292072534561157\n",
            "epoch: 9/10,    batch: 15072/15469    Discriminator_loss: 0.38528507947921753  Generator_loss: 1.1503024101257324\n",
            "epoch: 9/10,    batch: 15073/15469    Discriminator_loss: 0.37560778856277466  Generator_loss: 1.1709423065185547\n",
            "epoch: 9/10,    batch: 15074/15469    Discriminator_loss: 0.36617040634155273  Generator_loss: 1.1919429302215576\n",
            "epoch: 9/10,    batch: 15075/15469    Discriminator_loss: 0.35697484016418457  Generator_loss: 1.2132244110107422\n",
            "epoch: 9/10,    batch: 15076/15469    Discriminator_loss: 0.3479865789413452  Generator_loss: 1.234693169593811\n",
            "epoch: 9/10,    batch: 15077/15469    Discriminator_loss: 0.33936670422554016  Generator_loss: 1.2559478282928467\n",
            "epoch: 9/10,    batch: 15078/15469    Discriminator_loss: 0.33132973313331604  Generator_loss: 1.27712082862854\n",
            "epoch: 9/10,    batch: 15079/15469    Discriminator_loss: 0.3235495388507843  Generator_loss: 1.2973508834838867\n",
            "epoch: 9/10,    batch: 15080/15469    Discriminator_loss: 0.316617488861084  Generator_loss: 1.3158762454986572\n",
            "epoch: 9/10,    batch: 15081/15469    Discriminator_loss: 0.3104664385318756  Generator_loss: 1.333061695098877\n",
            "epoch: 9/10,    batch: 15082/15469    Discriminator_loss: 0.3048195242881775  Generator_loss: 1.3487675189971924\n",
            "epoch: 9/10,    batch: 15083/15469    Discriminator_loss: 0.29999086260795593  Generator_loss: 1.3627331256866455\n",
            "epoch: 9/10,    batch: 15084/15469    Discriminator_loss: 0.2957790791988373  Generator_loss: 1.3748446702957153\n",
            "epoch: 9/10,    batch: 15085/15469    Discriminator_loss: 0.2920704782009125  Generator_loss: 1.3855228424072266\n",
            "epoch: 9/10,    batch: 15086/15469    Discriminator_loss: 0.2888728380203247  Generator_loss: 1.3950693607330322\n",
            "epoch: 9/10,    batch: 15087/15469    Discriminator_loss: 0.2860400378704071  Generator_loss: 1.4036455154418945\n",
            "epoch: 9/10,    batch: 15088/15469    Discriminator_loss: 0.2832496762275696  Generator_loss: 1.411339521408081\n",
            "epoch: 9/10,    batch: 15089/15469    Discriminator_loss: 0.2804953455924988  Generator_loss: 1.419290542602539\n",
            "epoch: 9/10,    batch: 15090/15469    Discriminator_loss: 0.27794262766838074  Generator_loss: 1.4268200397491455\n",
            "epoch: 9/10,    batch: 15091/15469    Discriminator_loss: 0.27537569403648376  Generator_loss: 1.4346867799758911\n",
            "epoch: 9/10,    batch: 15092/15469    Discriminator_loss: 0.27276232838630676  Generator_loss: 1.4425688982009888\n",
            "epoch: 9/10,    batch: 15093/15469    Discriminator_loss: 0.26994526386260986  Generator_loss: 1.4513347148895264\n",
            "epoch: 9/10,    batch: 15094/15469    Discriminator_loss: 0.2671116292476654  Generator_loss: 1.4604575634002686\n",
            "epoch: 9/10,    batch: 15095/15469    Discriminator_loss: 0.26407212018966675  Generator_loss: 1.4701423645019531\n",
            "epoch: 9/10,    batch: 15096/15469    Discriminator_loss: 0.26093828678131104  Generator_loss: 1.480480670928955\n",
            "epoch: 9/10,    batch: 15097/15469    Discriminator_loss: 0.25772228837013245  Generator_loss: 1.4911143779754639\n",
            "epoch: 9/10,    batch: 15098/15469    Discriminator_loss: 0.2544481158256531  Generator_loss: 1.50238835811615\n",
            "epoch: 9/10,    batch: 15099/15469    Discriminator_loss: 0.2510688900947571  Generator_loss: 1.5138540267944336\n",
            "epoch: 9/10,    batch: 15100/15469    Discriminator_loss: 0.24773964285850525  Generator_loss: 1.5256367921829224\n",
            "epoch: 9/10,    batch: 15101/15469    Discriminator_loss: 0.24443385004997253  Generator_loss: 1.5375547409057617\n",
            "epoch: 9/10,    batch: 15102/15469    Discriminator_loss: 0.24117808043956757  Generator_loss: 1.5494133234024048\n",
            "epoch: 9/10,    batch: 15103/15469    Discriminator_loss: 0.23796088993549347  Generator_loss: 1.5612943172454834\n",
            "epoch: 9/10,    batch: 15104/15469    Discriminator_loss: 0.23488134145736694  Generator_loss: 1.5728931427001953\n",
            "epoch: 9/10,    batch: 15105/15469    Discriminator_loss: 0.23189163208007812  Generator_loss: 1.584373950958252\n",
            "epoch: 9/10,    batch: 15106/15469    Discriminator_loss: 0.2289687842130661  Generator_loss: 1.5955908298492432\n",
            "epoch: 9/10,    batch: 15107/15469    Discriminator_loss: 0.22619909048080444  Generator_loss: 1.6065278053283691\n",
            "epoch: 9/10,    batch: 15108/15469    Discriminator_loss: 0.22353346645832062  Generator_loss: 1.6171839237213135\n",
            "epoch: 9/10,    batch: 15109/15469    Discriminator_loss: 0.2209319770336151  Generator_loss: 1.6275522708892822\n",
            "epoch: 9/10,    batch: 15110/15469    Discriminator_loss: 0.21843796968460083  Generator_loss: 1.6377358436584473\n",
            "epoch: 9/10,    batch: 15111/15469    Discriminator_loss: 0.2160429209470749  Generator_loss: 1.6476435661315918\n",
            "epoch: 9/10,    batch: 15112/15469    Discriminator_loss: 0.21368494629859924  Generator_loss: 1.6574026346206665\n",
            "epoch: 9/10,    batch: 15113/15469    Discriminator_loss: 0.21142607927322388  Generator_loss: 1.666940689086914\n",
            "epoch: 9/10,    batch: 15114/15469    Discriminator_loss: 0.20921295881271362  Generator_loss: 1.676387906074524\n",
            "epoch: 9/10,    batch: 15115/15469    Discriminator_loss: 0.20703192055225372  Generator_loss: 1.6857619285583496\n",
            "epoch: 9/10,    batch: 15116/15469    Discriminator_loss: 0.2049097716808319  Generator_loss: 1.6950533390045166\n",
            "epoch: 9/10,    batch: 15117/15469    Discriminator_loss: 0.20280064642429352  Generator_loss: 1.7043148279190063\n",
            "epoch: 9/10,    batch: 15118/15469    Discriminator_loss: 0.20073719322681427  Generator_loss: 1.713423252105713\n",
            "epoch: 9/10,    batch: 15119/15469    Discriminator_loss: 0.19870059192180634  Generator_loss: 1.7226710319519043\n",
            "epoch: 9/10,    batch: 15120/15469    Discriminator_loss: 0.19666893780231476  Generator_loss: 1.7317585945129395\n",
            "epoch: 9/10,    batch: 15121/15469    Discriminator_loss: 0.19469091296195984  Generator_loss: 1.740983009338379\n",
            "epoch: 9/10,    batch: 15122/15469    Discriminator_loss: 0.19272910058498383  Generator_loss: 1.7500946521759033\n",
            "epoch: 9/10,    batch: 15123/15469    Discriminator_loss: 0.1907828450202942  Generator_loss: 1.759202003479004\n",
            "epoch: 9/10,    batch: 15124/15469    Discriminator_loss: 0.18889832496643066  Generator_loss: 1.7684016227722168\n",
            "epoch: 9/10,    batch: 15125/15469    Discriminator_loss: 0.18701723217964172  Generator_loss: 1.777418851852417\n",
            "epoch: 9/10,    batch: 15126/15469    Discriminator_loss: 0.18515938520431519  Generator_loss: 1.7865179777145386\n",
            "epoch: 9/10,    batch: 15127/15469    Discriminator_loss: 0.18335101008415222  Generator_loss: 1.7954761981964111\n",
            "epoch: 9/10,    batch: 15128/15469    Discriminator_loss: 0.18154430389404297  Generator_loss: 1.8043994903564453\n",
            "epoch: 9/10,    batch: 15129/15469    Discriminator_loss: 0.17978040874004364  Generator_loss: 1.813318133354187\n",
            "epoch: 9/10,    batch: 15130/15469    Discriminator_loss: 0.1780579686164856  Generator_loss: 1.822108507156372\n",
            "epoch: 9/10,    batch: 15131/15469    Discriminator_loss: 0.1763533651828766  Generator_loss: 1.830773115158081\n",
            "epoch: 9/10,    batch: 15132/15469    Discriminator_loss: 0.17468921840190887  Generator_loss: 1.8394687175750732\n",
            "epoch: 9/10,    batch: 15133/15469    Discriminator_loss: 0.17306706309318542  Generator_loss: 1.8480767011642456\n",
            "epoch: 9/10,    batch: 15134/15469    Discriminator_loss: 0.17146648466587067  Generator_loss: 1.8565614223480225\n",
            "epoch: 9/10,    batch: 15135/15469    Discriminator_loss: 0.16989514231681824  Generator_loss: 1.8649258613586426\n",
            "epoch: 9/10,    batch: 15136/15469    Discriminator_loss: 0.16835343837738037  Generator_loss: 1.8732078075408936\n",
            "epoch: 9/10,    batch: 15137/15469    Discriminator_loss: 0.16684822738170624  Generator_loss: 1.8814406394958496\n",
            "epoch: 9/10,    batch: 15138/15469    Discriminator_loss: 0.16538110375404358  Generator_loss: 1.8896280527114868\n",
            "epoch: 9/10,    batch: 15139/15469    Discriminator_loss: 0.1639217585325241  Generator_loss: 1.8975828886032104\n",
            "epoch: 9/10,    batch: 15140/15469    Discriminator_loss: 0.16251666843891144  Generator_loss: 1.9055566787719727\n",
            "epoch: 9/10,    batch: 15141/15469    Discriminator_loss: 0.1611197292804718  Generator_loss: 1.9134395122528076\n",
            "epoch: 9/10,    batch: 15142/15469    Discriminator_loss: 0.15976521372795105  Generator_loss: 1.9212217330932617\n",
            "epoch: 9/10,    batch: 15143/15469    Discriminator_loss: 0.15843702852725983  Generator_loss: 1.928917407989502\n",
            "epoch: 9/10,    batch: 15144/15469    Discriminator_loss: 0.15712933242321014  Generator_loss: 1.9365510940551758\n",
            "epoch: 9/10,    batch: 15145/15469    Discriminator_loss: 1.6503396034240723  Generator_loss: 1.9329618215560913\n",
            "epoch: 9/10,    batch: 15146/15469    Discriminator_loss: 2.0688047409057617  Generator_loss: 1.9190202951431274\n",
            "epoch: 9/10,    batch: 15147/15469    Discriminator_loss: 2.0918593406677246  Generator_loss: 1.8983869552612305\n",
            "epoch: 9/10,    batch: 15148/15469    Discriminator_loss: 2.0719892978668213  Generator_loss: 1.8737907409667969\n",
            "epoch: 9/10,    batch: 15149/15469    Discriminator_loss: 1.997180461883545  Generator_loss: 1.8464808464050293\n",
            "epoch: 9/10,    batch: 15150/15469    Discriminator_loss: 1.9915202856063843  Generator_loss: 1.8178099393844604\n",
            "epoch: 9/10,    batch: 15151/15469    Discriminator_loss: 1.973785400390625  Generator_loss: 1.7881662845611572\n",
            "epoch: 9/10,    batch: 15152/15469    Discriminator_loss: 1.941545009613037  Generator_loss: 1.7577283382415771\n",
            "epoch: 9/10,    batch: 15153/15469    Discriminator_loss: 1.8965933322906494  Generator_loss: 1.7271164655685425\n",
            "epoch: 9/10,    batch: 15154/15469    Discriminator_loss: 1.8711717128753662  Generator_loss: 1.69661545753479\n",
            "epoch: 9/10,    batch: 15155/15469    Discriminator_loss: 1.8593095541000366  Generator_loss: 1.6661466360092163\n",
            "epoch: 9/10,    batch: 15156/15469    Discriminator_loss: 1.8582135438919067  Generator_loss: 1.6361547708511353\n",
            "epoch: 9/10,    batch: 15157/15469    Discriminator_loss: 1.8115358352661133  Generator_loss: 1.6069613695144653\n",
            "epoch: 9/10,    batch: 15158/15469    Discriminator_loss: 1.793652057647705  Generator_loss: 1.578636884689331\n",
            "epoch: 9/10,    batch: 15159/15469    Discriminator_loss: 1.7890093326568604  Generator_loss: 1.5512452125549316\n",
            "epoch: 9/10,    batch: 15160/15469    Discriminator_loss: 1.7820613384246826  Generator_loss: 1.5249089002609253\n",
            "epoch: 9/10,    batch: 15161/15469    Discriminator_loss: 1.7729249000549316  Generator_loss: 1.4996542930603027\n",
            "epoch: 9/10,    batch: 15162/15469    Discriminator_loss: 1.7804162502288818  Generator_loss: 1.4755618572235107\n",
            "epoch: 9/10,    batch: 15163/15469    Discriminator_loss: 1.7576309442520142  Generator_loss: 1.4525327682495117\n",
            "epoch: 9/10,    batch: 15164/15469    Discriminator_loss: 1.7398840188980103  Generator_loss: 1.430592656135559\n",
            "epoch: 9/10,    batch: 15165/15469    Discriminator_loss: 0.414295494556427  Generator_loss: 1.4139630794525146\n",
            "epoch: 9/10,    batch: 15166/15469    Discriminator_loss: 0.2824765145778656  Generator_loss: 1.4021539688110352\n",
            "epoch: 9/10,    batch: 15167/15469    Discriminator_loss: 0.28453466296195984  Generator_loss: 1.3940205574035645\n",
            "epoch: 9/10,    batch: 15168/15469    Discriminator_loss: 0.28665032982826233  Generator_loss: 1.388859748840332\n",
            "epoch: 9/10,    batch: 15169/15469    Discriminator_loss: 0.2878682017326355  Generator_loss: 1.386099100112915\n",
            "epoch: 9/10,    batch: 15170/15469    Discriminator_loss: 1.1613402366638184  Generator_loss: 1.3786836862564087\n",
            "epoch: 9/10,    batch: 15171/15469    Discriminator_loss: 1.2575767040252686  Generator_loss: 1.3685039281845093\n",
            "epoch: 9/10,    batch: 15172/15469    Discriminator_loss: 1.241266131401062  Generator_loss: 1.3568339347839355\n",
            "epoch: 9/10,    batch: 15173/15469    Discriminator_loss: 0.33263733983039856  Generator_loss: 1.34999418258667\n",
            "epoch: 9/10,    batch: 15174/15469    Discriminator_loss: 0.30111607909202576  Generator_loss: 1.3481764793395996\n",
            "epoch: 9/10,    batch: 15175/15469    Discriminator_loss: 0.3008931577205658  Generator_loss: 1.350760579109192\n",
            "epoch: 9/10,    batch: 15176/15469    Discriminator_loss: 0.2993742823600769  Generator_loss: 1.356987714767456\n",
            "epoch: 9/10,    batch: 15177/15469    Discriminator_loss: 0.29687872529029846  Generator_loss: 1.3658726215362549\n",
            "epoch: 9/10,    batch: 15178/15469    Discriminator_loss: 0.29381293058395386  Generator_loss: 1.3757493495941162\n",
            "epoch: 9/10,    batch: 15179/15469    Discriminator_loss: 0.2907199263572693  Generator_loss: 1.3855886459350586\n",
            "epoch: 9/10,    batch: 15180/15469    Discriminator_loss: 0.2881450057029724  Generator_loss: 1.3938400745391846\n",
            "epoch: 9/10,    batch: 15181/15469    Discriminator_loss: 0.2861361801624298  Generator_loss: 1.3996968269348145\n",
            "epoch: 9/10,    batch: 15182/15469    Discriminator_loss: 0.28489091992378235  Generator_loss: 1.4033734798431396\n",
            "epoch: 9/10,    batch: 15183/15469    Discriminator_loss: 0.2841663360595703  Generator_loss: 1.4053750038146973\n",
            "epoch: 9/10,    batch: 15184/15469    Discriminator_loss: 0.2838538587093353  Generator_loss: 1.406116008758545\n",
            "epoch: 9/10,    batch: 15185/15469    Discriminator_loss: 0.2835957109928131  Generator_loss: 1.4066531658172607\n",
            "epoch: 9/10,    batch: 15186/15469    Discriminator_loss: 0.2832435369491577  Generator_loss: 1.4072062969207764\n",
            "epoch: 9/10,    batch: 15187/15469    Discriminator_loss: 0.2826896607875824  Generator_loss: 1.4084465503692627\n",
            "epoch: 9/10,    batch: 15188/15469    Discriminator_loss: 0.2818201780319214  Generator_loss: 1.410933494567871\n",
            "epoch: 9/10,    batch: 15189/15469    Discriminator_loss: 0.28050148487091064  Generator_loss: 1.4147143363952637\n",
            "epoch: 9/10,    batch: 15190/15469    Discriminator_loss: 0.27882757782936096  Generator_loss: 1.4197876453399658\n",
            "epoch: 9/10,    batch: 15191/15469    Discriminator_loss: 0.27679556608200073  Generator_loss: 1.4261043071746826\n",
            "epoch: 9/10,    batch: 15192/15469    Discriminator_loss: 0.2744586765766144  Generator_loss: 1.4334182739257812\n",
            "epoch: 9/10,    batch: 15193/15469    Discriminator_loss: 0.2719166576862335  Generator_loss: 1.4415137767791748\n",
            "epoch: 9/10,    batch: 15194/15469    Discriminator_loss: 0.26924166083335876  Generator_loss: 1.4501781463623047\n",
            "epoch: 9/10,    batch: 15195/15469    Discriminator_loss: 0.2664864659309387  Generator_loss: 1.459211826324463\n",
            "epoch: 9/10,    batch: 15196/15469    Discriminator_loss: 0.2637433111667633  Generator_loss: 1.4684462547302246\n",
            "epoch: 9/10,    batch: 15197/15469    Discriminator_loss: 0.26096293330192566  Generator_loss: 1.4776506423950195\n",
            "epoch: 9/10,    batch: 15198/15469    Discriminator_loss: 0.25831082463264465  Generator_loss: 1.4867520332336426\n",
            "epoch: 9/10,    batch: 15199/15469    Discriminator_loss: 0.25570082664489746  Generator_loss: 1.4956612586975098\n",
            "epoch: 9/10,    batch: 15200/15469    Discriminator_loss: 0.25310248136520386  Generator_loss: 1.5046067237854004\n",
            "epoch: 9/10,    batch: 15201/15469    Discriminator_loss: 0.2505992650985718  Generator_loss: 1.5132055282592773\n",
            "epoch: 9/10,    batch: 15202/15469    Discriminator_loss: 0.24822431802749634  Generator_loss: 1.521785020828247\n",
            "epoch: 9/10,    batch: 15203/15469    Discriminator_loss: 0.2458554208278656  Generator_loss: 1.530198574066162\n",
            "epoch: 9/10,    batch: 15204/15469    Discriminator_loss: 0.24358050525188446  Generator_loss: 1.538392424583435\n",
            "epoch: 9/10,    batch: 15205/15469    Discriminator_loss: 0.24129104614257812  Generator_loss: 1.5466666221618652\n",
            "epoch: 9/10,    batch: 15206/15469    Discriminator_loss: 0.2390884906053543  Generator_loss: 1.5548384189605713\n",
            "epoch: 9/10,    batch: 15207/15469    Discriminator_loss: 0.2368556261062622  Generator_loss: 1.5630290508270264\n",
            "epoch: 9/10,    batch: 15208/15469    Discriminator_loss: 0.23467236757278442  Generator_loss: 1.5712685585021973\n",
            "epoch: 9/10,    batch: 15209/15469    Discriminator_loss: 0.23246891796588898  Generator_loss: 1.5794947147369385\n",
            "epoch: 9/10,    batch: 15210/15469    Discriminator_loss: 0.23031124472618103  Generator_loss: 1.5878164768218994\n",
            "epoch: 9/10,    batch: 15211/15469    Discriminator_loss: 0.22813443839550018  Generator_loss: 1.5962072610855103\n",
            "epoch: 9/10,    batch: 15212/15469    Discriminator_loss: 0.2259778380393982  Generator_loss: 1.6046143770217896\n",
            "epoch: 9/10,    batch: 15213/15469    Discriminator_loss: 0.22382470965385437  Generator_loss: 1.6131246089935303\n",
            "epoch: 9/10,    batch: 15214/15469    Discriminator_loss: 0.22169223427772522  Generator_loss: 1.6216762065887451\n",
            "epoch: 9/10,    batch: 15215/15469    Discriminator_loss: 0.21955518424510956  Generator_loss: 1.6302807331085205\n",
            "epoch: 9/10,    batch: 15216/15469    Discriminator_loss: 0.21742695569992065  Generator_loss: 1.63893723487854\n",
            "epoch: 9/10,    batch: 15217/15469    Discriminator_loss: 0.2153264880180359  Generator_loss: 1.6475567817687988\n",
            "epoch: 9/10,    batch: 15218/15469    Discriminator_loss: 0.21324029564857483  Generator_loss: 1.6562474966049194\n",
            "epoch: 9/10,    batch: 15219/15469    Discriminator_loss: 0.21118022501468658  Generator_loss: 1.664959192276001\n",
            "epoch: 9/10,    batch: 15220/15469    Discriminator_loss: 0.2091311514377594  Generator_loss: 1.6736292839050293\n",
            "epoch: 9/10,    batch: 15221/15469    Discriminator_loss: 0.2071240246295929  Generator_loss: 1.6823580265045166\n",
            "epoch: 9/10,    batch: 15222/15469    Discriminator_loss: 0.20510603487491608  Generator_loss: 1.6910319328308105\n",
            "epoch: 9/10,    batch: 15223/15469    Discriminator_loss: 0.2031468152999878  Generator_loss: 1.699697732925415\n",
            "epoch: 9/10,    batch: 15224/15469    Discriminator_loss: 0.20119541883468628  Generator_loss: 1.708322286605835\n",
            "epoch: 9/10,    batch: 15225/15469    Discriminator_loss: 0.19929274916648865  Generator_loss: 1.7169787883758545\n",
            "epoch: 9/10,    batch: 15226/15469    Discriminator_loss: 0.1974099576473236  Generator_loss: 1.7255909442901611\n",
            "epoch: 9/10,    batch: 15227/15469    Discriminator_loss: 0.1955450028181076  Generator_loss: 1.7341337203979492\n",
            "epoch: 9/10,    batch: 15228/15469    Discriminator_loss: 0.19371162354946136  Generator_loss: 1.7426154613494873\n",
            "epoch: 9/10,    batch: 15229/15469    Discriminator_loss: 0.19190099835395813  Generator_loss: 1.7511494159698486\n",
            "epoch: 9/10,    batch: 15230/15469    Discriminator_loss: 0.1901232749223709  Generator_loss: 1.7595915794372559\n",
            "epoch: 9/10,    batch: 15231/15469    Discriminator_loss: 0.18836531043052673  Generator_loss: 1.7680308818817139\n",
            "epoch: 9/10,    batch: 15232/15469    Discriminator_loss: 0.18663141131401062  Generator_loss: 1.7764041423797607\n",
            "epoch: 9/10,    batch: 15233/15469    Discriminator_loss: 0.18491749465465546  Generator_loss: 1.7848122119903564\n",
            "epoch: 9/10,    batch: 15234/15469    Discriminator_loss: 0.18322530388832092  Generator_loss: 1.793192744255066\n",
            "epoch: 9/10,    batch: 15235/15469    Discriminator_loss: 0.18156135082244873  Generator_loss: 1.801530361175537\n",
            "epoch: 9/10,    batch: 15236/15469    Discriminator_loss: 0.17990946769714355  Generator_loss: 1.8098169565200806\n",
            "epoch: 9/10,    batch: 15237/15469    Discriminator_loss: 0.17828358709812164  Generator_loss: 1.81815505027771\n",
            "epoch: 9/10,    batch: 15238/15469    Discriminator_loss: 0.17666664719581604  Generator_loss: 1.8264415264129639\n",
            "epoch: 9/10,    batch: 15239/15469    Discriminator_loss: 0.1750722974538803  Generator_loss: 1.83473801612854\n",
            "epoch: 9/10,    batch: 15240/15469    Discriminator_loss: 0.17350822687149048  Generator_loss: 1.8430100679397583\n",
            "epoch: 9/10,    batch: 15241/15469    Discriminator_loss: 0.17193980515003204  Generator_loss: 1.851293683052063\n",
            "epoch: 9/10,    batch: 15242/15469    Discriminator_loss: 0.17040114104747772  Generator_loss: 1.859510898590088\n",
            "epoch: 9/10,    batch: 15243/15469    Discriminator_loss: 0.16887731850147247  Generator_loss: 1.8677409887313843\n",
            "epoch: 9/10,    batch: 15244/15469    Discriminator_loss: 0.1673789620399475  Generator_loss: 1.875947117805481\n",
            "epoch: 9/10,    batch: 15245/15469    Discriminator_loss: 0.16588757932186127  Generator_loss: 1.884134292602539\n",
            "epoch: 9/10,    batch: 15246/15469    Discriminator_loss: 0.16442205011844635  Generator_loss: 1.892384648323059\n",
            "epoch: 9/10,    batch: 15247/15469    Discriminator_loss: 0.16297271847724915  Generator_loss: 1.9005162715911865\n",
            "epoch: 9/10,    batch: 15248/15469    Discriminator_loss: 0.16153694689273834  Generator_loss: 1.908685564994812\n",
            "epoch: 9/10,    batch: 15249/15469    Discriminator_loss: 0.16012339293956757  Generator_loss: 1.9167903661727905\n",
            "epoch: 9/10,    batch: 15250/15469    Discriminator_loss: 0.15871793031692505  Generator_loss: 1.924851417541504\n",
            "epoch: 9/10,    batch: 15251/15469    Discriminator_loss: 0.15733861923217773  Generator_loss: 1.9329807758331299\n",
            "epoch: 9/10,    batch: 15252/15469    Discriminator_loss: 0.1559731811285019  Generator_loss: 1.940971851348877\n",
            "epoch: 9/10,    batch: 15253/15469    Discriminator_loss: 0.1546250730752945  Generator_loss: 1.9490759372711182\n",
            "epoch: 9/10,    batch: 15254/15469    Discriminator_loss: 0.1532934159040451  Generator_loss: 1.9570891857147217\n",
            "epoch: 9/10,    batch: 15255/15469    Discriminator_loss: 0.15198162198066711  Generator_loss: 1.9649977684020996\n",
            "epoch: 9/10,    batch: 15256/15469    Discriminator_loss: 0.15067879855632782  Generator_loss: 1.9729549884796143\n",
            "epoch: 9/10,    batch: 15257/15469    Discriminator_loss: 0.1494055688381195  Generator_loss: 1.9808542728424072\n",
            "epoch: 9/10,    batch: 15258/15469    Discriminator_loss: 0.14814142882823944  Generator_loss: 1.9887641668319702\n",
            "epoch: 9/10,    batch: 15259/15469    Discriminator_loss: 0.14688700437545776  Generator_loss: 1.9966044425964355\n",
            "epoch: 9/10,    batch: 15260/15469    Discriminator_loss: 0.14566347002983093  Generator_loss: 2.0043885707855225\n",
            "epoch: 9/10,    batch: 15261/15469    Discriminator_loss: 0.14444130659103394  Generator_loss: 2.012192964553833\n",
            "epoch: 9/10,    batch: 15262/15469    Discriminator_loss: 0.14323867857456207  Generator_loss: 2.019998073577881\n",
            "epoch: 9/10,    batch: 15263/15469    Discriminator_loss: 0.14205320179462433  Generator_loss: 2.02775502204895\n",
            "epoch: 9/10,    batch: 15264/15469    Discriminator_loss: 0.14088641107082367  Generator_loss: 2.035486936569214\n",
            "epoch: 9/10,    batch: 15265/15469    Discriminator_loss: 0.1397176831960678  Generator_loss: 2.043124198913574\n",
            "epoch: 9/10,    batch: 15266/15469    Discriminator_loss: 0.13857734203338623  Generator_loss: 2.050785779953003\n",
            "epoch: 9/10,    batch: 15267/15469    Discriminator_loss: 0.13744518160820007  Generator_loss: 2.0584640502929688\n",
            "epoch: 9/10,    batch: 15268/15469    Discriminator_loss: 0.13632681965827942  Generator_loss: 2.0660548210144043\n",
            "epoch: 9/10,    batch: 15269/15469    Discriminator_loss: 0.13523545861244202  Generator_loss: 2.0736405849456787\n",
            "epoch: 9/10,    batch: 15270/15469    Discriminator_loss: 0.13414272665977478  Generator_loss: 2.081221580505371\n",
            "epoch: 9/10,    batch: 15271/15469    Discriminator_loss: 0.13306181132793427  Generator_loss: 2.0887527465820312\n",
            "epoch: 9/10,    batch: 15272/15469    Discriminator_loss: 0.13199308514595032  Generator_loss: 2.096233606338501\n",
            "epoch: 9/10,    batch: 15273/15469    Discriminator_loss: 0.1309462934732437  Generator_loss: 2.103696823120117\n",
            "epoch: 9/10,    batch: 15274/15469    Discriminator_loss: 0.12991203367710114  Generator_loss: 2.111198663711548\n",
            "epoch: 9/10,    batch: 15275/15469    Discriminator_loss: 0.12888303399085999  Generator_loss: 2.118640422821045\n",
            "epoch: 9/10,    batch: 15276/15469    Discriminator_loss: 0.12786656618118286  Generator_loss: 2.126053810119629\n",
            "epoch: 9/10,    batch: 15277/15469    Discriminator_loss: 0.12687207758426666  Generator_loss: 2.1334354877471924\n",
            "epoch: 9/10,    batch: 15278/15469    Discriminator_loss: 0.26814234256744385  Generator_loss: 2.1398043632507324\n",
            "epoch: 9/10,    batch: 15279/15469    Discriminator_loss: 0.1358899176120758  Generator_loss: 2.1464571952819824\n",
            "epoch: 9/10,    batch: 15280/15469    Discriminator_loss: 0.13131506741046906  Generator_loss: 2.1532082557678223\n",
            "epoch: 9/10,    batch: 15281/15469    Discriminator_loss: 0.12830574810504913  Generator_loss: 2.1600453853607178\n",
            "epoch: 9/10,    batch: 15282/15469    Discriminator_loss: 0.1262202262878418  Generator_loss: 2.166891574859619\n",
            "epoch: 9/10,    batch: 15283/15469    Discriminator_loss: 0.12446136772632599  Generator_loss: 2.173877716064453\n",
            "epoch: 9/10,    batch: 15284/15469    Discriminator_loss: 0.12323054671287537  Generator_loss: 2.180844306945801\n",
            "epoch: 9/10,    batch: 15285/15469    Discriminator_loss: 0.12292361259460449  Generator_loss: 2.1878490447998047\n",
            "epoch: 9/10,    batch: 15286/15469    Discriminator_loss: 0.12073801457881927  Generator_loss: 2.194896697998047\n",
            "epoch: 9/10,    batch: 15287/15469    Discriminator_loss: 0.1195150762796402  Generator_loss: 2.201892614364624\n",
            "epoch: 9/10,    batch: 15288/15469    Discriminator_loss: 0.11839264631271362  Generator_loss: 2.2089781761169434\n",
            "epoch: 9/10,    batch: 15289/15469    Discriminator_loss: 0.11746890842914581  Generator_loss: 2.21604061126709\n",
            "epoch: 9/10,    batch: 15290/15469    Discriminator_loss: 0.11634581536054611  Generator_loss: 2.2230682373046875\n",
            "epoch: 9/10,    batch: 15291/15469    Discriminator_loss: 0.1153491735458374  Generator_loss: 2.2302193641662598\n",
            "epoch: 9/10,    batch: 15292/15469    Discriminator_loss: 0.11463982611894608  Generator_loss: 2.2371678352355957\n",
            "epoch: 9/10,    batch: 15293/15469    Discriminator_loss: 0.11369765549898148  Generator_loss: 2.2442569732666016\n",
            "epoch: 9/10,    batch: 15294/15469    Discriminator_loss: 0.11271978169679642  Generator_loss: 2.2512269020080566\n",
            "epoch: 9/10,    batch: 15295/15469    Discriminator_loss: 0.1116892620921135  Generator_loss: 2.2583017349243164\n",
            "epoch: 9/10,    batch: 15296/15469    Discriminator_loss: 0.11076584458351135  Generator_loss: 2.265244960784912\n",
            "epoch: 9/10,    batch: 15297/15469    Discriminator_loss: 0.10991118848323822  Generator_loss: 2.2721965312957764\n",
            "epoch: 9/10,    batch: 15298/15469    Discriminator_loss: 0.1089344397187233  Generator_loss: 2.279153347015381\n",
            "epoch: 9/10,    batch: 15299/15469    Discriminator_loss: 0.11621109396219254  Generator_loss: 2.286065101623535\n",
            "epoch: 9/10,    batch: 15300/15469    Discriminator_loss: 0.10726717114448547  Generator_loss: 2.2929296493530273\n",
            "epoch: 9/10,    batch: 15301/15469    Discriminator_loss: 0.1064230427145958  Generator_loss: 2.299771785736084\n",
            "epoch: 9/10,    batch: 15302/15469    Discriminator_loss: 0.10559991747140884  Generator_loss: 2.3066182136535645\n",
            "epoch: 9/10,    batch: 15303/15469    Discriminator_loss: 0.10488027334213257  Generator_loss: 2.3133890628814697\n",
            "epoch: 9/10,    batch: 15304/15469    Discriminator_loss: 0.10417072474956512  Generator_loss: 2.320202350616455\n",
            "epoch: 9/10,    batch: 15305/15469    Discriminator_loss: 0.1034398153424263  Generator_loss: 2.326960563659668\n",
            "epoch: 9/10,    batch: 15306/15469    Discriminator_loss: 0.10268750041723251  Generator_loss: 2.333658218383789\n",
            "epoch: 9/10,    batch: 15307/15469    Discriminator_loss: 0.10193758457899094  Generator_loss: 2.340315580368042\n",
            "epoch: 9/10,    batch: 15308/15469    Discriminator_loss: 0.10123042017221451  Generator_loss: 2.34696102142334\n",
            "epoch: 9/10,    batch: 15309/15469    Discriminator_loss: 0.1005316972732544  Generator_loss: 2.353641986846924\n",
            "epoch: 9/10,    batch: 15310/15469    Discriminator_loss: 0.09983665496110916  Generator_loss: 2.360259532928467\n",
            "epoch: 9/10,    batch: 15311/15469    Discriminator_loss: 0.0991552546620369  Generator_loss: 2.366729974746704\n",
            "epoch: 9/10,    batch: 15312/15469    Discriminator_loss: 0.0984836220741272  Generator_loss: 2.3732190132141113\n",
            "epoch: 9/10,    batch: 15313/15469    Discriminator_loss: 0.09781074523925781  Generator_loss: 2.3797764778137207\n",
            "epoch: 9/10,    batch: 15314/15469    Discriminator_loss: 0.09716770052909851  Generator_loss: 2.386157512664795\n",
            "epoch: 9/10,    batch: 15315/15469    Discriminator_loss: 0.09650824964046478  Generator_loss: 2.392570972442627\n",
            "epoch: 9/10,    batch: 15316/15469    Discriminator_loss: 0.09587370604276657  Generator_loss: 2.3989293575286865\n",
            "epoch: 9/10,    batch: 15317/15469    Discriminator_loss: 0.09523825347423553  Generator_loss: 2.4051764011383057\n",
            "epoch: 9/10,    batch: 15318/15469    Discriminator_loss: 0.09461303055286407  Generator_loss: 2.4114603996276855\n",
            "epoch: 9/10,    batch: 15319/15469    Discriminator_loss: 0.09398862719535828  Generator_loss: 2.4177002906799316\n",
            "epoch: 9/10,    batch: 15320/15469    Discriminator_loss: 0.09338760375976562  Generator_loss: 2.423879623413086\n",
            "epoch: 9/10,    batch: 15321/15469    Discriminator_loss: 0.09279078245162964  Generator_loss: 2.4300894737243652\n",
            "epoch: 9/10,    batch: 15322/15469    Discriminator_loss: 0.09219813346862793  Generator_loss: 2.4361658096313477\n",
            "epoch: 9/10,    batch: 15323/15469    Discriminator_loss: 0.09161362051963806  Generator_loss: 2.4421942234039307\n",
            "epoch: 9/10,    batch: 15324/15469    Discriminator_loss: 0.09103939682245255  Generator_loss: 2.448237657546997\n",
            "epoch: 9/10,    batch: 15325/15469    Discriminator_loss: 0.09046873450279236  Generator_loss: 2.454235076904297\n",
            "epoch: 9/10,    batch: 15326/15469    Discriminator_loss: 0.08991069346666336  Generator_loss: 2.4601738452911377\n",
            "epoch: 9/10,    batch: 15327/15469    Discriminator_loss: 0.08935493230819702  Generator_loss: 2.466076374053955\n",
            "epoch: 9/10,    batch: 15328/15469    Discriminator_loss: 0.08881258964538574  Generator_loss: 2.4719648361206055\n",
            "epoch: 9/10,    batch: 15329/15469    Discriminator_loss: 0.08827514201402664  Generator_loss: 2.4777567386627197\n",
            "epoch: 9/10,    batch: 15330/15469    Discriminator_loss: 0.08774002641439438  Generator_loss: 2.4835734367370605\n",
            "epoch: 9/10,    batch: 15331/15469    Discriminator_loss: 0.0872117355465889  Generator_loss: 2.4893715381622314\n",
            "epoch: 9/10,    batch: 15332/15469    Discriminator_loss: 0.08669349551200867  Generator_loss: 2.4950995445251465\n",
            "epoch: 9/10,    batch: 15333/15469    Discriminator_loss: 0.08618050813674927  Generator_loss: 2.5008316040039062\n",
            "epoch: 9/10,    batch: 15334/15469    Discriminator_loss: 0.08566634356975555  Generator_loss: 2.5064644813537598\n",
            "epoch: 9/10,    batch: 15335/15469    Discriminator_loss: 0.08515597879886627  Generator_loss: 2.512202739715576\n",
            "epoch: 9/10,    batch: 15336/15469    Discriminator_loss: 0.08466117084026337  Generator_loss: 2.5178375244140625\n",
            "epoch: 9/10,    batch: 15337/15469    Discriminator_loss: 0.08416765928268433  Generator_loss: 2.523571491241455\n",
            "epoch: 9/10,    batch: 15338/15469    Discriminator_loss: 0.08367439359426498  Generator_loss: 2.529170274734497\n",
            "epoch: 9/10,    batch: 15339/15469    Discriminator_loss: 0.08318164199590683  Generator_loss: 2.5348024368286133\n",
            "epoch: 9/10,    batch: 15340/15469    Discriminator_loss: 0.08269602060317993  Generator_loss: 2.5404305458068848\n",
            "epoch: 9/10,    batch: 15341/15469    Discriminator_loss: 0.08221394568681717  Generator_loss: 2.546081781387329\n",
            "epoch: 9/10,    batch: 15342/15469    Discriminator_loss: 0.08173380047082901  Generator_loss: 2.5517377853393555\n",
            "epoch: 9/10,    batch: 15343/15469    Discriminator_loss: 0.08125419914722443  Generator_loss: 2.557420253753662\n",
            "epoch: 9/10,    batch: 15344/15469    Discriminator_loss: 0.08077088743448257  Generator_loss: 2.5630900859832764\n",
            "epoch: 9/10,    batch: 15345/15469    Discriminator_loss: 0.08029583096504211  Generator_loss: 2.5688483715057373\n",
            "epoch: 9/10,    batch: 15346/15469    Discriminator_loss: 0.07981836050748825  Generator_loss: 2.5745675563812256\n",
            "epoch: 9/10,    batch: 15347/15469    Discriminator_loss: 0.07934455573558807  Generator_loss: 2.5803394317626953\n",
            "epoch: 9/10,    batch: 15348/15469    Discriminator_loss: 0.07887290418148041  Generator_loss: 2.5861525535583496\n",
            "epoch: 9/10,    batch: 15349/15469    Discriminator_loss: 0.07839974015951157  Generator_loss: 2.5919547080993652\n",
            "epoch: 9/10,    batch: 15350/15469    Discriminator_loss: 0.07792460173368454  Generator_loss: 2.5978665351867676\n",
            "epoch: 9/10,    batch: 15351/15469    Discriminator_loss: 0.07745654135942459  Generator_loss: 2.603743076324463\n",
            "epoch: 9/10,    batch: 15352/15469    Discriminator_loss: 0.07698091864585876  Generator_loss: 2.6096978187561035\n",
            "epoch: 9/10,    batch: 15353/15469    Discriminator_loss: 0.07650738954544067  Generator_loss: 2.615668773651123\n",
            "epoch: 9/10,    batch: 15354/15469    Discriminator_loss: 0.07603377103805542  Generator_loss: 2.6217002868652344\n",
            "epoch: 9/10,    batch: 15355/15469    Discriminator_loss: 0.07555315643548965  Generator_loss: 2.627760887145996\n",
            "epoch: 9/10,    batch: 15356/15469    Discriminator_loss: 0.07507260143756866  Generator_loss: 2.633924961090088\n",
            "epoch: 9/10,    batch: 15357/15469    Discriminator_loss: 0.0745931938290596  Generator_loss: 2.6401658058166504\n",
            "epoch: 9/10,    batch: 15358/15469    Discriminator_loss: 0.07411361485719681  Generator_loss: 2.646423816680908\n",
            "epoch: 9/10,    batch: 15359/15469    Discriminator_loss: 0.07363243401050568  Generator_loss: 2.6528079509735107\n",
            "epoch: 9/10,    batch: 15360/15469    Discriminator_loss: 0.07314030826091766  Generator_loss: 2.659287929534912\n",
            "epoch: 9/10,    batch: 15361/15469    Discriminator_loss: 0.07264944165945053  Generator_loss: 2.6658406257629395\n",
            "epoch: 9/10,    batch: 15362/15469    Discriminator_loss: 0.07215435802936554  Generator_loss: 2.672464370727539\n",
            "epoch: 9/10,    batch: 15363/15469    Discriminator_loss: 0.07165549695491791  Generator_loss: 2.6792776584625244\n",
            "epoch: 9/10,    batch: 15364/15469    Discriminator_loss: 0.0711483359336853  Generator_loss: 2.6861023902893066\n",
            "epoch: 9/10,    batch: 15365/15469    Discriminator_loss: 0.0706406980752945  Generator_loss: 2.6931562423706055\n",
            "epoch: 9/10,    batch: 15366/15469    Discriminator_loss: 0.0701235830783844  Generator_loss: 2.700305461883545\n",
            "epoch: 9/10,    batch: 15367/15469    Discriminator_loss: 0.06960057467222214  Generator_loss: 2.707609176635742\n",
            "epoch: 9/10,    batch: 15368/15469    Discriminator_loss: 0.06907349824905396  Generator_loss: 2.715036392211914\n",
            "epoch: 9/10,    batch: 15369/15469    Discriminator_loss: 0.06853672862052917  Generator_loss: 2.722667694091797\n",
            "epoch: 9/10,    batch: 15370/15469    Discriminator_loss: 0.06799528747797012  Generator_loss: 2.730428695678711\n",
            "epoch: 9/10,    batch: 15371/15469    Discriminator_loss: 0.0674428939819336  Generator_loss: 2.7383460998535156\n",
            "epoch: 9/10,    batch: 15372/15469    Discriminator_loss: 0.06688719242811203  Generator_loss: 2.746471405029297\n",
            "epoch: 9/10,    batch: 15373/15469    Discriminator_loss: 0.06632435321807861  Generator_loss: 2.754728317260742\n",
            "epoch: 9/10,    batch: 15374/15469    Discriminator_loss: 0.06575335562229156  Generator_loss: 2.763216495513916\n",
            "epoch: 9/10,    batch: 15375/15469    Discriminator_loss: 0.06518038362264633  Generator_loss: 2.771825075149536\n",
            "epoch: 9/10,    batch: 15376/15469    Discriminator_loss: 0.06459818035364151  Generator_loss: 2.7806148529052734\n",
            "epoch: 9/10,    batch: 15377/15469    Discriminator_loss: 0.06401976197957993  Generator_loss: 2.7895097732543945\n",
            "epoch: 9/10,    batch: 15378/15469    Discriminator_loss: 0.06343711167573929  Generator_loss: 2.798628807067871\n",
            "epoch: 9/10,    batch: 15379/15469    Discriminator_loss: 0.06282259523868561  Generator_loss: 2.8078293800354004\n",
            "epoch: 9/10,    batch: 15380/15469    Discriminator_loss: 0.06222372129559517  Generator_loss: 2.8172574043273926\n",
            "epoch: 9/10,    batch: 15381/15469    Discriminator_loss: 0.061622824519872665  Generator_loss: 2.8267502784729004\n",
            "epoch: 9/10,    batch: 15382/15469    Discriminator_loss: 0.06101466342806816  Generator_loss: 2.8364386558532715\n",
            "epoch: 9/10,    batch: 15383/15469    Discriminator_loss: 0.06042061001062393  Generator_loss: 2.846212863922119\n",
            "epoch: 9/10,    batch: 15384/15469    Discriminator_loss: 0.05980125069618225  Generator_loss: 2.856128692626953\n",
            "epoch: 9/10,    batch: 15385/15469    Discriminator_loss: 0.059199605137109756  Generator_loss: 2.8662078380584717\n",
            "epoch: 9/10,    batch: 15386/15469    Discriminator_loss: 0.058574628084897995  Generator_loss: 2.876330852508545\n",
            "epoch: 9/10,    batch: 15387/15469    Discriminator_loss: 0.05797268822789192  Generator_loss: 2.8866384029388428\n",
            "epoch: 9/10,    batch: 15388/15469    Discriminator_loss: 0.057361867278814316  Generator_loss: 2.8969855308532715\n",
            "epoch: 9/10,    batch: 15389/15469    Discriminator_loss: 0.05675546079874039  Generator_loss: 2.9074301719665527\n",
            "epoch: 9/10,    batch: 15390/15469    Discriminator_loss: 0.056195784360170364  Generator_loss: 2.9179396629333496\n",
            "epoch: 9/10,    batch: 15391/15469    Discriminator_loss: 0.05553837865591049  Generator_loss: 2.928544521331787\n",
            "epoch: 9/10,    batch: 15392/15469    Discriminator_loss: 0.05493537336587906  Generator_loss: 2.9391682147979736\n",
            "epoch: 9/10,    batch: 15393/15469    Discriminator_loss: 0.05434129014611244  Generator_loss: 2.9498491287231445\n",
            "epoch: 9/10,    batch: 15394/15469    Discriminator_loss: 0.05375119298696518  Generator_loss: 2.960590124130249\n",
            "epoch: 9/10,    batch: 15395/15469    Discriminator_loss: 0.05316408351063728  Generator_loss: 2.9712862968444824\n",
            "epoch: 9/10,    batch: 15396/15469    Discriminator_loss: 0.05258817598223686  Generator_loss: 2.981996536254883\n",
            "epoch: 9/10,    batch: 15397/15469    Discriminator_loss: 0.05201689526438713  Generator_loss: 2.992647886276245\n",
            "epoch: 9/10,    batch: 15398/15469    Discriminator_loss: 0.0514533631503582  Generator_loss: 3.0033016204833984\n",
            "epoch: 9/10,    batch: 15399/15469    Discriminator_loss: 0.05090158060193062  Generator_loss: 3.0138778686523438\n",
            "epoch: 9/10,    batch: 15400/15469    Discriminator_loss: 0.05036119744181633  Generator_loss: 3.0244083404541016\n",
            "epoch: 9/10,    batch: 15401/15469    Discriminator_loss: 0.04983028024435043  Generator_loss: 3.034766674041748\n",
            "epoch: 9/10,    batch: 15402/15469    Discriminator_loss: 0.04931436479091644  Generator_loss: 3.0450122356414795\n",
            "epoch: 9/10,    batch: 15403/15469    Discriminator_loss: 0.048813775181770325  Generator_loss: 3.055060386657715\n",
            "epoch: 9/10,    batch: 15404/15469    Discriminator_loss: 0.04832516610622406  Generator_loss: 3.0648601055145264\n",
            "epoch: 9/10,    batch: 15405/15469    Discriminator_loss: 0.04785940796136856  Generator_loss: 3.074406623840332\n",
            "epoch: 9/10,    batch: 15406/15469    Discriminator_loss: 0.04741421341896057  Generator_loss: 3.083698034286499\n",
            "epoch: 9/10,    batch: 15407/15469    Discriminator_loss: 0.046990636736154556  Generator_loss: 3.0924503803253174\n",
            "epoch: 9/10,    batch: 15408/15469    Discriminator_loss: 0.046592216938734055  Generator_loss: 3.100837230682373\n",
            "epoch: 9/10,    batch: 15409/15469    Discriminator_loss: 0.046232905238866806  Generator_loss: 3.1085638999938965\n",
            "epoch: 9/10,    batch: 15410/15469    Discriminator_loss: 0.045914627611637115  Generator_loss: 3.1154065132141113\n",
            "epoch: 9/10,    batch: 15411/15469    Discriminator_loss: 0.04564804211258888  Generator_loss: 3.1211938858032227\n",
            "epoch: 9/10,    batch: 15412/15469    Discriminator_loss: 0.046400271356105804  Generator_loss: 3.12579345703125\n",
            "epoch: 9/10,    batch: 15413/15469    Discriminator_loss: 0.046088531613349915  Generator_loss: 3.1281814575195312\n",
            "epoch: 9/10,    batch: 15414/15469    Discriminator_loss: 0.04602379351854324  Generator_loss: 3.1281180381774902\n",
            "epoch: 9/10,    batch: 15415/15469    Discriminator_loss: 0.046150173991918564  Generator_loss: 3.1230454444885254\n",
            "epoch: 9/10,    batch: 15416/15469    Discriminator_loss: 0.04679667577147484  Generator_loss: 3.110154628753662\n",
            "epoch: 9/10,    batch: 15417/15469    Discriminator_loss: 0.04814436659216881  Generator_loss: 3.0824220180511475\n",
            "epoch: 9/10,    batch: 15418/15469    Discriminator_loss: 0.051389627158641815  Generator_loss: 3.023364782333374\n",
            "epoch: 9/10,    batch: 15419/15469    Discriminator_loss: 0.05887175723910332  Generator_loss: 2.904801607131958\n",
            "epoch: 9/10,    batch: 15420/15469    Discriminator_loss: 0.09103676676750183  Generator_loss: 2.6561787128448486\n",
            "epoch: 9/10,    batch: 15421/15469    Discriminator_loss: 0.3668655753135681  Generator_loss: 2.2879080772399902\n",
            "epoch: 9/10,    batch: 15422/15469    Discriminator_loss: 0.4986227750778198  Generator_loss: 2.7107748985290527\n",
            "epoch: 9/10,    batch: 15423/15469    Discriminator_loss: 0.21137386560440063  Generator_loss: 3.4954631328582764\n",
            "epoch: 9/10,    batch: 15424/15469    Discriminator_loss: 0.06713765859603882  Generator_loss: 4.061642169952393\n",
            "epoch: 9/10,    batch: 15425/15469    Discriminator_loss: 0.034510914236307144  Generator_loss: 4.36363410949707\n",
            "epoch: 9/10,    batch: 15426/15469    Discriminator_loss: 0.026166431605815887  Generator_loss: 4.484314918518066\n",
            "epoch: 9/10,    batch: 15427/15469    Discriminator_loss: 0.0260944701731205  Generator_loss: 4.459731101989746\n",
            "epoch: 9/10,    batch: 15428/15469    Discriminator_loss: 0.034288693219423294  Generator_loss: 4.308428764343262\n",
            "epoch: 9/10,    batch: 15429/15469    Discriminator_loss: 0.04466715455055237  Generator_loss: 4.052614688873291\n",
            "epoch: 9/10,    batch: 15430/15469    Discriminator_loss: 0.06657388806343079  Generator_loss: 3.801476001739502\n",
            "epoch: 9/10,    batch: 15431/15469    Discriminator_loss: 0.0828537866473198  Generator_loss: 3.791003704071045\n",
            "epoch: 9/10,    batch: 15432/15469    Discriminator_loss: 0.0757424458861351  Generator_loss: 4.377474308013916\n",
            "epoch: 9/10,    batch: 15433/15469    Discriminator_loss: 0.027008993551135063  Generator_loss: 5.845269203186035\n",
            "epoch: 9/10,    batch: 15434/15469    Discriminator_loss: 0.00441006850451231  Generator_loss: 6.774909496307373\n",
            "epoch: 9/10,    batch: 15435/15469    Discriminator_loss: 0.0015728911384940147  Generator_loss: 7.053078651428223\n",
            "epoch: 9/10,    batch: 15436/15469    Discriminator_loss: 0.005149033851921558  Generator_loss: 7.128316879272461\n",
            "epoch: 9/10,    batch: 15437/15469    Discriminator_loss: 0.0189469326287508  Generator_loss: 7.098188400268555\n",
            "epoch: 9/10,    batch: 15438/15469    Discriminator_loss: 0.014311863109469414  Generator_loss: 7.002068519592285\n",
            "epoch: 9/10,    batch: 15439/15469    Discriminator_loss: 0.004525631666183472  Generator_loss: 6.8559770584106445\n",
            "epoch: 9/10,    batch: 15440/15469    Discriminator_loss: 0.001968023832887411  Generator_loss: 6.659529685974121\n",
            "epoch: 9/10,    batch: 15441/15469    Discriminator_loss: 0.002522207796573639  Generator_loss: 6.382716178894043\n",
            "epoch: 9/10,    batch: 15442/15469    Discriminator_loss: 0.003541594371199608  Generator_loss: 6.037137508392334\n",
            "epoch: 9/10,    batch: 15443/15469    Discriminator_loss: 0.004898738116025925  Generator_loss: 5.705881595611572\n",
            "epoch: 9/10,    batch: 15444/15469    Discriminator_loss: 0.00645806547254324  Generator_loss: 5.42048978805542\n",
            "epoch: 9/10,    batch: 15445/15469    Discriminator_loss: 0.008579983375966549  Generator_loss: 5.1316118240356445\n",
            "epoch: 9/10,    batch: 15446/15469    Discriminator_loss: 0.01072717271745205  Generator_loss: 4.892561912536621\n",
            "epoch: 9/10,    batch: 15447/15469    Discriminator_loss: 0.015183988027274609  Generator_loss: 4.732317924499512\n",
            "epoch: 9/10,    batch: 15448/15469    Discriminator_loss: 0.01766778528690338  Generator_loss: 4.7377519607543945\n",
            "epoch: 9/10,    batch: 15449/15469    Discriminator_loss: 0.013440234586596489  Generator_loss: 5.00897741317749\n",
            "epoch: 9/10,    batch: 15450/15469    Discriminator_loss: 0.00858163833618164  Generator_loss: 5.3589558601379395\n",
            "epoch: 9/10,    batch: 15451/15469    Discriminator_loss: 0.006097700912505388  Generator_loss: 5.619451522827148\n",
            "epoch: 9/10,    batch: 15452/15469    Discriminator_loss: 0.004928390495479107  Generator_loss: 5.78141975402832\n",
            "epoch: 9/10,    batch: 15453/15469    Discriminator_loss: 0.005063837394118309  Generator_loss: 5.876663684844971\n",
            "epoch: 9/10,    batch: 15454/15469    Discriminator_loss: 0.00438651442527771  Generator_loss: 5.929365158081055\n",
            "epoch: 9/10,    batch: 15455/15469    Discriminator_loss: 0.0046350909397006035  Generator_loss: 5.933698654174805\n",
            "epoch: 9/10,    batch: 15456/15469    Discriminator_loss: 0.0037023136392235756  Generator_loss: 5.894462585449219\n",
            "epoch: 9/10,    batch: 15457/15469    Discriminator_loss: 0.005248710513114929  Generator_loss: 5.81412410736084\n",
            "epoch: 9/10,    batch: 15458/15469    Discriminator_loss: 0.004857278428971767  Generator_loss: 5.711670875549316\n",
            "epoch: 9/10,    batch: 15459/15469    Discriminator_loss: 0.005824511870741844  Generator_loss: 5.6003217697143555\n",
            "epoch: 9/10,    batch: 15460/15469    Discriminator_loss: 0.006683863699436188  Generator_loss: 5.488409042358398\n",
            "epoch: 9/10,    batch: 15461/15469    Discriminator_loss: 0.009054041467607021  Generator_loss: 5.37088680267334\n",
            "epoch: 9/10,    batch: 15462/15469    Discriminator_loss: 0.006861620116978884  Generator_loss: 5.256603240966797\n",
            "epoch: 9/10,    batch: 15463/15469    Discriminator_loss: 0.011329930275678635  Generator_loss: 5.129593849182129\n",
            "epoch: 9/10,    batch: 15464/15469    Discriminator_loss: 0.007636991795152426  Generator_loss: 5.015666484832764\n",
            "epoch: 9/10,    batch: 15465/15469    Discriminator_loss: 0.008244197815656662  Generator_loss: 4.91298246383667\n",
            "epoch: 9/10,    batch: 15466/15469    Discriminator_loss: 0.008865498937666416  Generator_loss: 4.824079513549805\n",
            "epoch: 9/10,    batch: 15467/15469    Discriminator_loss: 0.009566995315253735  Generator_loss: 4.740730285644531\n",
            "epoch: 9/10,    batch: 15468/15469    Discriminator_loss: 0.01066143810749054  Generator_loss: 4.654860973358154\n",
            "  adding: checkpoints/ (stored 0%)\n",
            "  adding: checkpoints/generator/ (stored 0%)\n",
            "  adding: checkpoints/generator/gen_epoch3.h5 (deflated 9%)\n",
            "  adding: checkpoints/generator/gen_epoch9.h5 (deflated 9%)\n",
            "  adding: checkpoints/generator/gen_epoch1.h5 (deflated 9%)\n",
            "  adding: checkpoints/generator/gen_epoch2.h5 (deflated 9%)\n",
            "  adding: checkpoints/generator/gen_epoch5.h5 (deflated 9%)\n",
            "  adding: checkpoints/generator/gen_epoch4.h5 (deflated 9%)\n",
            "  adding: checkpoints/generator/gen_epoch0.h5 (deflated 9%)\n",
            "  adding: checkpoints/generator/gen_epoch7.h5 (deflated 9%)\n",
            "  adding: checkpoints/generator/gen_epoch8.h5 (deflated 9%)\n",
            "  adding: checkpoints/generator/gen_epoch6.h5 (deflated 9%)\n",
            "  adding: checkpoints/discriminator/ (stored 0%)\n",
            "  adding: checkpoints/discriminator/discr_epoch2.h5 (deflated 12%)\n",
            "  adding: checkpoints/discriminator/discr_epoch8.h5 (deflated 12%)\n",
            "  adding: checkpoints/discriminator/discr_epoch4.h5 (deflated 12%)\n",
            "  adding: checkpoints/discriminator/discr_epoch9.h5 (deflated 12%)\n",
            "  adding: checkpoints/discriminator/discr_epoch6.h5 (deflated 12%)\n",
            "  adding: checkpoints/discriminator/discr_epoch0.h5 (deflated 12%)\n",
            "  adding: checkpoints/discriminator/discr_epoch5.h5 (deflated 12%)\n",
            "  adding: checkpoints/discriminator/discr_epoch3.h5 (deflated 12%)\n",
            "  adding: checkpoints/discriminator/discr_epoch7.h5 (deflated 12%)\n",
            "  adding: checkpoints/discriminator/discr_epoch1.h5 (deflated 12%)\n"
          ]
        }
      ],
      "source": [
        "adversarial_training(generator, discriminator, train_dataset, TRAIN_BATCH_SIZE, WINDOW_SIZE, LATENT_VAR_SIZE, training_loss, discr_optimizer, gen_optimizer, \"./drive/MyDrive/ml-applications/TimeSeriesAnomalyDetection_project/saved_checkpoints_swat\", 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSWerpRs_of-"
      },
      "source": [
        "# Encoder Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPPJfItp_vl3"
      },
      "outputs": [],
      "source": [
        "def encoder_training(encoder, generator, data, latent_size, checkpoints_backup_path, num_epochs=1):\n",
        "  '''\n",
        "  Encoder must be trained on regular data only.\n",
        "\n",
        "  The purpose of the encoder is to learn the inverse mapping of the generator,\n",
        "  i.e. how to map a time sequence to the latent space.\n",
        "  '''\n",
        "\n",
        "  # setup checkpoint saving\n",
        "  directory_path = \"./encoder_checkpoints\"\n",
        "\n",
        "  if not os.path.isdir('encoder_checkpoints'):\n",
        "    ! mkdir encoder_checkpoints\n",
        "\n",
        "  loss_fn = tf.keras.losses.MeanAbsoluteError()\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "\n",
        "  print(\"epoch: 0/%d\")\n",
        "  for epoch in range(3, num_epochs):\n",
        "    sum_loss = 0\n",
        "    for i, (time_sequences, labels) in enumerate(data):\n",
        "      with tf.GradientTape() as tape:\n",
        "        latent_output = encoder(time_sequences)\n",
        "        generated_seq = generator(latent_output)\n",
        "\n",
        "        enc_loss = loss_fn(time_sequences, generated_seq)\n",
        "\n",
        "      gradients = tape.gradient(enc_loss, encoder.trainable_weights)\n",
        "      optimizer.apply_gradients(zip(gradients, encoder.trainable_weights))\n",
        "\n",
        "      sum_loss += enc_loss\n",
        "\n",
        "      print(\n",
        "        f\"  epoch: {epoch}/{num_epochs},    batch: {i}/{len(data)}    Encoder_loss: {enc_loss}\"\n",
        "      )\n",
        "\n",
        "    print(\n",
        "      f\"epoch: {epoch}/{num_epochs},  Tot_epoch_loss: {sum_loss}\"\n",
        "    )\n",
        "\n",
        "    encoder.save_weights(directory_path + f\"/encoder_epoch{epoch}.h5\")\n",
        "\n",
        "    now = datetime.datetime.now()\n",
        "    zip_name = f\"encoder_checkpoints_{str(now.date())+'_'+str(now.time())}.zip\"\n",
        "\n",
        "    !zip -r {zip_name} ./encoder_checkpoints\n",
        "    !cp {zip_name} {checkpoints_backup_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtpdwr0yHKtb"
      },
      "source": [
        "## IMPORTANT: run this cell only if you want to load previously saved checkpoints for generator and discriminator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwOFDnfxHJ8x",
        "outputId": "33ce531a-bca3-4790-a704-6bedf93e269b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  ./checkpoints_2023-08-16_00:13:33.763024.zip\n",
            "   creating: ./checkpoints/\n",
            "   creating: ./checkpoints/generator/\n",
            "  inflating: ./checkpoints/generator/gen_epoch3.h5  \n",
            "  inflating: ./checkpoints/generator/gen_epoch9.h5  \n",
            "  inflating: ./checkpoints/generator/gen_epoch1.h5  \n",
            "  inflating: ./checkpoints/generator/gen_epoch2.h5  \n",
            "  inflating: ./checkpoints/generator/gen_epoch5.h5  \n",
            "  inflating: ./checkpoints/generator/gen_epoch4.h5  \n",
            "  inflating: ./checkpoints/generator/gen_epoch0.h5  \n",
            "  inflating: ./checkpoints/generator/gen_epoch7.h5  \n",
            "  inflating: ./checkpoints/generator/gen_epoch8.h5  \n",
            "  inflating: ./checkpoints/generator/gen_epoch6.h5  \n",
            "   creating: ./checkpoints/discriminator/\n",
            "  inflating: ./checkpoints/discriminator/discr_epoch2.h5  \n",
            "  inflating: ./checkpoints/discriminator/discr_epoch8.h5  \n",
            "  inflating: ./checkpoints/discriminator/discr_epoch4.h5  \n",
            "  inflating: ./checkpoints/discriminator/discr_epoch9.h5  \n",
            "  inflating: ./checkpoints/discriminator/discr_epoch6.h5  \n",
            "  inflating: ./checkpoints/discriminator/discr_epoch0.h5  \n",
            "  inflating: ./checkpoints/discriminator/discr_epoch5.h5  \n",
            "  inflating: ./checkpoints/discriminator/discr_epoch3.h5  \n",
            "  inflating: ./checkpoints/discriminator/discr_epoch7.h5  \n",
            "  inflating: ./checkpoints/discriminator/discr_epoch1.h5  \n"
          ]
        }
      ],
      "source": [
        "generator = init_generator(in_dim=(WINDOW_SIZE, LATENT_VAR_SIZE), out_dim=(WINDOW_SIZE, SAMPLE_SIZE))\n",
        "discriminator = init_discriminator(in_dim=(WINDOW_SIZE, SAMPLE_SIZE), out_dim=1)\n",
        "\n",
        "# download and uzip latest generator and discriminator checkpoints\n",
        "!cp ./drive/MyDrive/ml-applications/TimeSeriesAnomalyDetection_project/saved_checkpoints_swat/checkpoints_2023-08-16_00:13:33.763024.zip .\n",
        "!unzip ./checkpoints_2023-08-16_00:13:33.763024.zip -d .\n",
        "\n",
        "epoch = 9\n",
        "generator.load_weights(f\"./checkpoints/generator/gen_epoch{epoch}.h5\")\n",
        "discriminator.load_weights(f\"./checkpoints/discriminator/discr_epoch{epoch}.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWLk-Ux2zqNZ"
      },
      "source": [
        "## Start Encoder Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMYNvwsBzm4u",
        "outputId": "41e3a0dc-5216-4a8b-9273-bcc4d6c99fc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0/%d\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f9ec0e42a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f9ec0e42a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  epoch: 9/10,    batch: 10481/15469    Encoder_loss: 0.3253931999206543\n",
            "  epoch: 9/10,    batch: 10482/15469    Encoder_loss: 0.3244699537754059\n",
            "  epoch: 9/10,    batch: 10483/15469    Encoder_loss: 0.3230203092098236\n",
            "  epoch: 9/10,    batch: 10484/15469    Encoder_loss: 0.32254114747047424\n",
            "  epoch: 9/10,    batch: 10485/15469    Encoder_loss: 0.32126784324645996\n",
            "  epoch: 9/10,    batch: 10486/15469    Encoder_loss: 0.3221737742424011\n",
            "  epoch: 9/10,    batch: 10487/15469    Encoder_loss: 0.321905255317688\n",
            "  epoch: 9/10,    batch: 10488/15469    Encoder_loss: 0.3201863765716553\n",
            "  epoch: 9/10,    batch: 10489/15469    Encoder_loss: 0.31932350993156433\n",
            "  epoch: 9/10,    batch: 10490/15469    Encoder_loss: 0.318231463432312\n",
            "  epoch: 9/10,    batch: 10491/15469    Encoder_loss: 0.3177185654640198\n",
            "  epoch: 9/10,    batch: 10492/15469    Encoder_loss: 0.3175451159477234\n",
            "  epoch: 9/10,    batch: 10493/15469    Encoder_loss: 0.31939420104026794\n",
            "  epoch: 9/10,    batch: 10494/15469    Encoder_loss: 0.31840306520462036\n",
            "  epoch: 9/10,    batch: 10495/15469    Encoder_loss: 0.31574973464012146\n",
            "  epoch: 9/10,    batch: 10496/15469    Encoder_loss: 0.31733614206314087\n",
            "  epoch: 9/10,    batch: 10497/15469    Encoder_loss: 0.3159325122833252\n",
            "  epoch: 9/10,    batch: 10498/15469    Encoder_loss: 0.318025678396225\n",
            "  epoch: 9/10,    batch: 10499/15469    Encoder_loss: 0.3156541883945465\n",
            "  epoch: 9/10,    batch: 10500/15469    Encoder_loss: 0.31387168169021606\n",
            "  epoch: 9/10,    batch: 10501/15469    Encoder_loss: 0.31339678168296814\n",
            "  epoch: 9/10,    batch: 10502/15469    Encoder_loss: 0.3132591247558594\n",
            "  epoch: 9/10,    batch: 10503/15469    Encoder_loss: 0.3138832449913025\n",
            "  epoch: 9/10,    batch: 10504/15469    Encoder_loss: 0.31383147835731506\n",
            "  epoch: 9/10,    batch: 10505/15469    Encoder_loss: 0.31143128871917725\n",
            "  epoch: 9/10,    batch: 10506/15469    Encoder_loss: 0.3133198022842407\n",
            "  epoch: 9/10,    batch: 10507/15469    Encoder_loss: 0.31325218081474304\n",
            "  epoch: 9/10,    batch: 10508/15469    Encoder_loss: 0.3106536865234375\n",
            "  epoch: 9/10,    batch: 10509/15469    Encoder_loss: 0.3124719262123108\n",
            "  epoch: 9/10,    batch: 10510/15469    Encoder_loss: 0.31201544404029846\n",
            "  epoch: 9/10,    batch: 10511/15469    Encoder_loss: 0.31205159425735474\n",
            "  epoch: 9/10,    batch: 10512/15469    Encoder_loss: 0.3116998076438904\n",
            "  epoch: 9/10,    batch: 10513/15469    Encoder_loss: 0.3106672167778015\n",
            "  epoch: 9/10,    batch: 10514/15469    Encoder_loss: 0.3112039566040039\n",
            "  epoch: 9/10,    batch: 10515/15469    Encoder_loss: 0.3116082549095154\n",
            "  epoch: 9/10,    batch: 10516/15469    Encoder_loss: 0.3115333020687103\n",
            "  epoch: 9/10,    batch: 10517/15469    Encoder_loss: 0.31071892380714417\n",
            "  epoch: 9/10,    batch: 10518/15469    Encoder_loss: 0.30931541323661804\n",
            "  epoch: 9/10,    batch: 10519/15469    Encoder_loss: 0.3111155927181244\n",
            "  epoch: 9/10,    batch: 10520/15469    Encoder_loss: 0.30986061692237854\n",
            "  epoch: 9/10,    batch: 10521/15469    Encoder_loss: 0.3110288977622986\n",
            "  epoch: 9/10,    batch: 10522/15469    Encoder_loss: 0.3086169362068176\n",
            "  epoch: 9/10,    batch: 10523/15469    Encoder_loss: 0.30879348516464233\n",
            "  epoch: 9/10,    batch: 10524/15469    Encoder_loss: 0.30882272124290466\n",
            "  epoch: 9/10,    batch: 10525/15469    Encoder_loss: 0.3088141083717346\n",
            "  epoch: 9/10,    batch: 10526/15469    Encoder_loss: 0.3473387360572815\n",
            "  epoch: 9/10,    batch: 10527/15469    Encoder_loss: 0.3935975432395935\n",
            "  epoch: 9/10,    batch: 10528/15469    Encoder_loss: 0.3946894705295563\n",
            "  epoch: 9/10,    batch: 10529/15469    Encoder_loss: 0.44514578580856323\n",
            "  epoch: 9/10,    batch: 10530/15469    Encoder_loss: 0.49391070008277893\n",
            "  epoch: 9/10,    batch: 10531/15469    Encoder_loss: 0.4955093264579773\n",
            "  epoch: 9/10,    batch: 10532/15469    Encoder_loss: 0.4941492974758148\n",
            "  epoch: 9/10,    batch: 10533/15469    Encoder_loss: 0.49429851770401\n",
            "  epoch: 9/10,    batch: 10534/15469    Encoder_loss: 0.4964316487312317\n",
            "  epoch: 9/10,    batch: 10535/15469    Encoder_loss: 0.49322015047073364\n",
            "  epoch: 9/10,    batch: 10536/15469    Encoder_loss: 0.4970143735408783\n",
            "  epoch: 9/10,    batch: 10537/15469    Encoder_loss: 0.4982627034187317\n",
            "  epoch: 9/10,    batch: 10538/15469    Encoder_loss: 0.496803343296051\n",
            "  epoch: 9/10,    batch: 10539/15469    Encoder_loss: 0.49769967794418335\n",
            "  epoch: 9/10,    batch: 10540/15469    Encoder_loss: 0.4984152317047119\n",
            "  epoch: 9/10,    batch: 10541/15469    Encoder_loss: 0.4994140565395355\n",
            "  epoch: 9/10,    batch: 10542/15469    Encoder_loss: 0.5010389685630798\n",
            "  epoch: 9/10,    batch: 10543/15469    Encoder_loss: 0.5032134056091309\n",
            "  epoch: 9/10,    batch: 10544/15469    Encoder_loss: 0.5033903121948242\n",
            "  epoch: 9/10,    batch: 10545/15469    Encoder_loss: 0.5033696293830872\n",
            "  epoch: 9/10,    batch: 10546/15469    Encoder_loss: 0.5061976909637451\n",
            "  epoch: 9/10,    batch: 10547/15469    Encoder_loss: 0.5067278742790222\n",
            "  epoch: 9/10,    batch: 10548/15469    Encoder_loss: 0.5093605518341064\n",
            "  epoch: 9/10,    batch: 10549/15469    Encoder_loss: 0.509294867515564\n",
            "  epoch: 9/10,    batch: 10550/15469    Encoder_loss: 0.5087035894393921\n",
            "  epoch: 9/10,    batch: 10551/15469    Encoder_loss: 0.5117687582969666\n",
            "  epoch: 9/10,    batch: 10552/15469    Encoder_loss: 0.5819379687309265\n",
            "  epoch: 9/10,    batch: 10553/15469    Encoder_loss: 0.8220715522766113\n",
            "  epoch: 9/10,    batch: 10554/15469    Encoder_loss: 0.9673913717269897\n",
            "  epoch: 9/10,    batch: 10555/15469    Encoder_loss: 0.9036938548088074\n",
            "  epoch: 9/10,    batch: 10556/15469    Encoder_loss: 0.8038036227226257\n",
            "  epoch: 9/10,    batch: 10557/15469    Encoder_loss: 0.8031511306762695\n",
            "  epoch: 9/10,    batch: 10558/15469    Encoder_loss: 0.8043702840805054\n",
            "  epoch: 9/10,    batch: 10559/15469    Encoder_loss: 0.7686684131622314\n",
            "  epoch: 9/10,    batch: 10560/15469    Encoder_loss: 0.6934967637062073\n",
            "  epoch: 9/10,    batch: 10561/15469    Encoder_loss: 0.6998419761657715\n",
            "  epoch: 9/10,    batch: 10562/15469    Encoder_loss: 0.6396945714950562\n",
            "  epoch: 9/10,    batch: 10563/15469    Encoder_loss: 0.6240366697311401\n",
            "  epoch: 9/10,    batch: 10564/15469    Encoder_loss: 0.6460660099983215\n",
            "  epoch: 9/10,    batch: 10565/15469    Encoder_loss: 0.6806477308273315\n",
            "  epoch: 9/10,    batch: 10566/15469    Encoder_loss: 0.6649417281150818\n",
            "  epoch: 9/10,    batch: 10567/15469    Encoder_loss: 0.655317485332489\n",
            "  epoch: 9/10,    batch: 10568/15469    Encoder_loss: 0.647954523563385\n",
            "  epoch: 9/10,    batch: 10569/15469    Encoder_loss: 0.5924094915390015\n",
            "  epoch: 9/10,    batch: 10570/15469    Encoder_loss: 0.6009913682937622\n",
            "  epoch: 9/10,    batch: 10571/15469    Encoder_loss: 0.6060796976089478\n",
            "  epoch: 9/10,    batch: 10572/15469    Encoder_loss: 0.6073209643363953\n",
            "  epoch: 9/10,    batch: 10573/15469    Encoder_loss: 0.6052327156066895\n",
            "  epoch: 9/10,    batch: 10574/15469    Encoder_loss: 0.6064542531967163\n",
            "  epoch: 9/10,    batch: 10575/15469    Encoder_loss: 0.6065987348556519\n",
            "  epoch: 9/10,    batch: 10576/15469    Encoder_loss: 0.609264075756073\n",
            "  epoch: 9/10,    batch: 10577/15469    Encoder_loss: 0.6075056791305542\n",
            "  epoch: 9/10,    batch: 10578/15469    Encoder_loss: 0.6077281832695007\n",
            "  epoch: 9/10,    batch: 10579/15469    Encoder_loss: 0.6082617044448853\n",
            "  epoch: 9/10,    batch: 10580/15469    Encoder_loss: 0.6111623048782349\n",
            "  epoch: 9/10,    batch: 10581/15469    Encoder_loss: 0.6111422181129456\n",
            "  epoch: 9/10,    batch: 10582/15469    Encoder_loss: 0.6165310740470886\n",
            "  epoch: 9/10,    batch: 10583/15469    Encoder_loss: 0.6150771379470825\n",
            "  epoch: 9/10,    batch: 10584/15469    Encoder_loss: 0.6177375912666321\n",
            "  epoch: 9/10,    batch: 10585/15469    Encoder_loss: 0.6175333261489868\n",
            "  epoch: 9/10,    batch: 10586/15469    Encoder_loss: 0.6179295778274536\n",
            "  epoch: 9/10,    batch: 10587/15469    Encoder_loss: 0.6220125555992126\n",
            "  epoch: 9/10,    batch: 10588/15469    Encoder_loss: 0.6305186748504639\n",
            "  epoch: 9/10,    batch: 10589/15469    Encoder_loss: 0.6551777720451355\n",
            "  epoch: 9/10,    batch: 10590/15469    Encoder_loss: 0.5801335573196411\n",
            "  epoch: 9/10,    batch: 10591/15469    Encoder_loss: 0.561691403388977\n",
            "  epoch: 9/10,    batch: 10592/15469    Encoder_loss: 0.560815155506134\n",
            "  epoch: 9/10,    batch: 10593/15469    Encoder_loss: 0.560167670249939\n",
            "  epoch: 9/10,    batch: 10594/15469    Encoder_loss: 0.559756875038147\n",
            "  epoch: 9/10,    batch: 10595/15469    Encoder_loss: 0.5604366064071655\n",
            "  epoch: 9/10,    batch: 10596/15469    Encoder_loss: 0.5679565668106079\n",
            "  epoch: 9/10,    batch: 10597/15469    Encoder_loss: 0.5024991631507874\n",
            "  epoch: 9/10,    batch: 10598/15469    Encoder_loss: 0.4601500630378723\n",
            "  epoch: 9/10,    batch: 10599/15469    Encoder_loss: 0.4594743251800537\n",
            "  epoch: 9/10,    batch: 10600/15469    Encoder_loss: 0.45883694291114807\n",
            "  epoch: 9/10,    batch: 10601/15469    Encoder_loss: 0.45650714635849\n",
            "  epoch: 9/10,    batch: 10602/15469    Encoder_loss: 0.4547019600868225\n",
            "  epoch: 9/10,    batch: 10603/15469    Encoder_loss: 0.4526919722557068\n",
            "  epoch: 9/10,    batch: 10604/15469    Encoder_loss: 0.4525238871574402\n",
            "  epoch: 9/10,    batch: 10605/15469    Encoder_loss: 0.44906187057495117\n",
            "  epoch: 9/10,    batch: 10606/15469    Encoder_loss: 0.44786542654037476\n",
            "  epoch: 9/10,    batch: 10607/15469    Encoder_loss: 0.4465982913970947\n",
            "  epoch: 9/10,    batch: 10608/15469    Encoder_loss: 0.4434855580329895\n",
            "  epoch: 9/10,    batch: 10609/15469    Encoder_loss: 0.49396371841430664\n",
            "  epoch: 9/10,    batch: 10610/15469    Encoder_loss: 0.5251838564872742\n",
            "  epoch: 9/10,    batch: 10611/15469    Encoder_loss: 0.5230859518051147\n",
            "  epoch: 9/10,    batch: 10612/15469    Encoder_loss: 0.5199764966964722\n",
            "  epoch: 9/10,    batch: 10613/15469    Encoder_loss: 0.516740620136261\n",
            "  epoch: 9/10,    batch: 10614/15469    Encoder_loss: 0.5152567625045776\n",
            "  epoch: 9/10,    batch: 10615/15469    Encoder_loss: 0.5110163688659668\n",
            "  epoch: 9/10,    batch: 10616/15469    Encoder_loss: 0.5077031254768372\n",
            "  epoch: 9/10,    batch: 10617/15469    Encoder_loss: 0.50506192445755\n",
            "  epoch: 9/10,    batch: 10618/15469    Encoder_loss: 0.5018395185470581\n",
            "  epoch: 9/10,    batch: 10619/15469    Encoder_loss: 0.501604437828064\n",
            "  epoch: 9/10,    batch: 10620/15469    Encoder_loss: 0.5005898475646973\n",
            "  epoch: 9/10,    batch: 10621/15469    Encoder_loss: 0.4970664381980896\n",
            "  epoch: 9/10,    batch: 10622/15469    Encoder_loss: 0.4961478114128113\n",
            "  epoch: 9/10,    batch: 10623/15469    Encoder_loss: 0.4947703778743744\n",
            "  epoch: 9/10,    batch: 10624/15469    Encoder_loss: 0.49222248792648315\n",
            "  epoch: 9/10,    batch: 10625/15469    Encoder_loss: 0.490950345993042\n",
            "  epoch: 9/10,    batch: 10626/15469    Encoder_loss: 0.489959716796875\n",
            "  epoch: 9/10,    batch: 10627/15469    Encoder_loss: 0.4884865880012512\n",
            "  epoch: 9/10,    batch: 10628/15469    Encoder_loss: 0.4879712462425232\n",
            "  epoch: 9/10,    batch: 10629/15469    Encoder_loss: 0.4858451783657074\n",
            "  epoch: 9/10,    batch: 10630/15469    Encoder_loss: 0.4852535128593445\n",
            "  epoch: 9/10,    batch: 10631/15469    Encoder_loss: 0.47190696001052856\n",
            "  epoch: 9/10,    batch: 10632/15469    Encoder_loss: 0.4482444226741791\n",
            "  epoch: 9/10,    batch: 10633/15469    Encoder_loss: 0.44873055815696716\n",
            "  epoch: 9/10,    batch: 10634/15469    Encoder_loss: 0.44822582602500916\n",
            "  epoch: 9/10,    batch: 10635/15469    Encoder_loss: 0.504228413105011\n",
            "  epoch: 9/10,    batch: 10636/15469    Encoder_loss: 0.548106849193573\n",
            "  epoch: 9/10,    batch: 10637/15469    Encoder_loss: 0.5479349493980408\n",
            "  epoch: 9/10,    batch: 10638/15469    Encoder_loss: 0.5549662113189697\n",
            "  epoch: 9/10,    batch: 10639/15469    Encoder_loss: 0.5147119760513306\n",
            "  epoch: 9/10,    batch: 10640/15469    Encoder_loss: 0.5158266425132751\n",
            "  epoch: 9/10,    batch: 10641/15469    Encoder_loss: 0.5167649388313293\n",
            "  epoch: 9/10,    batch: 10642/15469    Encoder_loss: 0.5192363262176514\n",
            "  epoch: 9/10,    batch: 10643/15469    Encoder_loss: 0.5192055106163025\n",
            "  epoch: 9/10,    batch: 10644/15469    Encoder_loss: 0.5194873213768005\n",
            "  epoch: 9/10,    batch: 10645/15469    Encoder_loss: 0.5201637148857117\n",
            "  epoch: 9/10,    batch: 10646/15469    Encoder_loss: 0.5208603143692017\n",
            "  epoch: 9/10,    batch: 10647/15469    Encoder_loss: 0.5248218774795532\n",
            "  epoch: 9/10,    batch: 10648/15469    Encoder_loss: 0.5512861609458923\n",
            "  epoch: 9/10,    batch: 10649/15469    Encoder_loss: 0.5144404172897339\n",
            "  epoch: 9/10,    batch: 10650/15469    Encoder_loss: 0.5204156041145325\n",
            "  epoch: 9/10,    batch: 10651/15469    Encoder_loss: 0.4656812250614166\n",
            "  epoch: 9/10,    batch: 10652/15469    Encoder_loss: 0.4411952495574951\n",
            "  epoch: 9/10,    batch: 10653/15469    Encoder_loss: 0.4416116774082184\n",
            "  epoch: 9/10,    batch: 10654/15469    Encoder_loss: 0.4409254193305969\n",
            "  epoch: 9/10,    batch: 10655/15469    Encoder_loss: 0.44340330362319946\n",
            "  epoch: 9/10,    batch: 10656/15469    Encoder_loss: 0.4441984295845032\n",
            "  epoch: 9/10,    batch: 10657/15469    Encoder_loss: 0.44455432891845703\n",
            "  epoch: 9/10,    batch: 10658/15469    Encoder_loss: 0.4441457688808441\n",
            "  epoch: 9/10,    batch: 10659/15469    Encoder_loss: 0.44497424364089966\n",
            "  epoch: 9/10,    batch: 10660/15469    Encoder_loss: 0.4429284930229187\n",
            "  epoch: 9/10,    batch: 10661/15469    Encoder_loss: 0.44729503989219666\n",
            "  epoch: 9/10,    batch: 10662/15469    Encoder_loss: 0.44812074303627014\n",
            "  epoch: 9/10,    batch: 10663/15469    Encoder_loss: 0.4489509165287018\n",
            "  epoch: 9/10,    batch: 10664/15469    Encoder_loss: 0.44742703437805176\n",
            "  epoch: 9/10,    batch: 10665/15469    Encoder_loss: 0.44911980628967285\n",
            "  epoch: 9/10,    batch: 10666/15469    Encoder_loss: 0.4499942362308502\n",
            "  epoch: 9/10,    batch: 10667/15469    Encoder_loss: 0.45109206438064575\n",
            "  epoch: 9/10,    batch: 10668/15469    Encoder_loss: 0.4536309838294983\n",
            "  epoch: 9/10,    batch: 10669/15469    Encoder_loss: 0.4522225260734558\n",
            "  epoch: 9/10,    batch: 10670/15469    Encoder_loss: 0.4532153010368347\n",
            "  epoch: 9/10,    batch: 10671/15469    Encoder_loss: 0.4551554322242737\n",
            "  epoch: 9/10,    batch: 10672/15469    Encoder_loss: 0.45576930046081543\n",
            "  epoch: 9/10,    batch: 10673/15469    Encoder_loss: 0.4555794596672058\n",
            "  epoch: 9/10,    batch: 10674/15469    Encoder_loss: 0.46059808135032654\n",
            "  epoch: 9/10,    batch: 10675/15469    Encoder_loss: 0.4862493574619293\n",
            "  epoch: 9/10,    batch: 10676/15469    Encoder_loss: 0.49257078766822815\n",
            "  epoch: 9/10,    batch: 10677/15469    Encoder_loss: 0.4927655756473541\n",
            "  epoch: 9/10,    batch: 10678/15469    Encoder_loss: 0.49696025252342224\n",
            "  epoch: 9/10,    batch: 10679/15469    Encoder_loss: 0.49875831604003906\n",
            "  epoch: 9/10,    batch: 10680/15469    Encoder_loss: 0.6405462026596069\n",
            "  epoch: 9/10,    batch: 10681/15469    Encoder_loss: 0.8353222012519836\n",
            "  epoch: 9/10,    batch: 10682/15469    Encoder_loss: 0.8024042248725891\n",
            "  epoch: 9/10,    batch: 10683/15469    Encoder_loss: 0.6019667983055115\n",
            "  epoch: 9/10,    batch: 10684/15469    Encoder_loss: 0.5788499116897583\n",
            "  epoch: 9/10,    batch: 10685/15469    Encoder_loss: 0.5781407356262207\n",
            "  epoch: 9/10,    batch: 10686/15469    Encoder_loss: 0.5618553161621094\n",
            "  epoch: 9/10,    batch: 10687/15469    Encoder_loss: 0.47678542137145996\n",
            "  epoch: 9/10,    batch: 10688/15469    Encoder_loss: 0.4702121615409851\n",
            "  epoch: 9/10,    batch: 10689/15469    Encoder_loss: 0.4238506257534027\n",
            "  epoch: 9/10,    batch: 10690/15469    Encoder_loss: 0.38832393288612366\n",
            "  epoch: 9/10,    batch: 10691/15469    Encoder_loss: 0.3867484927177429\n",
            "  epoch: 9/10,    batch: 10692/15469    Encoder_loss: 0.3857746124267578\n",
            "  epoch: 9/10,    batch: 10693/15469    Encoder_loss: 0.383871853351593\n",
            "  epoch: 9/10,    batch: 10694/15469    Encoder_loss: 0.3825591504573822\n",
            "  epoch: 9/10,    batch: 10695/15469    Encoder_loss: 0.3812587857246399\n",
            "  epoch: 9/10,    batch: 10696/15469    Encoder_loss: 0.38665372133255005\n",
            "  epoch: 9/10,    batch: 10697/15469    Encoder_loss: 0.3938906788825989\n",
            "  epoch: 9/10,    batch: 10698/15469    Encoder_loss: 0.39619359374046326\n",
            "  epoch: 9/10,    batch: 10699/15469    Encoder_loss: 0.3937090337276459\n",
            "  epoch: 9/10,    batch: 10700/15469    Encoder_loss: 0.39217469096183777\n",
            "  epoch: 9/10,    batch: 10701/15469    Encoder_loss: 0.3906271457672119\n",
            "  epoch: 9/10,    batch: 10702/15469    Encoder_loss: 0.38945895433425903\n",
            "  epoch: 9/10,    batch: 10703/15469    Encoder_loss: 0.39042770862579346\n",
            "  epoch: 9/10,    batch: 10704/15469    Encoder_loss: 0.3872276544570923\n",
            "  epoch: 9/10,    batch: 10705/15469    Encoder_loss: 0.3840789198875427\n",
            "  epoch: 9/10,    batch: 10706/15469    Encoder_loss: 0.38312116265296936\n",
            "  epoch: 9/10,    batch: 10707/15469    Encoder_loss: 0.3813766539096832\n",
            "  epoch: 9/10,    batch: 10708/15469    Encoder_loss: 0.38075581192970276\n",
            "  epoch: 9/10,    batch: 10709/15469    Encoder_loss: 0.37927860021591187\n",
            "  epoch: 9/10,    batch: 10710/15469    Encoder_loss: 0.377570778131485\n",
            "  epoch: 9/10,    batch: 10711/15469    Encoder_loss: 0.37631210684776306\n",
            "  epoch: 9/10,    batch: 10712/15469    Encoder_loss: 0.3758939802646637\n",
            "  epoch: 9/10,    batch: 10713/15469    Encoder_loss: 0.37484073638916016\n",
            "  epoch: 9/10,    batch: 10714/15469    Encoder_loss: 0.3733882009983063\n",
            "  epoch: 9/10,    batch: 10715/15469    Encoder_loss: 0.3725234866142273\n",
            "  epoch: 9/10,    batch: 10716/15469    Encoder_loss: 0.3725183308124542\n",
            "  epoch: 9/10,    batch: 10717/15469    Encoder_loss: 0.3706456124782562\n",
            "  epoch: 9/10,    batch: 10718/15469    Encoder_loss: 0.36994031071662903\n",
            "  epoch: 9/10,    batch: 10719/15469    Encoder_loss: 0.3687708377838135\n",
            "  epoch: 9/10,    batch: 10720/15469    Encoder_loss: 0.3753903806209564\n",
            "  epoch: 9/10,    batch: 10721/15469    Encoder_loss: 0.33296290040016174\n",
            "  epoch: 9/10,    batch: 10722/15469    Encoder_loss: 0.3319515585899353\n",
            "  epoch: 9/10,    batch: 10723/15469    Encoder_loss: 0.33283406496047974\n",
            "  epoch: 9/10,    batch: 10724/15469    Encoder_loss: 0.3318790793418884\n",
            "  epoch: 9/10,    batch: 10725/15469    Encoder_loss: 0.3305317461490631\n",
            "  epoch: 9/10,    batch: 10726/15469    Encoder_loss: 0.32919541001319885\n",
            "  epoch: 9/10,    batch: 10727/15469    Encoder_loss: 0.32926884293556213\n",
            "  epoch: 9/10,    batch: 10728/15469    Encoder_loss: 0.32861852645874023\n",
            "  epoch: 9/10,    batch: 10729/15469    Encoder_loss: 0.3275682330131531\n",
            "  epoch: 9/10,    batch: 10730/15469    Encoder_loss: 0.3274562954902649\n",
            "  epoch: 9/10,    batch: 10731/15469    Encoder_loss: 0.3266429603099823\n",
            "  epoch: 9/10,    batch: 10732/15469    Encoder_loss: 0.325780987739563\n",
            "  epoch: 9/10,    batch: 10733/15469    Encoder_loss: 0.32539016008377075\n",
            "  epoch: 9/10,    batch: 10734/15469    Encoder_loss: 0.3251116871833801\n",
            "  epoch: 9/10,    batch: 10735/15469    Encoder_loss: 0.3239890933036804\n",
            "  epoch: 9/10,    batch: 10736/15469    Encoder_loss: 0.3234800398349762\n",
            "  epoch: 9/10,    batch: 10737/15469    Encoder_loss: 0.32431676983833313\n",
            "  epoch: 9/10,    batch: 10738/15469    Encoder_loss: 0.32349592447280884\n",
            "  epoch: 9/10,    batch: 10739/15469    Encoder_loss: 0.32461869716644287\n",
            "  epoch: 9/10,    batch: 10740/15469    Encoder_loss: 0.32326993346214294\n",
            "  epoch: 9/10,    batch: 10741/15469    Encoder_loss: 0.3225511908531189\n",
            "  epoch: 9/10,    batch: 10742/15469    Encoder_loss: 0.3207828104496002\n",
            "  epoch: 9/10,    batch: 10743/15469    Encoder_loss: 0.3221396803855896\n",
            "  epoch: 9/10,    batch: 10744/15469    Encoder_loss: 0.32109159231185913\n",
            "  epoch: 9/10,    batch: 10745/15469    Encoder_loss: 0.32101744413375854\n",
            "  epoch: 9/10,    batch: 10746/15469    Encoder_loss: 0.3201349675655365\n",
            "  epoch: 9/10,    batch: 10747/15469    Encoder_loss: 0.32010218501091003\n",
            "  epoch: 9/10,    batch: 10748/15469    Encoder_loss: 0.31981340050697327\n",
            "  epoch: 9/10,    batch: 10749/15469    Encoder_loss: 0.31864723563194275\n",
            "  epoch: 9/10,    batch: 10750/15469    Encoder_loss: 0.3192727565765381\n",
            "  epoch: 9/10,    batch: 10751/15469    Encoder_loss: 0.3182564079761505\n",
            "  epoch: 9/10,    batch: 10752/15469    Encoder_loss: 0.31797438859939575\n",
            "  epoch: 9/10,    batch: 10753/15469    Encoder_loss: 0.31745555996894836\n",
            "  epoch: 9/10,    batch: 10754/15469    Encoder_loss: 0.3161277174949646\n",
            "  epoch: 9/10,    batch: 10755/15469    Encoder_loss: 0.31758204102516174\n",
            "  epoch: 9/10,    batch: 10756/15469    Encoder_loss: 0.31659263372421265\n",
            "  epoch: 9/10,    batch: 10757/15469    Encoder_loss: 0.3139151930809021\n",
            "  epoch: 9/10,    batch: 10758/15469    Encoder_loss: 0.31456121802330017\n",
            "  epoch: 9/10,    batch: 10759/15469    Encoder_loss: 0.3130109906196594\n",
            "  epoch: 9/10,    batch: 10760/15469    Encoder_loss: 0.3152284026145935\n",
            "  epoch: 9/10,    batch: 10761/15469    Encoder_loss: 0.31365299224853516\n",
            "  epoch: 9/10,    batch: 10762/15469    Encoder_loss: 0.3837239742279053\n",
            "  epoch: 9/10,    batch: 10763/15469    Encoder_loss: 0.633172869682312\n",
            "  epoch: 9/10,    batch: 10764/15469    Encoder_loss: 0.7652475237846375\n",
            "  epoch: 9/10,    batch: 10765/15469    Encoder_loss: 0.6744577884674072\n",
            "  epoch: 9/10,    batch: 10766/15469    Encoder_loss: 0.5004605054855347\n",
            "  epoch: 9/10,    batch: 10767/15469    Encoder_loss: 0.4997439384460449\n",
            "  epoch: 9/10,    batch: 10768/15469    Encoder_loss: 0.4990818500518799\n",
            "  epoch: 9/10,    batch: 10769/15469    Encoder_loss: 0.4679834246635437\n",
            "  epoch: 9/10,    batch: 10770/15469    Encoder_loss: 0.3878992199897766\n",
            "  epoch: 9/10,    batch: 10771/15469    Encoder_loss: 0.3941655457019806\n",
            "  epoch: 9/10,    batch: 10772/15469    Encoder_loss: 0.33537209033966064\n",
            "  epoch: 9/10,    batch: 10773/15469    Encoder_loss: 0.31524020433425903\n",
            "  epoch: 9/10,    batch: 10774/15469    Encoder_loss: 0.3160052001476288\n",
            "  epoch: 9/10,    batch: 10775/15469    Encoder_loss: 0.3162519335746765\n",
            "  epoch: 9/10,    batch: 10776/15469    Encoder_loss: 0.31325846910476685\n",
            "  epoch: 9/10,    batch: 10777/15469    Encoder_loss: 0.3138752579689026\n",
            "  epoch: 9/10,    batch: 10778/15469    Encoder_loss: 0.316895067691803\n",
            "  epoch: 9/10,    batch: 10779/15469    Encoder_loss: 0.32433944940567017\n",
            "  epoch: 9/10,    batch: 10780/15469    Encoder_loss: 0.3300037086009979\n",
            "  epoch: 9/10,    batch: 10781/15469    Encoder_loss: 0.33083194494247437\n",
            "  epoch: 9/10,    batch: 10782/15469    Encoder_loss: 0.3300530016422272\n",
            "  epoch: 9/10,    batch: 10783/15469    Encoder_loss: 0.3288475573062897\n",
            "  epoch: 9/10,    batch: 10784/15469    Encoder_loss: 0.32593387365341187\n",
            "  epoch: 9/10,    batch: 10785/15469    Encoder_loss: 0.3256344199180603\n",
            "  epoch: 9/10,    batch: 10786/15469    Encoder_loss: 0.32668954133987427\n",
            "  epoch: 9/10,    batch: 10787/15469    Encoder_loss: 0.3249436020851135\n",
            "  epoch: 9/10,    batch: 10788/15469    Encoder_loss: 0.33709147572517395\n",
            "  epoch: 9/10,    batch: 10789/15469    Encoder_loss: 0.4084901213645935\n",
            "  epoch: 9/10,    batch: 10790/15469    Encoder_loss: 0.40888914465904236\n",
            "  epoch: 9/10,    batch: 10791/15469    Encoder_loss: 0.4093272089958191\n",
            "  epoch: 9/10,    batch: 10792/15469    Encoder_loss: 0.4842151701450348\n",
            "  epoch: 9/10,    batch: 10793/15469    Encoder_loss: 0.5081267356872559\n",
            "  epoch: 9/10,    batch: 10794/15469    Encoder_loss: 0.5079721212387085\n",
            "  epoch: 9/10,    batch: 10795/15469    Encoder_loss: 0.507690966129303\n",
            "  epoch: 9/10,    batch: 10796/15469    Encoder_loss: 0.5058620572090149\n",
            "  epoch: 9/10,    batch: 10797/15469    Encoder_loss: 0.5068835020065308\n",
            "  epoch: 9/10,    batch: 10798/15469    Encoder_loss: 0.506443202495575\n",
            "  epoch: 9/10,    batch: 10799/15469    Encoder_loss: 0.5077319741249084\n",
            "  epoch: 9/10,    batch: 10800/15469    Encoder_loss: 0.5100733041763306\n",
            "  epoch: 9/10,    batch: 10801/15469    Encoder_loss: 0.5083447098731995\n",
            "  epoch: 9/10,    batch: 10802/15469    Encoder_loss: 0.5103808641433716\n",
            "  epoch: 9/10,    batch: 10803/15469    Encoder_loss: 0.5096563100814819\n",
            "  epoch: 9/10,    batch: 10804/15469    Encoder_loss: 0.5106300115585327\n",
            "  epoch: 9/10,    batch: 10805/15469    Encoder_loss: 0.5124529600143433\n",
            "  epoch: 9/10,    batch: 10806/15469    Encoder_loss: 0.5127442479133606\n",
            "  epoch: 9/10,    batch: 10807/15469    Encoder_loss: 0.5136581063270569\n",
            "  epoch: 9/10,    batch: 10808/15469    Encoder_loss: 0.514735996723175\n",
            "  epoch: 9/10,    batch: 10809/15469    Encoder_loss: 0.5154153108596802\n",
            "  epoch: 9/10,    batch: 10810/15469    Encoder_loss: 0.5162587761878967\n",
            "  epoch: 9/10,    batch: 10811/15469    Encoder_loss: 0.5203328132629395\n",
            "  epoch: 9/10,    batch: 10812/15469    Encoder_loss: 0.5185513496398926\n",
            "  epoch: 9/10,    batch: 10813/15469    Encoder_loss: 0.5204699039459229\n",
            "  epoch: 9/10,    batch: 10814/15469    Encoder_loss: 0.5203880071640015\n",
            "  epoch: 9/10,    batch: 10815/15469    Encoder_loss: 0.5190366506576538\n",
            "  epoch: 9/10,    batch: 10816/15469    Encoder_loss: 0.5219912528991699\n",
            "  epoch: 9/10,    batch: 10817/15469    Encoder_loss: 0.5217117071151733\n",
            "  epoch: 9/10,    batch: 10818/15469    Encoder_loss: 0.5226716995239258\n",
            "  epoch: 9/10,    batch: 10819/15469    Encoder_loss: 0.5237151980400085\n",
            "  epoch: 9/10,    batch: 10820/15469    Encoder_loss: 0.5246574282646179\n",
            "  epoch: 9/10,    batch: 10821/15469    Encoder_loss: 0.5248991250991821\n",
            "  epoch: 9/10,    batch: 10822/15469    Encoder_loss: 0.5248188376426697\n",
            "  epoch: 9/10,    batch: 10823/15469    Encoder_loss: 0.5261611938476562\n",
            "  epoch: 9/10,    batch: 10824/15469    Encoder_loss: 0.5275941491127014\n",
            "  epoch: 9/10,    batch: 10825/15469    Encoder_loss: 0.5292176008224487\n",
            "  epoch: 9/10,    batch: 10826/15469    Encoder_loss: 0.5370553731918335\n",
            "  epoch: 9/10,    batch: 10827/15469    Encoder_loss: 0.5730528831481934\n",
            "  epoch: 9/10,    batch: 10828/15469    Encoder_loss: 0.5758177042007446\n",
            "  epoch: 9/10,    batch: 10829/15469    Encoder_loss: 0.5549424886703491\n",
            "  epoch: 9/10,    batch: 10830/15469    Encoder_loss: 0.5542128682136536\n",
            "  epoch: 9/10,    batch: 10831/15469    Encoder_loss: 0.4871489405632019\n",
            "  epoch: 9/10,    batch: 10832/15469    Encoder_loss: 0.4798491895198822\n",
            "  epoch: 9/10,    batch: 10833/15469    Encoder_loss: 0.4781184792518616\n",
            "  epoch: 9/10,    batch: 10834/15469    Encoder_loss: 0.47935834527015686\n",
            "  epoch: 9/10,    batch: 10835/15469    Encoder_loss: 0.47862130403518677\n",
            "  epoch: 9/10,    batch: 10836/15469    Encoder_loss: 0.4789118766784668\n",
            "  epoch: 9/10,    batch: 10837/15469    Encoder_loss: 0.47958508133888245\n",
            "  epoch: 9/10,    batch: 10838/15469    Encoder_loss: 0.47940611839294434\n",
            "  epoch: 9/10,    batch: 10839/15469    Encoder_loss: 0.47924375534057617\n",
            "  epoch: 9/10,    batch: 10840/15469    Encoder_loss: 0.47960740327835083\n",
            "  epoch: 9/10,    batch: 10841/15469    Encoder_loss: 0.47869881987571716\n",
            "  epoch: 9/10,    batch: 10842/15469    Encoder_loss: 0.48142361640930176\n",
            "  epoch: 9/10,    batch: 10843/15469    Encoder_loss: 0.48082488775253296\n",
            "  epoch: 9/10,    batch: 10844/15469    Encoder_loss: 0.4809345304965973\n",
            "  epoch: 9/10,    batch: 10845/15469    Encoder_loss: 0.4804662764072418\n",
            "  epoch: 9/10,    batch: 10846/15469    Encoder_loss: 0.4823465943336487\n",
            "  epoch: 9/10,    batch: 10847/15469    Encoder_loss: 0.4797864854335785\n",
            "  epoch: 9/10,    batch: 10848/15469    Encoder_loss: 0.4824613034725189\n",
            "  epoch: 9/10,    batch: 10849/15469    Encoder_loss: 0.48151150345802307\n",
            "  epoch: 9/10,    batch: 10850/15469    Encoder_loss: 0.48093563318252563\n",
            "  epoch: 9/10,    batch: 10851/15469    Encoder_loss: 0.4830794334411621\n",
            "  epoch: 9/10,    batch: 10852/15469    Encoder_loss: 0.48303088545799255\n",
            "  epoch: 9/10,    batch: 10853/15469    Encoder_loss: 0.48341694474220276\n",
            "  epoch: 9/10,    batch: 10854/15469    Encoder_loss: 0.48425784707069397\n",
            "  epoch: 9/10,    batch: 10855/15469    Encoder_loss: 0.48379355669021606\n",
            "  epoch: 9/10,    batch: 10856/15469    Encoder_loss: 0.48385754227638245\n",
            "  epoch: 9/10,    batch: 10857/15469    Encoder_loss: 0.483522891998291\n",
            "  epoch: 9/10,    batch: 10858/15469    Encoder_loss: 0.4916396737098694\n",
            "  epoch: 9/10,    batch: 10859/15469    Encoder_loss: 0.42917507886886597\n",
            "  epoch: 9/10,    batch: 10860/15469    Encoder_loss: 0.38458651304244995\n",
            "  epoch: 9/10,    batch: 10861/15469    Encoder_loss: 0.38505449891090393\n",
            "  epoch: 9/10,    batch: 10862/15469    Encoder_loss: 0.3838566541671753\n",
            "  epoch: 9/10,    batch: 10863/15469    Encoder_loss: 0.3816208243370056\n",
            "  epoch: 9/10,    batch: 10864/15469    Encoder_loss: 0.3798065781593323\n",
            "  epoch: 9/10,    batch: 10865/15469    Encoder_loss: 0.38038474321365356\n",
            "  epoch: 9/10,    batch: 10866/15469    Encoder_loss: 0.3803289532661438\n",
            "  epoch: 9/10,    batch: 10867/15469    Encoder_loss: 0.37869301438331604\n",
            "  epoch: 9/10,    batch: 10868/15469    Encoder_loss: 0.3778311312198639\n",
            "  epoch: 9/10,    batch: 10869/15469    Encoder_loss: 0.3756357729434967\n",
            "  epoch: 9/10,    batch: 10870/15469    Encoder_loss: 0.3742625415325165\n",
            "  epoch: 9/10,    batch: 10871/15469    Encoder_loss: 0.37376394867897034\n",
            "  epoch: 9/10,    batch: 10872/15469    Encoder_loss: 0.3723117709159851\n",
            "  epoch: 9/10,    batch: 10873/15469    Encoder_loss: 0.37115100026130676\n",
            "  epoch: 9/10,    batch: 10874/15469    Encoder_loss: 0.3713636100292206\n",
            "  epoch: 9/10,    batch: 10875/15469    Encoder_loss: 0.37158623337745667\n",
            "  epoch: 9/10,    batch: 10876/15469    Encoder_loss: 0.37082967162132263\n",
            "  epoch: 9/10,    batch: 10877/15469    Encoder_loss: 0.37002435326576233\n",
            "  epoch: 9/10,    batch: 10878/15469    Encoder_loss: 0.3690202534198761\n",
            "  epoch: 9/10,    batch: 10879/15469    Encoder_loss: 0.3684512972831726\n",
            "  epoch: 9/10,    batch: 10880/15469    Encoder_loss: 0.3681271970272064\n",
            "  epoch: 9/10,    batch: 10881/15469    Encoder_loss: 0.36721116304397583\n",
            "  epoch: 9/10,    batch: 10882/15469    Encoder_loss: 0.4278152883052826\n",
            "  epoch: 9/10,    batch: 10883/15469    Encoder_loss: 0.6683300137519836\n",
            "  epoch: 9/10,    batch: 10884/15469    Encoder_loss: 0.8828639984130859\n",
            "  epoch: 9/10,    batch: 10885/15469    Encoder_loss: 0.8260830640792847\n",
            "  epoch: 9/10,    batch: 10886/15469    Encoder_loss: 0.6463468670845032\n",
            "  epoch: 9/10,    batch: 10887/15469    Encoder_loss: 0.646181046962738\n",
            "  epoch: 9/10,    batch: 10888/15469    Encoder_loss: 0.6443005204200745\n",
            "  epoch: 9/10,    batch: 10889/15469    Encoder_loss: 0.6145109534263611\n",
            "  epoch: 9/10,    batch: 10890/15469    Encoder_loss: 0.5297898054122925\n",
            "  epoch: 9/10,    batch: 10891/15469    Encoder_loss: 0.5362429022789001\n",
            "  epoch: 9/10,    batch: 10892/15469    Encoder_loss: 0.4767497181892395\n",
            "  epoch: 9/10,    batch: 10893/15469    Encoder_loss: 0.4563935399055481\n",
            "  epoch: 9/10,    batch: 10894/15469    Encoder_loss: 0.456409752368927\n",
            "  epoch: 9/10,    batch: 10895/15469    Encoder_loss: 0.456702321767807\n",
            "  epoch: 9/10,    batch: 10896/15469    Encoder_loss: 0.45685139298439026\n",
            "  epoch: 9/10,    batch: 10897/15469    Encoder_loss: 0.45677104592323303\n",
            "  epoch: 9/10,    batch: 10898/15469    Encoder_loss: 0.46048250794410706\n",
            "  epoch: 9/10,    batch: 10899/15469    Encoder_loss: 0.46613040566444397\n",
            "  epoch: 9/10,    batch: 10900/15469    Encoder_loss: 0.4633501470088959\n",
            "  epoch: 9/10,    batch: 10901/15469    Encoder_loss: 0.4428810179233551\n",
            "  epoch: 9/10,    batch: 10902/15469    Encoder_loss: 0.44493746757507324\n",
            "  epoch: 9/10,    batch: 10903/15469    Encoder_loss: 0.44203123450279236\n",
            "  epoch: 9/10,    batch: 10904/15469    Encoder_loss: 0.44114041328430176\n",
            "  epoch: 9/10,    batch: 10905/15469    Encoder_loss: 0.44269880652427673\n",
            "  epoch: 9/10,    batch: 10906/15469    Encoder_loss: 0.4443742334842682\n",
            "  epoch: 9/10,    batch: 10907/15469    Encoder_loss: 0.44512009620666504\n",
            "  epoch: 9/10,    batch: 10908/15469    Encoder_loss: 0.4478756785392761\n",
            "  epoch: 9/10,    batch: 10909/15469    Encoder_loss: 0.44570451974868774\n",
            "  epoch: 9/10,    batch: 10910/15469    Encoder_loss: 0.44817957282066345\n",
            "  epoch: 9/10,    batch: 10911/15469    Encoder_loss: 0.44802799820899963\n",
            "  epoch: 9/10,    batch: 10912/15469    Encoder_loss: 0.44900161027908325\n",
            "  epoch: 9/10,    batch: 10913/15469    Encoder_loss: 0.4487825334072113\n",
            "  epoch: 9/10,    batch: 10914/15469    Encoder_loss: 0.44940105080604553\n",
            "  epoch: 9/10,    batch: 10915/15469    Encoder_loss: 0.45212045311927795\n",
            "  epoch: 9/10,    batch: 10916/15469    Encoder_loss: 0.4532925486564636\n",
            "  epoch: 9/10,    batch: 10917/15469    Encoder_loss: 0.45134177803993225\n",
            "  epoch: 9/10,    batch: 10918/15469    Encoder_loss: 0.469127357006073\n",
            "  epoch: 9/10,    batch: 10919/15469    Encoder_loss: 0.4888629913330078\n",
            "  epoch: 9/10,    batch: 10920/15469    Encoder_loss: 0.41596779227256775\n",
            "  epoch: 9/10,    batch: 10921/15469    Encoder_loss: 0.38997572660446167\n",
            "  epoch: 9/10,    batch: 10922/15469    Encoder_loss: 0.39007794857025146\n",
            "  epoch: 9/10,    batch: 10923/15469    Encoder_loss: 0.3909367322921753\n",
            "  epoch: 9/10,    batch: 10924/15469    Encoder_loss: 0.38941261172294617\n",
            "  epoch: 9/10,    batch: 10925/15469    Encoder_loss: 0.3887706995010376\n",
            "  epoch: 9/10,    batch: 10926/15469    Encoder_loss: 0.3865485191345215\n",
            "  epoch: 9/10,    batch: 10927/15469    Encoder_loss: 0.3861055374145508\n",
            "  epoch: 9/10,    batch: 10928/15469    Encoder_loss: 0.38462018966674805\n",
            "  epoch: 9/10,    batch: 10929/15469    Encoder_loss: 0.3832686245441437\n",
            "  epoch: 9/10,    batch: 10930/15469    Encoder_loss: 0.3833238482475281\n",
            "  epoch: 9/10,    batch: 10931/15469    Encoder_loss: 0.38159826397895813\n",
            "  epoch: 9/10,    batch: 10932/15469    Encoder_loss: 0.3819081485271454\n",
            "  epoch: 9/10,    batch: 10933/15469    Encoder_loss: 0.381120502948761\n",
            "  epoch: 9/10,    batch: 10934/15469    Encoder_loss: 0.3796918988227844\n",
            "  epoch: 9/10,    batch: 10935/15469    Encoder_loss: 0.37808772921562195\n",
            "  epoch: 9/10,    batch: 10936/15469    Encoder_loss: 0.378039687871933\n",
            "  epoch: 9/10,    batch: 10937/15469    Encoder_loss: 0.3777598738670349\n",
            "  epoch: 9/10,    batch: 10938/15469    Encoder_loss: 0.3755877912044525\n",
            "  epoch: 9/10,    batch: 10939/15469    Encoder_loss: 0.375929057598114\n",
            "  epoch: 9/10,    batch: 10940/15469    Encoder_loss: 0.4433029592037201\n",
            "  epoch: 9/10,    batch: 10941/15469    Encoder_loss: 0.45888203382492065\n",
            "  epoch: 9/10,    batch: 10942/15469    Encoder_loss: 0.5808366537094116\n",
            "  epoch: 9/10,    batch: 10943/15469    Encoder_loss: 0.8233559727668762\n",
            "  epoch: 9/10,    batch: 10944/15469    Encoder_loss: 0.9080020189285278\n",
            "  epoch: 9/10,    batch: 10945/15469    Encoder_loss: 0.7404334545135498\n",
            "  epoch: 9/10,    batch: 10946/15469    Encoder_loss: 0.6350866556167603\n",
            "  epoch: 9/10,    batch: 10947/15469    Encoder_loss: 0.635493278503418\n",
            "  epoch: 9/10,    batch: 10948/15469    Encoder_loss: 0.6319758296012878\n",
            "  epoch: 9/10,    batch: 10949/15469    Encoder_loss: 0.5701825618743896\n",
            "  epoch: 9/10,    batch: 10950/15469    Encoder_loss: 0.5160746574401855\n",
            "  epoch: 9/10,    batch: 10951/15469    Encoder_loss: 0.5154705047607422\n",
            "  epoch: 9/10,    batch: 10952/15469    Encoder_loss: 0.4466985762119293\n",
            "  epoch: 9/10,    batch: 10953/15469    Encoder_loss: 0.4398043155670166\n",
            "  epoch: 9/10,    batch: 10954/15469    Encoder_loss: 0.4379552900791168\n",
            "  epoch: 9/10,    batch: 10955/15469    Encoder_loss: 0.4364015460014343\n",
            "  epoch: 9/10,    batch: 10956/15469    Encoder_loss: 0.4379717707633972\n",
            "  epoch: 9/10,    batch: 10957/15469    Encoder_loss: 0.4364814758300781\n",
            "  epoch: 9/10,    batch: 10958/15469    Encoder_loss: 0.4392855167388916\n",
            "  epoch: 9/10,    batch: 10959/15469    Encoder_loss: 0.4453333020210266\n",
            "  epoch: 9/10,    batch: 10960/15469    Encoder_loss: 0.4488510489463806\n",
            "  epoch: 9/10,    batch: 10961/15469    Encoder_loss: 0.4498137831687927\n",
            "  epoch: 9/10,    batch: 10962/15469    Encoder_loss: 0.4143519401550293\n",
            "  epoch: 9/10,    batch: 10963/15469    Encoder_loss: 0.4134986996650696\n",
            "  epoch: 9/10,    batch: 10964/15469    Encoder_loss: 0.412823349237442\n",
            "  epoch: 9/10,    batch: 10965/15469    Encoder_loss: 0.42821988463401794\n",
            "  epoch: 9/10,    batch: 10966/15469    Encoder_loss: 0.5158363580703735\n",
            "  epoch: 9/10,    batch: 10967/15469    Encoder_loss: 0.512235164642334\n",
            "  epoch: 9/10,    batch: 10968/15469    Encoder_loss: 0.5143128633499146\n",
            "  epoch: 9/10,    batch: 10969/15469    Encoder_loss: 0.5147235989570618\n",
            "  epoch: 9/10,    batch: 10970/15469    Encoder_loss: 0.5147624611854553\n",
            "  epoch: 9/10,    batch: 10971/15469    Encoder_loss: 0.5146493315696716\n",
            "  epoch: 9/10,    batch: 10972/15469    Encoder_loss: 0.5154822468757629\n",
            "  epoch: 9/10,    batch: 10973/15469    Encoder_loss: 0.516680121421814\n",
            "  epoch: 9/10,    batch: 10974/15469    Encoder_loss: 0.5183025598526001\n",
            "  epoch: 9/10,    batch: 10975/15469    Encoder_loss: 0.5191498398780823\n",
            "  epoch: 9/10,    batch: 10976/15469    Encoder_loss: 0.5189352035522461\n",
            "  epoch: 9/10,    batch: 10977/15469    Encoder_loss: 0.5202175974845886\n",
            "  epoch: 9/10,    batch: 10978/15469    Encoder_loss: 0.5213906764984131\n",
            "  epoch: 9/10,    batch: 10979/15469    Encoder_loss: 0.5495761036872864\n",
            "  epoch: 9/10,    batch: 10980/15469    Encoder_loss: 0.5135798454284668\n",
            "  epoch: 9/10,    batch: 10981/15469    Encoder_loss: 0.5193711519241333\n",
            "  epoch: 9/10,    batch: 10982/15469    Encoder_loss: 0.4682510793209076\n",
            "  epoch: 9/10,    batch: 10983/15469    Encoder_loss: 0.4398430287837982\n",
            "  epoch: 9/10,    batch: 10984/15469    Encoder_loss: 0.4401659369468689\n",
            "  epoch: 9/10,    batch: 10985/15469    Encoder_loss: 0.43946871161460876\n",
            "  epoch: 9/10,    batch: 10986/15469    Encoder_loss: 0.4403145909309387\n",
            "  epoch: 9/10,    batch: 10987/15469    Encoder_loss: 0.4419695734977722\n",
            "  epoch: 9/10,    batch: 10988/15469    Encoder_loss: 0.44127464294433594\n",
            "  epoch: 9/10,    batch: 10989/15469    Encoder_loss: 0.44313326478004456\n",
            "  epoch: 9/10,    batch: 10990/15469    Encoder_loss: 0.4435694217681885\n",
            "  epoch: 9/10,    batch: 10991/15469    Encoder_loss: 0.44217729568481445\n",
            "  epoch: 9/10,    batch: 10992/15469    Encoder_loss: 0.4441036581993103\n",
            "  epoch: 9/10,    batch: 10993/15469    Encoder_loss: 0.4441370964050293\n",
            "  epoch: 9/10,    batch: 10994/15469    Encoder_loss: 0.4450515806674957\n",
            "  epoch: 9/10,    batch: 10995/15469    Encoder_loss: 0.4453917145729065\n",
            "  epoch: 9/10,    batch: 10996/15469    Encoder_loss: 0.445684015750885\n",
            "  epoch: 9/10,    batch: 10997/15469    Encoder_loss: 0.4476589262485504\n",
            "  epoch: 9/10,    batch: 10998/15469    Encoder_loss: 0.44788089394569397\n",
            "  epoch: 9/10,    batch: 10999/15469    Encoder_loss: 0.4503193795681\n",
            "  epoch: 9/10,    batch: 11000/15469    Encoder_loss: 0.45220446586608887\n",
            "  epoch: 9/10,    batch: 11001/15469    Encoder_loss: 0.45100682973861694\n",
            "  epoch: 9/10,    batch: 11002/15469    Encoder_loss: 0.4518919587135315\n",
            "  epoch: 9/10,    batch: 11003/15469    Encoder_loss: 0.45209646224975586\n",
            "  epoch: 9/10,    batch: 11004/15469    Encoder_loss: 0.4521467089653015\n",
            "  epoch: 9/10,    batch: 11005/15469    Encoder_loss: 0.4670979976654053\n",
            "  epoch: 9/10,    batch: 11006/15469    Encoder_loss: 0.48311930894851685\n",
            "  epoch: 9/10,    batch: 11007/15469    Encoder_loss: 0.48959165811538696\n",
            "  epoch: 9/10,    batch: 11008/15469    Encoder_loss: 0.4897828996181488\n",
            "  epoch: 9/10,    batch: 11009/15469    Encoder_loss: 0.4919285178184509\n",
            "  epoch: 9/10,    batch: 11010/15469    Encoder_loss: 0.486572265625\n",
            "  epoch: 9/10,    batch: 11011/15469    Encoder_loss: 0.4090602397918701\n",
            "  epoch: 9/10,    batch: 11012/15469    Encoder_loss: 0.3925534188747406\n",
            "  epoch: 9/10,    batch: 11013/15469    Encoder_loss: 0.3918269872665405\n",
            "  epoch: 9/10,    batch: 11014/15469    Encoder_loss: 0.39074593782424927\n",
            "  epoch: 9/10,    batch: 11015/15469    Encoder_loss: 0.38883617520332336\n",
            "  epoch: 9/10,    batch: 11016/15469    Encoder_loss: 0.3882806897163391\n",
            "  epoch: 9/10,    batch: 11017/15469    Encoder_loss: 0.3879384398460388\n",
            "  epoch: 9/10,    batch: 11018/15469    Encoder_loss: 0.3865085244178772\n",
            "  epoch: 9/10,    batch: 11019/15469    Encoder_loss: 0.38370949029922485\n",
            "  epoch: 9/10,    batch: 11020/15469    Encoder_loss: 0.3838425576686859\n",
            "  epoch: 9/10,    batch: 11021/15469    Encoder_loss: 0.38308194279670715\n",
            "  epoch: 9/10,    batch: 11022/15469    Encoder_loss: 0.38160306215286255\n",
            "  epoch: 9/10,    batch: 11023/15469    Encoder_loss: 0.3806423246860504\n",
            "  epoch: 9/10,    batch: 11024/15469    Encoder_loss: 0.3788791000843048\n",
            "  epoch: 9/10,    batch: 11025/15469    Encoder_loss: 0.37970244884490967\n",
            "  epoch: 9/10,    batch: 11026/15469    Encoder_loss: 0.3786540627479553\n",
            "  epoch: 9/10,    batch: 11027/15469    Encoder_loss: 0.37685319781303406\n",
            "  epoch: 9/10,    batch: 11028/15469    Encoder_loss: 0.3760804831981659\n",
            "  epoch: 9/10,    batch: 11029/15469    Encoder_loss: 0.3759656548500061\n",
            "  epoch: 9/10,    batch: 11030/15469    Encoder_loss: 0.37646248936653137\n",
            "  epoch: 9/10,    batch: 11031/15469    Encoder_loss: 0.3758857846260071\n",
            "  epoch: 9/10,    batch: 11032/15469    Encoder_loss: 0.3730749487876892\n",
            "  epoch: 9/10,    batch: 11033/15469    Encoder_loss: 0.3725457787513733\n",
            "  epoch: 9/10,    batch: 11034/15469    Encoder_loss: 0.37266841530799866\n",
            "  epoch: 9/10,    batch: 11035/15469    Encoder_loss: 0.37036341428756714\n",
            "  epoch: 9/10,    batch: 11036/15469    Encoder_loss: 0.3703853189945221\n",
            "  epoch: 9/10,    batch: 11037/15469    Encoder_loss: 0.3698834180831909\n",
            "  epoch: 9/10,    batch: 11038/15469    Encoder_loss: 0.36772671341896057\n",
            "  epoch: 9/10,    batch: 11039/15469    Encoder_loss: 0.3677266240119934\n",
            "  epoch: 9/10,    batch: 11040/15469    Encoder_loss: 0.3672613203525543\n",
            "  epoch: 9/10,    batch: 11041/15469    Encoder_loss: 0.3654646873474121\n",
            "  epoch: 9/10,    batch: 11042/15469    Encoder_loss: 0.3675544559955597\n",
            "  epoch: 9/10,    batch: 11043/15469    Encoder_loss: 0.36416393518447876\n",
            "  epoch: 9/10,    batch: 11044/15469    Encoder_loss: 0.3643639087677002\n",
            "  epoch: 9/10,    batch: 11045/15469    Encoder_loss: 0.3629377484321594\n",
            "  epoch: 9/10,    batch: 11046/15469    Encoder_loss: 0.3615524470806122\n",
            "  epoch: 9/10,    batch: 11047/15469    Encoder_loss: 0.36211544275283813\n",
            "  epoch: 9/10,    batch: 11048/15469    Encoder_loss: 0.36110934615135193\n",
            "  epoch: 9/10,    batch: 11049/15469    Encoder_loss: 0.3613266944885254\n",
            "  epoch: 9/10,    batch: 11050/15469    Encoder_loss: 0.35843393206596375\n",
            "  epoch: 9/10,    batch: 11051/15469    Encoder_loss: 0.36334237456321716\n",
            "  epoch: 9/10,    batch: 11052/15469    Encoder_loss: 0.41946810483932495\n",
            "  epoch: 9/10,    batch: 11053/15469    Encoder_loss: 0.4081966280937195\n",
            "  epoch: 9/10,    batch: 11054/15469    Encoder_loss: 0.40874743461608887\n",
            "  epoch: 9/10,    batch: 11055/15469    Encoder_loss: 0.47235235571861267\n",
            "  epoch: 9/10,    batch: 11056/15469    Encoder_loss: 0.5070308446884155\n",
            "  epoch: 9/10,    batch: 11057/15469    Encoder_loss: 0.5077320337295532\n",
            "  epoch: 9/10,    batch: 11058/15469    Encoder_loss: 0.5088706016540527\n",
            "  epoch: 9/10,    batch: 11059/15469    Encoder_loss: 0.508612334728241\n",
            "  epoch: 9/10,    batch: 11060/15469    Encoder_loss: 0.5072411298751831\n",
            "  epoch: 9/10,    batch: 11061/15469    Encoder_loss: 0.507804811000824\n",
            "  epoch: 9/10,    batch: 11062/15469    Encoder_loss: 0.5089084506034851\n",
            "  epoch: 9/10,    batch: 11063/15469    Encoder_loss: 0.5113717913627625\n",
            "  epoch: 9/10,    batch: 11064/15469    Encoder_loss: 0.5109282732009888\n",
            "  epoch: 9/10,    batch: 11065/15469    Encoder_loss: 0.5114040970802307\n",
            "  epoch: 9/10,    batch: 11066/15469    Encoder_loss: 0.5119301676750183\n",
            "  epoch: 9/10,    batch: 11067/15469    Encoder_loss: 0.5130167007446289\n",
            "  epoch: 9/10,    batch: 11068/15469    Encoder_loss: 0.5140549540519714\n",
            "  epoch: 9/10,    batch: 11069/15469    Encoder_loss: 0.5134038925170898\n",
            "  epoch: 9/10,    batch: 11070/15469    Encoder_loss: 0.5140482783317566\n",
            "  epoch: 9/10,    batch: 11071/15469    Encoder_loss: 0.5167032480239868\n",
            "  epoch: 9/10,    batch: 11072/15469    Encoder_loss: 0.5174130201339722\n",
            "  epoch: 9/10,    batch: 11073/15469    Encoder_loss: 0.5188661813735962\n",
            "  epoch: 9/10,    batch: 11074/15469    Encoder_loss: 0.5197876691818237\n",
            "  epoch: 9/10,    batch: 11075/15469    Encoder_loss: 0.5198420286178589\n",
            "  epoch: 9/10,    batch: 11076/15469    Encoder_loss: 0.5210509300231934\n",
            "  epoch: 9/10,    batch: 11077/15469    Encoder_loss: 0.5211580991744995\n",
            "  epoch: 9/10,    batch: 11078/15469    Encoder_loss: 0.5214809775352478\n",
            "  epoch: 9/10,    batch: 11079/15469    Encoder_loss: 0.5238826870918274\n",
            "  epoch: 9/10,    batch: 11080/15469    Encoder_loss: 0.5229969024658203\n",
            "  epoch: 9/10,    batch: 11081/15469    Encoder_loss: 0.5245710611343384\n",
            "  epoch: 9/10,    batch: 11082/15469    Encoder_loss: 0.5258108377456665\n",
            "  epoch: 9/10,    batch: 11083/15469    Encoder_loss: 0.5272253751754761\n",
            "  epoch: 9/10,    batch: 11084/15469    Encoder_loss: 0.526322066783905\n",
            "  epoch: 9/10,    batch: 11085/15469    Encoder_loss: 0.5284513235092163\n",
            "  epoch: 9/10,    batch: 11086/15469    Encoder_loss: 0.5316206812858582\n",
            "  epoch: 9/10,    batch: 11087/15469    Encoder_loss: 0.5308429598808289\n",
            "  epoch: 9/10,    batch: 11088/15469    Encoder_loss: 0.5307409763336182\n",
            "  epoch: 9/10,    batch: 11089/15469    Encoder_loss: 0.5420832633972168\n",
            "  epoch: 9/10,    batch: 11090/15469    Encoder_loss: 0.5610653162002563\n",
            "  epoch: 9/10,    batch: 11091/15469    Encoder_loss: 0.5973492860794067\n",
            "  epoch: 9/10,    batch: 11092/15469    Encoder_loss: 0.6174097061157227\n",
            "  epoch: 9/10,    batch: 11093/15469    Encoder_loss: 0.8657430410385132\n",
            "  epoch: 9/10,    batch: 11094/15469    Encoder_loss: 0.9690313339233398\n",
            "  epoch: 9/10,    batch: 11095/15469    Encoder_loss: 0.830690860748291\n",
            "  epoch: 9/10,    batch: 11096/15469    Encoder_loss: 0.6699011921882629\n",
            "  epoch: 9/10,    batch: 11097/15469    Encoder_loss: 0.6697585582733154\n",
            "  epoch: 9/10,    batch: 11098/15469    Encoder_loss: 0.6677419543266296\n",
            "  epoch: 9/10,    batch: 11099/15469    Encoder_loss: 0.6337393522262573\n",
            "  epoch: 9/10,    batch: 11100/15469    Encoder_loss: 0.5559824109077454\n",
            "  epoch: 9/10,    batch: 11101/15469    Encoder_loss: 0.5644112825393677\n",
            "  epoch: 9/10,    batch: 11102/15469    Encoder_loss: 0.5058085322380066\n",
            "  epoch: 9/10,    batch: 11103/15469    Encoder_loss: 0.485651433467865\n",
            "  epoch: 9/10,    batch: 11104/15469    Encoder_loss: 0.4860963821411133\n",
            "  epoch: 9/10,    batch: 11105/15469    Encoder_loss: 0.4859158396720886\n",
            "  epoch: 9/10,    batch: 11106/15469    Encoder_loss: 0.48509836196899414\n",
            "  epoch: 9/10,    batch: 11107/15469    Encoder_loss: 0.4871501326560974\n",
            "  epoch: 9/10,    batch: 11108/15469    Encoder_loss: 0.4890912175178528\n",
            "  epoch: 9/10,    batch: 11109/15469    Encoder_loss: 0.4956267476081848\n",
            "  epoch: 9/10,    batch: 11110/15469    Encoder_loss: 0.5004639625549316\n",
            "  epoch: 9/10,    batch: 11111/15469    Encoder_loss: 0.5044428110122681\n",
            "  epoch: 9/10,    batch: 11112/15469    Encoder_loss: 0.502856969833374\n",
            "  epoch: 9/10,    batch: 11113/15469    Encoder_loss: 0.5016974210739136\n",
            "  epoch: 9/10,    batch: 11114/15469    Encoder_loss: 0.5015575289726257\n",
            "  epoch: 9/10,    batch: 11115/15469    Encoder_loss: 0.4995599389076233\n",
            "  epoch: 9/10,    batch: 11116/15469    Encoder_loss: 0.4991784989833832\n",
            "  epoch: 9/10,    batch: 11117/15469    Encoder_loss: 0.5000534057617188\n",
            "  epoch: 9/10,    batch: 11118/15469    Encoder_loss: 0.49941444396972656\n",
            "  epoch: 9/10,    batch: 11119/15469    Encoder_loss: 0.5001327991485596\n",
            "  epoch: 9/10,    batch: 11120/15469    Encoder_loss: 0.4993213713169098\n",
            "  epoch: 9/10,    batch: 11121/15469    Encoder_loss: 0.49926233291625977\n",
            "  epoch: 9/10,    batch: 11122/15469    Encoder_loss: 0.5022627115249634\n",
            "  epoch: 9/10,    batch: 11123/15469    Encoder_loss: 0.4222620129585266\n",
            "  epoch: 9/10,    batch: 11124/15469    Encoder_loss: 0.39730381965637207\n",
            "  epoch: 9/10,    batch: 11125/15469    Encoder_loss: 0.3971458673477173\n",
            "  epoch: 9/10,    batch: 11126/15469    Encoder_loss: 0.3964044153690338\n",
            "  epoch: 9/10,    batch: 11127/15469    Encoder_loss: 0.3945978283882141\n",
            "  epoch: 9/10,    batch: 11128/15469    Encoder_loss: 0.39380788803100586\n",
            "  epoch: 9/10,    batch: 11129/15469    Encoder_loss: 0.392447292804718\n",
            "  epoch: 9/10,    batch: 11130/15469    Encoder_loss: 0.391949862241745\n",
            "  epoch: 9/10,    batch: 11131/15469    Encoder_loss: 0.38905802369117737\n",
            "  epoch: 9/10,    batch: 11132/15469    Encoder_loss: 0.3884388506412506\n",
            "  epoch: 9/10,    batch: 11133/15469    Encoder_loss: 0.3870231807231903\n",
            "  epoch: 9/10,    batch: 11134/15469    Encoder_loss: 0.3861832618713379\n",
            "  epoch: 9/10,    batch: 11135/15469    Encoder_loss: 0.3853091597557068\n",
            "  epoch: 9/10,    batch: 11136/15469    Encoder_loss: 0.3840835690498352\n",
            "  epoch: 9/10,    batch: 11137/15469    Encoder_loss: 0.38116613030433655\n",
            "  epoch: 9/10,    batch: 11138/15469    Encoder_loss: 0.3809848129749298\n",
            "  epoch: 9/10,    batch: 11139/15469    Encoder_loss: 0.37851643562316895\n",
            "  epoch: 9/10,    batch: 11140/15469    Encoder_loss: 0.3791255056858063\n",
            "  epoch: 9/10,    batch: 11141/15469    Encoder_loss: 0.37767812609672546\n",
            "  epoch: 9/10,    batch: 11142/15469    Encoder_loss: 0.37669023871421814\n",
            "  epoch: 9/10,    batch: 11143/15469    Encoder_loss: 0.3751448690891266\n",
            "  epoch: 9/10,    batch: 11144/15469    Encoder_loss: 0.3737211227416992\n",
            "  epoch: 9/10,    batch: 11145/15469    Encoder_loss: 0.37269049882888794\n",
            "  epoch: 9/10,    batch: 11146/15469    Encoder_loss: 0.3716495931148529\n",
            "  epoch: 9/10,    batch: 11147/15469    Encoder_loss: 0.3711496889591217\n",
            "  epoch: 9/10,    batch: 11148/15469    Encoder_loss: 0.36848878860473633\n",
            "  epoch: 9/10,    batch: 11149/15469    Encoder_loss: 0.36800599098205566\n",
            "  epoch: 9/10,    batch: 11150/15469    Encoder_loss: 0.36712446808815\n",
            "  epoch: 9/10,    batch: 11151/15469    Encoder_loss: 0.36491328477859497\n",
            "  epoch: 9/10,    batch: 11152/15469    Encoder_loss: 0.3640243411064148\n",
            "  epoch: 9/10,    batch: 11153/15469    Encoder_loss: 0.363153874874115\n",
            "  epoch: 9/10,    batch: 11154/15469    Encoder_loss: 0.3617735803127289\n",
            "  epoch: 9/10,    batch: 11155/15469    Encoder_loss: 0.3605830669403076\n",
            "  epoch: 9/10,    batch: 11156/15469    Encoder_loss: 0.35957765579223633\n",
            "  epoch: 9/10,    batch: 11157/15469    Encoder_loss: 0.35681307315826416\n",
            "  epoch: 9/10,    batch: 11158/15469    Encoder_loss: 0.3571321368217468\n",
            "  epoch: 9/10,    batch: 11159/15469    Encoder_loss: 0.3567793071269989\n",
            "  epoch: 9/10,    batch: 11160/15469    Encoder_loss: 0.3566821813583374\n",
            "  epoch: 9/10,    batch: 11161/15469    Encoder_loss: 0.35471111536026\n",
            "  epoch: 9/10,    batch: 11162/15469    Encoder_loss: 0.35539084672927856\n",
            "  epoch: 9/10,    batch: 11163/15469    Encoder_loss: 0.35428187251091003\n",
            "  epoch: 9/10,    batch: 11164/15469    Encoder_loss: 0.35315921902656555\n",
            "  epoch: 9/10,    batch: 11165/15469    Encoder_loss: 0.31775718927383423\n",
            "  epoch: 9/10,    batch: 11166/15469    Encoder_loss: 0.31768831610679626\n",
            "  epoch: 9/10,    batch: 11167/15469    Encoder_loss: 0.3169703483581543\n",
            "  epoch: 9/10,    batch: 11168/15469    Encoder_loss: 0.3158304691314697\n",
            "  epoch: 9/10,    batch: 11169/15469    Encoder_loss: 0.3167819082736969\n",
            "  epoch: 9/10,    batch: 11170/15469    Encoder_loss: 0.3144497275352478\n",
            "  epoch: 9/10,    batch: 11171/15469    Encoder_loss: 0.31549271941185\n",
            "  epoch: 9/10,    batch: 11172/15469    Encoder_loss: 0.3132097125053406\n",
            "  epoch: 9/10,    batch: 11173/15469    Encoder_loss: 0.31296417117118835\n",
            "  epoch: 9/10,    batch: 11174/15469    Encoder_loss: 0.31326061487197876\n",
            "  epoch: 9/10,    batch: 11175/15469    Encoder_loss: 0.3130417764186859\n",
            "  epoch: 9/10,    batch: 11176/15469    Encoder_loss: 0.3115794062614441\n",
            "  epoch: 9/10,    batch: 11177/15469    Encoder_loss: 0.31161072850227356\n",
            "  epoch: 9/10,    batch: 11178/15469    Encoder_loss: 0.3109275698661804\n",
            "  epoch: 9/10,    batch: 11179/15469    Encoder_loss: 0.31178832054138184\n",
            "  epoch: 9/10,    batch: 11180/15469    Encoder_loss: 0.31176719069480896\n",
            "  epoch: 9/10,    batch: 11181/15469    Encoder_loss: 0.31009382009506226\n",
            "  epoch: 9/10,    batch: 11182/15469    Encoder_loss: 0.30894866585731506\n",
            "  epoch: 9/10,    batch: 11183/15469    Encoder_loss: 0.3087937831878662\n",
            "  epoch: 9/10,    batch: 11184/15469    Encoder_loss: 0.3085615038871765\n",
            "  epoch: 9/10,    batch: 11185/15469    Encoder_loss: 0.3088056445121765\n",
            "  epoch: 9/10,    batch: 11186/15469    Encoder_loss: 0.30885016918182373\n",
            "  epoch: 9/10,    batch: 11187/15469    Encoder_loss: 0.30900588631629944\n",
            "  epoch: 9/10,    batch: 11188/15469    Encoder_loss: 0.30696552991867065\n",
            "  epoch: 9/10,    batch: 11189/15469    Encoder_loss: 0.3070007562637329\n",
            "  epoch: 9/10,    batch: 11190/15469    Encoder_loss: 0.3081803321838379\n",
            "  epoch: 9/10,    batch: 11191/15469    Encoder_loss: 0.3077008128166199\n",
            "  epoch: 9/10,    batch: 11192/15469    Encoder_loss: 0.3062986731529236\n",
            "  epoch: 9/10,    batch: 11193/15469    Encoder_loss: 0.30629295110702515\n",
            "  epoch: 9/10,    batch: 11194/15469    Encoder_loss: 0.3065222203731537\n",
            "  epoch: 9/10,    batch: 11195/15469    Encoder_loss: 0.3063426613807678\n",
            "  epoch: 9/10,    batch: 11196/15469    Encoder_loss: 0.30585619807243347\n",
            "  epoch: 9/10,    batch: 11197/15469    Encoder_loss: 0.3068210184574127\n",
            "  epoch: 9/10,    batch: 11198/15469    Encoder_loss: 0.3061525225639343\n",
            "  epoch: 9/10,    batch: 11199/15469    Encoder_loss: 0.30409854650497437\n",
            "  epoch: 9/10,    batch: 11200/15469    Encoder_loss: 0.30499130487442017\n",
            "  epoch: 9/10,    batch: 11201/15469    Encoder_loss: 0.3054831027984619\n",
            "  epoch: 9/10,    batch: 11202/15469    Encoder_loss: 0.30406779050827026\n",
            "  epoch: 9/10,    batch: 11203/15469    Encoder_loss: 0.30501648783683777\n",
            "  epoch: 9/10,    batch: 11204/15469    Encoder_loss: 0.3340238332748413\n",
            "  epoch: 9/10,    batch: 11205/15469    Encoder_loss: 0.5363363027572632\n",
            "  epoch: 9/10,    batch: 11206/15469    Encoder_loss: 0.738924503326416\n",
            "  epoch: 9/10,    batch: 11207/15469    Encoder_loss: 0.7175500392913818\n",
            "  epoch: 9/10,    batch: 11208/15469    Encoder_loss: 0.5068167448043823\n",
            "  epoch: 9/10,    batch: 11209/15469    Encoder_loss: 0.490764856338501\n",
            "  epoch: 9/10,    batch: 11210/15469    Encoder_loss: 0.4907103478908539\n",
            "  epoch: 9/10,    batch: 11211/15469    Encoder_loss: 0.47793856263160706\n",
            "  epoch: 9/10,    batch: 11212/15469    Encoder_loss: 0.44536781311035156\n",
            "  epoch: 9/10,    batch: 11213/15469    Encoder_loss: 0.6791725754737854\n",
            "  epoch: 9/10,    batch: 11214/15469    Encoder_loss: 0.829079270362854\n",
            "  epoch: 9/10,    batch: 11215/15469    Encoder_loss: 0.7681764364242554\n",
            "  epoch: 9/10,    batch: 11216/15469    Encoder_loss: 0.5928340554237366\n",
            "  epoch: 9/10,    batch: 11217/15469    Encoder_loss: 0.5907131433486938\n",
            "  epoch: 9/10,    batch: 11218/15469    Encoder_loss: 0.5897186994552612\n",
            "  epoch: 9/10,    batch: 11219/15469    Encoder_loss: 0.5607874393463135\n",
            "  epoch: 9/10,    batch: 11220/15469    Encoder_loss: 0.4795044958591461\n",
            "  epoch: 9/10,    batch: 11221/15469    Encoder_loss: 0.494594007730484\n",
            "  epoch: 9/10,    batch: 11222/15469    Encoder_loss: 0.44477978348731995\n",
            "  epoch: 9/10,    batch: 11223/15469    Encoder_loss: 0.42467036843299866\n",
            "  epoch: 9/10,    batch: 11224/15469    Encoder_loss: 0.4251013398170471\n",
            "  epoch: 9/10,    batch: 11225/15469    Encoder_loss: 0.42271023988723755\n",
            "  epoch: 9/10,    batch: 11226/15469    Encoder_loss: 0.42374569177627563\n",
            "  epoch: 9/10,    batch: 11227/15469    Encoder_loss: 0.42596086859703064\n",
            "  epoch: 9/10,    batch: 11228/15469    Encoder_loss: 0.4234418272972107\n",
            "  epoch: 9/10,    batch: 11229/15469    Encoder_loss: 0.4291796088218689\n",
            "  epoch: 9/10,    batch: 11230/15469    Encoder_loss: 0.43517622351646423\n",
            "  epoch: 9/10,    batch: 11231/15469    Encoder_loss: 0.4416332244873047\n",
            "  epoch: 9/10,    batch: 11232/15469    Encoder_loss: 0.441034734249115\n",
            "  epoch: 9/10,    batch: 11233/15469    Encoder_loss: 0.43840956687927246\n",
            "  epoch: 9/10,    batch: 11234/15469    Encoder_loss: 0.4395173192024231\n",
            "  epoch: 9/10,    batch: 11235/15469    Encoder_loss: 0.43854326009750366\n",
            "  epoch: 9/10,    batch: 11236/15469    Encoder_loss: 0.4384785592556\n",
            "  epoch: 9/10,    batch: 11237/15469    Encoder_loss: 0.4393951892852783\n",
            "  epoch: 9/10,    batch: 11238/15469    Encoder_loss: 0.44070354104042053\n",
            "  epoch: 9/10,    batch: 11239/15469    Encoder_loss: 0.4426942765712738\n",
            "  epoch: 9/10,    batch: 11240/15469    Encoder_loss: 0.44367632269859314\n",
            "  epoch: 9/10,    batch: 11241/15469    Encoder_loss: 0.4445721507072449\n",
            "  epoch: 9/10,    batch: 11242/15469    Encoder_loss: 0.44494467973709106\n",
            "  epoch: 9/10,    batch: 11243/15469    Encoder_loss: 0.4458974301815033\n",
            "  epoch: 9/10,    batch: 11244/15469    Encoder_loss: 0.44789376854896545\n",
            "  epoch: 9/10,    batch: 11245/15469    Encoder_loss: 0.4476791322231293\n",
            "  epoch: 9/10,    batch: 11246/15469    Encoder_loss: 0.44818124175071716\n",
            "  epoch: 9/10,    batch: 11247/15469    Encoder_loss: 0.45054060220718384\n",
            "  epoch: 9/10,    batch: 11248/15469    Encoder_loss: 0.4534513056278229\n",
            "  epoch: 9/10,    batch: 11249/15469    Encoder_loss: 0.4736115038394928\n",
            "  epoch: 9/10,    batch: 11250/15469    Encoder_loss: 0.3974902033805847\n",
            "  epoch: 9/10,    batch: 11251/15469    Encoder_loss: 0.3872009515762329\n",
            "  epoch: 9/10,    batch: 11252/15469    Encoder_loss: 0.3860689103603363\n",
            "  epoch: 9/10,    batch: 11253/15469    Encoder_loss: 0.3857605755329132\n",
            "  epoch: 9/10,    batch: 11254/15469    Encoder_loss: 0.38339048624038696\n",
            "  epoch: 9/10,    batch: 11255/15469    Encoder_loss: 0.38161271810531616\n",
            "  epoch: 9/10,    batch: 11256/15469    Encoder_loss: 0.3811948299407959\n",
            "  epoch: 9/10,    batch: 11257/15469    Encoder_loss: 0.3798687756061554\n",
            "  epoch: 9/10,    batch: 11258/15469    Encoder_loss: 0.37988799810409546\n",
            "  epoch: 9/10,    batch: 11259/15469    Encoder_loss: 0.3790596127510071\n",
            "  epoch: 9/10,    batch: 11260/15469    Encoder_loss: 0.3795033097267151\n",
            "  epoch: 9/10,    batch: 11261/15469    Encoder_loss: 0.37903183698654175\n",
            "  epoch: 9/10,    batch: 11262/15469    Encoder_loss: 0.3768707811832428\n",
            "  epoch: 9/10,    batch: 11263/15469    Encoder_loss: 0.37566232681274414\n",
            "  epoch: 9/10,    batch: 11264/15469    Encoder_loss: 0.3747170567512512\n",
            "  epoch: 9/10,    batch: 11265/15469    Encoder_loss: 0.37370750308036804\n",
            "  epoch: 9/10,    batch: 11266/15469    Encoder_loss: 0.41773051023483276\n",
            "  epoch: 9/10,    batch: 11267/15469    Encoder_loss: 0.4550299644470215\n",
            "  epoch: 9/10,    batch: 11268/15469    Encoder_loss: 0.45483535528182983\n",
            "  epoch: 9/10,    batch: 11269/15469    Encoder_loss: 0.4516375660896301\n",
            "  epoch: 9/10,    batch: 11270/15469    Encoder_loss: 0.44973474740982056\n",
            "  epoch: 9/10,    batch: 11271/15469    Encoder_loss: 0.4473764896392822\n",
            "  epoch: 9/10,    batch: 11272/15469    Encoder_loss: 0.44523030519485474\n",
            "  epoch: 9/10,    batch: 11273/15469    Encoder_loss: 0.44429436326026917\n",
            "  epoch: 9/10,    batch: 11274/15469    Encoder_loss: 0.4407164454460144\n",
            "  epoch: 9/10,    batch: 11275/15469    Encoder_loss: 0.4402526915073395\n",
            "  epoch: 9/10,    batch: 11276/15469    Encoder_loss: 0.43869104981422424\n",
            "  epoch: 9/10,    batch: 11277/15469    Encoder_loss: 0.43847742676734924\n",
            "  epoch: 9/10,    batch: 11278/15469    Encoder_loss: 0.43690061569213867\n",
            "  epoch: 9/10,    batch: 11279/15469    Encoder_loss: 0.43381571769714355\n",
            "  epoch: 9/10,    batch: 11280/15469    Encoder_loss: 0.433826208114624\n",
            "  epoch: 9/10,    batch: 11281/15469    Encoder_loss: 0.43422508239746094\n",
            "  epoch: 9/10,    batch: 11282/15469    Encoder_loss: 0.4317972660064697\n",
            "  epoch: 9/10,    batch: 11283/15469    Encoder_loss: 0.43093419075012207\n",
            "  epoch: 9/10,    batch: 11284/15469    Encoder_loss: 0.42968493700027466\n",
            "  epoch: 9/10,    batch: 11285/15469    Encoder_loss: 0.42853713035583496\n",
            "  epoch: 9/10,    batch: 11286/15469    Encoder_loss: 0.4292794167995453\n",
            "  epoch: 9/10,    batch: 11287/15469    Encoder_loss: 0.4294764995574951\n",
            "  epoch: 9/10,    batch: 11288/15469    Encoder_loss: 0.4276804029941559\n",
            "  epoch: 9/10,    batch: 11289/15469    Encoder_loss: 0.42863598465919495\n",
            "  epoch: 9/10,    batch: 11290/15469    Encoder_loss: 0.4340439438819885\n",
            "  epoch: 9/10,    batch: 11291/15469    Encoder_loss: 0.4005805552005768\n",
            "  epoch: 9/10,    batch: 11292/15469    Encoder_loss: 0.4672729969024658\n",
            "  epoch: 9/10,    batch: 11293/15469    Encoder_loss: 0.4957381784915924\n",
            "  epoch: 9/10,    batch: 11294/15469    Encoder_loss: 0.496997594833374\n",
            "  epoch: 9/10,    batch: 11295/15469    Encoder_loss: 0.49931010603904724\n",
            "  epoch: 9/10,    batch: 11296/15469    Encoder_loss: 0.4990600049495697\n",
            "  epoch: 9/10,    batch: 11297/15469    Encoder_loss: 0.5001521706581116\n",
            "  epoch: 9/10,    batch: 11298/15469    Encoder_loss: 0.5000156164169312\n",
            "  epoch: 9/10,    batch: 11299/15469    Encoder_loss: 0.5003780722618103\n",
            "  epoch: 9/10,    batch: 11300/15469    Encoder_loss: 0.5036570429801941\n",
            "  epoch: 9/10,    batch: 11301/15469    Encoder_loss: 0.5036672949790955\n",
            "  epoch: 9/10,    batch: 11302/15469    Encoder_loss: 0.5051045417785645\n",
            "  epoch: 9/10,    batch: 11303/15469    Encoder_loss: 0.5053023099899292\n",
            "  epoch: 9/10,    batch: 11304/15469    Encoder_loss: 0.5064108967781067\n",
            "  epoch: 9/10,    batch: 11305/15469    Encoder_loss: 0.5348094701766968\n",
            "  epoch: 9/10,    batch: 11306/15469    Encoder_loss: 0.5008031129837036\n",
            "  epoch: 9/10,    batch: 11307/15469    Encoder_loss: 0.5069252252578735\n",
            "  epoch: 9/10,    batch: 11308/15469    Encoder_loss: 0.45442742109298706\n",
            "  epoch: 9/10,    batch: 11309/15469    Encoder_loss: 0.4261177182197571\n",
            "  epoch: 9/10,    batch: 11310/15469    Encoder_loss: 0.427638441324234\n",
            "  epoch: 9/10,    batch: 11311/15469    Encoder_loss: 0.4286312460899353\n",
            "  epoch: 9/10,    batch: 11312/15469    Encoder_loss: 0.42986851930618286\n",
            "  epoch: 9/10,    batch: 11313/15469    Encoder_loss: 0.4290052354335785\n",
            "  epoch: 9/10,    batch: 11314/15469    Encoder_loss: 0.49000993371009827\n",
            "  epoch: 9/10,    batch: 11315/15469    Encoder_loss: 0.5157004594802856\n",
            "  epoch: 9/10,    batch: 11316/15469    Encoder_loss: 0.5166125297546387\n",
            "  epoch: 9/10,    batch: 11317/15469    Encoder_loss: 0.515434205532074\n",
            "  epoch: 9/10,    batch: 11318/15469    Encoder_loss: 0.5878432989120483\n",
            "  epoch: 9/10,    batch: 11319/15469    Encoder_loss: 0.6182850003242493\n",
            "  epoch: 9/10,    batch: 11320/15469    Encoder_loss: 0.6202958822250366\n",
            "  epoch: 9/10,    batch: 11321/15469    Encoder_loss: 0.6224048137664795\n",
            "  epoch: 9/10,    batch: 11322/15469    Encoder_loss: 0.6232320070266724\n",
            "  epoch: 9/10,    batch: 11323/15469    Encoder_loss: 0.6236947774887085\n",
            "  epoch: 9/10,    batch: 11324/15469    Encoder_loss: 0.6260843276977539\n",
            "  epoch: 9/10,    batch: 11325/15469    Encoder_loss: 0.6275960803031921\n",
            "  epoch: 9/10,    batch: 11326/15469    Encoder_loss: 0.6303291320800781\n",
            "  epoch: 9/10,    batch: 11327/15469    Encoder_loss: 0.63066565990448\n",
            "  epoch: 9/10,    batch: 11328/15469    Encoder_loss: 0.6343098878860474\n",
            "  epoch: 9/10,    batch: 11329/15469    Encoder_loss: 0.6359453201293945\n",
            "  epoch: 9/10,    batch: 11330/15469    Encoder_loss: 0.6372305154800415\n",
            "  epoch: 9/10,    batch: 11331/15469    Encoder_loss: 0.6387448310852051\n",
            "  epoch: 9/10,    batch: 11332/15469    Encoder_loss: 0.667841374874115\n",
            "  epoch: 9/10,    batch: 11333/15469    Encoder_loss: 0.6768115162849426\n",
            "  epoch: 9/10,    batch: 11334/15469    Encoder_loss: 0.6798729300498962\n",
            "  epoch: 9/10,    batch: 11335/15469    Encoder_loss: 0.6805927753448486\n",
            "  epoch: 9/10,    batch: 11336/15469    Encoder_loss: 0.6815489530563354\n",
            "  epoch: 9/10,    batch: 11337/15469    Encoder_loss: 0.598646342754364\n",
            "  epoch: 9/10,    batch: 11338/15469    Encoder_loss: 0.5851011276245117\n",
            "  epoch: 9/10,    batch: 11339/15469    Encoder_loss: 0.5854879021644592\n",
            "  epoch: 9/10,    batch: 11340/15469    Encoder_loss: 0.5856986045837402\n",
            "  epoch: 9/10,    batch: 11341/15469    Encoder_loss: 0.585713803768158\n",
            "  epoch: 9/10,    batch: 11342/15469    Encoder_loss: 0.5867564082145691\n",
            "  epoch: 9/10,    batch: 11343/15469    Encoder_loss: 0.5871885418891907\n",
            "  epoch: 9/10,    batch: 11344/15469    Encoder_loss: 0.5876255631446838\n",
            "  epoch: 9/10,    batch: 11345/15469    Encoder_loss: 0.5885264277458191\n",
            "  epoch: 9/10,    batch: 11346/15469    Encoder_loss: 0.5883681774139404\n",
            "  epoch: 9/10,    batch: 11347/15469    Encoder_loss: 0.5883451700210571\n",
            "  epoch: 9/10,    batch: 11348/15469    Encoder_loss: 0.5884757041931152\n",
            "  epoch: 9/10,    batch: 11349/15469    Encoder_loss: 0.5891210436820984\n",
            "  epoch: 9/10,    batch: 11350/15469    Encoder_loss: 0.5892595648765564\n",
            "  epoch: 9/10,    batch: 11351/15469    Encoder_loss: 0.5881376266479492\n",
            "  epoch: 9/10,    batch: 11352/15469    Encoder_loss: 0.6055938005447388\n",
            "  epoch: 9/10,    batch: 11353/15469    Encoder_loss: 0.628960907459259\n",
            "  epoch: 9/10,    batch: 11354/15469    Encoder_loss: 0.6380449533462524\n",
            "  epoch: 9/10,    batch: 11355/15469    Encoder_loss: 0.6130945682525635\n",
            "  epoch: 9/10,    batch: 11356/15469    Encoder_loss: 0.6152105331420898\n",
            "  epoch: 9/10,    batch: 11357/15469    Encoder_loss: 0.551522433757782\n",
            "  epoch: 9/10,    batch: 11358/15469    Encoder_loss: 0.5352693200111389\n",
            "  epoch: 9/10,    batch: 11359/15469    Encoder_loss: 0.5331636667251587\n",
            "  epoch: 9/10,    batch: 11360/15469    Encoder_loss: 0.5345431566238403\n",
            "  epoch: 9/10,    batch: 11361/15469    Encoder_loss: 0.5324586629867554\n",
            "  epoch: 9/10,    batch: 11362/15469    Encoder_loss: 0.532704770565033\n",
            "  epoch: 9/10,    batch: 11363/15469    Encoder_loss: 0.5300920009613037\n",
            "  epoch: 9/10,    batch: 11364/15469    Encoder_loss: 0.5296415090560913\n",
            "  epoch: 9/10,    batch: 11365/15469    Encoder_loss: 0.5300719141960144\n",
            "  epoch: 9/10,    batch: 11366/15469    Encoder_loss: 0.5304888486862183\n",
            "  epoch: 9/10,    batch: 11367/15469    Encoder_loss: 0.5301647186279297\n",
            "  epoch: 9/10,    batch: 11368/15469    Encoder_loss: 0.5280822515487671\n",
            "  epoch: 9/10,    batch: 11369/15469    Encoder_loss: 0.5284225940704346\n",
            "  epoch: 9/10,    batch: 11370/15469    Encoder_loss: 0.5289446115493774\n",
            "  epoch: 9/10,    batch: 11371/15469    Encoder_loss: 0.5281994342803955\n",
            "  epoch: 9/10,    batch: 11372/15469    Encoder_loss: 0.5285055637359619\n",
            "  epoch: 9/10,    batch: 11373/15469    Encoder_loss: 0.528095543384552\n",
            "  epoch: 9/10,    batch: 11374/15469    Encoder_loss: 0.5280657410621643\n",
            "  epoch: 9/10,    batch: 11375/15469    Encoder_loss: 0.5286101698875427\n",
            "  epoch: 9/10,    batch: 11376/15469    Encoder_loss: 0.5264458060264587\n",
            "  epoch: 9/10,    batch: 11377/15469    Encoder_loss: 0.52904212474823\n",
            "  epoch: 9/10,    batch: 11378/15469    Encoder_loss: 0.517876386642456\n",
            "  epoch: 9/10,    batch: 11379/15469    Encoder_loss: 0.4927200675010681\n",
            "  epoch: 9/10,    batch: 11380/15469    Encoder_loss: 0.4936322569847107\n",
            "  epoch: 9/10,    batch: 11381/15469    Encoder_loss: 0.4922245144844055\n",
            "  epoch: 9/10,    batch: 11382/15469    Encoder_loss: 0.4929688572883606\n",
            "  epoch: 9/10,    batch: 11383/15469    Encoder_loss: 0.494626522064209\n",
            "  epoch: 9/10,    batch: 11384/15469    Encoder_loss: 0.49479764699935913\n",
            "  epoch: 9/10,    batch: 11385/15469    Encoder_loss: 0.4926389753818512\n",
            "  epoch: 9/10,    batch: 11386/15469    Encoder_loss: 0.4069995880126953\n",
            "  epoch: 9/10,    batch: 11387/15469    Encoder_loss: 0.39368200302124023\n",
            "  epoch: 9/10,    batch: 11388/15469    Encoder_loss: 0.39262571930885315\n",
            "  epoch: 9/10,    batch: 11389/15469    Encoder_loss: 0.392531156539917\n",
            "  epoch: 9/10,    batch: 11390/15469    Encoder_loss: 0.3915240168571472\n",
            "  epoch: 9/10,    batch: 11391/15469    Encoder_loss: 0.38908451795578003\n",
            "  epoch: 9/10,    batch: 11392/15469    Encoder_loss: 0.38720446825027466\n",
            "  epoch: 9/10,    batch: 11393/15469    Encoder_loss: 0.3863232731819153\n",
            "  epoch: 9/10,    batch: 11394/15469    Encoder_loss: 0.3860066533088684\n",
            "  epoch: 9/10,    batch: 11395/15469    Encoder_loss: 0.38402050733566284\n",
            "  epoch: 9/10,    batch: 11396/15469    Encoder_loss: 0.38318881392478943\n",
            "  epoch: 9/10,    batch: 11397/15469    Encoder_loss: 0.3817792534828186\n",
            "  epoch: 9/10,    batch: 11398/15469    Encoder_loss: 0.38159817457199097\n",
            "  epoch: 9/10,    batch: 11399/15469    Encoder_loss: 0.3802519738674164\n",
            "  epoch: 9/10,    batch: 11400/15469    Encoder_loss: 0.3811306357383728\n",
            "  epoch: 9/10,    batch: 11401/15469    Encoder_loss: 0.37926775217056274\n",
            "  epoch: 9/10,    batch: 11402/15469    Encoder_loss: 0.3772120475769043\n",
            "  epoch: 9/10,    batch: 11403/15469    Encoder_loss: 0.37669748067855835\n",
            "  epoch: 9/10,    batch: 11404/15469    Encoder_loss: 0.37542304396629333\n",
            "  epoch: 9/10,    batch: 11405/15469    Encoder_loss: 0.3743497431278229\n",
            "  epoch: 9/10,    batch: 11406/15469    Encoder_loss: 0.37203216552734375\n",
            "  epoch: 9/10,    batch: 11407/15469    Encoder_loss: 0.37144261598587036\n",
            "  epoch: 9/10,    batch: 11408/15469    Encoder_loss: 0.37062501907348633\n",
            "  epoch: 9/10,    batch: 11409/15469    Encoder_loss: 0.3696519732475281\n",
            "  epoch: 9/10,    batch: 11410/15469    Encoder_loss: 0.36903849244117737\n",
            "  epoch: 9/10,    batch: 11411/15469    Encoder_loss: 0.36742690205574036\n",
            "  epoch: 9/10,    batch: 11412/15469    Encoder_loss: 0.3659747242927551\n",
            "  epoch: 9/10,    batch: 11413/15469    Encoder_loss: 0.3655306398868561\n",
            "  epoch: 9/10,    batch: 11414/15469    Encoder_loss: 0.36378878355026245\n",
            "  epoch: 9/10,    batch: 11415/15469    Encoder_loss: 0.362297922372818\n",
            "  epoch: 9/10,    batch: 11416/15469    Encoder_loss: 0.36149564385414124\n",
            "  epoch: 9/10,    batch: 11417/15469    Encoder_loss: 0.36119502782821655\n",
            "  epoch: 9/10,    batch: 11418/15469    Encoder_loss: 0.38446345925331116\n",
            "  epoch: 9/10,    batch: 11419/15469    Encoder_loss: 0.5955840349197388\n",
            "  epoch: 9/10,    batch: 11420/15469    Encoder_loss: 0.7912788391113281\n",
            "  epoch: 9/10,    batch: 11421/15469    Encoder_loss: 0.7684904932975769\n",
            "  epoch: 9/10,    batch: 11422/15469    Encoder_loss: 0.562963604927063\n",
            "  epoch: 9/10,    batch: 11423/15469    Encoder_loss: 0.5420122742652893\n",
            "  epoch: 9/10,    batch: 11424/15469    Encoder_loss: 0.5422160029411316\n",
            "  epoch: 9/10,    batch: 11425/15469    Encoder_loss: 0.5277596116065979\n",
            "  epoch: 9/10,    batch: 11426/15469    Encoder_loss: 0.4479715824127197\n",
            "  epoch: 9/10,    batch: 11427/15469    Encoder_loss: 0.415635883808136\n",
            "  epoch: 9/10,    batch: 11428/15469    Encoder_loss: 0.3554624319076538\n",
            "  epoch: 9/10,    batch: 11429/15469    Encoder_loss: 0.32017016410827637\n",
            "  epoch: 9/10,    batch: 11430/15469    Encoder_loss: 0.3195641040802002\n",
            "  epoch: 9/10,    batch: 11431/15469    Encoder_loss: 0.3186618983745575\n",
            "  epoch: 9/10,    batch: 11432/15469    Encoder_loss: 0.3175716698169708\n",
            "  epoch: 9/10,    batch: 11433/15469    Encoder_loss: 0.3172655999660492\n",
            "  epoch: 9/10,    batch: 11434/15469    Encoder_loss: 0.3176911473274231\n",
            "  epoch: 9/10,    batch: 11435/15469    Encoder_loss: 0.3224075734615326\n",
            "  epoch: 9/10,    batch: 11436/15469    Encoder_loss: 0.3304046392440796\n",
            "  epoch: 9/10,    batch: 11437/15469    Encoder_loss: 0.33208712935447693\n",
            "  epoch: 9/10,    batch: 11438/15469    Encoder_loss: 0.3307756781578064\n",
            "  epoch: 9/10,    batch: 11439/15469    Encoder_loss: 0.3293159306049347\n",
            "  epoch: 9/10,    batch: 11440/15469    Encoder_loss: 0.32767826318740845\n",
            "  epoch: 9/10,    batch: 11441/15469    Encoder_loss: 0.32802262902259827\n",
            "  epoch: 9/10,    batch: 11442/15469    Encoder_loss: 0.3266030251979828\n",
            "  epoch: 9/10,    batch: 11443/15469    Encoder_loss: 0.32548823952674866\n",
            "  epoch: 9/10,    batch: 11444/15469    Encoder_loss: 0.3247031569480896\n",
            "  epoch: 9/10,    batch: 11445/15469    Encoder_loss: 0.32428252696990967\n",
            "  epoch: 9/10,    batch: 11446/15469    Encoder_loss: 0.3239491879940033\n",
            "  epoch: 9/10,    batch: 11447/15469    Encoder_loss: 0.32278239727020264\n",
            "  epoch: 9/10,    batch: 11448/15469    Encoder_loss: 0.3212776780128479\n",
            "  epoch: 9/10,    batch: 11449/15469    Encoder_loss: 0.3218007981777191\n",
            "  epoch: 9/10,    batch: 11450/15469    Encoder_loss: 0.3235243558883667\n",
            "  epoch: 9/10,    batch: 11451/15469    Encoder_loss: 0.32070475816726685\n",
            "  epoch: 9/10,    batch: 11452/15469    Encoder_loss: 0.3211783766746521\n",
            "  epoch: 9/10,    batch: 11453/15469    Encoder_loss: 0.31989938020706177\n",
            "  epoch: 9/10,    batch: 11454/15469    Encoder_loss: 0.3189719319343567\n",
            "  epoch: 9/10,    batch: 11455/15469    Encoder_loss: 0.31885677576065063\n",
            "  epoch: 9/10,    batch: 11456/15469    Encoder_loss: 0.3197217583656311\n",
            "  epoch: 9/10,    batch: 11457/15469    Encoder_loss: 0.31860917806625366\n",
            "  epoch: 9/10,    batch: 11458/15469    Encoder_loss: 0.3170239329338074\n",
            "  epoch: 9/10,    batch: 11459/15469    Encoder_loss: 0.3162078559398651\n",
            "  epoch: 9/10,    batch: 11460/15469    Encoder_loss: 0.31543177366256714\n",
            "  epoch: 9/10,    batch: 11461/15469    Encoder_loss: 0.31625106930732727\n",
            "  epoch: 9/10,    batch: 11462/15469    Encoder_loss: 0.3152506947517395\n",
            "  epoch: 9/10,    batch: 11463/15469    Encoder_loss: 0.3138083815574646\n",
            "  epoch: 9/10,    batch: 11464/15469    Encoder_loss: 0.31280776858329773\n",
            "  epoch: 9/10,    batch: 11465/15469    Encoder_loss: 0.312867671251297\n",
            "  epoch: 9/10,    batch: 11466/15469    Encoder_loss: 0.3118269741535187\n",
            "  epoch: 9/10,    batch: 11467/15469    Encoder_loss: 0.4207126200199127\n",
            "  epoch: 9/10,    batch: 11468/15469    Encoder_loss: 0.6604267954826355\n",
            "  epoch: 9/10,    batch: 11469/15469    Encoder_loss: 0.7650295495986938\n",
            "  epoch: 9/10,    batch: 11470/15469    Encoder_loss: 0.618683934211731\n",
            "  epoch: 9/10,    batch: 11471/15469    Encoder_loss: 0.4966363310813904\n",
            "  epoch: 9/10,    batch: 11472/15469    Encoder_loss: 0.49689632654190063\n",
            "  epoch: 9/10,    batch: 11473/15469    Encoder_loss: 0.4966115951538086\n",
            "  epoch: 9/10,    batch: 11474/15469    Encoder_loss: 0.4407370686531067\n",
            "  epoch: 9/10,    batch: 11475/15469    Encoder_loss: 0.38362807035446167\n",
            "  epoch: 9/10,    batch: 11476/15469    Encoder_loss: 0.3844178318977356\n",
            "  epoch: 9/10,    batch: 11477/15469    Encoder_loss: 0.31564369797706604\n",
            "  epoch: 9/10,    batch: 11478/15469    Encoder_loss: 0.3102759122848511\n",
            "  epoch: 9/10,    batch: 11479/15469    Encoder_loss: 0.3086627721786499\n",
            "  epoch: 9/10,    batch: 11480/15469    Encoder_loss: 0.3094747066497803\n",
            "  epoch: 9/10,    batch: 11481/15469    Encoder_loss: 0.30929601192474365\n",
            "  epoch: 9/10,    batch: 11482/15469    Encoder_loss: 0.30837273597717285\n",
            "  epoch: 9/10,    batch: 11483/15469    Encoder_loss: 0.3099796175956726\n",
            "  epoch: 9/10,    batch: 11484/15469    Encoder_loss: 0.31572166085243225\n",
            "  epoch: 9/10,    batch: 11485/15469    Encoder_loss: 0.32447510957717896\n",
            "  epoch: 9/10,    batch: 11486/15469    Encoder_loss: 0.3229684829711914\n",
            "  epoch: 9/10,    batch: 11487/15469    Encoder_loss: 0.3209100663661957\n",
            "  epoch: 9/10,    batch: 11488/15469    Encoder_loss: 0.3207032084465027\n",
            "  epoch: 9/10,    batch: 11489/15469    Encoder_loss: 0.31889966130256653\n",
            "  epoch: 9/10,    batch: 11490/15469    Encoder_loss: 0.316699743270874\n",
            "  epoch: 9/10,    batch: 11491/15469    Encoder_loss: 0.316829115152359\n",
            "  epoch: 9/10,    batch: 11492/15469    Encoder_loss: 0.31666573882102966\n",
            "  epoch: 9/10,    batch: 11493/15469    Encoder_loss: 0.31596535444259644\n",
            "  epoch: 9/10,    batch: 11494/15469    Encoder_loss: 0.31499186158180237\n",
            "  epoch: 9/10,    batch: 11495/15469    Encoder_loss: 0.3143612742424011\n",
            "  epoch: 9/10,    batch: 11496/15469    Encoder_loss: 0.3142697811126709\n",
            "  epoch: 9/10,    batch: 11497/15469    Encoder_loss: 0.31237712502479553\n",
            "  epoch: 9/10,    batch: 11498/15469    Encoder_loss: 0.31310585141181946\n",
            "  epoch: 9/10,    batch: 11499/15469    Encoder_loss: 0.311536580324173\n",
            "  epoch: 9/10,    batch: 11500/15469    Encoder_loss: 0.31127315759658813\n",
            "  epoch: 9/10,    batch: 11501/15469    Encoder_loss: 0.30998557806015015\n",
            "  epoch: 9/10,    batch: 11502/15469    Encoder_loss: 0.3104730248451233\n",
            "  epoch: 9/10,    batch: 11503/15469    Encoder_loss: 0.30908268690109253\n",
            "  epoch: 9/10,    batch: 11504/15469    Encoder_loss: 0.30959707498550415\n",
            "  epoch: 9/10,    batch: 11505/15469    Encoder_loss: 0.31017279624938965\n",
            "  epoch: 9/10,    batch: 11506/15469    Encoder_loss: 0.30907395482063293\n",
            "  epoch: 9/10,    batch: 11507/15469    Encoder_loss: 0.30850401520729065\n",
            "  epoch: 9/10,    batch: 11508/15469    Encoder_loss: 0.30790549516677856\n",
            "  epoch: 9/10,    batch: 11509/15469    Encoder_loss: 0.3073023855686188\n",
            "  epoch: 9/10,    batch: 11510/15469    Encoder_loss: 0.30696195363998413\n",
            "  epoch: 9/10,    batch: 11511/15469    Encoder_loss: 0.30662769079208374\n",
            "  epoch: 9/10,    batch: 11512/15469    Encoder_loss: 0.30652981996536255\n",
            "  epoch: 9/10,    batch: 11513/15469    Encoder_loss: 0.3055897653102875\n",
            "  epoch: 9/10,    batch: 11514/15469    Encoder_loss: 0.3058599531650543\n",
            "  epoch: 9/10,    batch: 11515/15469    Encoder_loss: 0.3063194751739502\n",
            "  epoch: 9/10,    batch: 11516/15469    Encoder_loss: 0.30501651763916016\n",
            "  epoch: 9/10,    batch: 11517/15469    Encoder_loss: 0.30544334650039673\n",
            "  epoch: 9/10,    batch: 11518/15469    Encoder_loss: 0.3043074309825897\n",
            "  epoch: 9/10,    batch: 11519/15469    Encoder_loss: 0.3050779700279236\n",
            "  epoch: 9/10,    batch: 11520/15469    Encoder_loss: 0.3049062192440033\n",
            "  epoch: 9/10,    batch: 11521/15469    Encoder_loss: 0.30366453528404236\n",
            "  epoch: 9/10,    batch: 11522/15469    Encoder_loss: 0.30398693680763245\n",
            "  epoch: 9/10,    batch: 11523/15469    Encoder_loss: 0.3029082417488098\n",
            "  epoch: 9/10,    batch: 11524/15469    Encoder_loss: 0.30417534708976746\n",
            "  epoch: 9/10,    batch: 11525/15469    Encoder_loss: 0.30416327714920044\n",
            "  epoch: 9/10,    batch: 11526/15469    Encoder_loss: 0.30286163091659546\n",
            "  epoch: 9/10,    batch: 11527/15469    Encoder_loss: 0.3027997612953186\n",
            "  epoch: 9/10,    batch: 11528/15469    Encoder_loss: 0.3018324673175812\n",
            "  epoch: 9/10,    batch: 11529/15469    Encoder_loss: 0.3018481731414795\n",
            "  epoch: 9/10,    batch: 11530/15469    Encoder_loss: 0.30054330825805664\n",
            "  epoch: 9/10,    batch: 11531/15469    Encoder_loss: 0.30151692032814026\n",
            "  epoch: 9/10,    batch: 11532/15469    Encoder_loss: 0.301675021648407\n",
            "  epoch: 9/10,    batch: 11533/15469    Encoder_loss: 0.3014144003391266\n",
            "  epoch: 9/10,    batch: 11534/15469    Encoder_loss: 0.30247950553894043\n",
            "  epoch: 9/10,    batch: 11535/15469    Encoder_loss: 0.30184006690979004\n",
            "  epoch: 9/10,    batch: 11536/15469    Encoder_loss: 0.30124205350875854\n",
            "  epoch: 9/10,    batch: 11537/15469    Encoder_loss: 0.301739364862442\n",
            "  epoch: 9/10,    batch: 11538/15469    Encoder_loss: 0.32592934370040894\n",
            "  epoch: 9/10,    batch: 11539/15469    Encoder_loss: 0.5236722826957703\n",
            "  epoch: 9/10,    batch: 11540/15469    Encoder_loss: 0.7880865931510925\n",
            "  epoch: 9/10,    batch: 11541/15469    Encoder_loss: 0.8124629855155945\n",
            "  epoch: 9/10,    batch: 11542/15469    Encoder_loss: 0.6124933958053589\n",
            "  epoch: 9/10,    batch: 11543/15469    Encoder_loss: 0.5873442888259888\n",
            "  epoch: 9/10,    batch: 11544/15469    Encoder_loss: 0.5861711502075195\n",
            "  epoch: 9/10,    batch: 11545/15469    Encoder_loss: 0.5744942426681519\n",
            "  epoch: 9/10,    batch: 11546/15469    Encoder_loss: 0.4891131818294525\n",
            "  epoch: 9/10,    batch: 11547/15469    Encoder_loss: 0.47938698530197144\n",
            "  epoch: 9/10,    batch: 11548/15469    Encoder_loss: 0.4423019289970398\n",
            "  epoch: 9/10,    batch: 11549/15469    Encoder_loss: 0.39950743317604065\n",
            "  epoch: 9/10,    batch: 11550/15469    Encoder_loss: 0.39974725246429443\n",
            "  epoch: 9/10,    batch: 11551/15469    Encoder_loss: 0.40111246705055237\n",
            "  epoch: 9/10,    batch: 11552/15469    Encoder_loss: 0.40100985765457153\n",
            "  epoch: 9/10,    batch: 11553/15469    Encoder_loss: 0.40296220779418945\n",
            "  epoch: 9/10,    batch: 11554/15469    Encoder_loss: 0.4037849009037018\n",
            "  epoch: 9/10,    batch: 11555/15469    Encoder_loss: 0.4076075553894043\n",
            "  epoch: 9/10,    batch: 11556/15469    Encoder_loss: 0.4118308126926422\n",
            "  epoch: 9/10,    batch: 11557/15469    Encoder_loss: 0.4194985032081604\n",
            "  epoch: 9/10,    batch: 11558/15469    Encoder_loss: 0.4225805699825287\n",
            "  epoch: 9/10,    batch: 11559/15469    Encoder_loss: 0.4241059422492981\n",
            "  epoch: 9/10,    batch: 11560/15469    Encoder_loss: 0.4261341691017151\n",
            "  epoch: 9/10,    batch: 11561/15469    Encoder_loss: 0.42549076676368713\n",
            "  epoch: 9/10,    batch: 11562/15469    Encoder_loss: 0.4262150228023529\n",
            "  epoch: 9/10,    batch: 11563/15469    Encoder_loss: 0.42860308289527893\n",
            "  epoch: 9/10,    batch: 11564/15469    Encoder_loss: 0.4303620755672455\n",
            "  epoch: 9/10,    batch: 11565/15469    Encoder_loss: 0.4301205277442932\n",
            "  epoch: 9/10,    batch: 11566/15469    Encoder_loss: 0.42973923683166504\n",
            "  epoch: 9/10,    batch: 11567/15469    Encoder_loss: 0.4304885268211365\n",
            "  epoch: 9/10,    batch: 11568/15469    Encoder_loss: 0.43114203214645386\n",
            "  epoch: 9/10,    batch: 11569/15469    Encoder_loss: 0.43148073554039\n",
            "  epoch: 9/10,    batch: 11570/15469    Encoder_loss: 0.4342012405395508\n",
            "  epoch: 9/10,    batch: 11571/15469    Encoder_loss: 0.43619078397750854\n",
            "  epoch: 9/10,    batch: 11572/15469    Encoder_loss: 0.43724942207336426\n",
            "  epoch: 9/10,    batch: 11573/15469    Encoder_loss: 0.43750032782554626\n",
            "  epoch: 9/10,    batch: 11574/15469    Encoder_loss: 0.4390036463737488\n",
            "  epoch: 9/10,    batch: 11575/15469    Encoder_loss: 0.46706342697143555\n",
            "  epoch: 9/10,    batch: 11576/15469    Encoder_loss: 0.44309520721435547\n",
            "  epoch: 9/10,    batch: 11577/15469    Encoder_loss: 0.37825730443000793\n",
            "  epoch: 9/10,    batch: 11578/15469    Encoder_loss: 0.416609525680542\n",
            "  epoch: 9/10,    batch: 11579/15469    Encoder_loss: 0.4593058228492737\n",
            "  epoch: 9/10,    batch: 11580/15469    Encoder_loss: 0.4604063928127289\n",
            "  epoch: 9/10,    batch: 11581/15469    Encoder_loss: 0.4852651059627533\n",
            "  epoch: 9/10,    batch: 11582/15469    Encoder_loss: 0.5605961084365845\n",
            "  epoch: 9/10,    batch: 11583/15469    Encoder_loss: 0.5581333041191101\n",
            "  epoch: 9/10,    batch: 11584/15469    Encoder_loss: 0.5578533411026001\n",
            "  epoch: 9/10,    batch: 11585/15469    Encoder_loss: 0.5584688782691956\n",
            "  epoch: 9/10,    batch: 11586/15469    Encoder_loss: 0.5561770796775818\n",
            "  epoch: 9/10,    batch: 11587/15469    Encoder_loss: 0.5553961396217346\n",
            "  epoch: 9/10,    batch: 11588/15469    Encoder_loss: 0.5572508573532104\n",
            "  epoch: 9/10,    batch: 11589/15469    Encoder_loss: 0.5575604438781738\n",
            "  epoch: 9/10,    batch: 11590/15469    Encoder_loss: 0.566950261592865\n",
            "  epoch: 9/10,    batch: 11591/15469    Encoder_loss: 0.6380405426025391\n",
            "  epoch: 9/10,    batch: 11592/15469    Encoder_loss: 0.6402379274368286\n",
            "  epoch: 9/10,    batch: 11593/15469    Encoder_loss: 0.6388428807258606\n",
            "  epoch: 9/10,    batch: 11594/15469    Encoder_loss: 0.6395021677017212\n",
            "  epoch: 9/10,    batch: 11595/15469    Encoder_loss: 0.6373149156570435\n",
            "  epoch: 9/10,    batch: 11596/15469    Encoder_loss: 0.6368992924690247\n",
            "  epoch: 9/10,    batch: 11597/15469    Encoder_loss: 0.6358687281608582\n",
            "  epoch: 9/10,    batch: 11598/15469    Encoder_loss: 0.6361021399497986\n",
            "  epoch: 9/10,    batch: 11599/15469    Encoder_loss: 0.6362504959106445\n",
            "  epoch: 9/10,    batch: 11600/15469    Encoder_loss: 0.6351819634437561\n",
            "  epoch: 9/10,    batch: 11601/15469    Encoder_loss: 0.63531094789505\n",
            "  epoch: 9/10,    batch: 11602/15469    Encoder_loss: 0.6351569890975952\n",
            "  epoch: 9/10,    batch: 11603/15469    Encoder_loss: 0.6358637809753418\n",
            "  epoch: 9/10,    batch: 11604/15469    Encoder_loss: 0.6354748010635376\n",
            "  epoch: 9/10,    batch: 11605/15469    Encoder_loss: 0.6358181238174438\n",
            "  epoch: 9/10,    batch: 11606/15469    Encoder_loss: 0.6360207796096802\n",
            "  epoch: 9/10,    batch: 11607/15469    Encoder_loss: 0.6371361017227173\n",
            "  epoch: 9/10,    batch: 11608/15469    Encoder_loss: 0.6359619498252869\n",
            "  epoch: 9/10,    batch: 11609/15469    Encoder_loss: 0.6371201276779175\n",
            "  epoch: 9/10,    batch: 11610/15469    Encoder_loss: 0.638891339302063\n",
            "  epoch: 9/10,    batch: 11611/15469    Encoder_loss: 0.6383317708969116\n",
            "  epoch: 9/10,    batch: 11612/15469    Encoder_loss: 0.6395810842514038\n",
            "  epoch: 9/10,    batch: 11613/15469    Encoder_loss: 0.6405142545700073\n",
            "  epoch: 9/10,    batch: 11614/15469    Encoder_loss: 0.6415603160858154\n",
            "  epoch: 9/10,    batch: 11615/15469    Encoder_loss: 0.6427068710327148\n",
            "  epoch: 9/10,    batch: 11616/15469    Encoder_loss: 0.6511024236679077\n",
            "  epoch: 9/10,    batch: 11617/15469    Encoder_loss: 0.7380510568618774\n",
            "  epoch: 9/10,    batch: 11618/15469    Encoder_loss: 0.7657288312911987\n",
            "  epoch: 9/10,    batch: 11619/15469    Encoder_loss: 0.7366787195205688\n",
            "  epoch: 9/10,    batch: 11620/15469    Encoder_loss: 0.7387140393257141\n",
            "  epoch: 9/10,    batch: 11621/15469    Encoder_loss: 0.6728537082672119\n",
            "  epoch: 9/10,    batch: 11622/15469    Encoder_loss: 0.6674098372459412\n",
            "  epoch: 9/10,    batch: 11623/15469    Encoder_loss: 0.6673575043678284\n",
            "  epoch: 9/10,    batch: 11624/15469    Encoder_loss: 0.6670453548431396\n",
            "  epoch: 9/10,    batch: 11625/15469    Encoder_loss: 0.6680346727371216\n",
            "  epoch: 9/10,    batch: 11626/15469    Encoder_loss: 0.6702377200126648\n",
            "  epoch: 9/10,    batch: 11627/15469    Encoder_loss: 0.6715332269668579\n",
            "  epoch: 9/10,    batch: 11628/15469    Encoder_loss: 0.6733501553535461\n",
            "  epoch: 9/10,    batch: 11629/15469    Encoder_loss: 0.6999279260635376\n",
            "  epoch: 9/10,    batch: 11630/15469    Encoder_loss: 0.6740237474441528\n",
            "  epoch: 9/10,    batch: 11631/15469    Encoder_loss: 0.6702815890312195\n",
            "  epoch: 9/10,    batch: 11632/15469    Encoder_loss: 0.6471777558326721\n",
            "  epoch: 9/10,    batch: 11633/15469    Encoder_loss: 0.5946587920188904\n",
            "  epoch: 9/10,    batch: 11634/15469    Encoder_loss: 0.5951873660087585\n",
            "  epoch: 9/10,    batch: 11635/15469    Encoder_loss: 0.5979634523391724\n",
            "  epoch: 9/10,    batch: 11636/15469    Encoder_loss: 0.59862220287323\n",
            "  epoch: 9/10,    batch: 11637/15469    Encoder_loss: 0.5993179082870483\n",
            "  epoch: 9/10,    batch: 11638/15469    Encoder_loss: 0.6016298532485962\n",
            "  epoch: 9/10,    batch: 11639/15469    Encoder_loss: 0.6014363169670105\n",
            "  epoch: 9/10,    batch: 11640/15469    Encoder_loss: 0.6028672456741333\n",
            "  epoch: 9/10,    batch: 11641/15469    Encoder_loss: 0.6046299934387207\n",
            "  epoch: 9/10,    batch: 11642/15469    Encoder_loss: 0.6065317392349243\n",
            "  epoch: 9/10,    batch: 11643/15469    Encoder_loss: 0.5069524645805359\n",
            "  epoch: 9/10,    batch: 11644/15469    Encoder_loss: 0.3474782109260559\n",
            "  epoch: 9/10,    batch: 11645/15469    Encoder_loss: 0.34703606367111206\n",
            "  epoch: 9/10,    batch: 11646/15469    Encoder_loss: 0.3465762138366699\n",
            "  epoch: 9/10,    batch: 11647/15469    Encoder_loss: 0.34580737352371216\n",
            "  epoch: 9/10,    batch: 11648/15469    Encoder_loss: 0.3440314829349518\n",
            "  epoch: 9/10,    batch: 11649/15469    Encoder_loss: 0.344819575548172\n",
            "  epoch: 9/10,    batch: 11650/15469    Encoder_loss: 0.3411161005496979\n",
            "  epoch: 9/10,    batch: 11651/15469    Encoder_loss: 0.3399151861667633\n",
            "  epoch: 9/10,    batch: 11652/15469    Encoder_loss: 0.339114248752594\n",
            "  epoch: 9/10,    batch: 11653/15469    Encoder_loss: 0.3387187123298645\n",
            "  epoch: 9/10,    batch: 11654/15469    Encoder_loss: 0.33931928873062134\n",
            "  epoch: 9/10,    batch: 11655/15469    Encoder_loss: 0.3358200192451477\n",
            "  epoch: 9/10,    batch: 11656/15469    Encoder_loss: 0.334364116191864\n",
            "  epoch: 9/10,    batch: 11657/15469    Encoder_loss: 0.3344074487686157\n",
            "  epoch: 9/10,    batch: 11658/15469    Encoder_loss: 0.3321288228034973\n",
            "  epoch: 9/10,    batch: 11659/15469    Encoder_loss: 0.33230847120285034\n",
            "  epoch: 9/10,    batch: 11660/15469    Encoder_loss: 0.3315781354904175\n",
            "  epoch: 9/10,    batch: 11661/15469    Encoder_loss: 0.33047276735305786\n",
            "  epoch: 9/10,    batch: 11662/15469    Encoder_loss: 0.3278564512729645\n",
            "  epoch: 9/10,    batch: 11663/15469    Encoder_loss: 0.3288975954055786\n",
            "  epoch: 9/10,    batch: 11664/15469    Encoder_loss: 0.3270314633846283\n",
            "  epoch: 9/10,    batch: 11665/15469    Encoder_loss: 0.3279087245464325\n",
            "  epoch: 9/10,    batch: 11666/15469    Encoder_loss: 0.32637470960617065\n",
            "  epoch: 9/10,    batch: 11667/15469    Encoder_loss: 0.32431718707084656\n",
            "  epoch: 9/10,    batch: 11668/15469    Encoder_loss: 0.323714941740036\n",
            "  epoch: 9/10,    batch: 11669/15469    Encoder_loss: 0.3240256905555725\n",
            "  epoch: 9/10,    batch: 11670/15469    Encoder_loss: 0.32245245575904846\n",
            "  epoch: 9/10,    batch: 11671/15469    Encoder_loss: 0.3216943144798279\n",
            "  epoch: 9/10,    batch: 11672/15469    Encoder_loss: 0.3206110894680023\n",
            "  epoch: 9/10,    batch: 11673/15469    Encoder_loss: 0.3198665380477905\n",
            "  epoch: 9/10,    batch: 11674/15469    Encoder_loss: 0.32107457518577576\n",
            "  epoch: 9/10,    batch: 11675/15469    Encoder_loss: 0.31890422105789185\n",
            "  epoch: 9/10,    batch: 11676/15469    Encoder_loss: 0.3180389702320099\n",
            "  epoch: 9/10,    batch: 11677/15469    Encoder_loss: 0.31598788499832153\n",
            "  epoch: 9/10,    batch: 11678/15469    Encoder_loss: 0.3152869641780853\n",
            "  epoch: 9/10,    batch: 11679/15469    Encoder_loss: 0.31542307138442993\n",
            "  epoch: 9/10,    batch: 11680/15469    Encoder_loss: 0.31411057710647583\n",
            "  epoch: 9/10,    batch: 11681/15469    Encoder_loss: 0.31403985619544983\n",
            "  epoch: 9/10,    batch: 11682/15469    Encoder_loss: 0.3134118914604187\n",
            "  epoch: 9/10,    batch: 11683/15469    Encoder_loss: 0.3123530149459839\n",
            "  epoch: 9/10,    batch: 11684/15469    Encoder_loss: 0.3150281012058258\n",
            "  epoch: 9/10,    batch: 11685/15469    Encoder_loss: 0.2793640196323395\n",
            "  epoch: 9/10,    batch: 11686/15469    Encoder_loss: 0.27756956219673157\n",
            "  epoch: 9/10,    batch: 11687/15469    Encoder_loss: 0.27768898010253906\n",
            "  epoch: 9/10,    batch: 11688/15469    Encoder_loss: 0.27815037965774536\n",
            "  epoch: 9/10,    batch: 11689/15469    Encoder_loss: 0.2764163613319397\n",
            "  epoch: 9/10,    batch: 11690/15469    Encoder_loss: 0.27795568108558655\n",
            "  epoch: 9/10,    batch: 11691/15469    Encoder_loss: 0.35188111662864685\n",
            "  epoch: 9/10,    batch: 11692/15469    Encoder_loss: 0.3596895635128021\n",
            "  epoch: 9/10,    batch: 11693/15469    Encoder_loss: 0.3586201071739197\n",
            "  epoch: 9/10,    batch: 11694/15469    Encoder_loss: 0.3592938780784607\n",
            "  epoch: 9/10,    batch: 11695/15469    Encoder_loss: 0.4205254018306732\n",
            "  epoch: 9/10,    batch: 11696/15469    Encoder_loss: 0.45942458510398865\n",
            "  epoch: 9/10,    batch: 11697/15469    Encoder_loss: 0.4601097106933594\n",
            "  epoch: 9/10,    batch: 11698/15469    Encoder_loss: 0.4632622301578522\n",
            "  epoch: 9/10,    batch: 11699/15469    Encoder_loss: 0.46434634923934937\n",
            "  epoch: 9/10,    batch: 11700/15469    Encoder_loss: 0.46317005157470703\n",
            "  epoch: 9/10,    batch: 11701/15469    Encoder_loss: 0.464287132024765\n",
            "  epoch: 9/10,    batch: 11702/15469    Encoder_loss: 0.4645019471645355\n",
            "  epoch: 9/10,    batch: 11703/15469    Encoder_loss: 0.46643370389938354\n",
            "  epoch: 9/10,    batch: 11704/15469    Encoder_loss: 0.46730881929397583\n",
            "  epoch: 9/10,    batch: 11705/15469    Encoder_loss: 0.4657362997531891\n",
            "  epoch: 9/10,    batch: 11706/15469    Encoder_loss: 0.46684589982032776\n",
            "  epoch: 9/10,    batch: 11707/15469    Encoder_loss: 0.4678031802177429\n",
            "  epoch: 9/10,    batch: 11708/15469    Encoder_loss: 0.4712866246700287\n",
            "  epoch: 9/10,    batch: 11709/15469    Encoder_loss: 0.47279518842697144\n",
            "  epoch: 9/10,    batch: 11710/15469    Encoder_loss: 0.47361063957214355\n",
            "  epoch: 9/10,    batch: 11711/15469    Encoder_loss: 0.4727098345756531\n",
            "  epoch: 9/10,    batch: 11712/15469    Encoder_loss: 0.4734209477901459\n",
            "  epoch: 9/10,    batch: 11713/15469    Encoder_loss: 0.47532519698143005\n",
            "  epoch: 9/10,    batch: 11714/15469    Encoder_loss: 0.47800707817077637\n",
            "  epoch: 9/10,    batch: 11715/15469    Encoder_loss: 0.47930383682250977\n",
            "  epoch: 9/10,    batch: 11716/15469    Encoder_loss: 0.48022231459617615\n",
            "  epoch: 9/10,    batch: 11717/15469    Encoder_loss: 0.48052507638931274\n",
            "  epoch: 9/10,    batch: 11718/15469    Encoder_loss: 0.4814179539680481\n",
            "  epoch: 9/10,    batch: 11719/15469    Encoder_loss: 0.48258185386657715\n",
            "  epoch: 9/10,    batch: 11720/15469    Encoder_loss: 0.4839840829372406\n",
            "  epoch: 9/10,    batch: 11721/15469    Encoder_loss: 0.4859384596347809\n",
            "  epoch: 9/10,    batch: 11722/15469    Encoder_loss: 0.48641419410705566\n",
            "  epoch: 9/10,    batch: 11723/15469    Encoder_loss: 0.48735445737838745\n",
            "  epoch: 9/10,    batch: 11724/15469    Encoder_loss: 0.48950329422950745\n",
            "  epoch: 9/10,    batch: 11725/15469    Encoder_loss: 0.48910021781921387\n",
            "  epoch: 9/10,    batch: 11726/15469    Encoder_loss: 0.5501071810722351\n",
            "  epoch: 9/10,    batch: 11727/15469    Encoder_loss: 0.7918006181716919\n",
            "  epoch: 9/10,    batch: 11728/15469    Encoder_loss: 0.9556329846382141\n",
            "  epoch: 9/10,    batch: 11729/15469    Encoder_loss: 0.8971308469772339\n",
            "  epoch: 9/10,    batch: 11730/15469    Encoder_loss: 0.7180259823799133\n",
            "  epoch: 9/10,    batch: 11731/15469    Encoder_loss: 0.7055417895317078\n",
            "  epoch: 9/10,    batch: 11732/15469    Encoder_loss: 0.7020438313484192\n",
            "  epoch: 9/10,    batch: 11733/15469    Encoder_loss: 0.6053053140640259\n",
            "  epoch: 9/10,    batch: 11734/15469    Encoder_loss: 0.5208476185798645\n",
            "  epoch: 9/10,    batch: 11735/15469    Encoder_loss: 0.5244988203048706\n",
            "  epoch: 9/10,    batch: 11736/15469    Encoder_loss: 0.4670041501522064\n",
            "  epoch: 9/10,    batch: 11737/15469    Encoder_loss: 0.4469086229801178\n",
            "  epoch: 9/10,    batch: 11738/15469    Encoder_loss: 0.4503253996372223\n",
            "  epoch: 9/10,    batch: 11739/15469    Encoder_loss: 0.4485781788825989\n",
            "  epoch: 9/10,    batch: 11740/15469    Encoder_loss: 0.44870781898498535\n",
            "  epoch: 9/10,    batch: 11741/15469    Encoder_loss: 0.4468544125556946\n",
            "  epoch: 9/10,    batch: 11742/15469    Encoder_loss: 0.45010966062545776\n",
            "  epoch: 9/10,    batch: 11743/15469    Encoder_loss: 0.4523286819458008\n",
            "  epoch: 9/10,    batch: 11744/15469    Encoder_loss: 0.45961886644363403\n",
            "  epoch: 9/10,    batch: 11745/15469    Encoder_loss: 0.46159958839416504\n",
            "  epoch: 9/10,    batch: 11746/15469    Encoder_loss: 0.46156901121139526\n",
            "  epoch: 9/10,    batch: 11747/15469    Encoder_loss: 0.46099019050598145\n",
            "  epoch: 9/10,    batch: 11748/15469    Encoder_loss: 0.46042299270629883\n",
            "  epoch: 9/10,    batch: 11749/15469    Encoder_loss: 0.46045994758605957\n",
            "  epoch: 9/10,    batch: 11750/15469    Encoder_loss: 0.4597410559654236\n",
            "  epoch: 9/10,    batch: 11751/15469    Encoder_loss: 0.45944681763648987\n",
            "  epoch: 9/10,    batch: 11752/15469    Encoder_loss: 0.4584047794342041\n",
            "  epoch: 9/10,    batch: 11753/15469    Encoder_loss: 0.45945867896080017\n",
            "  epoch: 9/10,    batch: 11754/15469    Encoder_loss: 0.46057161688804626\n",
            "  epoch: 9/10,    batch: 11755/15469    Encoder_loss: 0.45895448327064514\n",
            "  epoch: 9/10,    batch: 11756/15469    Encoder_loss: 0.45874160528182983\n",
            "  epoch: 9/10,    batch: 11757/15469    Encoder_loss: 0.4604574143886566\n",
            "  epoch: 9/10,    batch: 11758/15469    Encoder_loss: 0.4587318003177643\n",
            "  epoch: 9/10,    batch: 11759/15469    Encoder_loss: 0.46186694502830505\n",
            "  epoch: 9/10,    batch: 11760/15469    Encoder_loss: 0.4612998068332672\n",
            "  epoch: 9/10,    batch: 11761/15469    Encoder_loss: 0.4470343589782715\n",
            "  epoch: 9/10,    batch: 11762/15469    Encoder_loss: 0.363071471452713\n",
            "  epoch: 9/10,    batch: 11763/15469    Encoder_loss: 0.35776689648628235\n",
            "  epoch: 9/10,    batch: 11764/15469    Encoder_loss: 0.35547760128974915\n",
            "  epoch: 9/10,    batch: 11765/15469    Encoder_loss: 0.3543521463871002\n",
            "  epoch: 9/10,    batch: 11766/15469    Encoder_loss: 0.3520354628562927\n",
            "  epoch: 9/10,    batch: 11767/15469    Encoder_loss: 0.3518611192703247\n",
            "  epoch: 9/10,    batch: 11768/15469    Encoder_loss: 0.3511412739753723\n",
            "  epoch: 9/10,    batch: 11769/15469    Encoder_loss: 0.3503802418708801\n",
            "  epoch: 9/10,    batch: 11770/15469    Encoder_loss: 0.3489244282245636\n",
            "  epoch: 9/10,    batch: 11771/15469    Encoder_loss: 0.34696269035339355\n",
            "  epoch: 9/10,    batch: 11772/15469    Encoder_loss: 0.3452151119709015\n",
            "  epoch: 9/10,    batch: 11773/15469    Encoder_loss: 0.3434331715106964\n",
            "  epoch: 9/10,    batch: 11774/15469    Encoder_loss: 0.3421621322631836\n",
            "  epoch: 9/10,    batch: 11775/15469    Encoder_loss: 0.3416159152984619\n",
            "  epoch: 9/10,    batch: 11776/15469    Encoder_loss: 0.338975191116333\n",
            "  epoch: 9/10,    batch: 11777/15469    Encoder_loss: 0.33906692266464233\n",
            "  epoch: 9/10,    batch: 11778/15469    Encoder_loss: 0.3384196162223816\n",
            "  epoch: 9/10,    batch: 11779/15469    Encoder_loss: 0.33609873056411743\n",
            "  epoch: 9/10,    batch: 11780/15469    Encoder_loss: 0.33617502450942993\n",
            "  epoch: 9/10,    batch: 11781/15469    Encoder_loss: 0.3355749249458313\n",
            "  epoch: 9/10,    batch: 11782/15469    Encoder_loss: 0.3325483202934265\n",
            "  epoch: 9/10,    batch: 11783/15469    Encoder_loss: 0.33171015977859497\n",
            "  epoch: 9/10,    batch: 11784/15469    Encoder_loss: 0.33064642548561096\n",
            "  epoch: 9/10,    batch: 11785/15469    Encoder_loss: 0.32901519536972046\n",
            "  epoch: 9/10,    batch: 11786/15469    Encoder_loss: 0.3279821276664734\n",
            "  epoch: 9/10,    batch: 11787/15469    Encoder_loss: 0.32684290409088135\n",
            "  epoch: 9/10,    batch: 11788/15469    Encoder_loss: 0.325924813747406\n",
            "  epoch: 9/10,    batch: 11789/15469    Encoder_loss: 0.325764000415802\n",
            "  epoch: 9/10,    batch: 11790/15469    Encoder_loss: 0.32332339882850647\n",
            "  epoch: 9/10,    batch: 11791/15469    Encoder_loss: 0.3217328190803528\n",
            "  epoch: 9/10,    batch: 11792/15469    Encoder_loss: 0.3214069902896881\n",
            "  epoch: 9/10,    batch: 11793/15469    Encoder_loss: 0.3224274218082428\n",
            "  epoch: 9/10,    batch: 11794/15469    Encoder_loss: 0.31949383020401\n",
            "  epoch: 9/10,    batch: 11795/15469    Encoder_loss: 0.31843602657318115\n",
            "  epoch: 9/10,    batch: 11796/15469    Encoder_loss: 0.3191661238670349\n",
            "  epoch: 9/10,    batch: 11797/15469    Encoder_loss: 0.31794190406799316\n",
            "  epoch: 9/10,    batch: 11798/15469    Encoder_loss: 0.31574004888534546\n",
            "  epoch: 9/10,    batch: 11799/15469    Encoder_loss: 0.3150634765625\n",
            "  epoch: 9/10,    batch: 11800/15469    Encoder_loss: 0.3137747049331665\n",
            "  epoch: 9/10,    batch: 11801/15469    Encoder_loss: 0.3132639229297638\n",
            "  epoch: 9/10,    batch: 11802/15469    Encoder_loss: 0.31621044874191284\n",
            "  epoch: 9/10,    batch: 11803/15469    Encoder_loss: 0.2966536283493042\n",
            "  epoch: 9/10,    batch: 11804/15469    Encoder_loss: 0.27733710408210754\n",
            "  epoch: 9/10,    batch: 11805/15469    Encoder_loss: 0.2781757414340973\n",
            "  epoch: 9/10,    batch: 11806/15469    Encoder_loss: 0.27799221873283386\n",
            "  epoch: 9/10,    batch: 11807/15469    Encoder_loss: 0.2753705084323883\n",
            "  epoch: 9/10,    batch: 11808/15469    Encoder_loss: 0.27469268441200256\n",
            "  epoch: 9/10,    batch: 11809/15469    Encoder_loss: 0.2742864787578583\n",
            "  epoch: 9/10,    batch: 11810/15469    Encoder_loss: 0.27455851435661316\n",
            "  epoch: 9/10,    batch: 11811/15469    Encoder_loss: 0.2740260064601898\n",
            "  epoch: 9/10,    batch: 11812/15469    Encoder_loss: 0.27511972188949585\n",
            "  epoch: 9/10,    batch: 11813/15469    Encoder_loss: 0.2743857502937317\n",
            "  epoch: 9/10,    batch: 11814/15469    Encoder_loss: 0.2741164267063141\n",
            "  epoch: 9/10,    batch: 11815/15469    Encoder_loss: 0.27473706007003784\n",
            "  epoch: 9/10,    batch: 11816/15469    Encoder_loss: 0.27429503202438354\n",
            "  epoch: 9/10,    batch: 11817/15469    Encoder_loss: 0.2731911242008209\n",
            "  epoch: 9/10,    batch: 11818/15469    Encoder_loss: 0.2756739556789398\n",
            "  epoch: 9/10,    batch: 11819/15469    Encoder_loss: 0.2732548713684082\n",
            "  epoch: 9/10,    batch: 11820/15469    Encoder_loss: 0.27330639958381653\n",
            "  epoch: 9/10,    batch: 11821/15469    Encoder_loss: 0.27337855100631714\n",
            "  epoch: 9/10,    batch: 11822/15469    Encoder_loss: 0.27236834168434143\n",
            "  epoch: 9/10,    batch: 11823/15469    Encoder_loss: 0.2717214524745941\n",
            "  epoch: 9/10,    batch: 11824/15469    Encoder_loss: 0.2726287245750427\n",
            "  epoch: 9/10,    batch: 11825/15469    Encoder_loss: 0.27082952857017517\n",
            "  epoch: 9/10,    batch: 11826/15469    Encoder_loss: 0.2725093960762024\n",
            "  epoch: 9/10,    batch: 11827/15469    Encoder_loss: 0.27195197343826294\n",
            "  epoch: 9/10,    batch: 11828/15469    Encoder_loss: 0.27159374952316284\n",
            "  epoch: 9/10,    batch: 11829/15469    Encoder_loss: 0.27075904607772827\n",
            "  epoch: 9/10,    batch: 11830/15469    Encoder_loss: 0.2711148262023926\n",
            "  epoch: 9/10,    batch: 11831/15469    Encoder_loss: 0.27197664976119995\n",
            "  epoch: 9/10,    batch: 11832/15469    Encoder_loss: 0.2715167999267578\n",
            "  epoch: 9/10,    batch: 11833/15469    Encoder_loss: 0.27110204100608826\n",
            "  epoch: 9/10,    batch: 11834/15469    Encoder_loss: 0.2705303132534027\n",
            "  epoch: 9/10,    batch: 11835/15469    Encoder_loss: 0.27112728357315063\n",
            "  epoch: 9/10,    batch: 11836/15469    Encoder_loss: 0.2714737355709076\n",
            "  epoch: 9/10,    batch: 11837/15469    Encoder_loss: 0.27065813541412354\n",
            "  epoch: 9/10,    batch: 11838/15469    Encoder_loss: 0.30964452028274536\n",
            "  epoch: 9/10,    batch: 11839/15469    Encoder_loss: 0.35469406843185425\n",
            "  epoch: 9/10,    batch: 11840/15469    Encoder_loss: 0.3535824120044708\n",
            "  epoch: 9/10,    batch: 11841/15469    Encoder_loss: 0.3760206699371338\n",
            "  epoch: 9/10,    batch: 11842/15469    Encoder_loss: 0.4633747637271881\n",
            "  epoch: 9/10,    batch: 11843/15469    Encoder_loss: 0.6145071983337402\n",
            "  epoch: 9/10,    batch: 11844/15469    Encoder_loss: 0.8596771955490112\n",
            "  epoch: 9/10,    batch: 11845/15469    Encoder_loss: 0.9042395353317261\n",
            "  epoch: 9/10,    batch: 11846/15469    Encoder_loss: 0.7046473026275635\n",
            "  epoch: 9/10,    batch: 11847/15469    Encoder_loss: 0.6442363858222961\n",
            "  epoch: 9/10,    batch: 11848/15469    Encoder_loss: 0.6462925672531128\n",
            "  epoch: 9/10,    batch: 11849/15469    Encoder_loss: 0.6428526639938354\n",
            "  epoch: 9/10,    batch: 11850/15469    Encoder_loss: 0.567440390586853\n",
            "  epoch: 9/10,    batch: 11851/15469    Encoder_loss: 0.5362921953201294\n",
            "  epoch: 9/10,    batch: 11852/15469    Encoder_loss: 0.5266206860542297\n",
            "  epoch: 9/10,    batch: 11853/15469    Encoder_loss: 0.465217262506485\n",
            "  epoch: 9/10,    batch: 11854/15469    Encoder_loss: 0.46530359983444214\n",
            "  epoch: 9/10,    batch: 11855/15469    Encoder_loss: 0.46641334891319275\n",
            "  epoch: 9/10,    batch: 11856/15469    Encoder_loss: 0.46737033128738403\n",
            "  epoch: 9/10,    batch: 11857/15469    Encoder_loss: 0.46969279646873474\n",
            "  epoch: 9/10,    batch: 11858/15469    Encoder_loss: 0.4720337986946106\n",
            "  epoch: 9/10,    batch: 11859/15469    Encoder_loss: 0.47724953293800354\n",
            "  epoch: 9/10,    batch: 11860/15469    Encoder_loss: 0.48475655913352966\n",
            "  epoch: 9/10,    batch: 11861/15469    Encoder_loss: 0.4899662435054779\n",
            "  epoch: 9/10,    batch: 11862/15469    Encoder_loss: 0.48932093381881714\n",
            "  epoch: 9/10,    batch: 11863/15469    Encoder_loss: 0.4873606562614441\n",
            "  epoch: 9/10,    batch: 11864/15469    Encoder_loss: 0.4882761836051941\n",
            "  epoch: 9/10,    batch: 11865/15469    Encoder_loss: 0.48807722330093384\n",
            "  epoch: 9/10,    batch: 11866/15469    Encoder_loss: 0.4904170036315918\n",
            "  epoch: 9/10,    batch: 11867/15469    Encoder_loss: 0.49016982316970825\n",
            "  epoch: 9/10,    batch: 11868/15469    Encoder_loss: 0.49156850576400757\n",
            "  epoch: 9/10,    batch: 11869/15469    Encoder_loss: 0.4922221601009369\n",
            "  epoch: 9/10,    batch: 11870/15469    Encoder_loss: 0.4920884072780609\n",
            "  epoch: 9/10,    batch: 11871/15469    Encoder_loss: 0.49286919832229614\n",
            "  epoch: 9/10,    batch: 11872/15469    Encoder_loss: 0.493058443069458\n",
            "  epoch: 9/10,    batch: 11873/15469    Encoder_loss: 0.4947725832462311\n",
            "  epoch: 9/10,    batch: 11874/15469    Encoder_loss: 0.49555841088294983\n",
            "  epoch: 9/10,    batch: 11875/15469    Encoder_loss: 0.495373398065567\n",
            "  epoch: 9/10,    batch: 11876/15469    Encoder_loss: 0.5131205320358276\n",
            "  epoch: 9/10,    batch: 11877/15469    Encoder_loss: 0.5444397926330566\n",
            "  epoch: 9/10,    batch: 11878/15469    Encoder_loss: 0.5387463569641113\n",
            "  epoch: 9/10,    batch: 11879/15469    Encoder_loss: 0.5224002599716187\n",
            "  epoch: 9/10,    batch: 11880/15469    Encoder_loss: 0.5215239524841309\n",
            "  epoch: 9/10,    batch: 11881/15469    Encoder_loss: 0.455058753490448\n",
            "  epoch: 9/10,    batch: 11882/15469    Encoder_loss: 0.4476235806941986\n",
            "  epoch: 9/10,    batch: 11883/15469    Encoder_loss: 0.4489414691925049\n",
            "  epoch: 9/10,    batch: 11884/15469    Encoder_loss: 0.4481629729270935\n",
            "  epoch: 9/10,    batch: 11885/15469    Encoder_loss: 0.4487190842628479\n",
            "  epoch: 9/10,    batch: 11886/15469    Encoder_loss: 0.4485061764717102\n",
            "  epoch: 9/10,    batch: 11887/15469    Encoder_loss: 0.44980955123901367\n",
            "  epoch: 9/10,    batch: 11888/15469    Encoder_loss: 0.44773736596107483\n",
            "  epoch: 9/10,    batch: 11889/15469    Encoder_loss: 0.4469124674797058\n",
            "  epoch: 9/10,    batch: 11890/15469    Encoder_loss: 0.4489058554172516\n",
            "  epoch: 9/10,    batch: 11891/15469    Encoder_loss: 0.4488105773925781\n",
            "  epoch: 9/10,    batch: 11892/15469    Encoder_loss: 0.44860395789146423\n",
            "  epoch: 9/10,    batch: 11893/15469    Encoder_loss: 0.4494014382362366\n",
            "  epoch: 9/10,    batch: 11894/15469    Encoder_loss: 0.45059338212013245\n",
            "  epoch: 9/10,    batch: 11895/15469    Encoder_loss: 0.44935178756713867\n",
            "  epoch: 9/10,    batch: 11896/15469    Encoder_loss: 0.4495978355407715\n",
            "  epoch: 9/10,    batch: 11897/15469    Encoder_loss: 0.44818824529647827\n",
            "  epoch: 9/10,    batch: 11898/15469    Encoder_loss: 0.45019254088401794\n",
            "  epoch: 9/10,    batch: 11899/15469    Encoder_loss: 0.45027726888656616\n",
            "  epoch: 9/10,    batch: 11900/15469    Encoder_loss: 0.44923096895217896\n",
            "  epoch: 9/10,    batch: 11901/15469    Encoder_loss: 0.4486214518547058\n",
            "  epoch: 9/10,    batch: 11902/15469    Encoder_loss: 0.45027366280555725\n",
            "  epoch: 9/10,    batch: 11903/15469    Encoder_loss: 0.4493369162082672\n",
            "  epoch: 9/10,    batch: 11904/15469    Encoder_loss: 0.4499228596687317\n",
            "  epoch: 9/10,    batch: 11905/15469    Encoder_loss: 0.44886988401412964\n",
            "  epoch: 9/10,    batch: 11906/15469    Encoder_loss: 0.4474925398826599\n",
            "  epoch: 9/10,    batch: 11907/15469    Encoder_loss: 0.4482482075691223\n",
            "  epoch: 9/10,    batch: 11908/15469    Encoder_loss: 0.45119017362594604\n",
            "  epoch: 9/10,    batch: 11909/15469    Encoder_loss: 0.43273240327835083\n",
            "  epoch: 9/10,    batch: 11910/15469    Encoder_loss: 0.35072702169418335\n",
            "  epoch: 9/10,    batch: 11911/15469    Encoder_loss: 0.34613916277885437\n",
            "  epoch: 9/10,    batch: 11912/15469    Encoder_loss: 0.34605085849761963\n",
            "  epoch: 9/10,    batch: 11913/15469    Encoder_loss: 0.34480875730514526\n",
            "  epoch: 9/10,    batch: 11914/15469    Encoder_loss: 0.34476786851882935\n",
            "  epoch: 9/10,    batch: 11915/15469    Encoder_loss: 0.34162935614585876\n",
            "  epoch: 9/10,    batch: 11916/15469    Encoder_loss: 0.33947744965553284\n",
            "  epoch: 9/10,    batch: 11917/15469    Encoder_loss: 0.33800336718559265\n",
            "  epoch: 9/10,    batch: 11918/15469    Encoder_loss: 0.33660516142845154\n",
            "  epoch: 9/10,    batch: 11919/15469    Encoder_loss: 0.3354090750217438\n",
            "  epoch: 9/10,    batch: 11920/15469    Encoder_loss: 0.336284339427948\n",
            "  epoch: 9/10,    batch: 11921/15469    Encoder_loss: 0.33369627594947815\n",
            "  epoch: 9/10,    batch: 11922/15469    Encoder_loss: 0.33281373977661133\n",
            "  epoch: 9/10,    batch: 11923/15469    Encoder_loss: 0.33300337195396423\n",
            "  epoch: 9/10,    batch: 11924/15469    Encoder_loss: 0.33228206634521484\n",
            "  epoch: 9/10,    batch: 11925/15469    Encoder_loss: 0.3307584524154663\n",
            "  epoch: 9/10,    batch: 11926/15469    Encoder_loss: 0.3277304768562317\n",
            "  epoch: 9/10,    batch: 11927/15469    Encoder_loss: 0.32839444279670715\n",
            "  epoch: 9/10,    batch: 11928/15469    Encoder_loss: 0.3278150260448456\n",
            "  epoch: 9/10,    batch: 11929/15469    Encoder_loss: 0.32529717683792114\n",
            "  epoch: 9/10,    batch: 11930/15469    Encoder_loss: 0.3254970908164978\n",
            "  epoch: 9/10,    batch: 11931/15469    Encoder_loss: 0.32429075241088867\n",
            "  epoch: 9/10,    batch: 11932/15469    Encoder_loss: 0.3225798010826111\n",
            "  epoch: 9/10,    batch: 11933/15469    Encoder_loss: 0.3223286271095276\n",
            "  epoch: 9/10,    batch: 11934/15469    Encoder_loss: 0.32154250144958496\n",
            "  epoch: 9/10,    batch: 11935/15469    Encoder_loss: 0.320811927318573\n",
            "  epoch: 9/10,    batch: 11936/15469    Encoder_loss: 0.32043108344078064\n",
            "  epoch: 9/10,    batch: 11937/15469    Encoder_loss: 0.318337619304657\n",
            "  epoch: 9/10,    batch: 11938/15469    Encoder_loss: 0.3177298903465271\n",
            "  epoch: 9/10,    batch: 11939/15469    Encoder_loss: 0.31627392768859863\n",
            "  epoch: 9/10,    batch: 11940/15469    Encoder_loss: 0.3149232268333435\n",
            "  epoch: 9/10,    batch: 11941/15469    Encoder_loss: 0.31561240553855896\n",
            "  epoch: 9/10,    batch: 11942/15469    Encoder_loss: 0.31335535645484924\n",
            "  epoch: 9/10,    batch: 11943/15469    Encoder_loss: 0.31462952494621277\n",
            "  epoch: 9/10,    batch: 11944/15469    Encoder_loss: 0.31220611929893494\n",
            "  epoch: 9/10,    batch: 11945/15469    Encoder_loss: 0.3113860487937927\n",
            "  epoch: 9/10,    batch: 11946/15469    Encoder_loss: 0.3118709921836853\n",
            "  epoch: 9/10,    batch: 11947/15469    Encoder_loss: 0.31067603826522827\n",
            "  epoch: 9/10,    batch: 11948/15469    Encoder_loss: 0.3089830279350281\n",
            "  epoch: 9/10,    batch: 11949/15469    Encoder_loss: 0.30910658836364746\n",
            "  epoch: 9/10,    batch: 11950/15469    Encoder_loss: 0.309515118598938\n",
            "  epoch: 9/10,    batch: 11951/15469    Encoder_loss: 0.3010527789592743\n",
            "  epoch: 9/10,    batch: 11952/15469    Encoder_loss: 0.274989515542984\n",
            "  epoch: 9/10,    batch: 11953/15469    Encoder_loss: 0.27406299114227295\n",
            "  epoch: 9/10,    batch: 11954/15469    Encoder_loss: 0.2727000117301941\n",
            "  epoch: 9/10,    batch: 11955/15469    Encoder_loss: 0.27260836958885193\n",
            "  epoch: 9/10,    batch: 11956/15469    Encoder_loss: 0.2734505832195282\n",
            "  epoch: 9/10,    batch: 11957/15469    Encoder_loss: 0.2722003757953644\n",
            "  epoch: 9/10,    batch: 11958/15469    Encoder_loss: 0.27232733368873596\n",
            "  epoch: 9/10,    batch: 11959/15469    Encoder_loss: 0.27301543951034546\n",
            "  epoch: 9/10,    batch: 11960/15469    Encoder_loss: 0.2721203863620758\n",
            "  epoch: 9/10,    batch: 11961/15469    Encoder_loss: 0.31429043412208557\n",
            "  epoch: 9/10,    batch: 11962/15469    Encoder_loss: 0.35738828778266907\n",
            "  epoch: 9/10,    batch: 11963/15469    Encoder_loss: 0.3561828136444092\n",
            "  epoch: 9/10,    batch: 11964/15469    Encoder_loss: 0.3559494614601135\n",
            "  epoch: 9/10,    batch: 11965/15469    Encoder_loss: 0.39192479848861694\n",
            "  epoch: 9/10,    batch: 11966/15469    Encoder_loss: 0.45749631524086\n",
            "  epoch: 9/10,    batch: 11967/15469    Encoder_loss: 0.45660215616226196\n",
            "  epoch: 9/10,    batch: 11968/15469    Encoder_loss: 0.45888209342956543\n",
            "  epoch: 9/10,    batch: 11969/15469    Encoder_loss: 0.45971250534057617\n",
            "  epoch: 9/10,    batch: 11970/15469    Encoder_loss: 0.4592224657535553\n",
            "  epoch: 9/10,    batch: 11971/15469    Encoder_loss: 0.460643470287323\n",
            "  epoch: 9/10,    batch: 11972/15469    Encoder_loss: 0.4602484703063965\n",
            "  epoch: 9/10,    batch: 11973/15469    Encoder_loss: 0.4618082046508789\n",
            "  epoch: 9/10,    batch: 11974/15469    Encoder_loss: 0.4625377655029297\n",
            "  epoch: 9/10,    batch: 11975/15469    Encoder_loss: 0.46472153067588806\n",
            "  epoch: 9/10,    batch: 11976/15469    Encoder_loss: 0.4633517265319824\n",
            "  epoch: 9/10,    batch: 11977/15469    Encoder_loss: 0.46541041135787964\n",
            "  epoch: 9/10,    batch: 11978/15469    Encoder_loss: 0.4675025939941406\n",
            "  epoch: 9/10,    batch: 11979/15469    Encoder_loss: 0.4678572714328766\n",
            "  epoch: 9/10,    batch: 11980/15469    Encoder_loss: 0.46906203031539917\n",
            "  epoch: 9/10,    batch: 11981/15469    Encoder_loss: 0.46962204575538635\n",
            "  epoch: 9/10,    batch: 11982/15469    Encoder_loss: 0.4710007309913635\n",
            "  epoch: 9/10,    batch: 11983/15469    Encoder_loss: 0.4736485481262207\n",
            "  epoch: 9/10,    batch: 11984/15469    Encoder_loss: 0.4740879535675049\n",
            "  epoch: 9/10,    batch: 11985/15469    Encoder_loss: 0.4746329188346863\n",
            "  epoch: 9/10,    batch: 11986/15469    Encoder_loss: 0.47450631856918335\n",
            "  epoch: 9/10,    batch: 11987/15469    Encoder_loss: 0.47711697220802307\n",
            "  epoch: 9/10,    batch: 11988/15469    Encoder_loss: 0.47795066237449646\n",
            "  epoch: 9/10,    batch: 11989/15469    Encoder_loss: 0.47975268959999084\n",
            "  epoch: 9/10,    batch: 11990/15469    Encoder_loss: 0.4806477427482605\n",
            "  epoch: 9/10,    batch: 11991/15469    Encoder_loss: 0.4832323491573334\n",
            "  epoch: 9/10,    batch: 11992/15469    Encoder_loss: 0.5108270645141602\n",
            "  epoch: 9/10,    batch: 11993/15469    Encoder_loss: 0.7088190913200378\n",
            "  epoch: 9/10,    batch: 11994/15469    Encoder_loss: 0.9163961410522461\n",
            "  epoch: 9/10,    batch: 11995/15469    Encoder_loss: 0.9014949798583984\n",
            "  epoch: 9/10,    batch: 11996/15469    Encoder_loss: 0.6929677128791809\n",
            "  epoch: 9/10,    batch: 11997/15469    Encoder_loss: 0.6760179400444031\n",
            "  epoch: 9/10,    batch: 11998/15469    Encoder_loss: 0.6936692595481873\n",
            "  epoch: 9/10,    batch: 11999/15469    Encoder_loss: 0.6942561268806458\n",
            "  epoch: 9/10,    batch: 12000/15469    Encoder_loss: 0.6425645351409912\n",
            "  epoch: 9/10,    batch: 12001/15469    Encoder_loss: 0.5988666415214539\n",
            "  epoch: 9/10,    batch: 12002/15469    Encoder_loss: 0.5679461359977722\n",
            "  epoch: 9/10,    batch: 12003/15469    Encoder_loss: 0.47426193952560425\n",
            "  epoch: 9/10,    batch: 12004/15469    Encoder_loss: 0.44356974959373474\n",
            "  epoch: 9/10,    batch: 12005/15469    Encoder_loss: 0.44486433267593384\n",
            "  epoch: 9/10,    batch: 12006/15469    Encoder_loss: 0.44487419724464417\n",
            "  epoch: 9/10,    batch: 12007/15469    Encoder_loss: 0.4439160227775574\n",
            "  epoch: 9/10,    batch: 12008/15469    Encoder_loss: 0.4451773762702942\n",
            "  epoch: 9/10,    batch: 12009/15469    Encoder_loss: 0.4502395987510681\n",
            "  epoch: 9/10,    batch: 12010/15469    Encoder_loss: 0.45833057165145874\n",
            "  epoch: 9/10,    batch: 12011/15469    Encoder_loss: 0.4628845751285553\n",
            "  epoch: 9/10,    batch: 12012/15469    Encoder_loss: 0.46273645758628845\n",
            "  epoch: 9/10,    batch: 12013/15469    Encoder_loss: 0.4614112973213196\n",
            "  epoch: 9/10,    batch: 12014/15469    Encoder_loss: 0.46064287424087524\n",
            "  epoch: 9/10,    batch: 12015/15469    Encoder_loss: 0.4596741795539856\n",
            "  epoch: 9/10,    batch: 12016/15469    Encoder_loss: 0.45825791358947754\n",
            "  epoch: 9/10,    batch: 12017/15469    Encoder_loss: 0.4583560526371002\n",
            "  epoch: 9/10,    batch: 12018/15469    Encoder_loss: 0.4584294259548187\n",
            "  epoch: 9/10,    batch: 12019/15469    Encoder_loss: 0.45674723386764526\n",
            "  epoch: 9/10,    batch: 12020/15469    Encoder_loss: 0.4569668769836426\n",
            "  epoch: 9/10,    batch: 12021/15469    Encoder_loss: 0.45694178342819214\n",
            "  epoch: 9/10,    batch: 12022/15469    Encoder_loss: 0.4560914933681488\n",
            "  epoch: 9/10,    batch: 12023/15469    Encoder_loss: 0.4572661519050598\n",
            "  epoch: 9/10,    batch: 12024/15469    Encoder_loss: 0.4575674533843994\n",
            "  epoch: 9/10,    batch: 12025/15469    Encoder_loss: 0.4570232927799225\n",
            "  epoch: 9/10,    batch: 12026/15469    Encoder_loss: 0.4571072459220886\n",
            "  epoch: 9/10,    batch: 12027/15469    Encoder_loss: 0.45692452788352966\n",
            "  epoch: 9/10,    batch: 12028/15469    Encoder_loss: 0.45739513635635376\n",
            "  epoch: 9/10,    batch: 12029/15469    Encoder_loss: 0.45744746923446655\n",
            "  epoch: 9/10,    batch: 12030/15469    Encoder_loss: 0.45841318368911743\n",
            "  epoch: 9/10,    batch: 12031/15469    Encoder_loss: 0.4616129398345947\n",
            "  epoch: 9/10,    batch: 12032/15469    Encoder_loss: 0.37527817487716675\n",
            "  epoch: 9/10,    batch: 12033/15469    Encoder_loss: 0.35528892278671265\n",
            "  epoch: 9/10,    batch: 12034/15469    Encoder_loss: 0.35355836153030396\n",
            "  epoch: 9/10,    batch: 12035/15469    Encoder_loss: 0.3521515429019928\n",
            "  epoch: 9/10,    batch: 12036/15469    Encoder_loss: 0.35121238231658936\n",
            "  epoch: 9/10,    batch: 12037/15469    Encoder_loss: 0.34854722023010254\n",
            "  epoch: 9/10,    batch: 12038/15469    Encoder_loss: 0.34763848781585693\n",
            "  epoch: 9/10,    batch: 12039/15469    Encoder_loss: 0.34679657220840454\n",
            "  epoch: 9/10,    batch: 12040/15469    Encoder_loss: 0.3451736271381378\n",
            "  epoch: 9/10,    batch: 12041/15469    Encoder_loss: 0.3445466458797455\n",
            "  epoch: 9/10,    batch: 12042/15469    Encoder_loss: 0.34352248907089233\n",
            "  epoch: 9/10,    batch: 12043/15469    Encoder_loss: 0.3429063856601715\n",
            "  epoch: 9/10,    batch: 12044/15469    Encoder_loss: 0.339861661195755\n",
            "  epoch: 9/10,    batch: 12045/15469    Encoder_loss: 0.33840376138687134\n",
            "  epoch: 9/10,    batch: 12046/15469    Encoder_loss: 0.33883216977119446\n",
            "  epoch: 9/10,    batch: 12047/15469    Encoder_loss: 0.33785369992256165\n",
            "  epoch: 9/10,    batch: 12048/15469    Encoder_loss: 0.33583444356918335\n",
            "  epoch: 9/10,    batch: 12049/15469    Encoder_loss: 0.332631915807724\n",
            "  epoch: 9/10,    batch: 12050/15469    Encoder_loss: 0.3332492411136627\n",
            "  epoch: 9/10,    batch: 12051/15469    Encoder_loss: 0.33168283104896545\n",
            "  epoch: 9/10,    batch: 12052/15469    Encoder_loss: 0.329944908618927\n",
            "  epoch: 9/10,    batch: 12053/15469    Encoder_loss: 0.3293777108192444\n",
            "  epoch: 9/10,    batch: 12054/15469    Encoder_loss: 0.32845908403396606\n",
            "  epoch: 9/10,    batch: 12055/15469    Encoder_loss: 0.32792940735816956\n",
            "  epoch: 9/10,    batch: 12056/15469    Encoder_loss: 0.3273654878139496\n",
            "  epoch: 9/10,    batch: 12057/15469    Encoder_loss: 0.32525333762168884\n",
            "  epoch: 9/10,    batch: 12058/15469    Encoder_loss: 0.3242785334587097\n",
            "  epoch: 9/10,    batch: 12059/15469    Encoder_loss: 0.3228026032447815\n",
            "  epoch: 9/10,    batch: 12060/15469    Encoder_loss: 0.3212653398513794\n",
            "  epoch: 9/10,    batch: 12061/15469    Encoder_loss: 0.32099515199661255\n",
            "  epoch: 9/10,    batch: 12062/15469    Encoder_loss: 0.32082268595695496\n",
            "  epoch: 9/10,    batch: 12063/15469    Encoder_loss: 0.31813979148864746\n",
            "  epoch: 9/10,    batch: 12064/15469    Encoder_loss: 0.31647008657455444\n",
            "  epoch: 9/10,    batch: 12065/15469    Encoder_loss: 0.31658726930618286\n",
            "  epoch: 9/10,    batch: 12066/15469    Encoder_loss: 0.3155243992805481\n",
            "  epoch: 9/10,    batch: 12067/15469    Encoder_loss: 0.31403377652168274\n",
            "  epoch: 9/10,    batch: 12068/15469    Encoder_loss: 0.31337064504623413\n",
            "  epoch: 9/10,    batch: 12069/15469    Encoder_loss: 0.3121952414512634\n",
            "  epoch: 9/10,    batch: 12070/15469    Encoder_loss: 0.3120543956756592\n",
            "  epoch: 9/10,    batch: 12071/15469    Encoder_loss: 0.3111100196838379\n",
            "  epoch: 9/10,    batch: 12072/15469    Encoder_loss: 0.31494563817977905\n",
            "  epoch: 9/10,    batch: 12073/15469    Encoder_loss: 0.298233300447464\n",
            "  epoch: 9/10,    batch: 12074/15469    Encoder_loss: 0.2764754295349121\n",
            "  epoch: 9/10,    batch: 12075/15469    Encoder_loss: 0.27499526739120483\n",
            "  epoch: 9/10,    batch: 12076/15469    Encoder_loss: 0.27456507086753845\n",
            "  epoch: 9/10,    batch: 12077/15469    Encoder_loss: 0.2745136320590973\n",
            "  epoch: 9/10,    batch: 12078/15469    Encoder_loss: 0.2740367352962494\n",
            "  epoch: 9/10,    batch: 12079/15469    Encoder_loss: 0.27287596464157104\n",
            "  epoch: 9/10,    batch: 12080/15469    Encoder_loss: 0.2723265290260315\n",
            "  epoch: 9/10,    batch: 12081/15469    Encoder_loss: 0.27319303154945374\n",
            "  epoch: 9/10,    batch: 12082/15469    Encoder_loss: 0.2714857757091522\n",
            "  epoch: 9/10,    batch: 12083/15469    Encoder_loss: 0.2715381681919098\n",
            "  epoch: 9/10,    batch: 12084/15469    Encoder_loss: 0.2716730535030365\n",
            "  epoch: 9/10,    batch: 12085/15469    Encoder_loss: 0.27213793992996216\n",
            "  epoch: 9/10,    batch: 12086/15469    Encoder_loss: 0.2716020941734314\n",
            "  epoch: 9/10,    batch: 12087/15469    Encoder_loss: 0.2711648643016815\n",
            "  epoch: 9/10,    batch: 12088/15469    Encoder_loss: 0.2718281149864197\n",
            "  epoch: 9/10,    batch: 12089/15469    Encoder_loss: 0.27135592699050903\n",
            "  epoch: 9/10,    batch: 12090/15469    Encoder_loss: 0.2720552682876587\n",
            "  epoch: 9/10,    batch: 12091/15469    Encoder_loss: 0.2713438868522644\n",
            "  epoch: 9/10,    batch: 12092/15469    Encoder_loss: 0.2697385847568512\n",
            "  epoch: 9/10,    batch: 12093/15469    Encoder_loss: 0.26929935812950134\n",
            "  epoch: 9/10,    batch: 12094/15469    Encoder_loss: 0.26585108041763306\n",
            "  epoch: 9/10,    batch: 12095/15469    Encoder_loss: 0.2641790211200714\n",
            "  epoch: 9/10,    batch: 12096/15469    Encoder_loss: 0.26578277349472046\n",
            "  epoch: 9/10,    batch: 12097/15469    Encoder_loss: 0.26471105217933655\n",
            "  epoch: 9/10,    batch: 12098/15469    Encoder_loss: 0.26579761505126953\n",
            "  epoch: 9/10,    batch: 12099/15469    Encoder_loss: 0.26592016220092773\n",
            "  epoch: 9/10,    batch: 12100/15469    Encoder_loss: 0.26502951979637146\n",
            "  epoch: 9/10,    batch: 12101/15469    Encoder_loss: 0.2653653621673584\n",
            "  epoch: 9/10,    batch: 12102/15469    Encoder_loss: 0.2655305862426758\n",
            "  epoch: 9/10,    batch: 12103/15469    Encoder_loss: 0.2667752504348755\n",
            "  epoch: 9/10,    batch: 12104/15469    Encoder_loss: 0.265337735414505\n",
            "  epoch: 9/10,    batch: 12105/15469    Encoder_loss: 0.26556989550590515\n",
            "  epoch: 9/10,    batch: 12106/15469    Encoder_loss: 0.31118136644363403\n",
            "  epoch: 9/10,    batch: 12107/15469    Encoder_loss: 0.34930628538131714\n",
            "  epoch: 9/10,    batch: 12108/15469    Encoder_loss: 0.34923985600471497\n",
            "  epoch: 9/10,    batch: 12109/15469    Encoder_loss: 0.34856536984443665\n",
            "  epoch: 9/10,    batch: 12110/15469    Encoder_loss: 0.4161863327026367\n",
            "  epoch: 9/10,    batch: 12111/15469    Encoder_loss: 0.4500845968723297\n",
            "  epoch: 9/10,    batch: 12112/15469    Encoder_loss: 0.4512249529361725\n",
            "  epoch: 9/10,    batch: 12113/15469    Encoder_loss: 0.4506736397743225\n",
            "  epoch: 9/10,    batch: 12114/15469    Encoder_loss: 0.4518129229545593\n",
            "  epoch: 9/10,    batch: 12115/15469    Encoder_loss: 0.45203715562820435\n",
            "  epoch: 9/10,    batch: 12116/15469    Encoder_loss: 0.45423048734664917\n",
            "  epoch: 9/10,    batch: 12117/15469    Encoder_loss: 0.45381322503089905\n",
            "  epoch: 9/10,    batch: 12118/15469    Encoder_loss: 0.45595043897628784\n",
            "  epoch: 9/10,    batch: 12119/15469    Encoder_loss: 0.4567689299583435\n",
            "  epoch: 9/10,    batch: 12120/15469    Encoder_loss: 0.458763986825943\n",
            "  epoch: 9/10,    batch: 12121/15469    Encoder_loss: 0.4596130847930908\n",
            "  epoch: 9/10,    batch: 12122/15469    Encoder_loss: 0.45916563272476196\n",
            "  epoch: 9/10,    batch: 12123/15469    Encoder_loss: 0.4623526632785797\n",
            "  epoch: 9/10,    batch: 12124/15469    Encoder_loss: 0.46434903144836426\n",
            "  epoch: 9/10,    batch: 12125/15469    Encoder_loss: 0.46493619680404663\n",
            "  epoch: 9/10,    batch: 12126/15469    Encoder_loss: 0.46662235260009766\n",
            "  epoch: 9/10,    batch: 12127/15469    Encoder_loss: 0.46858733892440796\n",
            "  epoch: 9/10,    batch: 12128/15469    Encoder_loss: 0.5078570246696472\n",
            "  epoch: 9/10,    batch: 12129/15469    Encoder_loss: 0.5557007193565369\n",
            "  epoch: 9/10,    batch: 12130/15469    Encoder_loss: 0.56162029504776\n",
            "  epoch: 9/10,    batch: 12131/15469    Encoder_loss: 0.6569799184799194\n",
            "  epoch: 9/10,    batch: 12132/15469    Encoder_loss: 0.6599202752113342\n",
            "  epoch: 9/10,    batch: 12133/15469    Encoder_loss: 0.6630577445030212\n",
            "  epoch: 9/10,    batch: 12134/15469    Encoder_loss: 0.6642533540725708\n",
            "  epoch: 9/10,    batch: 12135/15469    Encoder_loss: 0.6652846932411194\n",
            "  epoch: 9/10,    batch: 12136/15469    Encoder_loss: 0.6662683486938477\n",
            "  epoch: 9/10,    batch: 12137/15469    Encoder_loss: 0.667676568031311\n",
            "  epoch: 9/10,    batch: 12138/15469    Encoder_loss: 0.6712343096733093\n",
            "  epoch: 9/10,    batch: 12139/15469    Encoder_loss: 0.6725870370864868\n",
            "  epoch: 9/10,    batch: 12140/15469    Encoder_loss: 0.6737302541732788\n",
            "  epoch: 9/10,    batch: 12141/15469    Encoder_loss: 0.6765149831771851\n",
            "  epoch: 9/10,    batch: 12142/15469    Encoder_loss: 0.6786857843399048\n",
            "  epoch: 9/10,    batch: 12143/15469    Encoder_loss: 0.6874290704727173\n",
            "  epoch: 9/10,    batch: 12144/15469    Encoder_loss: 0.7263461351394653\n",
            "  epoch: 9/10,    batch: 12145/15469    Encoder_loss: 0.7374102473258972\n",
            "  epoch: 9/10,    batch: 12146/15469    Encoder_loss: 0.7115413546562195\n",
            "  epoch: 9/10,    batch: 12147/15469    Encoder_loss: 0.7135676145553589\n",
            "  epoch: 9/10,    batch: 12148/15469    Encoder_loss: 0.6518898606300354\n",
            "  epoch: 9/10,    batch: 12149/15469    Encoder_loss: 0.6410869359970093\n",
            "  epoch: 9/10,    batch: 12150/15469    Encoder_loss: 0.6412041783332825\n",
            "  epoch: 9/10,    batch: 12151/15469    Encoder_loss: 0.64423006772995\n",
            "  epoch: 9/10,    batch: 12152/15469    Encoder_loss: 0.6464443206787109\n",
            "  epoch: 9/10,    batch: 12153/15469    Encoder_loss: 0.6457828879356384\n",
            "  epoch: 9/10,    batch: 12154/15469    Encoder_loss: 0.6478408575057983\n",
            "  epoch: 9/10,    batch: 12155/15469    Encoder_loss: 0.6500347852706909\n",
            "  epoch: 9/10,    batch: 12156/15469    Encoder_loss: 0.6515640020370483\n",
            "  epoch: 9/10,    batch: 12157/15469    Encoder_loss: 0.6539660692214966\n",
            "  epoch: 9/10,    batch: 12158/15469    Encoder_loss: 0.654667854309082\n",
            "  epoch: 9/10,    batch: 12159/15469    Encoder_loss: 0.6567283272743225\n",
            "  epoch: 9/10,    batch: 12160/15469    Encoder_loss: 0.6565850973129272\n",
            "  epoch: 9/10,    batch: 12161/15469    Encoder_loss: 0.6587618589401245\n",
            "  epoch: 9/10,    batch: 12162/15469    Encoder_loss: 0.6610032320022583\n",
            "  epoch: 9/10,    batch: 12163/15469    Encoder_loss: 0.6611908674240112\n",
            "  epoch: 9/10,    batch: 12164/15469    Encoder_loss: 0.6624933481216431\n",
            "  epoch: 9/10,    batch: 12165/15469    Encoder_loss: 0.6713061332702637\n",
            "  epoch: 9/10,    batch: 12166/15469    Encoder_loss: 0.6934806108474731\n",
            "  epoch: 9/10,    batch: 12167/15469    Encoder_loss: 0.7269493341445923\n",
            "  epoch: 9/10,    batch: 12168/15469    Encoder_loss: 0.693720817565918\n",
            "  epoch: 9/10,    batch: 12169/15469    Encoder_loss: 0.6971232295036316\n",
            "  epoch: 9/10,    batch: 12170/15469    Encoder_loss: 0.6638354659080505\n",
            "  epoch: 9/10,    batch: 12171/15469    Encoder_loss: 0.6179805397987366\n",
            "  epoch: 9/10,    batch: 12172/15469    Encoder_loss: 0.6175838708877563\n",
            "  epoch: 9/10,    batch: 12173/15469    Encoder_loss: 0.6188294887542725\n",
            "  epoch: 9/10,    batch: 12174/15469    Encoder_loss: 0.619594395160675\n",
            "  epoch: 9/10,    batch: 12175/15469    Encoder_loss: 0.621282696723938\n",
            "  epoch: 9/10,    batch: 12176/15469    Encoder_loss: 0.6040655374526978\n",
            "  epoch: 9/10,    batch: 12177/15469    Encoder_loss: 0.5247014164924622\n",
            "  epoch: 9/10,    batch: 12178/15469    Encoder_loss: 0.5205256342887878\n",
            "  epoch: 9/10,    batch: 12179/15469    Encoder_loss: 0.5195620059967041\n",
            "  epoch: 9/10,    batch: 12180/15469    Encoder_loss: 0.5191952586174011\n",
            "  epoch: 9/10,    batch: 12181/15469    Encoder_loss: 0.5173167586326599\n",
            "  epoch: 9/10,    batch: 12182/15469    Encoder_loss: 0.5170205235481262\n",
            "  epoch: 9/10,    batch: 12183/15469    Encoder_loss: 0.5168343186378479\n",
            "  epoch: 9/10,    batch: 12184/15469    Encoder_loss: 0.5162359476089478\n",
            "  epoch: 9/10,    batch: 12185/15469    Encoder_loss: 0.5169221758842468\n",
            "  epoch: 9/10,    batch: 12186/15469    Encoder_loss: 0.516168475151062\n",
            "  epoch: 9/10,    batch: 12187/15469    Encoder_loss: 0.514797568321228\n",
            "  epoch: 9/10,    batch: 12188/15469    Encoder_loss: 0.5139039754867554\n",
            "  epoch: 9/10,    batch: 12189/15469    Encoder_loss: 0.5139027833938599\n",
            "  epoch: 9/10,    batch: 12190/15469    Encoder_loss: 0.5140975713729858\n",
            "  epoch: 9/10,    batch: 12191/15469    Encoder_loss: 0.5152601599693298\n",
            "  epoch: 9/10,    batch: 12192/15469    Encoder_loss: 0.5125089883804321\n",
            "  epoch: 9/10,    batch: 12193/15469    Encoder_loss: 0.5123435258865356\n",
            "  epoch: 9/10,    batch: 12194/15469    Encoder_loss: 0.5114626884460449\n",
            "  epoch: 9/10,    batch: 12195/15469    Encoder_loss: 0.5113239288330078\n",
            "  epoch: 9/10,    batch: 12196/15469    Encoder_loss: 0.5114489197731018\n",
            "  epoch: 9/10,    batch: 12197/15469    Encoder_loss: 0.5115451216697693\n",
            "  epoch: 9/10,    batch: 12198/15469    Encoder_loss: 0.5081591606140137\n",
            "  epoch: 9/10,    batch: 12199/15469    Encoder_loss: 0.42264407873153687\n",
            "  epoch: 9/10,    batch: 12200/15469    Encoder_loss: 0.40678292512893677\n",
            "  epoch: 9/10,    batch: 12201/15469    Encoder_loss: 0.40566954016685486\n",
            "  epoch: 9/10,    batch: 12202/15469    Encoder_loss: 0.4042205810546875\n",
            "  epoch: 9/10,    batch: 12203/15469    Encoder_loss: 0.4025222659111023\n",
            "  epoch: 9/10,    batch: 12204/15469    Encoder_loss: 0.4005909562110901\n",
            "  epoch: 9/10,    batch: 12205/15469    Encoder_loss: 0.39909565448760986\n",
            "  epoch: 9/10,    batch: 12206/15469    Encoder_loss: 0.3971666693687439\n",
            "  epoch: 9/10,    batch: 12207/15469    Encoder_loss: 0.3955135941505432\n",
            "  epoch: 9/10,    batch: 12208/15469    Encoder_loss: 0.3933456540107727\n",
            "  epoch: 9/10,    batch: 12209/15469    Encoder_loss: 0.3921872079372406\n",
            "  epoch: 9/10,    batch: 12210/15469    Encoder_loss: 0.3893504738807678\n",
            "  epoch: 9/10,    batch: 12211/15469    Encoder_loss: 0.3887636065483093\n",
            "  epoch: 9/10,    batch: 12212/15469    Encoder_loss: 0.3878400921821594\n",
            "  epoch: 9/10,    batch: 12213/15469    Encoder_loss: 0.3858192265033722\n",
            "  epoch: 9/10,    batch: 12214/15469    Encoder_loss: 0.38422852754592896\n",
            "  epoch: 9/10,    batch: 12215/15469    Encoder_loss: 0.3834693729877472\n",
            "  epoch: 9/10,    batch: 12216/15469    Encoder_loss: 0.38094061613082886\n",
            "  epoch: 9/10,    batch: 12217/15469    Encoder_loss: 0.38500314950942993\n",
            "  epoch: 9/10,    batch: 12218/15469    Encoder_loss: 0.3451506793498993\n",
            "  epoch: 9/10,    batch: 12219/15469    Encoder_loss: 0.3440123200416565\n",
            "  epoch: 9/10,    batch: 12220/15469    Encoder_loss: 0.3420395851135254\n",
            "  epoch: 9/10,    batch: 12221/15469    Encoder_loss: 0.34004539251327515\n",
            "  epoch: 9/10,    batch: 12222/15469    Encoder_loss: 0.3398345410823822\n",
            "  epoch: 9/10,    batch: 12223/15469    Encoder_loss: 0.33986619114875793\n",
            "  epoch: 9/10,    batch: 12224/15469    Encoder_loss: 0.33835360407829285\n",
            "  epoch: 9/10,    batch: 12225/15469    Encoder_loss: 0.33625808358192444\n",
            "  epoch: 9/10,    batch: 12226/15469    Encoder_loss: 0.3340184688568115\n",
            "  epoch: 9/10,    batch: 12227/15469    Encoder_loss: 0.3349224627017975\n",
            "  epoch: 9/10,    batch: 12228/15469    Encoder_loss: 0.33320415019989014\n",
            "  epoch: 9/10,    batch: 12229/15469    Encoder_loss: 0.33279386162757874\n",
            "  epoch: 9/10,    batch: 12230/15469    Encoder_loss: 0.33090752363204956\n",
            "  epoch: 9/10,    batch: 12231/15469    Encoder_loss: 0.3297335207462311\n",
            "  epoch: 9/10,    batch: 12232/15469    Encoder_loss: 0.32945990562438965\n",
            "  epoch: 9/10,    batch: 12233/15469    Encoder_loss: 0.3284575641155243\n",
            "  epoch: 9/10,    batch: 12234/15469    Encoder_loss: 0.32910841703414917\n",
            "  epoch: 9/10,    batch: 12235/15469    Encoder_loss: 0.32737061381340027\n",
            "  epoch: 9/10,    batch: 12236/15469    Encoder_loss: 0.32697945833206177\n",
            "  epoch: 9/10,    batch: 12237/15469    Encoder_loss: 0.3259887397289276\n",
            "  epoch: 9/10,    batch: 12238/15469    Encoder_loss: 0.3263619542121887\n",
            "  epoch: 9/10,    batch: 12239/15469    Encoder_loss: 0.3307185769081116\n",
            "  epoch: 9/10,    batch: 12240/15469    Encoder_loss: 0.2981850206851959\n",
            "  epoch: 9/10,    batch: 12241/15469    Encoder_loss: 0.28812775015830994\n",
            "  epoch: 9/10,    batch: 12242/15469    Encoder_loss: 0.287852942943573\n",
            "  epoch: 9/10,    batch: 12243/15469    Encoder_loss: 0.28675174713134766\n",
            "  epoch: 9/10,    batch: 12244/15469    Encoder_loss: 0.28528892993927\n",
            "  epoch: 9/10,    batch: 12245/15469    Encoder_loss: 0.28631335496902466\n",
            "  epoch: 9/10,    batch: 12246/15469    Encoder_loss: 0.28631460666656494\n",
            "  epoch: 9/10,    batch: 12247/15469    Encoder_loss: 0.2858084738254547\n",
            "  epoch: 9/10,    batch: 12248/15469    Encoder_loss: 0.2857055366039276\n",
            "  epoch: 9/10,    batch: 12249/15469    Encoder_loss: 0.28489482402801514\n",
            "  epoch: 9/10,    batch: 12250/15469    Encoder_loss: 0.2839621901512146\n",
            "  epoch: 9/10,    batch: 12251/15469    Encoder_loss: 0.2851009964942932\n",
            "  epoch: 9/10,    batch: 12252/15469    Encoder_loss: 0.28385040163993835\n",
            "  epoch: 9/10,    batch: 12253/15469    Encoder_loss: 0.28364259004592896\n",
            "  epoch: 9/10,    batch: 12254/15469    Encoder_loss: 0.2822924256324768\n",
            "  epoch: 9/10,    batch: 12255/15469    Encoder_loss: 0.282121866941452\n",
            "  epoch: 9/10,    batch: 12256/15469    Encoder_loss: 0.28202059864997864\n",
            "  epoch: 9/10,    batch: 12257/15469    Encoder_loss: 0.28256526589393616\n",
            "  epoch: 9/10,    batch: 12258/15469    Encoder_loss: 0.2900363504886627\n",
            "  epoch: 9/10,    batch: 12259/15469    Encoder_loss: 0.4372456669807434\n",
            "  epoch: 9/10,    batch: 12260/15469    Encoder_loss: 0.676630437374115\n",
            "  epoch: 9/10,    batch: 12261/15469    Encoder_loss: 0.7289511561393738\n",
            "  epoch: 9/10,    batch: 12262/15469    Encoder_loss: 0.5351794362068176\n",
            "  epoch: 9/10,    batch: 12263/15469    Encoder_loss: 0.46658214926719666\n",
            "  epoch: 9/10,    batch: 12264/15469    Encoder_loss: 0.467562198638916\n",
            "  epoch: 9/10,    batch: 12265/15469    Encoder_loss: 0.4640931189060211\n",
            "  epoch: 9/10,    batch: 12266/15469    Encoder_loss: 0.3941146433353424\n",
            "  epoch: 9/10,    batch: 12267/15469    Encoder_loss: 0.3546958863735199\n",
            "  epoch: 9/10,    batch: 12268/15469    Encoder_loss: 0.34429845213890076\n",
            "  epoch: 9/10,    batch: 12269/15469    Encoder_loss: 0.2838357090950012\n",
            "  epoch: 9/10,    batch: 12270/15469    Encoder_loss: 0.28146839141845703\n",
            "  epoch: 9/10,    batch: 12271/15469    Encoder_loss: 0.2798934578895569\n",
            "  epoch: 9/10,    batch: 12272/15469    Encoder_loss: 0.28133538365364075\n",
            "  epoch: 9/10,    batch: 12273/15469    Encoder_loss: 0.28055495023727417\n",
            "  epoch: 9/10,    batch: 12274/15469    Encoder_loss: 0.2810230553150177\n",
            "  epoch: 9/10,    batch: 12275/15469    Encoder_loss: 0.28566688299179077\n",
            "  epoch: 9/10,    batch: 12276/15469    Encoder_loss: 0.2948218882083893\n",
            "  epoch: 9/10,    batch: 12277/15469    Encoder_loss: 0.2980615496635437\n",
            "  epoch: 9/10,    batch: 12278/15469    Encoder_loss: 0.2970762550830841\n",
            "  epoch: 9/10,    batch: 12279/15469    Encoder_loss: 0.29538875818252563\n",
            "  epoch: 9/10,    batch: 12280/15469    Encoder_loss: 0.29346442222595215\n",
            "  epoch: 9/10,    batch: 12281/15469    Encoder_loss: 0.38954561948776245\n",
            "  epoch: 9/10,    batch: 12282/15469    Encoder_loss: 0.6342824101448059\n",
            "  epoch: 9/10,    batch: 12283/15469    Encoder_loss: 0.7427096962928772\n",
            "  epoch: 9/10,    batch: 12284/15469    Encoder_loss: 0.5949989557266235\n",
            "  epoch: 9/10,    batch: 12285/15469    Encoder_loss: 0.474232017993927\n",
            "  epoch: 9/10,    batch: 12286/15469    Encoder_loss: 0.4736652374267578\n",
            "  epoch: 9/10,    batch: 12287/15469    Encoder_loss: 0.4730251729488373\n",
            "  epoch: 9/10,    batch: 12288/15469    Encoder_loss: 0.41953930258750916\n",
            "  epoch: 9/10,    batch: 12289/15469    Encoder_loss: 0.35930001735687256\n",
            "  epoch: 9/10,    batch: 12290/15469    Encoder_loss: 0.36261439323425293\n",
            "  epoch: 9/10,    batch: 12291/15469    Encoder_loss: 0.29870176315307617\n",
            "  epoch: 9/10,    batch: 12292/15469    Encoder_loss: 0.2870068848133087\n",
            "  epoch: 9/10,    batch: 12293/15469    Encoder_loss: 0.2862040400505066\n",
            "  epoch: 9/10,    batch: 12294/15469    Encoder_loss: 0.2853541076183319\n",
            "  epoch: 9/10,    batch: 12295/15469    Encoder_loss: 0.28343504667282104\n",
            "  epoch: 9/10,    batch: 12296/15469    Encoder_loss: 0.28363293409347534\n",
            "  epoch: 9/10,    batch: 12297/15469    Encoder_loss: 0.28484874963760376\n",
            "  epoch: 9/10,    batch: 12298/15469    Encoder_loss: 0.28864580392837524\n",
            "  epoch: 9/10,    batch: 12299/15469    Encoder_loss: 0.29150310158729553\n",
            "  epoch: 9/10,    batch: 12300/15469    Encoder_loss: 0.29371368885040283\n",
            "  epoch: 9/10,    batch: 12301/15469    Encoder_loss: 0.29268088936805725\n",
            "  epoch: 9/10,    batch: 12302/15469    Encoder_loss: 0.29181498289108276\n",
            "  epoch: 9/10,    batch: 12303/15469    Encoder_loss: 0.29126593470573425\n",
            "  epoch: 9/10,    batch: 12304/15469    Encoder_loss: 0.28900420665740967\n",
            "  epoch: 9/10,    batch: 12305/15469    Encoder_loss: 0.28790169954299927\n",
            "  epoch: 9/10,    batch: 12306/15469    Encoder_loss: 0.2861745357513428\n",
            "  epoch: 9/10,    batch: 12307/15469    Encoder_loss: 0.2875838577747345\n",
            "  epoch: 9/10,    batch: 12308/15469    Encoder_loss: 0.2867259085178375\n",
            "  epoch: 9/10,    batch: 12309/15469    Encoder_loss: 0.28594499826431274\n",
            "  epoch: 9/10,    batch: 12310/15469    Encoder_loss: 0.285481721162796\n",
            "  epoch: 9/10,    batch: 12311/15469    Encoder_loss: 0.28423699736595154\n",
            "  epoch: 9/10,    batch: 12312/15469    Encoder_loss: 0.28260868787765503\n",
            "  epoch: 9/10,    batch: 12313/15469    Encoder_loss: 0.28332892060279846\n",
            "  epoch: 9/10,    batch: 12314/15469    Encoder_loss: 0.2823181450366974\n",
            "  epoch: 9/10,    batch: 12315/15469    Encoder_loss: 0.2828986644744873\n",
            "  epoch: 9/10,    batch: 12316/15469    Encoder_loss: 0.28199970722198486\n",
            "  epoch: 9/10,    batch: 12317/15469    Encoder_loss: 0.2801116704940796\n",
            "  epoch: 9/10,    batch: 12318/15469    Encoder_loss: 0.2809683680534363\n",
            "  epoch: 9/10,    batch: 12319/15469    Encoder_loss: 0.2797357439994812\n",
            "  epoch: 9/10,    batch: 12320/15469    Encoder_loss: 0.27998653054237366\n",
            "  epoch: 9/10,    batch: 12321/15469    Encoder_loss: 0.27980589866638184\n",
            "  epoch: 9/10,    batch: 12322/15469    Encoder_loss: 0.27679625153541565\n",
            "  epoch: 9/10,    batch: 12323/15469    Encoder_loss: 0.2785618305206299\n",
            "  epoch: 9/10,    batch: 12324/15469    Encoder_loss: 0.2774297595024109\n",
            "  epoch: 9/10,    batch: 12325/15469    Encoder_loss: 0.277606725692749\n",
            "  epoch: 9/10,    batch: 12326/15469    Encoder_loss: 0.2771344780921936\n",
            "  epoch: 9/10,    batch: 12327/15469    Encoder_loss: 0.2754560112953186\n",
            "  epoch: 9/10,    batch: 12328/15469    Encoder_loss: 0.2754289507865906\n",
            "  epoch: 9/10,    batch: 12329/15469    Encoder_loss: 0.2752079367637634\n",
            "  epoch: 9/10,    batch: 12330/15469    Encoder_loss: 0.27455925941467285\n",
            "  epoch: 9/10,    batch: 12331/15469    Encoder_loss: 0.2746948301792145\n",
            "  epoch: 9/10,    batch: 12332/15469    Encoder_loss: 0.27356135845184326\n",
            "  epoch: 9/10,    batch: 12333/15469    Encoder_loss: 0.27328741550445557\n",
            "  epoch: 9/10,    batch: 12334/15469    Encoder_loss: 0.2720758318901062\n",
            "  epoch: 9/10,    batch: 12335/15469    Encoder_loss: 0.2721233367919922\n",
            "  epoch: 9/10,    batch: 12336/15469    Encoder_loss: 0.27148932218551636\n",
            "  epoch: 9/10,    batch: 12337/15469    Encoder_loss: 0.2705981731414795\n",
            "  epoch: 9/10,    batch: 12338/15469    Encoder_loss: 0.2712370753288269\n",
            "  epoch: 9/10,    batch: 12339/15469    Encoder_loss: 0.26937103271484375\n",
            "  epoch: 9/10,    batch: 12340/15469    Encoder_loss: 0.2701638340950012\n",
            "  epoch: 9/10,    batch: 12341/15469    Encoder_loss: 0.27090656757354736\n",
            "  epoch: 9/10,    batch: 12342/15469    Encoder_loss: 0.27030786871910095\n",
            "  epoch: 9/10,    batch: 12343/15469    Encoder_loss: 0.2694109082221985\n",
            "  epoch: 9/10,    batch: 12344/15469    Encoder_loss: 0.2694108784198761\n",
            "  epoch: 9/10,    batch: 12345/15469    Encoder_loss: 0.26808619499206543\n",
            "  epoch: 9/10,    batch: 12346/15469    Encoder_loss: 0.2694486677646637\n",
            "  epoch: 9/10,    batch: 12347/15469    Encoder_loss: 0.26812565326690674\n",
            "  epoch: 9/10,    batch: 12348/15469    Encoder_loss: 0.268745481967926\n",
            "  epoch: 9/10,    batch: 12349/15469    Encoder_loss: 0.2683539092540741\n",
            "  epoch: 9/10,    batch: 12350/15469    Encoder_loss: 0.2664206624031067\n",
            "  epoch: 9/10,    batch: 12351/15469    Encoder_loss: 0.26793143153190613\n",
            "  epoch: 9/10,    batch: 12352/15469    Encoder_loss: 0.267782062292099\n",
            "  epoch: 9/10,    batch: 12353/15469    Encoder_loss: 0.26748567819595337\n",
            "  epoch: 9/10,    batch: 12354/15469    Encoder_loss: 0.26899564266204834\n",
            "  epoch: 9/10,    batch: 12355/15469    Encoder_loss: 0.26787224411964417\n",
            "  epoch: 9/10,    batch: 12356/15469    Encoder_loss: 0.2670251727104187\n",
            "  epoch: 9/10,    batch: 12357/15469    Encoder_loss: 0.2682899832725525\n",
            "  epoch: 9/10,    batch: 12358/15469    Encoder_loss: 0.2672506272792816\n",
            "  epoch: 9/10,    batch: 12359/15469    Encoder_loss: 0.26710817217826843\n",
            "  epoch: 9/10,    batch: 12360/15469    Encoder_loss: 0.2672637403011322\n",
            "  epoch: 9/10,    batch: 12361/15469    Encoder_loss: 0.2680240869522095\n",
            "  epoch: 9/10,    batch: 12362/15469    Encoder_loss: 0.26711350679397583\n",
            "  epoch: 9/10,    batch: 12363/15469    Encoder_loss: 0.2645600736141205\n",
            "  epoch: 9/10,    batch: 12364/15469    Encoder_loss: 0.26585477590560913\n",
            "  epoch: 9/10,    batch: 12365/15469    Encoder_loss: 0.2655467391014099\n",
            "  epoch: 9/10,    batch: 12366/15469    Encoder_loss: 0.26710325479507446\n",
            "  epoch: 9/10,    batch: 12367/15469    Encoder_loss: 0.26581570506095886\n",
            "  epoch: 9/10,    batch: 12368/15469    Encoder_loss: 0.26571157574653625\n",
            "  epoch: 9/10,    batch: 12369/15469    Encoder_loss: 0.26693195104599\n",
            "  epoch: 9/10,    batch: 12370/15469    Encoder_loss: 0.26586636900901794\n",
            "  epoch: 9/10,    batch: 12371/15469    Encoder_loss: 0.2656448781490326\n",
            "  epoch: 9/10,    batch: 12372/15469    Encoder_loss: 0.2660452723503113\n",
            "  epoch: 9/10,    batch: 12373/15469    Encoder_loss: 0.2659338414669037\n",
            "  epoch: 9/10,    batch: 12374/15469    Encoder_loss: 0.26444220542907715\n",
            "  epoch: 9/10,    batch: 12375/15469    Encoder_loss: 0.30725735425949097\n",
            "  epoch: 9/10,    batch: 12376/15469    Encoder_loss: 0.35213953256607056\n",
            "  epoch: 9/10,    batch: 12377/15469    Encoder_loss: 0.35113003849983215\n",
            "  epoch: 9/10,    batch: 12378/15469    Encoder_loss: 0.34940290451049805\n",
            "  epoch: 9/10,    batch: 12379/15469    Encoder_loss: 0.3656238317489624\n",
            "  epoch: 9/10,    batch: 12380/15469    Encoder_loss: 0.4530865550041199\n",
            "  epoch: 9/10,    batch: 12381/15469    Encoder_loss: 0.45028337836265564\n",
            "  epoch: 9/10,    batch: 12382/15469    Encoder_loss: 0.45328012108802795\n",
            "  epoch: 9/10,    batch: 12383/15469    Encoder_loss: 0.4542011320590973\n",
            "  epoch: 9/10,    batch: 12384/15469    Encoder_loss: 0.4538502097129822\n",
            "  epoch: 9/10,    batch: 12385/15469    Encoder_loss: 0.45400577783584595\n",
            "  epoch: 9/10,    batch: 12386/15469    Encoder_loss: 0.4549580216407776\n",
            "  epoch: 9/10,    batch: 12387/15469    Encoder_loss: 0.4560644030570984\n",
            "  epoch: 9/10,    batch: 12388/15469    Encoder_loss: 0.45716333389282227\n",
            "  epoch: 9/10,    batch: 12389/15469    Encoder_loss: 0.4586460590362549\n",
            "  epoch: 9/10,    batch: 12390/15469    Encoder_loss: 0.4601823687553406\n",
            "  epoch: 9/10,    batch: 12391/15469    Encoder_loss: 0.46175438165664673\n",
            "  epoch: 9/10,    batch: 12392/15469    Encoder_loss: 0.4626307487487793\n",
            "  epoch: 9/10,    batch: 12393/15469    Encoder_loss: 0.463834285736084\n",
            "  epoch: 9/10,    batch: 12394/15469    Encoder_loss: 0.46422863006591797\n",
            "  epoch: 9/10,    batch: 12395/15469    Encoder_loss: 0.4666934013366699\n",
            "  epoch: 9/10,    batch: 12396/15469    Encoder_loss: 0.4676376283168793\n",
            "  epoch: 9/10,    batch: 12397/15469    Encoder_loss: 0.46898865699768066\n",
            "  epoch: 9/10,    batch: 12398/15469    Encoder_loss: 0.4895396828651428\n",
            "  epoch: 9/10,    batch: 12399/15469    Encoder_loss: 0.5554425716400146\n",
            "  epoch: 9/10,    batch: 12400/15469    Encoder_loss: 0.5653542280197144\n",
            "  epoch: 9/10,    batch: 12401/15469    Encoder_loss: 0.6601735949516296\n",
            "  epoch: 9/10,    batch: 12402/15469    Encoder_loss: 0.6612290143966675\n",
            "  epoch: 9/10,    batch: 12403/15469    Encoder_loss: 0.6641708612442017\n",
            "  epoch: 9/10,    batch: 12404/15469    Encoder_loss: 0.6658259034156799\n",
            "  epoch: 9/10,    batch: 12405/15469    Encoder_loss: 0.6669179797172546\n",
            "  epoch: 9/10,    batch: 12406/15469    Encoder_loss: 0.6680213212966919\n",
            "  epoch: 9/10,    batch: 12407/15469    Encoder_loss: 0.6684790849685669\n",
            "  epoch: 9/10,    batch: 12408/15469    Encoder_loss: 0.6717312335968018\n",
            "  epoch: 9/10,    batch: 12409/15469    Encoder_loss: 0.6745830774307251\n",
            "  epoch: 9/10,    batch: 12410/15469    Encoder_loss: 0.6751521825790405\n",
            "  epoch: 9/10,    batch: 12411/15469    Encoder_loss: 0.6782458424568176\n",
            "  epoch: 9/10,    batch: 12412/15469    Encoder_loss: 0.678164541721344\n",
            "  epoch: 9/10,    batch: 12413/15469    Encoder_loss: 0.7031198740005493\n",
            "  epoch: 9/10,    batch: 12414/15469    Encoder_loss: 0.742518424987793\n",
            "  epoch: 9/10,    batch: 12415/15469    Encoder_loss: 0.7108531594276428\n",
            "  epoch: 9/10,    batch: 12416/15469    Encoder_loss: 0.7178407907485962\n",
            "  epoch: 9/10,    batch: 12417/15469    Encoder_loss: 0.6756986379623413\n",
            "  epoch: 9/10,    batch: 12418/15469    Encoder_loss: 0.6416018009185791\n",
            "  epoch: 9/10,    batch: 12419/15469    Encoder_loss: 0.6408249139785767\n",
            "  epoch: 9/10,    batch: 12420/15469    Encoder_loss: 0.6426030397415161\n",
            "  epoch: 9/10,    batch: 12421/15469    Encoder_loss: 0.643830418586731\n",
            "  epoch: 9/10,    batch: 12422/15469    Encoder_loss: 0.6456643342971802\n",
            "  epoch: 9/10,    batch: 12423/15469    Encoder_loss: 0.6472137570381165\n",
            "  epoch: 9/10,    batch: 12424/15469    Encoder_loss: 0.6474619507789612\n",
            "  epoch: 9/10,    batch: 12425/15469    Encoder_loss: 0.6492928266525269\n",
            "  epoch: 9/10,    batch: 12426/15469    Encoder_loss: 0.6508044004440308\n",
            "  epoch: 9/10,    batch: 12427/15469    Encoder_loss: 0.6519221067428589\n",
            "  epoch: 9/10,    batch: 12428/15469    Encoder_loss: 0.6537671089172363\n",
            "  epoch: 9/10,    batch: 12429/15469    Encoder_loss: 0.6552542448043823\n",
            "  epoch: 9/10,    batch: 12430/15469    Encoder_loss: 0.6560007333755493\n",
            "  epoch: 9/10,    batch: 12431/15469    Encoder_loss: 0.6579982042312622\n",
            "  epoch: 9/10,    batch: 12432/15469    Encoder_loss: 0.6601241230964661\n",
            "  epoch: 9/10,    batch: 12433/15469    Encoder_loss: 0.6606830358505249\n",
            "  epoch: 9/10,    batch: 12434/15469    Encoder_loss: 0.6615017056465149\n",
            "  epoch: 9/10,    batch: 12435/15469    Encoder_loss: 0.6746100187301636\n",
            "  epoch: 9/10,    batch: 12436/15469    Encoder_loss: 0.6941068768501282\n",
            "  epoch: 9/10,    batch: 12437/15469    Encoder_loss: 0.7098903656005859\n",
            "  epoch: 9/10,    batch: 12438/15469    Encoder_loss: 0.7176402807235718\n",
            "  epoch: 9/10,    batch: 12439/15469    Encoder_loss: 0.6901354193687439\n",
            "  epoch: 9/10,    batch: 12440/15469    Encoder_loss: 0.6938625574111938\n",
            "  epoch: 9/10,    batch: 12441/15469    Encoder_loss: 0.6332336664199829\n",
            "  epoch: 9/10,    batch: 12442/15469    Encoder_loss: 0.618409276008606\n",
            "  epoch: 9/10,    batch: 12443/15469    Encoder_loss: 0.6187198758125305\n",
            "  epoch: 9/10,    batch: 12444/15469    Encoder_loss: 0.6187974214553833\n",
            "  epoch: 9/10,    batch: 12445/15469    Encoder_loss: 0.6268466114997864\n",
            "  epoch: 9/10,    batch: 12446/15469    Encoder_loss: 0.5518298149108887\n",
            "  epoch: 9/10,    batch: 12447/15469    Encoder_loss: 0.5209236741065979\n",
            "  epoch: 9/10,    batch: 12448/15469    Encoder_loss: 0.5201206803321838\n",
            "  epoch: 9/10,    batch: 12449/15469    Encoder_loss: 0.519011378288269\n",
            "  epoch: 9/10,    batch: 12450/15469    Encoder_loss: 0.5182120203971863\n",
            "  epoch: 9/10,    batch: 12451/15469    Encoder_loss: 0.517774760723114\n",
            "  epoch: 9/10,    batch: 12452/15469    Encoder_loss: 0.5160950422286987\n",
            "  epoch: 9/10,    batch: 12453/15469    Encoder_loss: 0.5152165293693542\n",
            "  epoch: 9/10,    batch: 12454/15469    Encoder_loss: 0.5139186382293701\n",
            "  epoch: 9/10,    batch: 12455/15469    Encoder_loss: 0.5145079493522644\n",
            "  epoch: 9/10,    batch: 12456/15469    Encoder_loss: 0.5145995616912842\n",
            "  epoch: 9/10,    batch: 12457/15469    Encoder_loss: 0.5138012170791626\n",
            "  epoch: 9/10,    batch: 12458/15469    Encoder_loss: 0.5132409334182739\n",
            "  epoch: 9/10,    batch: 12459/15469    Encoder_loss: 0.5124126672744751\n",
            "  epoch: 9/10,    batch: 12460/15469    Encoder_loss: 0.5121525526046753\n",
            "  epoch: 9/10,    batch: 12461/15469    Encoder_loss: 0.5128668546676636\n",
            "  epoch: 9/10,    batch: 12462/15469    Encoder_loss: 0.5131074786186218\n",
            "  epoch: 9/10,    batch: 12463/15469    Encoder_loss: 0.5116666555404663\n",
            "  epoch: 9/10,    batch: 12464/15469    Encoder_loss: 0.5109437108039856\n",
            "  epoch: 9/10,    batch: 12465/15469    Encoder_loss: 0.5116086602210999\n",
            "  epoch: 9/10,    batch: 12466/15469    Encoder_loss: 0.5095144510269165\n",
            "  epoch: 9/10,    batch: 12467/15469    Encoder_loss: 0.5106300115585327\n",
            "  epoch: 9/10,    batch: 12468/15469    Encoder_loss: 0.509827733039856\n",
            "  epoch: 9/10,    batch: 12469/15469    Encoder_loss: 0.5106041431427002\n",
            "  epoch: 9/10,    batch: 12470/15469    Encoder_loss: 0.42475128173828125\n",
            "  epoch: 9/10,    batch: 12471/15469    Encoder_loss: 0.407527357339859\n",
            "  epoch: 9/10,    batch: 12472/15469    Encoder_loss: 0.4066014289855957\n",
            "  epoch: 9/10,    batch: 12473/15469    Encoder_loss: 0.402862548828125\n",
            "  epoch: 9/10,    batch: 12474/15469    Encoder_loss: 0.40105071663856506\n",
            "  epoch: 9/10,    batch: 12475/15469    Encoder_loss: 0.4005410075187683\n",
            "  epoch: 9/10,    batch: 12476/15469    Encoder_loss: 0.3986723721027374\n",
            "  epoch: 9/10,    batch: 12477/15469    Encoder_loss: 0.39646098017692566\n",
            "  epoch: 9/10,    batch: 12478/15469    Encoder_loss: 0.3949812352657318\n",
            "  epoch: 9/10,    batch: 12479/15469    Encoder_loss: 0.3944193720817566\n",
            "  epoch: 9/10,    batch: 12480/15469    Encoder_loss: 0.3925624489784241\n",
            "  epoch: 9/10,    batch: 12481/15469    Encoder_loss: 0.3895346522331238\n",
            "  epoch: 9/10,    batch: 12482/15469    Encoder_loss: 0.3881947994232178\n",
            "  epoch: 9/10,    batch: 12483/15469    Encoder_loss: 0.3859333395957947\n",
            "  epoch: 9/10,    batch: 12484/15469    Encoder_loss: 0.38375765085220337\n",
            "  epoch: 9/10,    batch: 12485/15469    Encoder_loss: 0.38312745094299316\n",
            "  epoch: 9/10,    batch: 12486/15469    Encoder_loss: 0.3826044201850891\n",
            "  epoch: 9/10,    batch: 12487/15469    Encoder_loss: 0.38577035069465637\n",
            "  epoch: 9/10,    batch: 12488/15469    Encoder_loss: 0.3594190180301666\n",
            "  epoch: 9/10,    batch: 12489/15469    Encoder_loss: 0.3435557186603546\n",
            "  epoch: 9/10,    batch: 12490/15469    Encoder_loss: 0.34180551767349243\n",
            "  epoch: 9/10,    batch: 12491/15469    Encoder_loss: 0.34070196747779846\n",
            "  epoch: 9/10,    batch: 12492/15469    Encoder_loss: 0.3397873342037201\n",
            "  epoch: 9/10,    batch: 12493/15469    Encoder_loss: 0.3385116159915924\n",
            "  epoch: 9/10,    batch: 12494/15469    Encoder_loss: 0.3385758399963379\n",
            "  epoch: 9/10,    batch: 12495/15469    Encoder_loss: 0.3379681706428528\n",
            "  epoch: 9/10,    batch: 12496/15469    Encoder_loss: 0.3361019492149353\n",
            "  epoch: 9/10,    batch: 12497/15469    Encoder_loss: 0.3339664041996002\n",
            "  epoch: 9/10,    batch: 12498/15469    Encoder_loss: 0.3334399461746216\n",
            "  epoch: 9/10,    batch: 12499/15469    Encoder_loss: 0.3325270712375641\n",
            "  epoch: 9/10,    batch: 12500/15469    Encoder_loss: 0.33109167218208313\n",
            "  epoch: 9/10,    batch: 12501/15469    Encoder_loss: 0.33133259415626526\n",
            "  epoch: 9/10,    batch: 12502/15469    Encoder_loss: 0.32991012930870056\n",
            "  epoch: 9/10,    batch: 12503/15469    Encoder_loss: 0.32911771535873413\n",
            "  epoch: 9/10,    batch: 12504/15469    Encoder_loss: 0.3279074728488922\n",
            "  epoch: 9/10,    batch: 12505/15469    Encoder_loss: 0.32675111293792725\n",
            "  epoch: 9/10,    batch: 12506/15469    Encoder_loss: 0.32749128341674805\n",
            "  epoch: 9/10,    batch: 12507/15469    Encoder_loss: 0.32409119606018066\n",
            "  epoch: 9/10,    batch: 12508/15469    Encoder_loss: 0.32467517256736755\n",
            "  epoch: 9/10,    batch: 12509/15469    Encoder_loss: 0.3240373730659485\n",
            "  epoch: 9/10,    batch: 12510/15469    Encoder_loss: 0.3247780203819275\n",
            "  epoch: 9/10,    batch: 12511/15469    Encoder_loss: 0.31352776288986206\n",
            "  epoch: 9/10,    batch: 12512/15469    Encoder_loss: 0.2875637710094452\n",
            "  epoch: 9/10,    batch: 12513/15469    Encoder_loss: 0.28700894117355347\n",
            "  epoch: 9/10,    batch: 12514/15469    Encoder_loss: 0.2864607572555542\n",
            "  epoch: 9/10,    batch: 12515/15469    Encoder_loss: 0.2867516279220581\n",
            "  epoch: 9/10,    batch: 12516/15469    Encoder_loss: 0.2865923345088959\n",
            "  epoch: 9/10,    batch: 12517/15469    Encoder_loss: 0.28623560070991516\n",
            "  epoch: 9/10,    batch: 12518/15469    Encoder_loss: 0.2861340641975403\n",
            "  epoch: 9/10,    batch: 12519/15469    Encoder_loss: 0.28511935472488403\n",
            "  epoch: 9/10,    batch: 12520/15469    Encoder_loss: 0.28507184982299805\n",
            "  epoch: 9/10,    batch: 12521/15469    Encoder_loss: 0.2843312621116638\n",
            "  epoch: 9/10,    batch: 12522/15469    Encoder_loss: 0.2845071852207184\n",
            "  epoch: 9/10,    batch: 12523/15469    Encoder_loss: 0.28470051288604736\n",
            "  epoch: 9/10,    batch: 12524/15469    Encoder_loss: 0.28387579321861267\n",
            "  epoch: 9/10,    batch: 12525/15469    Encoder_loss: 0.2831396758556366\n",
            "  epoch: 9/10,    batch: 12526/15469    Encoder_loss: 0.2826484739780426\n",
            "  epoch: 9/10,    batch: 12527/15469    Encoder_loss: 0.2829352617263794\n",
            "  epoch: 9/10,    batch: 12528/15469    Encoder_loss: 0.2857566773891449\n",
            "  epoch: 9/10,    batch: 12529/15469    Encoder_loss: 0.42798733711242676\n",
            "  epoch: 9/10,    batch: 12530/15469    Encoder_loss: 0.6757022738456726\n",
            "  epoch: 9/10,    batch: 12531/15469    Encoder_loss: 0.7329334616661072\n",
            "  epoch: 9/10,    batch: 12532/15469    Encoder_loss: 0.5474856495857239\n",
            "  epoch: 9/10,    batch: 12533/15469    Encoder_loss: 0.4674047827720642\n",
            "  epoch: 9/10,    batch: 12534/15469    Encoder_loss: 0.46708908677101135\n",
            "  epoch: 9/10,    batch: 12535/15469    Encoder_loss: 0.4674311578273773\n",
            "  epoch: 9/10,    batch: 12536/15469    Encoder_loss: 0.3989332318305969\n",
            "  epoch: 9/10,    batch: 12537/15469    Encoder_loss: 0.35456377267837524\n",
            "  epoch: 9/10,    batch: 12538/15469    Encoder_loss: 0.3485061228275299\n",
            "  epoch: 9/10,    batch: 12539/15469    Encoder_loss: 0.28440728783607483\n",
            "  epoch: 9/10,    batch: 12540/15469    Encoder_loss: 0.28176528215408325\n",
            "  epoch: 9/10,    batch: 12541/15469    Encoder_loss: 0.2817460000514984\n",
            "  epoch: 9/10,    batch: 12542/15469    Encoder_loss: 0.28078073263168335\n",
            "  epoch: 9/10,    batch: 12543/15469    Encoder_loss: 0.28103190660476685\n",
            "  epoch: 9/10,    batch: 12544/15469    Encoder_loss: 0.2829814553260803\n",
            "  epoch: 9/10,    batch: 12545/15469    Encoder_loss: 0.28867268562316895\n",
            "  epoch: 9/10,    batch: 12546/15469    Encoder_loss: 0.2970691919326782\n",
            "  epoch: 9/10,    batch: 12547/15469    Encoder_loss: 0.29917725920677185\n",
            "  epoch: 9/10,    batch: 12548/15469    Encoder_loss: 0.2956162989139557\n",
            "  epoch: 9/10,    batch: 12549/15469    Encoder_loss: 0.29301929473876953\n",
            "  epoch: 9/10,    batch: 12550/15469    Encoder_loss: 0.2902507781982422\n",
            "  epoch: 9/10,    batch: 12551/15469    Encoder_loss: 0.38878288865089417\n",
            "  epoch: 9/10,    batch: 12552/15469    Encoder_loss: 0.631575345993042\n",
            "  epoch: 9/10,    batch: 12553/15469    Encoder_loss: 0.7422081232070923\n",
            "  epoch: 9/10,    batch: 12554/15469    Encoder_loss: 0.6109468936920166\n",
            "  epoch: 9/10,    batch: 12555/15469    Encoder_loss: 0.4753292202949524\n",
            "  epoch: 9/10,    batch: 12556/15469    Encoder_loss: 0.47584661841392517\n",
            "  epoch: 9/10,    batch: 12557/15469    Encoder_loss: 0.47508469223976135\n",
            "  epoch: 9/10,    batch: 12558/15469    Encoder_loss: 0.42799386382102966\n",
            "  epoch: 9/10,    batch: 12559/15469    Encoder_loss: 0.35901063680648804\n",
            "  epoch: 9/10,    batch: 12560/15469    Encoder_loss: 0.36183562874794006\n",
            "  epoch: 9/10,    batch: 12561/15469    Encoder_loss: 0.2986566424369812\n",
            "  epoch: 9/10,    batch: 12562/15469    Encoder_loss: 0.28789228200912476\n",
            "  epoch: 9/10,    batch: 12563/15469    Encoder_loss: 0.2854125499725342\n",
            "  epoch: 9/10,    batch: 12564/15469    Encoder_loss: 0.2851220667362213\n",
            "  epoch: 9/10,    batch: 12565/15469    Encoder_loss: 0.2849850058555603\n",
            "  epoch: 9/10,    batch: 12566/15469    Encoder_loss: 0.28438422083854675\n",
            "  epoch: 9/10,    batch: 12567/15469    Encoder_loss: 0.28631776571273804\n",
            "  epoch: 9/10,    batch: 12568/15469    Encoder_loss: 0.29084447026252747\n",
            "  epoch: 9/10,    batch: 12569/15469    Encoder_loss: 0.29572510719299316\n",
            "  epoch: 9/10,    batch: 12570/15469    Encoder_loss: 0.29563432931900024\n",
            "  epoch: 9/10,    batch: 12571/15469    Encoder_loss: 0.2928691506385803\n",
            "  epoch: 9/10,    batch: 12572/15469    Encoder_loss: 0.2923430800437927\n",
            "  epoch: 9/10,    batch: 12573/15469    Encoder_loss: 0.29091793298721313\n",
            "  epoch: 9/10,    batch: 12574/15469    Encoder_loss: 0.2908061146736145\n",
            "  epoch: 9/10,    batch: 12575/15469    Encoder_loss: 0.2904600501060486\n",
            "  epoch: 9/10,    batch: 12576/15469    Encoder_loss: 0.2887716293334961\n",
            "  epoch: 9/10,    batch: 12577/15469    Encoder_loss: 0.2878498435020447\n",
            "  epoch: 9/10,    batch: 12578/15469    Encoder_loss: 0.2877689003944397\n",
            "  epoch: 9/10,    batch: 12579/15469    Encoder_loss: 0.28587624430656433\n",
            "  epoch: 9/10,    batch: 12580/15469    Encoder_loss: 0.2866309881210327\n",
            "  epoch: 9/10,    batch: 12581/15469    Encoder_loss: 0.28515100479125977\n",
            "  epoch: 9/10,    batch: 12582/15469    Encoder_loss: 0.2850803732872009\n",
            "  epoch: 9/10,    batch: 12583/15469    Encoder_loss: 0.2833779752254486\n",
            "  epoch: 9/10,    batch: 12584/15469    Encoder_loss: 0.28425461053848267\n",
            "  epoch: 9/10,    batch: 12585/15469    Encoder_loss: 0.28510767221450806\n",
            "  epoch: 9/10,    batch: 12586/15469    Encoder_loss: 0.2829328179359436\n",
            "  epoch: 9/10,    batch: 12587/15469    Encoder_loss: 0.28257399797439575\n",
            "  epoch: 9/10,    batch: 12588/15469    Encoder_loss: 0.28157907724380493\n",
            "  epoch: 9/10,    batch: 12589/15469    Encoder_loss: 0.2819008529186249\n",
            "  epoch: 9/10,    batch: 12590/15469    Encoder_loss: 0.28199076652526855\n",
            "  epoch: 9/10,    batch: 12591/15469    Encoder_loss: 0.28302323818206787\n",
            "  epoch: 9/10,    batch: 12592/15469    Encoder_loss: 0.28050851821899414\n",
            "  epoch: 9/10,    batch: 12593/15469    Encoder_loss: 0.2795626223087311\n",
            "  epoch: 9/10,    batch: 12594/15469    Encoder_loss: 0.2793053388595581\n",
            "  epoch: 9/10,    batch: 12595/15469    Encoder_loss: 0.27626410126686096\n",
            "  epoch: 9/10,    batch: 12596/15469    Encoder_loss: 0.2775377631187439\n",
            "  epoch: 9/10,    batch: 12597/15469    Encoder_loss: 0.27728432416915894\n",
            "  epoch: 9/10,    batch: 12598/15469    Encoder_loss: 0.2753694951534271\n",
            "  epoch: 9/10,    batch: 12599/15469    Encoder_loss: 0.27623939514160156\n",
            "  epoch: 9/10,    batch: 12600/15469    Encoder_loss: 0.2771911919116974\n",
            "  epoch: 9/10,    batch: 12601/15469    Encoder_loss: 0.2767968773841858\n",
            "  epoch: 9/10,    batch: 12602/15469    Encoder_loss: 0.2753838896751404\n",
            "  epoch: 9/10,    batch: 12603/15469    Encoder_loss: 0.27598434686660767\n",
            "  epoch: 9/10,    batch: 12604/15469    Encoder_loss: 0.27453750371932983\n",
            "  epoch: 9/10,    batch: 12605/15469    Encoder_loss: 0.27408191561698914\n",
            "  epoch: 9/10,    batch: 12606/15469    Encoder_loss: 0.2745509743690491\n",
            "  epoch: 9/10,    batch: 12607/15469    Encoder_loss: 0.2730382978916168\n",
            "  epoch: 9/10,    batch: 12608/15469    Encoder_loss: 0.2728663682937622\n",
            "  epoch: 9/10,    batch: 12609/15469    Encoder_loss: 0.27270057797431946\n",
            "  epoch: 9/10,    batch: 12610/15469    Encoder_loss: 0.2723062336444855\n",
            "  epoch: 9/10,    batch: 12611/15469    Encoder_loss: 0.2706514000892639\n",
            "  epoch: 9/10,    batch: 12612/15469    Encoder_loss: 0.2718852758407593\n",
            "  epoch: 9/10,    batch: 12613/15469    Encoder_loss: 0.27030450105667114\n",
            "  epoch: 9/10,    batch: 12614/15469    Encoder_loss: 0.2713266611099243\n",
            "  epoch: 9/10,    batch: 12615/15469    Encoder_loss: 0.2711232900619507\n",
            "  epoch: 9/10,    batch: 12616/15469    Encoder_loss: 0.2695702910423279\n",
            "  epoch: 9/10,    batch: 12617/15469    Encoder_loss: 0.2699032425880432\n",
            "  epoch: 9/10,    batch: 12618/15469    Encoder_loss: 0.269695520401001\n",
            "  epoch: 9/10,    batch: 12619/15469    Encoder_loss: 0.2683519721031189\n",
            "  epoch: 9/10,    batch: 12620/15469    Encoder_loss: 0.2702071964740753\n",
            "  epoch: 9/10,    batch: 12621/15469    Encoder_loss: 0.26832813024520874\n",
            "  epoch: 9/10,    batch: 12622/15469    Encoder_loss: 0.26845115423202515\n",
            "  epoch: 9/10,    batch: 12623/15469    Encoder_loss: 0.26766902208328247\n",
            "  epoch: 9/10,    batch: 12624/15469    Encoder_loss: 0.26753291487693787\n",
            "  epoch: 9/10,    batch: 12625/15469    Encoder_loss: 0.2691929340362549\n",
            "  epoch: 9/10,    batch: 12626/15469    Encoder_loss: 0.2678464353084564\n",
            "  epoch: 9/10,    batch: 12627/15469    Encoder_loss: 0.26807597279548645\n",
            "  epoch: 9/10,    batch: 12628/15469    Encoder_loss: 0.2686999440193176\n",
            "  epoch: 9/10,    batch: 12629/15469    Encoder_loss: 0.2664492130279541\n",
            "  epoch: 9/10,    batch: 12630/15469    Encoder_loss: 0.26799365878105164\n",
            "  epoch: 9/10,    batch: 12631/15469    Encoder_loss: 0.26684144139289856\n",
            "  epoch: 9/10,    batch: 12632/15469    Encoder_loss: 0.26677384972572327\n",
            "  epoch: 9/10,    batch: 12633/15469    Encoder_loss: 0.26640933752059937\n",
            "  epoch: 9/10,    batch: 12634/15469    Encoder_loss: 0.26687905192375183\n",
            "  epoch: 9/10,    batch: 12635/15469    Encoder_loss: 0.26730382442474365\n",
            "  epoch: 9/10,    batch: 12636/15469    Encoder_loss: 0.26815590262413025\n",
            "  epoch: 9/10,    batch: 12637/15469    Encoder_loss: 0.26658743619918823\n",
            "  epoch: 9/10,    batch: 12638/15469    Encoder_loss: 0.26617851853370667\n",
            "  epoch: 9/10,    batch: 12639/15469    Encoder_loss: 0.2670655846595764\n",
            "  epoch: 9/10,    batch: 12640/15469    Encoder_loss: 0.26663950085639954\n",
            "  epoch: 9/10,    batch: 12641/15469    Encoder_loss: 0.2676886320114136\n",
            "  epoch: 9/10,    batch: 12642/15469    Encoder_loss: 0.2661336064338684\n",
            "  epoch: 9/10,    batch: 12643/15469    Encoder_loss: 0.26748695969581604\n",
            "  epoch: 9/10,    batch: 12644/15469    Encoder_loss: 0.2683076858520508\n",
            "  epoch: 9/10,    batch: 12645/15469    Encoder_loss: 0.2674594521522522\n",
            "  epoch: 9/10,    batch: 12646/15469    Encoder_loss: 0.2673867344856262\n",
            "  epoch: 9/10,    batch: 12647/15469    Encoder_loss: 0.2671152353286743\n",
            "  epoch: 9/10,    batch: 12648/15469    Encoder_loss: 0.2706228792667389\n",
            "  epoch: 9/10,    batch: 12649/15469    Encoder_loss: 0.3445908725261688\n",
            "  epoch: 9/10,    batch: 12650/15469    Encoder_loss: 0.35081344842910767\n",
            "  epoch: 9/10,    batch: 12651/15469    Encoder_loss: 0.35225406289100647\n",
            "  epoch: 9/10,    batch: 12652/15469    Encoder_loss: 0.4475531578063965\n",
            "  epoch: 9/10,    batch: 12653/15469    Encoder_loss: 0.4522487223148346\n",
            "  epoch: 9/10,    batch: 12654/15469    Encoder_loss: 0.4524642527103424\n",
            "  epoch: 9/10,    batch: 12655/15469    Encoder_loss: 0.4535542130470276\n",
            "  epoch: 9/10,    batch: 12656/15469    Encoder_loss: 0.4550169110298157\n",
            "  epoch: 9/10,    batch: 12657/15469    Encoder_loss: 0.4545532166957855\n",
            "  epoch: 9/10,    batch: 12658/15469    Encoder_loss: 0.45763522386550903\n",
            "  epoch: 9/10,    batch: 12659/15469    Encoder_loss: 0.457610547542572\n",
            "  epoch: 9/10,    batch: 12660/15469    Encoder_loss: 0.45630717277526855\n",
            "  epoch: 9/10,    batch: 12661/15469    Encoder_loss: 0.4589598774909973\n",
            "  epoch: 9/10,    batch: 12662/15469    Encoder_loss: 0.4606720805168152\n",
            "  epoch: 9/10,    batch: 12663/15469    Encoder_loss: 0.4611263871192932\n",
            "  epoch: 9/10,    batch: 12664/15469    Encoder_loss: 0.4634307026863098\n",
            "  epoch: 9/10,    batch: 12665/15469    Encoder_loss: 0.464357852935791\n",
            "  epoch: 9/10,    batch: 12666/15469    Encoder_loss: 0.46704167127609253\n",
            "  epoch: 9/10,    batch: 12667/15469    Encoder_loss: 0.4965771734714508\n",
            "  epoch: 9/10,    batch: 12668/15469    Encoder_loss: 0.5531964302062988\n",
            "  epoch: 9/10,    batch: 12669/15469    Encoder_loss: 0.5548703074455261\n",
            "  epoch: 9/10,    batch: 12670/15469    Encoder_loss: 0.6063634753227234\n",
            "  epoch: 9/10,    batch: 12671/15469    Encoder_loss: 0.6570677757263184\n",
            "  epoch: 9/10,    batch: 12672/15469    Encoder_loss: 0.6598885655403137\n",
            "  epoch: 9/10,    batch: 12673/15469    Encoder_loss: 0.6630496978759766\n",
            "  epoch: 9/10,    batch: 12674/15469    Encoder_loss: 0.6658514142036438\n",
            "  epoch: 9/10,    batch: 12675/15469    Encoder_loss: 0.66728675365448\n",
            "  epoch: 9/10,    batch: 12676/15469    Encoder_loss: 0.6674469709396362\n",
            "  epoch: 9/10,    batch: 12677/15469    Encoder_loss: 0.67042076587677\n",
            "  epoch: 9/10,    batch: 12678/15469    Encoder_loss: 0.6717145442962646\n",
            "  epoch: 9/10,    batch: 12679/15469    Encoder_loss: 0.6772246360778809\n",
            "  epoch: 9/10,    batch: 12680/15469    Encoder_loss: 0.676997184753418\n",
            "  epoch: 9/10,    batch: 12681/15469    Encoder_loss: 0.6773546934127808\n",
            "  epoch: 9/10,    batch: 12682/15469    Encoder_loss: 0.6811330914497375\n",
            "  epoch: 9/10,    batch: 12683/15469    Encoder_loss: 0.6832577586174011\n",
            "  epoch: 9/10,    batch: 12684/15469    Encoder_loss: 0.6859294176101685\n",
            "  epoch: 9/10,    batch: 12685/15469    Encoder_loss: 0.6872462034225464\n",
            "  epoch: 9/10,    batch: 12686/15469    Encoder_loss: 0.7160239219665527\n",
            "  epoch: 9/10,    batch: 12687/15469    Encoder_loss: 0.7533012628555298\n",
            "  epoch: 9/10,    batch: 12688/15469    Encoder_loss: 0.7222961187362671\n",
            "  epoch: 9/10,    batch: 12689/15469    Encoder_loss: 0.7239941358566284\n",
            "  epoch: 9/10,    batch: 12690/15469    Encoder_loss: 0.6924924850463867\n",
            "  epoch: 9/10,    batch: 12691/15469    Encoder_loss: 0.6478102803230286\n",
            "  epoch: 9/10,    batch: 12692/15469    Encoder_loss: 0.6500226259231567\n",
            "  epoch: 9/10,    batch: 12693/15469    Encoder_loss: 0.6497185230255127\n",
            "  epoch: 9/10,    batch: 12694/15469    Encoder_loss: 0.6527032256126404\n",
            "  epoch: 9/10,    batch: 12695/15469    Encoder_loss: 0.6517778635025024\n",
            "  epoch: 9/10,    batch: 12696/15469    Encoder_loss: 0.6544156670570374\n",
            "  epoch: 9/10,    batch: 12697/15469    Encoder_loss: 0.6560074090957642\n",
            "  epoch: 9/10,    batch: 12698/15469    Encoder_loss: 0.6568734049797058\n",
            "  epoch: 9/10,    batch: 12699/15469    Encoder_loss: 0.658868670463562\n",
            "  epoch: 9/10,    batch: 12700/15469    Encoder_loss: 0.6614261269569397\n",
            "  epoch: 9/10,    batch: 12701/15469    Encoder_loss: 0.6617586016654968\n",
            "  epoch: 9/10,    batch: 12702/15469    Encoder_loss: 0.6622780561447144\n",
            "  epoch: 9/10,    batch: 12703/15469    Encoder_loss: 0.6659032702445984\n",
            "  epoch: 9/10,    batch: 12704/15469    Encoder_loss: 0.6683756709098816\n",
            "  epoch: 9/10,    batch: 12705/15469    Encoder_loss: 0.6980783343315125\n",
            "  epoch: 9/10,    batch: 12706/15469    Encoder_loss: 0.7274392247200012\n",
            "  epoch: 9/10,    batch: 12707/15469    Encoder_loss: 0.7013855576515198\n",
            "  epoch: 9/10,    batch: 12708/15469    Encoder_loss: 0.6959460377693176\n",
            "  epoch: 9/10,    batch: 12709/15469    Encoder_loss: 0.6761698126792908\n",
            "  epoch: 9/10,    batch: 12710/15469    Encoder_loss: 0.621146559715271\n",
            "  epoch: 9/10,    batch: 12711/15469    Encoder_loss: 0.6198304891586304\n",
            "  epoch: 9/10,    batch: 12712/15469    Encoder_loss: 0.6216410994529724\n",
            "  epoch: 9/10,    batch: 12713/15469    Encoder_loss: 0.6233628392219543\n",
            "  epoch: 9/10,    batch: 12714/15469    Encoder_loss: 0.6221352815628052\n",
            "  epoch: 9/10,    batch: 12715/15469    Encoder_loss: 0.6240595579147339\n",
            "  epoch: 9/10,    batch: 12716/15469    Encoder_loss: 0.6223987340927124\n",
            "  epoch: 9/10,    batch: 12717/15469    Encoder_loss: 0.623772144317627\n",
            "  epoch: 9/10,    batch: 12718/15469    Encoder_loss: 0.6275985836982727\n",
            "  epoch: 9/10,    batch: 12719/15469    Encoder_loss: 0.5921702980995178\n",
            "  epoch: 9/10,    batch: 12720/15469    Encoder_loss: 0.5270608067512512\n",
            "  epoch: 9/10,    batch: 12721/15469    Encoder_loss: 0.5275381207466125\n",
            "  epoch: 9/10,    batch: 12722/15469    Encoder_loss: 0.5266992449760437\n",
            "  epoch: 9/10,    batch: 12723/15469    Encoder_loss: 0.5241113901138306\n",
            "  epoch: 9/10,    batch: 12724/15469    Encoder_loss: 0.5233892798423767\n",
            "  epoch: 9/10,    batch: 12725/15469    Encoder_loss: 0.5234155058860779\n",
            "  epoch: 9/10,    batch: 12726/15469    Encoder_loss: 0.5220233798027039\n",
            "  epoch: 9/10,    batch: 12727/15469    Encoder_loss: 0.5219889283180237\n",
            "  epoch: 9/10,    batch: 12728/15469    Encoder_loss: 0.5229181051254272\n",
            "  epoch: 9/10,    batch: 12729/15469    Encoder_loss: 0.5225709676742554\n",
            "  epoch: 9/10,    batch: 12730/15469    Encoder_loss: 0.5211097002029419\n",
            "  epoch: 9/10,    batch: 12731/15469    Encoder_loss: 0.5221593976020813\n",
            "  epoch: 9/10,    batch: 12732/15469    Encoder_loss: 0.5199165344238281\n",
            "  epoch: 9/10,    batch: 12733/15469    Encoder_loss: 0.5196561217308044\n",
            "  epoch: 9/10,    batch: 12734/15469    Encoder_loss: 0.5195857882499695\n",
            "  epoch: 9/10,    batch: 12735/15469    Encoder_loss: 0.5180176496505737\n",
            "  epoch: 9/10,    batch: 12736/15469    Encoder_loss: 0.5193937420845032\n",
            "  epoch: 9/10,    batch: 12737/15469    Encoder_loss: 0.5259453058242798\n",
            "  epoch: 9/10,    batch: 12738/15469    Encoder_loss: 0.4505748748779297\n",
            "  epoch: 9/10,    batch: 12739/15469    Encoder_loss: 0.4132649898529053\n",
            "  epoch: 9/10,    batch: 12740/15469    Encoder_loss: 0.4120977520942688\n",
            "  epoch: 9/10,    batch: 12741/15469    Encoder_loss: 0.41049012541770935\n",
            "  epoch: 9/10,    batch: 12742/15469    Encoder_loss: 0.4090442657470703\n",
            "  epoch: 9/10,    batch: 12743/15469    Encoder_loss: 0.40799206495285034\n",
            "  epoch: 9/10,    batch: 12744/15469    Encoder_loss: 0.4061029851436615\n",
            "  epoch: 9/10,    batch: 12745/15469    Encoder_loss: 0.4039086401462555\n",
            "  epoch: 9/10,    batch: 12746/15469    Encoder_loss: 0.4013749957084656\n",
            "  epoch: 9/10,    batch: 12747/15469    Encoder_loss: 0.39997878670692444\n",
            "  epoch: 9/10,    batch: 12748/15469    Encoder_loss: 0.40013208985328674\n",
            "  epoch: 9/10,    batch: 12749/15469    Encoder_loss: 0.39830631017684937\n",
            "  epoch: 9/10,    batch: 12750/15469    Encoder_loss: 0.39577969908714294\n",
            "  epoch: 9/10,    batch: 12751/15469    Encoder_loss: 0.39417529106140137\n",
            "  epoch: 9/10,    batch: 12752/15469    Encoder_loss: 0.39133960008621216\n",
            "  epoch: 9/10,    batch: 12753/15469    Encoder_loss: 0.3913845717906952\n",
            "  epoch: 9/10,    batch: 12754/15469    Encoder_loss: 0.3891061842441559\n",
            "  epoch: 9/10,    batch: 12755/15469    Encoder_loss: 0.3860294222831726\n",
            "  epoch: 9/10,    batch: 12756/15469    Encoder_loss: 0.3849024772644043\n",
            "  epoch: 9/10,    batch: 12757/15469    Encoder_loss: 0.38247954845428467\n",
            "  epoch: 9/10,    batch: 12758/15469    Encoder_loss: 0.38381919264793396\n",
            "  epoch: 9/10,    batch: 12759/15469    Encoder_loss: 0.38051193952560425\n",
            "  epoch: 9/10,    batch: 12760/15469    Encoder_loss: 0.3881897032260895\n",
            "  epoch: 9/10,    batch: 12761/15469    Encoder_loss: 0.3521319031715393\n",
            "  epoch: 9/10,    batch: 12762/15469    Encoder_loss: 0.34345704317092896\n",
            "  epoch: 9/10,    batch: 12763/15469    Encoder_loss: 0.3410983681678772\n",
            "  epoch: 9/10,    batch: 12764/15469    Encoder_loss: 0.33972328901290894\n",
            "  epoch: 9/10,    batch: 12765/15469    Encoder_loss: 0.33907315135002136\n",
            "  epoch: 9/10,    batch: 12766/15469    Encoder_loss: 0.33779290318489075\n",
            "  epoch: 9/10,    batch: 12767/15469    Encoder_loss: 0.3378683030605316\n",
            "  epoch: 9/10,    batch: 12768/15469    Encoder_loss: 0.33652323484420776\n",
            "  epoch: 9/10,    batch: 12769/15469    Encoder_loss: 0.33719855546951294\n",
            "  epoch: 9/10,    batch: 12770/15469    Encoder_loss: 0.3352487087249756\n",
            "  epoch: 9/10,    batch: 12771/15469    Encoder_loss: 0.33346620202064514\n",
            "  epoch: 9/10,    batch: 12772/15469    Encoder_loss: 0.3326222598552704\n",
            "  epoch: 9/10,    batch: 12773/15469    Encoder_loss: 0.32994967699050903\n",
            "  epoch: 9/10,    batch: 12774/15469    Encoder_loss: 0.32858961820602417\n",
            "  epoch: 9/10,    batch: 12775/15469    Encoder_loss: 0.3287852108478546\n",
            "  epoch: 9/10,    batch: 12776/15469    Encoder_loss: 0.32971635460853577\n",
            "  epoch: 9/10,    batch: 12777/15469    Encoder_loss: 0.32739493250846863\n",
            "  epoch: 9/10,    batch: 12778/15469    Encoder_loss: 0.3262689709663391\n",
            "  epoch: 9/10,    batch: 12779/15469    Encoder_loss: 0.33188149333000183\n",
            "  epoch: 9/10,    batch: 12780/15469    Encoder_loss: 0.29088956117630005\n",
            "  epoch: 9/10,    batch: 12781/15469    Encoder_loss: 0.2909415364265442\n",
            "  epoch: 9/10,    batch: 12782/15469    Encoder_loss: 0.2905096113681793\n",
            "  epoch: 9/10,    batch: 12783/15469    Encoder_loss: 0.2912190556526184\n",
            "  epoch: 9/10,    batch: 12784/15469    Encoder_loss: 0.2911606431007385\n",
            "  epoch: 9/10,    batch: 12785/15469    Encoder_loss: 0.28946197032928467\n",
            "  epoch: 9/10,    batch: 12786/15469    Encoder_loss: 0.28984731435775757\n",
            "  epoch: 9/10,    batch: 12787/15469    Encoder_loss: 0.2892771363258362\n",
            "  epoch: 9/10,    batch: 12788/15469    Encoder_loss: 0.2897360324859619\n",
            "  epoch: 9/10,    batch: 12789/15469    Encoder_loss: 0.28870466351509094\n",
            "  epoch: 9/10,    batch: 12790/15469    Encoder_loss: 0.2885085940361023\n",
            "  epoch: 9/10,    batch: 12791/15469    Encoder_loss: 0.289338618516922\n",
            "  epoch: 9/10,    batch: 12792/15469    Encoder_loss: 0.28766876459121704\n",
            "  epoch: 9/10,    batch: 12793/15469    Encoder_loss: 0.2875327467918396\n",
            "  epoch: 9/10,    batch: 12794/15469    Encoder_loss: 0.2864752411842346\n",
            "  epoch: 9/10,    batch: 12795/15469    Encoder_loss: 0.2865954637527466\n",
            "  epoch: 9/10,    batch: 12796/15469    Encoder_loss: 0.28507936000823975\n",
            "  epoch: 9/10,    batch: 12797/15469    Encoder_loss: 0.2868029773235321\n",
            "  epoch: 9/10,    batch: 12798/15469    Encoder_loss: 0.2850121557712555\n",
            "  epoch: 9/10,    batch: 12799/15469    Encoder_loss: 0.28508245944976807\n",
            "  epoch: 9/10,    batch: 12800/15469    Encoder_loss: 0.28450411558151245\n",
            "  epoch: 9/10,    batch: 12801/15469    Encoder_loss: 0.28416645526885986\n",
            "  epoch: 9/10,    batch: 12802/15469    Encoder_loss: 0.3028702437877655\n",
            "  epoch: 9/10,    batch: 12803/15469    Encoder_loss: 0.48039665818214417\n",
            "  epoch: 9/10,    batch: 12804/15469    Encoder_loss: 0.7101070880889893\n",
            "  epoch: 9/10,    batch: 12805/15469    Encoder_loss: 0.7099922895431519\n",
            "  epoch: 9/10,    batch: 12806/15469    Encoder_loss: 0.5006298422813416\n",
            "  epoch: 9/10,    batch: 12807/15469    Encoder_loss: 0.4691263735294342\n",
            "  epoch: 9/10,    batch: 12808/15469    Encoder_loss: 0.46816760301589966\n",
            "  epoch: 9/10,    batch: 12809/15469    Encoder_loss: 0.4589753746986389\n",
            "  epoch: 9/10,    batch: 12810/15469    Encoder_loss: 0.3758764863014221\n",
            "  epoch: 9/10,    batch: 12811/15469    Encoder_loss: 0.3604215085506439\n",
            "  epoch: 9/10,    batch: 12812/15469    Encoder_loss: 0.33027347922325134\n",
            "  epoch: 9/10,    batch: 12813/15469    Encoder_loss: 0.28243717551231384\n",
            "  epoch: 9/10,    batch: 12814/15469    Encoder_loss: 0.28257831931114197\n",
            "  epoch: 9/10,    batch: 12815/15469    Encoder_loss: 0.2825689911842346\n",
            "  epoch: 9/10,    batch: 12816/15469    Encoder_loss: 0.2824142873287201\n",
            "  epoch: 9/10,    batch: 12817/15469    Encoder_loss: 0.2825910151004791\n",
            "  epoch: 9/10,    batch: 12818/15469    Encoder_loss: 0.2838350236415863\n",
            "  epoch: 9/10,    batch: 12819/15469    Encoder_loss: 0.29232051968574524\n",
            "  epoch: 9/10,    batch: 12820/15469    Encoder_loss: 0.30031996965408325\n",
            "  epoch: 9/10,    batch: 12821/15469    Encoder_loss: 0.399198442697525\n",
            "  epoch: 9/10,    batch: 12822/15469    Encoder_loss: 0.6264498829841614\n",
            "  epoch: 9/10,    batch: 12823/15469    Encoder_loss: 0.7478823065757751\n",
            "  epoch: 9/10,    batch: 12824/15469    Encoder_loss: 0.6253672242164612\n",
            "  epoch: 9/10,    batch: 12825/15469    Encoder_loss: 0.4811391234397888\n",
            "  epoch: 9/10,    batch: 12826/15469    Encoder_loss: 0.4781884551048279\n",
            "  epoch: 9/10,    batch: 12827/15469    Encoder_loss: 0.47839346528053284\n",
            "  epoch: 9/10,    batch: 12828/15469    Encoder_loss: 0.4380568563938141\n",
            "  epoch: 9/10,    batch: 12829/15469    Encoder_loss: 0.361975759267807\n",
            "  epoch: 9/10,    batch: 12830/15469    Encoder_loss: 0.36967888474464417\n",
            "  epoch: 9/10,    batch: 12831/15469    Encoder_loss: 0.3045889139175415\n",
            "  epoch: 9/10,    batch: 12832/15469    Encoder_loss: 0.29023098945617676\n",
            "  epoch: 9/10,    batch: 12833/15469    Encoder_loss: 0.2882441580295563\n",
            "  epoch: 9/10,    batch: 12834/15469    Encoder_loss: 0.2878609895706177\n",
            "  epoch: 9/10,    batch: 12835/15469    Encoder_loss: 0.28839030861854553\n",
            "  epoch: 9/10,    batch: 12836/15469    Encoder_loss: 0.2875210642814636\n",
            "  epoch: 9/10,    batch: 12837/15469    Encoder_loss: 0.28948917984962463\n",
            "  epoch: 9/10,    batch: 12838/15469    Encoder_loss: 0.2951366901397705\n",
            "  epoch: 9/10,    batch: 12839/15469    Encoder_loss: 0.2989349961280823\n",
            "  epoch: 9/10,    batch: 12840/15469    Encoder_loss: 0.3013365864753723\n",
            "  epoch: 9/10,    batch: 12841/15469    Encoder_loss: 0.2969505190849304\n",
            "  epoch: 9/10,    batch: 12842/15469    Encoder_loss: 0.29458802938461304\n",
            "  epoch: 9/10,    batch: 12843/15469    Encoder_loss: 0.2939097583293915\n",
            "  epoch: 9/10,    batch: 12844/15469    Encoder_loss: 0.29335129261016846\n",
            "  epoch: 9/10,    batch: 12845/15469    Encoder_loss: 0.2923262417316437\n",
            "  epoch: 9/10,    batch: 12846/15469    Encoder_loss: 0.29136908054351807\n",
            "  epoch: 9/10,    batch: 12847/15469    Encoder_loss: 0.2907574772834778\n",
            "  epoch: 9/10,    batch: 12848/15469    Encoder_loss: 0.2922767102718353\n",
            "  epoch: 9/10,    batch: 12849/15469    Encoder_loss: 0.28985220193862915\n",
            "  epoch: 9/10,    batch: 12850/15469    Encoder_loss: 0.289978951215744\n",
            "  epoch: 9/10,    batch: 12851/15469    Encoder_loss: 0.2883523106575012\n",
            "  epoch: 9/10,    batch: 12852/15469    Encoder_loss: 0.2887251377105713\n",
            "  epoch: 9/10,    batch: 12853/15469    Encoder_loss: 0.289002001285553\n",
            "  epoch: 9/10,    batch: 12854/15469    Encoder_loss: 0.2871232330799103\n",
            "  epoch: 9/10,    batch: 12855/15469    Encoder_loss: 0.28730306029319763\n",
            "  epoch: 9/10,    batch: 12856/15469    Encoder_loss: 0.28536802530288696\n",
            "  epoch: 9/10,    batch: 12857/15469    Encoder_loss: 0.28483763337135315\n",
            "  epoch: 9/10,    batch: 12858/15469    Encoder_loss: 0.28384286165237427\n",
            "  epoch: 9/10,    batch: 12859/15469    Encoder_loss: 0.28404539823532104\n",
            "  epoch: 9/10,    batch: 12860/15469    Encoder_loss: 0.2840587794780731\n",
            "  epoch: 9/10,    batch: 12861/15469    Encoder_loss: 0.2844257950782776\n",
            "  epoch: 9/10,    batch: 12862/15469    Encoder_loss: 0.284665048122406\n",
            "  epoch: 9/10,    batch: 12863/15469    Encoder_loss: 0.28350526094436646\n",
            "  epoch: 9/10,    batch: 12864/15469    Encoder_loss: 0.282564640045166\n",
            "  epoch: 9/10,    batch: 12865/15469    Encoder_loss: 0.28091195225715637\n",
            "  epoch: 9/10,    batch: 12866/15469    Encoder_loss: 0.28052619099617004\n",
            "  epoch: 9/10,    batch: 12867/15469    Encoder_loss: 0.2802230417728424\n",
            "  epoch: 9/10,    batch: 12868/15469    Encoder_loss: 0.2790543735027313\n",
            "  epoch: 9/10,    batch: 12869/15469    Encoder_loss: 0.2786640226840973\n",
            "  epoch: 9/10,    batch: 12870/15469    Encoder_loss: 0.27893003821372986\n",
            "  epoch: 9/10,    batch: 12871/15469    Encoder_loss: 0.2778246998786926\n",
            "  epoch: 9/10,    batch: 12872/15469    Encoder_loss: 0.27751538157463074\n",
            "  epoch: 9/10,    batch: 12873/15469    Encoder_loss: 0.27676114439964294\n",
            "  epoch: 9/10,    batch: 12874/15469    Encoder_loss: 0.27718260884284973\n",
            "  epoch: 9/10,    batch: 12875/15469    Encoder_loss: 0.2767673134803772\n",
            "  epoch: 9/10,    batch: 12876/15469    Encoder_loss: 0.2765321135520935\n",
            "  epoch: 9/10,    batch: 12877/15469    Encoder_loss: 0.2758013904094696\n",
            "  epoch: 9/10,    batch: 12878/15469    Encoder_loss: 0.2760378420352936\n",
            "  epoch: 9/10,    batch: 12879/15469    Encoder_loss: 0.2748704254627228\n",
            "  epoch: 9/10,    batch: 12880/15469    Encoder_loss: 0.274296373128891\n",
            "  epoch: 9/10,    batch: 12881/15469    Encoder_loss: 0.2732990086078644\n",
            "  epoch: 9/10,    batch: 12882/15469    Encoder_loss: 0.27307865023612976\n",
            "  epoch: 9/10,    batch: 12883/15469    Encoder_loss: 0.27414456009864807\n",
            "  epoch: 9/10,    batch: 12884/15469    Encoder_loss: 0.2725117802619934\n",
            "  epoch: 9/10,    batch: 12885/15469    Encoder_loss: 0.2728324830532074\n",
            "  epoch: 9/10,    batch: 12886/15469    Encoder_loss: 0.27148568630218506\n",
            "  epoch: 9/10,    batch: 12887/15469    Encoder_loss: 0.2713565230369568\n",
            "  epoch: 9/10,    batch: 12888/15469    Encoder_loss: 0.27212825417518616\n",
            "  epoch: 9/10,    batch: 12889/15469    Encoder_loss: 0.27052751183509827\n",
            "  epoch: 9/10,    batch: 12890/15469    Encoder_loss: 0.2703971862792969\n",
            "  epoch: 9/10,    batch: 12891/15469    Encoder_loss: 0.27195286750793457\n",
            "  epoch: 9/10,    batch: 12892/15469    Encoder_loss: 0.2695448100566864\n",
            "  epoch: 9/10,    batch: 12893/15469    Encoder_loss: 0.2697993218898773\n",
            "  epoch: 9/10,    batch: 12894/15469    Encoder_loss: 0.2689574062824249\n",
            "  epoch: 9/10,    batch: 12895/15469    Encoder_loss: 0.27095627784729004\n",
            "  epoch: 9/10,    batch: 12896/15469    Encoder_loss: 0.26850375533103943\n",
            "  epoch: 9/10,    batch: 12897/15469    Encoder_loss: 0.2683732509613037\n",
            "  epoch: 9/10,    batch: 12898/15469    Encoder_loss: 0.2680990695953369\n",
            "  epoch: 9/10,    batch: 12899/15469    Encoder_loss: 0.26868006587028503\n",
            "  epoch: 9/10,    batch: 12900/15469    Encoder_loss: 0.2680230736732483\n",
            "  epoch: 9/10,    batch: 12901/15469    Encoder_loss: 0.2689387798309326\n",
            "  epoch: 9/10,    batch: 12902/15469    Encoder_loss: 0.2669484615325928\n",
            "  epoch: 9/10,    batch: 12903/15469    Encoder_loss: 0.26656633615493774\n",
            "  epoch: 9/10,    batch: 12904/15469    Encoder_loss: 0.2679734528064728\n",
            "  epoch: 9/10,    batch: 12905/15469    Encoder_loss: 0.2664319574832916\n",
            "  epoch: 9/10,    batch: 12906/15469    Encoder_loss: 0.2673198878765106\n",
            "  epoch: 9/10,    batch: 12907/15469    Encoder_loss: 0.268442302942276\n",
            "  epoch: 9/10,    batch: 12908/15469    Encoder_loss: 0.2690216600894928\n",
            "  epoch: 9/10,    batch: 12909/15469    Encoder_loss: 0.2671348452568054\n",
            "  epoch: 9/10,    batch: 12910/15469    Encoder_loss: 0.26680755615234375\n",
            "  epoch: 9/10,    batch: 12911/15469    Encoder_loss: 0.2685495615005493\n",
            "  epoch: 9/10,    batch: 12912/15469    Encoder_loss: 0.26678168773651123\n",
            "  epoch: 9/10,    batch: 12913/15469    Encoder_loss: 0.2666689455509186\n",
            "  epoch: 9/10,    batch: 12914/15469    Encoder_loss: 0.2681264877319336\n",
            "  epoch: 9/10,    batch: 12915/15469    Encoder_loss: 0.26848939061164856\n",
            "  epoch: 9/10,    batch: 12916/15469    Encoder_loss: 0.26727303862571716\n",
            "  epoch: 9/10,    batch: 12917/15469    Encoder_loss: 0.26854151487350464\n",
            "  epoch: 9/10,    batch: 12918/15469    Encoder_loss: 0.26803940534591675\n",
            "  epoch: 9/10,    batch: 12919/15469    Encoder_loss: 0.26848113536834717\n",
            "  epoch: 9/10,    batch: 12920/15469    Encoder_loss: 0.26853427290916443\n",
            "  epoch: 9/10,    batch: 12921/15469    Encoder_loss: 0.2672918438911438\n",
            "  epoch: 9/10,    batch: 12922/15469    Encoder_loss: 0.2891099452972412\n",
            "  epoch: 9/10,    batch: 12923/15469    Encoder_loss: 0.35259583592414856\n",
            "  epoch: 9/10,    batch: 12924/15469    Encoder_loss: 0.35412245988845825\n",
            "  epoch: 9/10,    batch: 12925/15469    Encoder_loss: 0.40268588066101074\n",
            "  epoch: 9/10,    batch: 12926/15469    Encoder_loss: 0.4543163776397705\n",
            "  epoch: 9/10,    batch: 12927/15469    Encoder_loss: 0.4540642201900482\n",
            "  epoch: 9/10,    batch: 12928/15469    Encoder_loss: 0.45435506105422974\n",
            "  epoch: 9/10,    batch: 12929/15469    Encoder_loss: 0.45769181847572327\n",
            "  epoch: 9/10,    batch: 12930/15469    Encoder_loss: 0.4547574520111084\n",
            "  epoch: 9/10,    batch: 12931/15469    Encoder_loss: 0.45546454191207886\n",
            "  epoch: 9/10,    batch: 12932/15469    Encoder_loss: 0.4572790563106537\n",
            "  epoch: 9/10,    batch: 12933/15469    Encoder_loss: 0.4576670527458191\n",
            "  epoch: 9/10,    batch: 12934/15469    Encoder_loss: 0.4608992636203766\n",
            "  epoch: 9/10,    batch: 12935/15469    Encoder_loss: 0.4609120488166809\n",
            "  epoch: 9/10,    batch: 12936/15469    Encoder_loss: 0.4624806344509125\n",
            "  epoch: 9/10,    batch: 12937/15469    Encoder_loss: 0.5016579627990723\n",
            "  epoch: 9/10,    batch: 12938/15469    Encoder_loss: 0.5493041276931763\n",
            "  epoch: 9/10,    batch: 12939/15469    Encoder_loss: 0.5519973039627075\n",
            "  epoch: 9/10,    batch: 12940/15469    Encoder_loss: 0.6418901085853577\n",
            "  epoch: 9/10,    batch: 12941/15469    Encoder_loss: 0.6531981229782104\n",
            "  epoch: 9/10,    batch: 12942/15469    Encoder_loss: 0.6550663709640503\n",
            "  epoch: 9/10,    batch: 12943/15469    Encoder_loss: 0.659309983253479\n",
            "  epoch: 9/10,    batch: 12944/15469    Encoder_loss: 0.6601089835166931\n",
            "  epoch: 9/10,    batch: 12945/15469    Encoder_loss: 0.6612027287483215\n",
            "  epoch: 9/10,    batch: 12946/15469    Encoder_loss: 0.6622956991195679\n",
            "  epoch: 9/10,    batch: 12947/15469    Encoder_loss: 0.663192868232727\n",
            "  epoch: 9/10,    batch: 12948/15469    Encoder_loss: 0.6662483215332031\n",
            "  epoch: 9/10,    batch: 12949/15469    Encoder_loss: 0.6668389439582825\n",
            "  epoch: 9/10,    batch: 12950/15469    Encoder_loss: 0.670252799987793\n",
            "  epoch: 9/10,    batch: 12951/15469    Encoder_loss: 0.6726633906364441\n",
            "  epoch: 9/10,    batch: 12952/15469    Encoder_loss: 0.67497318983078\n",
            "  epoch: 9/10,    batch: 12953/15469    Encoder_loss: 0.6771024465560913\n",
            "  epoch: 9/10,    batch: 12954/15469    Encoder_loss: 0.6794880628585815\n",
            "  epoch: 9/10,    batch: 12955/15469    Encoder_loss: 0.6800656914710999\n",
            "  epoch: 9/10,    batch: 12956/15469    Encoder_loss: 0.685025155544281\n",
            "  epoch: 9/10,    batch: 12957/15469    Encoder_loss: 0.687092661857605\n",
            "  epoch: 9/10,    batch: 12958/15469    Encoder_loss: 0.6898411512374878\n",
            "  epoch: 9/10,    batch: 12959/15469    Encoder_loss: 0.6967540383338928\n",
            "  epoch: 9/10,    batch: 12960/15469    Encoder_loss: 0.7326249480247498\n",
            "  epoch: 9/10,    batch: 12961/15469    Encoder_loss: 0.7501424551010132\n",
            "  epoch: 9/10,    batch: 12962/15469    Encoder_loss: 0.7227662205696106\n",
            "  epoch: 9/10,    batch: 12963/15469    Encoder_loss: 0.7272225022315979\n",
            "  epoch: 9/10,    batch: 12964/15469    Encoder_loss: 0.6637097001075745\n",
            "  epoch: 9/10,    batch: 12965/15469    Encoder_loss: 0.6512042284011841\n",
            "  epoch: 9/10,    batch: 12966/15469    Encoder_loss: 0.6516814231872559\n",
            "  epoch: 9/10,    batch: 12967/15469    Encoder_loss: 0.6534262895584106\n",
            "  epoch: 9/10,    batch: 12968/15469    Encoder_loss: 0.6554800271987915\n",
            "  epoch: 9/10,    batch: 12969/15469    Encoder_loss: 0.6581049561500549\n",
            "  epoch: 9/10,    batch: 12970/15469    Encoder_loss: 0.6602116227149963\n",
            "  epoch: 9/10,    batch: 12971/15469    Encoder_loss: 0.6599230170249939\n",
            "  epoch: 9/10,    batch: 12972/15469    Encoder_loss: 0.6605050563812256\n",
            "  epoch: 9/10,    batch: 12973/15469    Encoder_loss: 0.6622832417488098\n",
            "  epoch: 9/10,    batch: 12974/15469    Encoder_loss: 0.6831143498420715\n",
            "  epoch: 9/10,    batch: 12975/15469    Encoder_loss: 0.695927619934082\n",
            "  epoch: 9/10,    batch: 12976/15469    Encoder_loss: 0.7281797528266907\n",
            "  epoch: 9/10,    batch: 12977/15469    Encoder_loss: 0.699285626411438\n",
            "  epoch: 9/10,    batch: 12978/15469    Encoder_loss: 0.6966726779937744\n",
            "  epoch: 9/10,    batch: 12979/15469    Encoder_loss: 0.6646621823310852\n",
            "  epoch: 9/10,    batch: 12980/15469    Encoder_loss: 0.6181069612503052\n",
            "  epoch: 9/10,    batch: 12981/15469    Encoder_loss: 0.6185270547866821\n",
            "  epoch: 9/10,    batch: 12982/15469    Encoder_loss: 0.6181870102882385\n",
            "  epoch: 9/10,    batch: 12983/15469    Encoder_loss: 0.6196993589401245\n",
            "  epoch: 9/10,    batch: 12984/15469    Encoder_loss: 0.620440661907196\n",
            "  epoch: 9/10,    batch: 12985/15469    Encoder_loss: 0.6226368546485901\n",
            "  epoch: 9/10,    batch: 12986/15469    Encoder_loss: 0.62177574634552\n",
            "  epoch: 9/10,    batch: 12987/15469    Encoder_loss: 0.623205304145813\n",
            "  epoch: 9/10,    batch: 12988/15469    Encoder_loss: 0.6236312985420227\n",
            "  epoch: 9/10,    batch: 12989/15469    Encoder_loss: 0.6237080693244934\n",
            "  epoch: 9/10,    batch: 12990/15469    Encoder_loss: 0.6260942220687866\n",
            "  epoch: 9/10,    batch: 12991/15469    Encoder_loss: 0.6276575922966003\n",
            "  epoch: 9/10,    batch: 12992/15469    Encoder_loss: 0.6038039326667786\n",
            "  epoch: 9/10,    batch: 12993/15469    Encoder_loss: 0.5299818515777588\n",
            "  epoch: 9/10,    batch: 12994/15469    Encoder_loss: 0.5279649496078491\n",
            "  epoch: 9/10,    batch: 12995/15469    Encoder_loss: 0.5267397165298462\n",
            "  epoch: 9/10,    batch: 12996/15469    Encoder_loss: 0.5262401103973389\n",
            "  epoch: 9/10,    batch: 12997/15469    Encoder_loss: 0.5265623331069946\n",
            "  epoch: 9/10,    batch: 12998/15469    Encoder_loss: 0.5249830484390259\n",
            "  epoch: 9/10,    batch: 12999/15469    Encoder_loss: 0.5249165296554565\n",
            "  epoch: 9/10,    batch: 13000/15469    Encoder_loss: 0.5234599113464355\n",
            "  epoch: 9/10,    batch: 13001/15469    Encoder_loss: 0.5227441787719727\n",
            "  epoch: 9/10,    batch: 13002/15469    Encoder_loss: 0.5215054750442505\n",
            "  epoch: 9/10,    batch: 13003/15469    Encoder_loss: 0.5202537775039673\n",
            "  epoch: 9/10,    batch: 13004/15469    Encoder_loss: 0.5195695161819458\n",
            "  epoch: 9/10,    batch: 13005/15469    Encoder_loss: 0.5207576751708984\n",
            "  epoch: 9/10,    batch: 13006/15469    Encoder_loss: 0.5202051401138306\n",
            "  epoch: 9/10,    batch: 13007/15469    Encoder_loss: 0.5245501399040222\n",
            "  epoch: 9/10,    batch: 13008/15469    Encoder_loss: 0.4407607614994049\n",
            "  epoch: 9/10,    batch: 13009/15469    Encoder_loss: 0.41775092482566833\n",
            "  epoch: 9/10,    batch: 13010/15469    Encoder_loss: 0.4147292971611023\n",
            "  epoch: 9/10,    batch: 13011/15469    Encoder_loss: 0.41298288106918335\n",
            "  epoch: 9/10,    batch: 13012/15469    Encoder_loss: 0.4108138978481293\n",
            "  epoch: 9/10,    batch: 13013/15469    Encoder_loss: 0.4094417095184326\n",
            "  epoch: 9/10,    batch: 13014/15469    Encoder_loss: 0.40827780961990356\n",
            "  epoch: 9/10,    batch: 13015/15469    Encoder_loss: 0.40508952736854553\n",
            "  epoch: 9/10,    batch: 13016/15469    Encoder_loss: 0.40427565574645996\n",
            "  epoch: 9/10,    batch: 13017/15469    Encoder_loss: 0.4030087888240814\n",
            "  epoch: 9/10,    batch: 13018/15469    Encoder_loss: 0.40057724714279175\n",
            "  epoch: 9/10,    batch: 13019/15469    Encoder_loss: 0.3999797999858856\n",
            "  epoch: 9/10,    batch: 13020/15469    Encoder_loss: 0.3962920904159546\n",
            "  epoch: 9/10,    batch: 13021/15469    Encoder_loss: 0.39577460289001465\n",
            "  epoch: 9/10,    batch: 13022/15469    Encoder_loss: 0.3939167857170105\n",
            "  epoch: 9/10,    batch: 13023/15469    Encoder_loss: 0.391483336687088\n",
            "  epoch: 9/10,    batch: 13024/15469    Encoder_loss: 0.3907504081726074\n",
            "  epoch: 9/10,    batch: 13025/15469    Encoder_loss: 0.3895999789237976\n",
            "  epoch: 9/10,    batch: 13026/15469    Encoder_loss: 0.38659483194351196\n",
            "  epoch: 9/10,    batch: 13027/15469    Encoder_loss: 0.3835325539112091\n",
            "  epoch: 9/10,    batch: 13028/15469    Encoder_loss: 0.383929044008255\n",
            "  epoch: 9/10,    batch: 13029/15469    Encoder_loss: 0.38347378373146057\n",
            "  epoch: 9/10,    batch: 13030/15469    Encoder_loss: 0.3801196217536926\n",
            "  epoch: 9/10,    batch: 13031/15469    Encoder_loss: 0.378709614276886\n",
            "  epoch: 9/10,    batch: 13032/15469    Encoder_loss: 0.37698131799697876\n",
            "  epoch: 9/10,    batch: 13033/15469    Encoder_loss: 0.38374945521354675\n",
            "  epoch: 9/10,    batch: 13034/15469    Encoder_loss: 0.35244596004486084\n",
            "  epoch: 9/10,    batch: 13035/15469    Encoder_loss: 0.3390212059020996\n",
            "  epoch: 9/10,    batch: 13036/15469    Encoder_loss: 0.3386096656322479\n",
            "  epoch: 9/10,    batch: 13037/15469    Encoder_loss: 0.3369559347629547\n",
            "  epoch: 9/10,    batch: 13038/15469    Encoder_loss: 0.335544615983963\n",
            "  epoch: 9/10,    batch: 13039/15469    Encoder_loss: 0.3353068232536316\n",
            "  epoch: 9/10,    batch: 13040/15469    Encoder_loss: 0.3341712951660156\n",
            "  epoch: 9/10,    batch: 13041/15469    Encoder_loss: 0.3336467742919922\n",
            "  epoch: 9/10,    batch: 13042/15469    Encoder_loss: 0.3325895369052887\n",
            "  epoch: 9/10,    batch: 13043/15469    Encoder_loss: 0.3313506245613098\n",
            "  epoch: 9/10,    batch: 13044/15469    Encoder_loss: 0.3290775418281555\n",
            "  epoch: 9/10,    batch: 13045/15469    Encoder_loss: 0.328185498714447\n",
            "  epoch: 9/10,    batch: 13046/15469    Encoder_loss: 0.3288480043411255\n",
            "  epoch: 9/10,    batch: 13047/15469    Encoder_loss: 0.32818812131881714\n",
            "  epoch: 9/10,    batch: 13048/15469    Encoder_loss: 0.3269270360469818\n",
            "  epoch: 9/10,    batch: 13049/15469    Encoder_loss: 0.32189643383026123\n",
            "  epoch: 9/10,    batch: 13050/15469    Encoder_loss: 0.29147249460220337\n",
            "  epoch: 9/10,    batch: 13051/15469    Encoder_loss: 0.29131531715393066\n",
            "  epoch: 9/10,    batch: 13052/15469    Encoder_loss: 0.2889663279056549\n",
            "  epoch: 9/10,    batch: 13053/15469    Encoder_loss: 0.2902170419692993\n",
            "  epoch: 9/10,    batch: 13054/15469    Encoder_loss: 0.28903475403785706\n",
            "  epoch: 9/10,    batch: 13055/15469    Encoder_loss: 0.2891412377357483\n",
            "  epoch: 9/10,    batch: 13056/15469    Encoder_loss: 0.28871944546699524\n",
            "  epoch: 9/10,    batch: 13057/15469    Encoder_loss: 0.28848978877067566\n",
            "  epoch: 9/10,    batch: 13058/15469    Encoder_loss: 0.28826838731765747\n",
            "  epoch: 9/10,    batch: 13059/15469    Encoder_loss: 0.2889237403869629\n",
            "  epoch: 9/10,    batch: 13060/15469    Encoder_loss: 0.2871314585208893\n",
            "  epoch: 9/10,    batch: 13061/15469    Encoder_loss: 0.28760355710983276\n",
            "  epoch: 9/10,    batch: 13062/15469    Encoder_loss: 0.2854216694831848\n",
            "  epoch: 9/10,    batch: 13063/15469    Encoder_loss: 0.2866670787334442\n",
            "  epoch: 9/10,    batch: 13064/15469    Encoder_loss: 0.2853681445121765\n",
            "  epoch: 9/10,    batch: 13065/15469    Encoder_loss: 0.28508907556533813\n",
            "  epoch: 9/10,    batch: 13066/15469    Encoder_loss: 0.28412505984306335\n",
            "  epoch: 9/10,    batch: 13067/15469    Encoder_loss: 0.28215351700782776\n",
            "  epoch: 9/10,    batch: 13068/15469    Encoder_loss: 0.2839978337287903\n",
            "  epoch: 9/10,    batch: 13069/15469    Encoder_loss: 0.2831370234489441\n",
            "  epoch: 9/10,    batch: 13070/15469    Encoder_loss: 0.2818956971168518\n",
            "  epoch: 9/10,    batch: 13071/15469    Encoder_loss: 0.28243693709373474\n",
            "  epoch: 9/10,    batch: 13072/15469    Encoder_loss: 0.2823558449745178\n",
            "  epoch: 9/10,    batch: 13073/15469    Encoder_loss: 0.28121575713157654\n",
            "  epoch: 9/10,    batch: 13074/15469    Encoder_loss: 0.28171366453170776\n",
            "  epoch: 9/10,    batch: 13075/15469    Encoder_loss: 0.2807188928127289\n",
            "  epoch: 9/10,    batch: 13076/15469    Encoder_loss: 0.3207698464393616\n",
            "  epoch: 9/10,    batch: 13077/15469    Encoder_loss: 0.5357345342636108\n",
            "  epoch: 9/10,    batch: 13078/15469    Encoder_loss: 0.7236208319664001\n",
            "  epoch: 9/10,    batch: 13079/15469    Encoder_loss: 0.671576738357544\n",
            "  epoch: 9/10,    batch: 13080/15469    Encoder_loss: 0.4737749695777893\n",
            "  epoch: 9/10,    batch: 13081/15469    Encoder_loss: 0.4660171568393707\n",
            "  epoch: 9/10,    batch: 13082/15469    Encoder_loss: 0.4657815098762512\n",
            "  epoch: 9/10,    batch: 13083/15469    Encoder_loss: 0.44828057289123535\n",
            "  epoch: 9/10,    batch: 13084/15469    Encoder_loss: 0.35694757103919983\n",
            "  epoch: 9/10,    batch: 13085/15469    Encoder_loss: 0.3617859184741974\n",
            "  epoch: 9/10,    batch: 13086/15469    Encoder_loss: 0.3113921880722046\n",
            "  epoch: 9/10,    batch: 13087/15469    Encoder_loss: 0.2798716425895691\n",
            "  epoch: 9/10,    batch: 13088/15469    Encoder_loss: 0.28042304515838623\n",
            "  epoch: 9/10,    batch: 13089/15469    Encoder_loss: 0.2797529995441437\n",
            "  epoch: 9/10,    batch: 13090/15469    Encoder_loss: 0.2807282507419586\n",
            "  epoch: 9/10,    batch: 13091/15469    Encoder_loss: 0.36070144176483154\n",
            "  epoch: 9/10,    batch: 13092/15469    Encoder_loss: 0.6116584539413452\n",
            "  epoch: 9/10,    batch: 13093/15469    Encoder_loss: 0.7411832213401794\n",
            "  epoch: 9/10,    batch: 13094/15469    Encoder_loss: 0.6170102953910828\n",
            "  epoch: 9/10,    batch: 13095/15469    Encoder_loss: 0.4821498990058899\n",
            "  epoch: 9/10,    batch: 13096/15469    Encoder_loss: 0.4798128008842468\n",
            "  epoch: 9/10,    batch: 13097/15469    Encoder_loss: 0.4781875014305115\n",
            "  epoch: 9/10,    batch: 13098/15469    Encoder_loss: 0.43039608001708984\n",
            "  epoch: 9/10,    batch: 13099/15469    Encoder_loss: 0.36474210023880005\n",
            "  epoch: 9/10,    batch: 13100/15469    Encoder_loss: 0.36704331636428833\n",
            "  epoch: 9/10,    batch: 13101/15469    Encoder_loss: 0.30633655190467834\n",
            "  epoch: 9/10,    batch: 13102/15469    Encoder_loss: 0.2886768579483032\n",
            "  epoch: 9/10,    batch: 13103/15469    Encoder_loss: 0.28899484872817993\n",
            "  epoch: 9/10,    batch: 13104/15469    Encoder_loss: 0.2882835268974304\n",
            "  epoch: 9/10,    batch: 13105/15469    Encoder_loss: 0.2880132794380188\n",
            "  epoch: 9/10,    batch: 13106/15469    Encoder_loss: 0.2878131568431854\n",
            "  epoch: 9/10,    batch: 13107/15469    Encoder_loss: 0.28806614875793457\n",
            "  epoch: 9/10,    batch: 13108/15469    Encoder_loss: 0.29344189167022705\n",
            "  epoch: 9/10,    batch: 13109/15469    Encoder_loss: 0.29819491505622864\n",
            "  epoch: 9/10,    batch: 13110/15469    Encoder_loss: 0.3001618981361389\n",
            "  epoch: 9/10,    batch: 13111/15469    Encoder_loss: 0.2952636778354645\n",
            "  epoch: 9/10,    batch: 13112/15469    Encoder_loss: 0.29489487409591675\n",
            "  epoch: 9/10,    batch: 13113/15469    Encoder_loss: 0.29427820444107056\n",
            "  epoch: 9/10,    batch: 13114/15469    Encoder_loss: 0.2946416437625885\n",
            "  epoch: 9/10,    batch: 13115/15469    Encoder_loss: 0.29331350326538086\n",
            "  epoch: 9/10,    batch: 13116/15469    Encoder_loss: 0.2928962707519531\n",
            "  epoch: 9/10,    batch: 13117/15469    Encoder_loss: 0.29184624552726746\n",
            "  epoch: 9/10,    batch: 13118/15469    Encoder_loss: 0.290290892124176\n",
            "  epoch: 9/10,    batch: 13119/15469    Encoder_loss: 0.29023101925849915\n",
            "  epoch: 9/10,    batch: 13120/15469    Encoder_loss: 0.28922244906425476\n",
            "  epoch: 9/10,    batch: 13121/15469    Encoder_loss: 0.2891063094139099\n",
            "  epoch: 9/10,    batch: 13122/15469    Encoder_loss: 0.28863656520843506\n",
            "  epoch: 9/10,    batch: 13123/15469    Encoder_loss: 0.2875978946685791\n",
            "  epoch: 9/10,    batch: 13124/15469    Encoder_loss: 0.2869265377521515\n",
            "  epoch: 9/10,    batch: 13125/15469    Encoder_loss: 0.2861703932285309\n",
            "  epoch: 9/10,    batch: 13126/15469    Encoder_loss: 0.28581181168556213\n",
            "  epoch: 9/10,    batch: 13127/15469    Encoder_loss: 0.2851078510284424\n",
            "  epoch: 9/10,    batch: 13128/15469    Encoder_loss: 0.2831481099128723\n",
            "  epoch: 9/10,    batch: 13129/15469    Encoder_loss: 0.28256756067276\n",
            "  epoch: 9/10,    batch: 13130/15469    Encoder_loss: 0.28464341163635254\n",
            "  epoch: 9/10,    batch: 13131/15469    Encoder_loss: 0.2819046676158905\n",
            "  epoch: 9/10,    batch: 13132/15469    Encoder_loss: 0.281943678855896\n",
            "  epoch: 9/10,    batch: 13133/15469    Encoder_loss: 0.2813502550125122\n",
            "  epoch: 9/10,    batch: 13134/15469    Encoder_loss: 0.28134945034980774\n",
            "  epoch: 9/10,    batch: 13135/15469    Encoder_loss: 0.2795025706291199\n",
            "  epoch: 9/10,    batch: 13136/15469    Encoder_loss: 0.28084301948547363\n",
            "  epoch: 9/10,    batch: 13137/15469    Encoder_loss: 0.27911657094955444\n",
            "  epoch: 9/10,    batch: 13138/15469    Encoder_loss: 0.2809367775917053\n",
            "  epoch: 9/10,    batch: 13139/15469    Encoder_loss: 0.27923864126205444\n",
            "  epoch: 9/10,    batch: 13140/15469    Encoder_loss: 0.2787953019142151\n",
            "  epoch: 9/10,    batch: 13141/15469    Encoder_loss: 0.27644434571266174\n",
            "  epoch: 9/10,    batch: 13142/15469    Encoder_loss: 0.27568307518959045\n",
            "  epoch: 9/10,    batch: 13143/15469    Encoder_loss: 0.2773507535457611\n",
            "  epoch: 9/10,    batch: 13144/15469    Encoder_loss: 0.27611637115478516\n",
            "  epoch: 9/10,    batch: 13145/15469    Encoder_loss: 0.2748482823371887\n",
            "  epoch: 9/10,    batch: 13146/15469    Encoder_loss: 0.27447304129600525\n",
            "  epoch: 9/10,    batch: 13147/15469    Encoder_loss: 0.2738771140575409\n",
            "  epoch: 9/10,    batch: 13148/15469    Encoder_loss: 0.27553117275238037\n",
            "  epoch: 9/10,    batch: 13149/15469    Encoder_loss: 0.2757255733013153\n",
            "  epoch: 9/10,    batch: 13150/15469    Encoder_loss: 0.27234160900115967\n",
            "  epoch: 9/10,    batch: 13151/15469    Encoder_loss: 0.2714208960533142\n",
            "  epoch: 9/10,    batch: 13152/15469    Encoder_loss: 0.27299416065216064\n",
            "  epoch: 9/10,    batch: 13153/15469    Encoder_loss: 0.2723182737827301\n",
            "  epoch: 9/10,    batch: 13154/15469    Encoder_loss: 0.2705008387565613\n",
            "  epoch: 9/10,    batch: 13155/15469    Encoder_loss: 0.2707715630531311\n",
            "  epoch: 9/10,    batch: 13156/15469    Encoder_loss: 0.2704669237136841\n",
            "  epoch: 9/10,    batch: 13157/15469    Encoder_loss: 0.26999303698539734\n",
            "  epoch: 9/10,    batch: 13158/15469    Encoder_loss: 0.2709367871284485\n",
            "  epoch: 9/10,    batch: 13159/15469    Encoder_loss: 0.2691875994205475\n",
            "  epoch: 9/10,    batch: 13160/15469    Encoder_loss: 0.2707580626010895\n",
            "  epoch: 9/10,    batch: 13161/15469    Encoder_loss: 0.26920104026794434\n",
            "  epoch: 9/10,    batch: 13162/15469    Encoder_loss: 0.26940909028053284\n",
            "  epoch: 9/10,    batch: 13163/15469    Encoder_loss: 0.2720586955547333\n",
            "  epoch: 9/10,    batch: 13164/15469    Encoder_loss: 0.2691712975502014\n",
            "  epoch: 9/10,    batch: 13165/15469    Encoder_loss: 0.2682090997695923\n",
            "  epoch: 9/10,    batch: 13166/15469    Encoder_loss: 0.2679062485694885\n",
            "  epoch: 9/10,    batch: 13167/15469    Encoder_loss: 0.2673581540584564\n",
            "  epoch: 9/10,    batch: 13168/15469    Encoder_loss: 0.2680109143257141\n",
            "  epoch: 9/10,    batch: 13169/15469    Encoder_loss: 0.2677507996559143\n",
            "  epoch: 9/10,    batch: 13170/15469    Encoder_loss: 0.2672012746334076\n",
            "  epoch: 9/10,    batch: 13171/15469    Encoder_loss: 0.2675577998161316\n",
            "  epoch: 9/10,    batch: 13172/15469    Encoder_loss: 0.26662659645080566\n",
            "  epoch: 9/10,    batch: 13173/15469    Encoder_loss: 0.2667909860610962\n",
            "  epoch: 9/10,    batch: 13174/15469    Encoder_loss: 0.2653118968009949\n",
            "  epoch: 9/10,    batch: 13175/15469    Encoder_loss: 0.26705202460289\n",
            "  epoch: 9/10,    batch: 13176/15469    Encoder_loss: 0.2665378153324127\n",
            "  epoch: 9/10,    batch: 13177/15469    Encoder_loss: 0.2672330439090729\n",
            "  epoch: 9/10,    batch: 13178/15469    Encoder_loss: 0.268785297870636\n",
            "  epoch: 9/10,    batch: 13179/15469    Encoder_loss: 0.26736339926719666\n",
            "  epoch: 9/10,    batch: 13180/15469    Encoder_loss: 0.2679300904273987\n",
            "  epoch: 9/10,    batch: 13181/15469    Encoder_loss: 0.2683350443840027\n",
            "  epoch: 9/10,    batch: 13182/15469    Encoder_loss: 0.26740676164627075\n",
            "  epoch: 9/10,    batch: 13183/15469    Encoder_loss: 0.2670515179634094\n",
            "  epoch: 9/10,    batch: 13184/15469    Encoder_loss: 0.26716071367263794\n",
            "  epoch: 9/10,    batch: 13185/15469    Encoder_loss: 0.2676286995410919\n",
            "  epoch: 9/10,    batch: 13186/15469    Encoder_loss: 0.26779669523239136\n",
            "  epoch: 9/10,    batch: 13187/15469    Encoder_loss: 0.26870015263557434\n",
            "  epoch: 9/10,    batch: 13188/15469    Encoder_loss: 0.26848840713500977\n",
            "  epoch: 9/10,    batch: 13189/15469    Encoder_loss: 0.26782554388046265\n",
            "  epoch: 9/10,    batch: 13190/15469    Encoder_loss: 0.2679513096809387\n",
            "  epoch: 9/10,    batch: 13191/15469    Encoder_loss: 0.26924800872802734\n",
            "  epoch: 9/10,    batch: 13192/15469    Encoder_loss: 0.268155038356781\n",
            "  epoch: 9/10,    batch: 13193/15469    Encoder_loss: 0.2685791850090027\n",
            "  epoch: 9/10,    batch: 13194/15469    Encoder_loss: 0.26807528734207153\n",
            "  epoch: 9/10,    batch: 13195/15469    Encoder_loss: 0.269392728805542\n",
            "  epoch: 9/10,    batch: 13196/15469    Encoder_loss: 0.30592575669288635\n",
            "  epoch: 9/10,    batch: 13197/15469    Encoder_loss: 0.5333727598190308\n",
            "  epoch: 9/10,    batch: 13198/15469    Encoder_loss: 0.7144145369529724\n",
            "  epoch: 9/10,    batch: 13199/15469    Encoder_loss: 0.7404583096504211\n",
            "  epoch: 9/10,    batch: 13200/15469    Encoder_loss: 0.569256603717804\n",
            "  epoch: 9/10,    batch: 13201/15469    Encoder_loss: 0.5552664399147034\n",
            "  epoch: 9/10,    batch: 13202/15469    Encoder_loss: 0.557336151599884\n",
            "  epoch: 9/10,    batch: 13203/15469    Encoder_loss: 0.5377737283706665\n",
            "  epoch: 9/10,    batch: 13204/15469    Encoder_loss: 0.4507853090763092\n",
            "  epoch: 9/10,    batch: 13205/15469    Encoder_loss: 0.5143977403640747\n",
            "  epoch: 9/10,    batch: 13206/15469    Encoder_loss: 0.4855152666568756\n",
            "  epoch: 9/10,    batch: 13207/15469    Encoder_loss: 0.4576600193977356\n",
            "  epoch: 9/10,    batch: 13208/15469    Encoder_loss: 0.5496693253517151\n",
            "  epoch: 9/10,    batch: 13209/15469    Encoder_loss: 0.5590183138847351\n",
            "  epoch: 9/10,    batch: 13210/15469    Encoder_loss: 0.5635077357292175\n",
            "  epoch: 9/10,    batch: 13211/15469    Encoder_loss: 0.5646988153457642\n",
            "  epoch: 9/10,    batch: 13212/15469    Encoder_loss: 0.565982460975647\n",
            "  epoch: 9/10,    batch: 13213/15469    Encoder_loss: 0.5750115513801575\n",
            "  epoch: 9/10,    batch: 13214/15469    Encoder_loss: 0.5880563259124756\n",
            "  epoch: 9/10,    batch: 13215/15469    Encoder_loss: 0.5931544303894043\n",
            "  epoch: 9/10,    batch: 13216/15469    Encoder_loss: 0.5934561491012573\n",
            "  epoch: 9/10,    batch: 13217/15469    Encoder_loss: 0.5925969481468201\n",
            "  epoch: 9/10,    batch: 13218/15469    Encoder_loss: 0.5890138149261475\n",
            "  epoch: 9/10,    batch: 13219/15469    Encoder_loss: 0.5912898778915405\n",
            "  epoch: 9/10,    batch: 13220/15469    Encoder_loss: 0.5919806957244873\n",
            "  epoch: 9/10,    batch: 13221/15469    Encoder_loss: 0.5953917503356934\n",
            "  epoch: 9/10,    batch: 13222/15469    Encoder_loss: 0.5983179211616516\n",
            "  epoch: 9/10,    batch: 13223/15469    Encoder_loss: 0.599675178527832\n",
            "  epoch: 9/10,    batch: 13224/15469    Encoder_loss: 0.6005679965019226\n",
            "  epoch: 9/10,    batch: 13225/15469    Encoder_loss: 0.6040432453155518\n",
            "  epoch: 9/10,    batch: 13226/15469    Encoder_loss: 0.6053670048713684\n",
            "  epoch: 9/10,    batch: 13227/15469    Encoder_loss: 0.6082334518432617\n",
            "  epoch: 9/10,    batch: 13228/15469    Encoder_loss: 0.6106562614440918\n",
            "  epoch: 9/10,    batch: 13229/15469    Encoder_loss: 0.6133425831794739\n",
            "  epoch: 9/10,    batch: 13230/15469    Encoder_loss: 0.6156796216964722\n",
            "  epoch: 9/10,    batch: 13231/15469    Encoder_loss: 0.6169475317001343\n",
            "  epoch: 9/10,    batch: 13232/15469    Encoder_loss: 0.6216264963150024\n",
            "  epoch: 9/10,    batch: 13233/15469    Encoder_loss: 0.6536399126052856\n",
            "  epoch: 9/10,    batch: 13234/15469    Encoder_loss: 0.588148295879364\n",
            "  epoch: 9/10,    batch: 13235/15469    Encoder_loss: 0.5625635385513306\n",
            "  epoch: 9/10,    batch: 13236/15469    Encoder_loss: 0.5614849328994751\n",
            "  epoch: 9/10,    batch: 13237/15469    Encoder_loss: 0.5631454586982727\n",
            "  epoch: 9/10,    batch: 13238/15469    Encoder_loss: 0.5618054270744324\n",
            "  epoch: 9/10,    batch: 13239/15469    Encoder_loss: 0.5635210871696472\n",
            "  epoch: 9/10,    batch: 13240/15469    Encoder_loss: 0.5650676488876343\n",
            "  epoch: 9/10,    batch: 13241/15469    Encoder_loss: 0.5647484064102173\n",
            "  epoch: 9/10,    batch: 13242/15469    Encoder_loss: 0.5721080303192139\n",
            "  epoch: 9/10,    batch: 13243/15469    Encoder_loss: 0.6003185510635376\n",
            "  epoch: 9/10,    batch: 13244/15469    Encoder_loss: 0.6188820600509644\n",
            "  epoch: 9/10,    batch: 13245/15469    Encoder_loss: 0.5897408723831177\n",
            "  epoch: 9/10,    batch: 13246/15469    Encoder_loss: 0.5919613838195801\n",
            "  epoch: 9/10,    batch: 13247/15469    Encoder_loss: 0.5280210971832275\n",
            "  epoch: 9/10,    batch: 13248/15469    Encoder_loss: 0.5128487348556519\n",
            "  epoch: 9/10,    batch: 13249/15469    Encoder_loss: 0.5115858912467957\n",
            "  epoch: 9/10,    batch: 13250/15469    Encoder_loss: 0.5666934251785278\n",
            "  epoch: 9/10,    batch: 13251/15469    Encoder_loss: 0.5949611067771912\n",
            "  epoch: 9/10,    batch: 13252/15469    Encoder_loss: 0.5941323041915894\n",
            "  epoch: 9/10,    batch: 13253/15469    Encoder_loss: 0.5933550000190735\n",
            "  epoch: 9/10,    batch: 13254/15469    Encoder_loss: 0.5914789438247681\n",
            "  epoch: 9/10,    batch: 13255/15469    Encoder_loss: 0.590282678604126\n",
            "  epoch: 9/10,    batch: 13256/15469    Encoder_loss: 0.5879484415054321\n",
            "  epoch: 9/10,    batch: 13257/15469    Encoder_loss: 0.5862160921096802\n",
            "  epoch: 9/10,    batch: 13258/15469    Encoder_loss: 0.5850345492362976\n",
            "  epoch: 9/10,    batch: 13259/15469    Encoder_loss: 0.5838364362716675\n",
            "  epoch: 9/10,    batch: 13260/15469    Encoder_loss: 0.5850152969360352\n",
            "  epoch: 9/10,    batch: 13261/15469    Encoder_loss: 0.5834505558013916\n",
            "  epoch: 9/10,    batch: 13262/15469    Encoder_loss: 0.5844118595123291\n",
            "  epoch: 9/10,    batch: 13263/15469    Encoder_loss: 0.5817680358886719\n",
            "  epoch: 9/10,    batch: 13264/15469    Encoder_loss: 0.5813230276107788\n",
            "  epoch: 9/10,    batch: 13265/15469    Encoder_loss: 0.5817528963088989\n",
            "  epoch: 9/10,    batch: 13266/15469    Encoder_loss: 0.5824748873710632\n",
            "  epoch: 9/10,    batch: 13267/15469    Encoder_loss: 0.5829730033874512\n",
            "  epoch: 9/10,    batch: 13268/15469    Encoder_loss: 0.5815803408622742\n",
            "  epoch: 9/10,    batch: 13269/15469    Encoder_loss: 0.5817096829414368\n",
            "  epoch: 9/10,    batch: 13270/15469    Encoder_loss: 0.5817615985870361\n",
            "  epoch: 9/10,    batch: 13271/15469    Encoder_loss: 0.5825932621955872\n",
            "  epoch: 9/10,    batch: 13272/15469    Encoder_loss: 0.5819315910339355\n",
            "  epoch: 9/10,    batch: 13273/15469    Encoder_loss: 0.582002580165863\n",
            "  epoch: 9/10,    batch: 13274/15469    Encoder_loss: 0.5842754244804382\n",
            "  epoch: 9/10,    batch: 13275/15469    Encoder_loss: 0.5649285316467285\n",
            "  epoch: 9/10,    batch: 13276/15469    Encoder_loss: 0.514750599861145\n",
            "  epoch: 9/10,    batch: 13277/15469    Encoder_loss: 0.5478886365890503\n",
            "  epoch: 9/10,    batch: 13278/15469    Encoder_loss: 0.5497258305549622\n",
            "  epoch: 9/10,    batch: 13279/15469    Encoder_loss: 0.5492405295372009\n",
            "  epoch: 9/10,    batch: 13280/15469    Encoder_loss: 0.5499145984649658\n",
            "  epoch: 9/10,    batch: 13281/15469    Encoder_loss: 0.5503160357475281\n",
            "  epoch: 9/10,    batch: 13282/15469    Encoder_loss: 0.550964891910553\n",
            "  epoch: 9/10,    batch: 13283/15469    Encoder_loss: 0.5511680841445923\n",
            "  epoch: 9/10,    batch: 13284/15469    Encoder_loss: 0.5512739419937134\n",
            "  epoch: 9/10,    batch: 13285/15469    Encoder_loss: 0.5518705248832703\n",
            "  epoch: 9/10,    batch: 13286/15469    Encoder_loss: 0.551543653011322\n",
            "  epoch: 9/10,    batch: 13287/15469    Encoder_loss: 0.551131010055542\n",
            "  epoch: 9/10,    batch: 13288/15469    Encoder_loss: 0.5651037096977234\n",
            "  epoch: 9/10,    batch: 13289/15469    Encoder_loss: 0.5678465366363525\n",
            "  epoch: 9/10,    batch: 13290/15469    Encoder_loss: 0.5415148138999939\n",
            "  epoch: 9/10,    batch: 13291/15469    Encoder_loss: 0.5417256355285645\n",
            "  epoch: 9/10,    batch: 13292/15469    Encoder_loss: 0.4785836338996887\n",
            "  epoch: 9/10,    batch: 13293/15469    Encoder_loss: 0.46771955490112305\n",
            "  epoch: 9/10,    batch: 13294/15469    Encoder_loss: 0.4664495587348938\n",
            "  epoch: 9/10,    batch: 13295/15469    Encoder_loss: 0.4682714343070984\n",
            "  epoch: 9/10,    batch: 13296/15469    Encoder_loss: 0.46865662932395935\n",
            "  epoch: 9/10,    batch: 13297/15469    Encoder_loss: 0.4676287770271301\n",
            "  epoch: 9/10,    batch: 13298/15469    Encoder_loss: 0.46770405769348145\n",
            "  epoch: 9/10,    batch: 13299/15469    Encoder_loss: 0.46734946966171265\n",
            "  epoch: 9/10,    batch: 13300/15469    Encoder_loss: 0.46780410408973694\n",
            "  epoch: 9/10,    batch: 13301/15469    Encoder_loss: 0.4679911732673645\n",
            "  epoch: 9/10,    batch: 13302/15469    Encoder_loss: 0.4669722616672516\n",
            "  epoch: 9/10,    batch: 13303/15469    Encoder_loss: 0.46932727098464966\n",
            "  epoch: 9/10,    batch: 13304/15469    Encoder_loss: 0.46778956055641174\n",
            "  epoch: 9/10,    batch: 13305/15469    Encoder_loss: 0.46835121512413025\n",
            "  epoch: 9/10,    batch: 13306/15469    Encoder_loss: 0.46728992462158203\n",
            "  epoch: 9/10,    batch: 13307/15469    Encoder_loss: 0.4661811292171478\n",
            "  epoch: 9/10,    batch: 13308/15469    Encoder_loss: 0.46663475036621094\n",
            "  epoch: 9/10,    batch: 13309/15469    Encoder_loss: 0.4665193557739258\n",
            "  epoch: 9/10,    batch: 13310/15469    Encoder_loss: 0.4683988690376282\n",
            "  epoch: 9/10,    batch: 13311/15469    Encoder_loss: 0.46660155057907104\n",
            "  epoch: 9/10,    batch: 13312/15469    Encoder_loss: 0.46800798177719116\n",
            "  epoch: 9/10,    batch: 13313/15469    Encoder_loss: 0.46800899505615234\n",
            "  epoch: 9/10,    batch: 13314/15469    Encoder_loss: 0.4671920835971832\n",
            "  epoch: 9/10,    batch: 13315/15469    Encoder_loss: 0.47014564275741577\n",
            "  epoch: 9/10,    batch: 13316/15469    Encoder_loss: 0.4986949861049652\n",
            "  epoch: 9/10,    batch: 13317/15469    Encoder_loss: 0.5004533529281616\n",
            "  epoch: 9/10,    batch: 13318/15469    Encoder_loss: 0.4702024459838867\n",
            "  epoch: 9/10,    batch: 13319/15469    Encoder_loss: 0.4732553958892822\n",
            "  epoch: 9/10,    batch: 13320/15469    Encoder_loss: 0.4365907311439514\n",
            "  epoch: 9/10,    batch: 13321/15469    Encoder_loss: 0.36974865198135376\n",
            "  epoch: 9/10,    batch: 13322/15469    Encoder_loss: 0.3702685236930847\n",
            "  epoch: 9/10,    batch: 13323/15469    Encoder_loss: 0.36796751618385315\n",
            "  epoch: 9/10,    batch: 13324/15469    Encoder_loss: 0.3667929768562317\n",
            "  epoch: 9/10,    batch: 13325/15469    Encoder_loss: 0.3654263913631439\n",
            "  epoch: 9/10,    batch: 13326/15469    Encoder_loss: 0.3640654981136322\n",
            "  epoch: 9/10,    batch: 13327/15469    Encoder_loss: 0.3646753430366516\n",
            "  epoch: 9/10,    batch: 13328/15469    Encoder_loss: 0.3630850315093994\n",
            "  epoch: 9/10,    batch: 13329/15469    Encoder_loss: 0.36047592759132385\n",
            "  epoch: 9/10,    batch: 13330/15469    Encoder_loss: 0.35956010222435\n",
            "  epoch: 9/10,    batch: 13331/15469    Encoder_loss: 0.35928648710250854\n",
            "  epoch: 9/10,    batch: 13332/15469    Encoder_loss: 0.35842078924179077\n",
            "  epoch: 9/10,    batch: 13333/15469    Encoder_loss: 0.35742855072021484\n",
            "  epoch: 9/10,    batch: 13334/15469    Encoder_loss: 0.35745173692703247\n",
            "  epoch: 9/10,    batch: 13335/15469    Encoder_loss: 0.35462188720703125\n",
            "  epoch: 9/10,    batch: 13336/15469    Encoder_loss: 0.3536359369754791\n",
            "  epoch: 9/10,    batch: 13337/15469    Encoder_loss: 0.35121503472328186\n",
            "  epoch: 9/10,    batch: 13338/15469    Encoder_loss: 0.35056522488594055\n",
            "  epoch: 9/10,    batch: 13339/15469    Encoder_loss: 0.3507193922996521\n",
            "  epoch: 9/10,    batch: 13340/15469    Encoder_loss: 0.3487823009490967\n",
            "  epoch: 9/10,    batch: 13341/15469    Encoder_loss: 0.34688159823417664\n",
            "  epoch: 9/10,    batch: 13342/15469    Encoder_loss: 0.34624847769737244\n",
            "  epoch: 9/10,    batch: 13343/15469    Encoder_loss: 0.3444426655769348\n",
            "  epoch: 9/10,    batch: 13344/15469    Encoder_loss: 0.3430247902870178\n",
            "  epoch: 9/10,    batch: 13345/15469    Encoder_loss: 0.3423658013343811\n",
            "  epoch: 9/10,    batch: 13346/15469    Encoder_loss: 0.3408655524253845\n",
            "  epoch: 9/10,    batch: 13347/15469    Encoder_loss: 0.3396379053592682\n",
            "  epoch: 9/10,    batch: 13348/15469    Encoder_loss: 0.3380955755710602\n",
            "  epoch: 9/10,    batch: 13349/15469    Encoder_loss: 0.3373151421546936\n",
            "  epoch: 9/10,    batch: 13350/15469    Encoder_loss: 0.3391803205013275\n",
            "  epoch: 9/10,    batch: 13351/15469    Encoder_loss: 0.3356689512729645\n",
            "  epoch: 9/10,    batch: 13352/15469    Encoder_loss: 0.336351215839386\n",
            "  epoch: 9/10,    batch: 13353/15469    Encoder_loss: 0.3351146876811981\n",
            "  epoch: 9/10,    batch: 13354/15469    Encoder_loss: 0.33185911178588867\n",
            "  epoch: 9/10,    batch: 13355/15469    Encoder_loss: 0.33087077736854553\n",
            "  epoch: 9/10,    batch: 13356/15469    Encoder_loss: 0.3306640386581421\n",
            "  epoch: 9/10,    batch: 13357/15469    Encoder_loss: 0.36953607201576233\n",
            "  epoch: 9/10,    batch: 13358/15469    Encoder_loss: 0.5948824882507324\n",
            "  epoch: 9/10,    batch: 13359/15469    Encoder_loss: 0.7711013555526733\n",
            "  epoch: 9/10,    batch: 13360/15469    Encoder_loss: 0.7329562902450562\n",
            "  epoch: 9/10,    batch: 13361/15469    Encoder_loss: 0.531398355960846\n",
            "  epoch: 9/10,    batch: 13362/15469    Encoder_loss: 0.4797385632991791\n",
            "  epoch: 9/10,    batch: 13363/15469    Encoder_loss: 0.48029112815856934\n",
            "  epoch: 9/10,    batch: 13364/15469    Encoder_loss: 0.4616757333278656\n",
            "  epoch: 9/10,    batch: 13365/15469    Encoder_loss: 0.3757195770740509\n",
            "  epoch: 9/10,    batch: 13366/15469    Encoder_loss: 0.37339431047439575\n",
            "  epoch: 9/10,    batch: 13367/15469    Encoder_loss: 0.32640910148620605\n",
            "  epoch: 9/10,    batch: 13368/15469    Encoder_loss: 0.29245054721832275\n",
            "  epoch: 9/10,    batch: 13369/15469    Encoder_loss: 0.2918590009212494\n",
            "  epoch: 9/10,    batch: 13370/15469    Encoder_loss: 0.2905714213848114\n",
            "  epoch: 9/10,    batch: 13371/15469    Encoder_loss: 0.2912587821483612\n",
            "  epoch: 9/10,    batch: 13372/15469    Encoder_loss: 0.29070261120796204\n",
            "  epoch: 9/10,    batch: 13373/15469    Encoder_loss: 0.29228445887565613\n",
            "  epoch: 9/10,    batch: 13374/15469    Encoder_loss: 0.301797479391098\n",
            "  epoch: 9/10,    batch: 13375/15469    Encoder_loss: 0.3082290291786194\n",
            "  epoch: 9/10,    batch: 13376/15469    Encoder_loss: 0.3075401186943054\n",
            "  epoch: 9/10,    batch: 13377/15469    Encoder_loss: 0.3045217990875244\n",
            "  epoch: 9/10,    batch: 13378/15469    Encoder_loss: 0.3036234676837921\n",
            "  epoch: 9/10,    batch: 13379/15469    Encoder_loss: 0.30325159430503845\n",
            "  epoch: 9/10,    batch: 13380/15469    Encoder_loss: 0.3002432882785797\n",
            "  epoch: 9/10,    batch: 13381/15469    Encoder_loss: 0.29999542236328125\n",
            "  epoch: 9/10,    batch: 13382/15469    Encoder_loss: 0.2988560199737549\n",
            "  epoch: 9/10,    batch: 13383/15469    Encoder_loss: 0.29775503277778625\n",
            "  epoch: 9/10,    batch: 13384/15469    Encoder_loss: 0.296800822019577\n",
            "  epoch: 9/10,    batch: 13385/15469    Encoder_loss: 0.2959582507610321\n",
            "  epoch: 9/10,    batch: 13386/15469    Encoder_loss: 0.2968750596046448\n",
            "  epoch: 9/10,    batch: 13387/15469    Encoder_loss: 0.2964405417442322\n",
            "  epoch: 9/10,    batch: 13388/15469    Encoder_loss: 0.29426997900009155\n",
            "  epoch: 9/10,    batch: 13389/15469    Encoder_loss: 0.295016348361969\n",
            "  epoch: 9/10,    batch: 13390/15469    Encoder_loss: 0.2931939363479614\n",
            "  epoch: 9/10,    batch: 13391/15469    Encoder_loss: 0.2928116023540497\n",
            "  epoch: 9/10,    batch: 13392/15469    Encoder_loss: 0.2937849760055542\n",
            "  epoch: 9/10,    batch: 13393/15469    Encoder_loss: 0.291606068611145\n",
            "  epoch: 9/10,    batch: 13394/15469    Encoder_loss: 0.2915992736816406\n",
            "  epoch: 9/10,    batch: 13395/15469    Encoder_loss: 0.29095855355262756\n",
            "  epoch: 9/10,    batch: 13396/15469    Encoder_loss: 0.29064834117889404\n",
            "  epoch: 9/10,    batch: 13397/15469    Encoder_loss: 0.28978589177131653\n",
            "  epoch: 9/10,    batch: 13398/15469    Encoder_loss: 0.2888101637363434\n",
            "  epoch: 9/10,    batch: 13399/15469    Encoder_loss: 0.28812941908836365\n",
            "  epoch: 9/10,    batch: 13400/15469    Encoder_loss: 0.2883577048778534\n",
            "  epoch: 9/10,    batch: 13401/15469    Encoder_loss: 0.28689688444137573\n",
            "  epoch: 9/10,    batch: 13402/15469    Encoder_loss: 0.30411162972450256\n",
            "  epoch: 9/10,    batch: 13403/15469    Encoder_loss: 0.4761330485343933\n",
            "  epoch: 9/10,    batch: 13404/15469    Encoder_loss: 0.7072833776473999\n",
            "  epoch: 9/10,    batch: 13405/15469    Encoder_loss: 0.7216389775276184\n",
            "  epoch: 9/10,    batch: 13406/15469    Encoder_loss: 0.5120953321456909\n",
            "  epoch: 9/10,    batch: 13407/15469    Encoder_loss: 0.4726766049861908\n",
            "  epoch: 9/10,    batch: 13408/15469    Encoder_loss: 0.4718407094478607\n",
            "  epoch: 9/10,    batch: 13409/15469    Encoder_loss: 0.4637531340122223\n",
            "  epoch: 9/10,    batch: 13410/15469    Encoder_loss: 0.3844151496887207\n",
            "  epoch: 9/10,    batch: 13411/15469    Encoder_loss: 0.36194857954978943\n",
            "  epoch: 9/10,    batch: 13412/15469    Encoder_loss: 0.3375728726387024\n",
            "  epoch: 9/10,    batch: 13413/15469    Encoder_loss: 0.28662794828414917\n",
            "  epoch: 9/10,    batch: 13414/15469    Encoder_loss: 0.28706949949264526\n",
            "  epoch: 9/10,    batch: 13415/15469    Encoder_loss: 0.284463107585907\n",
            "  epoch: 9/10,    batch: 13416/15469    Encoder_loss: 0.2843919098377228\n",
            "  epoch: 9/10,    batch: 13417/15469    Encoder_loss: 0.28597068786621094\n",
            "  epoch: 9/10,    batch: 13418/15469    Encoder_loss: 0.286864697933197\n",
            "  epoch: 9/10,    batch: 13419/15469    Encoder_loss: 0.29414135217666626\n",
            "  epoch: 9/10,    batch: 13420/15469    Encoder_loss: 0.3020901083946228\n",
            "  epoch: 9/10,    batch: 13421/15469    Encoder_loss: 0.3026179373264313\n",
            "  epoch: 9/10,    batch: 13422/15469    Encoder_loss: 0.29886963963508606\n",
            "  epoch: 9/10,    batch: 13423/15469    Encoder_loss: 0.29638436436653137\n",
            "  epoch: 9/10,    batch: 13424/15469    Encoder_loss: 0.2945061922073364\n",
            "  epoch: 9/10,    batch: 13425/15469    Encoder_loss: 0.294790655374527\n",
            "  epoch: 9/10,    batch: 13426/15469    Encoder_loss: 0.29242998361587524\n",
            "  epoch: 9/10,    batch: 13427/15469    Encoder_loss: 0.29108673334121704\n",
            "  epoch: 9/10,    batch: 13428/15469    Encoder_loss: 0.29117101430892944\n",
            "  epoch: 9/10,    batch: 13429/15469    Encoder_loss: 0.29078447818756104\n",
            "  epoch: 9/10,    batch: 13430/15469    Encoder_loss: 0.2900209426879883\n",
            "  epoch: 9/10,    batch: 13431/15469    Encoder_loss: 0.2875835597515106\n",
            "  epoch: 9/10,    batch: 13432/15469    Encoder_loss: 0.28891244530677795\n",
            "  epoch: 9/10,    batch: 13433/15469    Encoder_loss: 0.2894658148288727\n",
            "  epoch: 9/10,    batch: 13434/15469    Encoder_loss: 0.28739991784095764\n",
            "  epoch: 9/10,    batch: 13435/15469    Encoder_loss: 0.28983086347579956\n",
            "  epoch: 9/10,    batch: 13436/15469    Encoder_loss: 0.28765183687210083\n",
            "  epoch: 9/10,    batch: 13437/15469    Encoder_loss: 0.2856748700141907\n",
            "  epoch: 9/10,    batch: 13438/15469    Encoder_loss: 0.2867377996444702\n",
            "  epoch: 9/10,    batch: 13439/15469    Encoder_loss: 0.2858814597129822\n",
            "  epoch: 9/10,    batch: 13440/15469    Encoder_loss: 0.2857503890991211\n",
            "  epoch: 9/10,    batch: 13441/15469    Encoder_loss: 0.28535208106040955\n",
            "  epoch: 9/10,    batch: 13442/15469    Encoder_loss: 0.2841547131538391\n",
            "  epoch: 9/10,    batch: 13443/15469    Encoder_loss: 0.2844988703727722\n",
            "  epoch: 9/10,    batch: 13444/15469    Encoder_loss: 0.28395456075668335\n",
            "  epoch: 9/10,    batch: 13445/15469    Encoder_loss: 0.2830227017402649\n",
            "  epoch: 9/10,    batch: 13446/15469    Encoder_loss: 0.2819615602493286\n",
            "  epoch: 9/10,    batch: 13447/15469    Encoder_loss: 0.2824370861053467\n",
            "  epoch: 9/10,    batch: 13448/15469    Encoder_loss: 0.28358006477355957\n",
            "  epoch: 9/10,    batch: 13449/15469    Encoder_loss: 0.2816949486732483\n",
            "  epoch: 9/10,    batch: 13450/15469    Encoder_loss: 0.28106826543807983\n",
            "  epoch: 9/10,    batch: 13451/15469    Encoder_loss: 0.2811467945575714\n",
            "  epoch: 9/10,    batch: 13452/15469    Encoder_loss: 0.2804838716983795\n",
            "  epoch: 9/10,    batch: 13453/15469    Encoder_loss: 0.28067392110824585\n",
            "  epoch: 9/10,    batch: 13454/15469    Encoder_loss: 0.28208523988723755\n",
            "  epoch: 9/10,    batch: 13455/15469    Encoder_loss: 0.27915874123573303\n",
            "  epoch: 9/10,    batch: 13456/15469    Encoder_loss: 0.2804562449455261\n",
            "  epoch: 9/10,    batch: 13457/15469    Encoder_loss: 0.27987462282180786\n",
            "  epoch: 9/10,    batch: 13458/15469    Encoder_loss: 0.27777180075645447\n",
            "  epoch: 9/10,    batch: 13459/15469    Encoder_loss: 0.2794014811515808\n",
            "  epoch: 9/10,    batch: 13460/15469    Encoder_loss: 0.27815622091293335\n",
            "  epoch: 9/10,    batch: 13461/15469    Encoder_loss: 0.277983695268631\n",
            "  epoch: 9/10,    batch: 13462/15469    Encoder_loss: 0.27966347336769104\n",
            "  epoch: 9/10,    batch: 13463/15469    Encoder_loss: 0.2797901928424835\n",
            "  epoch: 9/10,    batch: 13464/15469    Encoder_loss: 0.2783867120742798\n",
            "  epoch: 9/10,    batch: 13465/15469    Encoder_loss: 0.27613401412963867\n",
            "  epoch: 9/10,    batch: 13466/15469    Encoder_loss: 0.2782087028026581\n",
            "  epoch: 9/10,    batch: 13467/15469    Encoder_loss: 0.2762804925441742\n",
            "  epoch: 9/10,    batch: 13468/15469    Encoder_loss: 0.2784883677959442\n",
            "  epoch: 9/10,    batch: 13469/15469    Encoder_loss: 0.27640727162361145\n",
            "  epoch: 9/10,    batch: 13470/15469    Encoder_loss: 0.276933491230011\n",
            "  epoch: 9/10,    batch: 13471/15469    Encoder_loss: 0.27685463428497314\n",
            "  epoch: 9/10,    batch: 13472/15469    Encoder_loss: 0.3061211109161377\n",
            "  epoch: 9/10,    batch: 13473/15469    Encoder_loss: 0.36147361993789673\n",
            "  epoch: 9/10,    batch: 13474/15469    Encoder_loss: 0.36116892099380493\n",
            "  epoch: 9/10,    batch: 13475/15469    Encoder_loss: 0.4304603636264801\n",
            "  epoch: 9/10,    batch: 13476/15469    Encoder_loss: 0.46267399191856384\n",
            "  epoch: 9/10,    batch: 13477/15469    Encoder_loss: 0.46373844146728516\n",
            "  epoch: 9/10,    batch: 13478/15469    Encoder_loss: 0.4625991880893707\n",
            "  epoch: 9/10,    batch: 13479/15469    Encoder_loss: 0.4655742645263672\n",
            "  epoch: 9/10,    batch: 13480/15469    Encoder_loss: 0.4647687077522278\n",
            "  epoch: 9/10,    batch: 13481/15469    Encoder_loss: 0.4649670124053955\n",
            "  epoch: 9/10,    batch: 13482/15469    Encoder_loss: 0.46478432416915894\n",
            "  epoch: 9/10,    batch: 13483/15469    Encoder_loss: 0.4656631052494049\n",
            "  epoch: 9/10,    batch: 13484/15469    Encoder_loss: 0.46575433015823364\n",
            "  epoch: 9/10,    batch: 13485/15469    Encoder_loss: 0.46530047059059143\n",
            "  epoch: 9/10,    batch: 13486/15469    Encoder_loss: 0.4670819640159607\n",
            "  epoch: 9/10,    batch: 13487/15469    Encoder_loss: 0.4695839285850525\n",
            "  epoch: 9/10,    batch: 13488/15469    Encoder_loss: 0.46927541494369507\n",
            "  epoch: 9/10,    batch: 13489/15469    Encoder_loss: 0.47050198912620544\n",
            "  epoch: 9/10,    batch: 13490/15469    Encoder_loss: 0.47102755308151245\n",
            "  epoch: 9/10,    batch: 13491/15469    Encoder_loss: 0.4723098874092102\n",
            "  epoch: 9/10,    batch: 13492/15469    Encoder_loss: 0.47370797395706177\n",
            "  epoch: 9/10,    batch: 13493/15469    Encoder_loss: 0.47349071502685547\n",
            "  epoch: 9/10,    batch: 13494/15469    Encoder_loss: 0.47499483823776245\n",
            "  epoch: 9/10,    batch: 13495/15469    Encoder_loss: 0.4755220413208008\n",
            "  epoch: 9/10,    batch: 13496/15469    Encoder_loss: 0.4760732650756836\n",
            "  epoch: 9/10,    batch: 13497/15469    Encoder_loss: 0.4783322215080261\n",
            "  epoch: 9/10,    batch: 13498/15469    Encoder_loss: 0.4784814715385437\n",
            "  epoch: 9/10,    batch: 13499/15469    Encoder_loss: 0.4802972376346588\n",
            "  epoch: 9/10,    batch: 13500/15469    Encoder_loss: 0.48097681999206543\n",
            "  epoch: 9/10,    batch: 13501/15469    Encoder_loss: 0.4834403097629547\n",
            "  epoch: 9/10,    batch: 13502/15469    Encoder_loss: 0.4823502004146576\n",
            "  epoch: 9/10,    batch: 13503/15469    Encoder_loss: 0.4842686057090759\n",
            "  epoch: 9/10,    batch: 13504/15469    Encoder_loss: 0.48641809821128845\n",
            "  epoch: 9/10,    batch: 13505/15469    Encoder_loss: 0.4891255497932434\n",
            "  epoch: 9/10,    batch: 13506/15469    Encoder_loss: 0.48899489641189575\n",
            "  epoch: 9/10,    batch: 13507/15469    Encoder_loss: 0.4906701147556305\n",
            "  epoch: 9/10,    batch: 13508/15469    Encoder_loss: 0.4917217791080475\n",
            "  epoch: 9/10,    batch: 13509/15469    Encoder_loss: 0.5002061128616333\n",
            "  epoch: 9/10,    batch: 13510/15469    Encoder_loss: 0.5242615938186646\n",
            "  epoch: 9/10,    batch: 13511/15469    Encoder_loss: 0.5503333806991577\n",
            "  epoch: 9/10,    batch: 13512/15469    Encoder_loss: 0.5268128514289856\n",
            "  epoch: 9/10,    batch: 13513/15469    Encoder_loss: 0.5210257768630981\n",
            "  epoch: 9/10,    batch: 13514/15469    Encoder_loss: 0.5042299628257751\n",
            "  epoch: 9/10,    batch: 13515/15469    Encoder_loss: 0.4454156458377838\n",
            "  epoch: 9/10,    batch: 13516/15469    Encoder_loss: 0.4452420771121979\n",
            "  epoch: 9/10,    batch: 13517/15469    Encoder_loss: 0.44593364000320435\n",
            "  epoch: 9/10,    batch: 13518/15469    Encoder_loss: 0.44590121507644653\n",
            "  epoch: 9/10,    batch: 13519/15469    Encoder_loss: 0.4460909366607666\n",
            "  epoch: 9/10,    batch: 13520/15469    Encoder_loss: 0.4464908540248871\n",
            "  epoch: 9/10,    batch: 13521/15469    Encoder_loss: 0.44635123014450073\n",
            "  epoch: 9/10,    batch: 13522/15469    Encoder_loss: 0.4632047116756439\n",
            "  epoch: 9/10,    batch: 13523/15469    Encoder_loss: 0.6321635842323303\n",
            "  epoch: 9/10,    batch: 13524/15469    Encoder_loss: 0.8615607023239136\n",
            "  epoch: 9/10,    batch: 13525/15469    Encoder_loss: 0.8835576772689819\n",
            "  epoch: 9/10,    batch: 13526/15469    Encoder_loss: 0.7045843601226807\n",
            "  epoch: 9/10,    batch: 13527/15469    Encoder_loss: 0.7339984178543091\n",
            "  epoch: 9/10,    batch: 13528/15469    Encoder_loss: 0.7328512072563171\n",
            "  epoch: 9/10,    batch: 13529/15469    Encoder_loss: 0.7275955677032471\n",
            "  epoch: 9/10,    batch: 13530/15469    Encoder_loss: 0.6456539034843445\n",
            "  epoch: 9/10,    batch: 13531/15469    Encoder_loss: 0.6252188682556152\n",
            "  epoch: 9/10,    batch: 13532/15469    Encoder_loss: 0.606397807598114\n",
            "  epoch: 9/10,    batch: 13533/15469    Encoder_loss: 0.5498083829879761\n",
            "  epoch: 9/10,    batch: 13534/15469    Encoder_loss: 0.5514675378799438\n",
            "  epoch: 9/10,    batch: 13535/15469    Encoder_loss: 0.5539287328720093\n",
            "  epoch: 9/10,    batch: 13536/15469    Encoder_loss: 0.5523933172225952\n",
            "  epoch: 9/10,    batch: 13537/15469    Encoder_loss: 0.5552948713302612\n",
            "  epoch: 9/10,    batch: 13538/15469    Encoder_loss: 0.5562982559204102\n",
            "  epoch: 9/10,    batch: 13539/15469    Encoder_loss: 0.562578558921814\n",
            "  epoch: 9/10,    batch: 13540/15469    Encoder_loss: 0.5718673467636108\n",
            "  epoch: 9/10,    batch: 13541/15469    Encoder_loss: 0.5794996619224548\n",
            "  epoch: 9/10,    batch: 13542/15469    Encoder_loss: 0.5864333510398865\n",
            "  epoch: 9/10,    batch: 13543/15469    Encoder_loss: 0.5410870909690857\n",
            "  epoch: 9/10,    batch: 13544/15469    Encoder_loss: 0.4800604283809662\n",
            "  epoch: 9/10,    batch: 13545/15469    Encoder_loss: 0.47784581780433655\n",
            "  epoch: 9/10,    batch: 13546/15469    Encoder_loss: 0.47729572653770447\n",
            "  epoch: 9/10,    batch: 13547/15469    Encoder_loss: 0.47558489441871643\n",
            "  epoch: 9/10,    batch: 13548/15469    Encoder_loss: 0.47564297914505005\n",
            "  epoch: 9/10,    batch: 13549/15469    Encoder_loss: 0.474738210439682\n",
            "  epoch: 9/10,    batch: 13550/15469    Encoder_loss: 0.4756453037261963\n",
            "  epoch: 9/10,    batch: 13551/15469    Encoder_loss: 0.4758710265159607\n",
            "  epoch: 9/10,    batch: 13552/15469    Encoder_loss: 0.4767557978630066\n",
            "  epoch: 9/10,    batch: 13553/15469    Encoder_loss: 0.4775099754333496\n",
            "  epoch: 9/10,    batch: 13554/15469    Encoder_loss: 0.4776820242404938\n",
            "  epoch: 9/10,    batch: 13555/15469    Encoder_loss: 0.47731471061706543\n",
            "  epoch: 9/10,    batch: 13556/15469    Encoder_loss: 0.47716957330703735\n",
            "  epoch: 9/10,    batch: 13557/15469    Encoder_loss: 0.4794577956199646\n",
            "  epoch: 9/10,    batch: 13558/15469    Encoder_loss: 0.4790521562099457\n",
            "  epoch: 9/10,    batch: 13559/15469    Encoder_loss: 0.4886167645454407\n",
            "  epoch: 9/10,    batch: 13560/15469    Encoder_loss: 0.4603351652622223\n",
            "  epoch: 9/10,    batch: 13561/15469    Encoder_loss: 0.4141905903816223\n",
            "  epoch: 9/10,    batch: 13562/15469    Encoder_loss: 0.41471561789512634\n",
            "  epoch: 9/10,    batch: 13563/15469    Encoder_loss: 0.413236141204834\n",
            "  epoch: 9/10,    batch: 13564/15469    Encoder_loss: 0.4114893972873688\n",
            "  epoch: 9/10,    batch: 13565/15469    Encoder_loss: 0.41006308794021606\n",
            "  epoch: 9/10,    batch: 13566/15469    Encoder_loss: 0.40878766775131226\n",
            "  epoch: 9/10,    batch: 13567/15469    Encoder_loss: 0.40609320998191833\n",
            "  epoch: 9/10,    batch: 13568/15469    Encoder_loss: 0.4050160050392151\n",
            "  epoch: 9/10,    batch: 13569/15469    Encoder_loss: 0.40272483229637146\n",
            "  epoch: 9/10,    batch: 13570/15469    Encoder_loss: 0.4033694267272949\n",
            "  epoch: 9/10,    batch: 13571/15469    Encoder_loss: 0.4016433656215668\n",
            "  epoch: 9/10,    batch: 13572/15469    Encoder_loss: 0.39741018414497375\n",
            "  epoch: 9/10,    batch: 13573/15469    Encoder_loss: 0.39624348282814026\n",
            "  epoch: 9/10,    batch: 13574/15469    Encoder_loss: 0.3935515880584717\n",
            "  epoch: 9/10,    batch: 13575/15469    Encoder_loss: 0.392962247133255\n",
            "  epoch: 9/10,    batch: 13576/15469    Encoder_loss: 0.39173489809036255\n",
            "  epoch: 9/10,    batch: 13577/15469    Encoder_loss: 0.42005762457847595\n",
            "  epoch: 9/10,    batch: 13578/15469    Encoder_loss: 0.4728265404701233\n",
            "  epoch: 9/10,    batch: 13579/15469    Encoder_loss: 0.47159916162490845\n",
            "  epoch: 9/10,    batch: 13580/15469    Encoder_loss: 0.4702218472957611\n",
            "  epoch: 9/10,    batch: 13581/15469    Encoder_loss: 0.4663822650909424\n",
            "  epoch: 9/10,    batch: 13582/15469    Encoder_loss: 0.4636734127998352\n",
            "  epoch: 9/10,    batch: 13583/15469    Encoder_loss: 0.4604731500148773\n",
            "  epoch: 9/10,    batch: 13584/15469    Encoder_loss: 0.46218281984329224\n",
            "  epoch: 9/10,    batch: 13585/15469    Encoder_loss: 0.4223875105381012\n",
            "  epoch: 9/10,    batch: 13586/15469    Encoder_loss: 0.4215730130672455\n",
            "  epoch: 9/10,    batch: 13587/15469    Encoder_loss: 0.4212462306022644\n",
            "  epoch: 9/10,    batch: 13588/15469    Encoder_loss: 0.4189468026161194\n",
            "  epoch: 9/10,    batch: 13589/15469    Encoder_loss: 0.42020291090011597\n",
            "  epoch: 9/10,    batch: 13590/15469    Encoder_loss: 0.41655445098876953\n",
            "  epoch: 9/10,    batch: 13591/15469    Encoder_loss: 0.4165290892124176\n",
            "  epoch: 9/10,    batch: 13592/15469    Encoder_loss: 0.4147518277168274\n",
            "  epoch: 9/10,    batch: 13593/15469    Encoder_loss: 0.4155302047729492\n",
            "  epoch: 9/10,    batch: 13594/15469    Encoder_loss: 0.4132830500602722\n",
            "  epoch: 9/10,    batch: 13595/15469    Encoder_loss: 0.4122329652309418\n",
            "  epoch: 9/10,    batch: 13596/15469    Encoder_loss: 0.41139769554138184\n",
            "  epoch: 9/10,    batch: 13597/15469    Encoder_loss: 0.4107535481452942\n",
            "  epoch: 9/10,    batch: 13598/15469    Encoder_loss: 0.41146811842918396\n",
            "  epoch: 9/10,    batch: 13599/15469    Encoder_loss: 0.411092609167099\n",
            "  epoch: 9/10,    batch: 13600/15469    Encoder_loss: 0.410365492105484\n",
            "  epoch: 9/10,    batch: 13601/15469    Encoder_loss: 0.4198065400123596\n",
            "  epoch: 9/10,    batch: 13602/15469    Encoder_loss: 0.3816535174846649\n",
            "  epoch: 9/10,    batch: 13603/15469    Encoder_loss: 0.37770694494247437\n",
            "  epoch: 9/10,    batch: 13604/15469    Encoder_loss: 0.45193424820899963\n",
            "  epoch: 9/10,    batch: 13605/15469    Encoder_loss: 0.4796779155731201\n",
            "  epoch: 9/10,    batch: 13606/15469    Encoder_loss: 0.4787462651729584\n",
            "  epoch: 9/10,    batch: 13607/15469    Encoder_loss: 0.48004189133644104\n",
            "  epoch: 9/10,    batch: 13608/15469    Encoder_loss: 0.48106664419174194\n",
            "  epoch: 9/10,    batch: 13609/15469    Encoder_loss: 0.48063772916793823\n",
            "  epoch: 9/10,    batch: 13610/15469    Encoder_loss: 0.4819304943084717\n",
            "  epoch: 9/10,    batch: 13611/15469    Encoder_loss: 0.48412618041038513\n",
            "  epoch: 9/10,    batch: 13612/15469    Encoder_loss: 0.486067533493042\n",
            "  epoch: 9/10,    batch: 13613/15469    Encoder_loss: 0.4856418967247009\n",
            "  epoch: 9/10,    batch: 13614/15469    Encoder_loss: 0.4863324761390686\n",
            "  epoch: 9/10,    batch: 13615/15469    Encoder_loss: 0.4940902590751648\n",
            "  epoch: 9/10,    batch: 13616/15469    Encoder_loss: 0.5166181325912476\n",
            "  epoch: 9/10,    batch: 13617/15469    Encoder_loss: 0.47977858781814575\n",
            "  epoch: 9/10,    batch: 13618/15469    Encoder_loss: 0.4859016537666321\n",
            "  epoch: 9/10,    batch: 13619/15469    Encoder_loss: 0.4294014871120453\n",
            "  epoch: 9/10,    batch: 13620/15469    Encoder_loss: 0.40744251012802124\n",
            "  epoch: 9/10,    batch: 13621/15469    Encoder_loss: 0.4074011743068695\n",
            "  epoch: 9/10,    batch: 13622/15469    Encoder_loss: 0.4094800353050232\n",
            "  epoch: 9/10,    batch: 13623/15469    Encoder_loss: 0.4249687194824219\n",
            "  epoch: 9/10,    batch: 13624/15469    Encoder_loss: 0.5876991748809814\n",
            "  epoch: 9/10,    batch: 13625/15469    Encoder_loss: 0.8267838358879089\n",
            "  epoch: 9/10,    batch: 13626/15469    Encoder_loss: 0.8425776362419128\n",
            "  epoch: 9/10,    batch: 13627/15469    Encoder_loss: 0.6417233347892761\n",
            "  epoch: 9/10,    batch: 13628/15469    Encoder_loss: 0.5983551740646362\n",
            "  epoch: 9/10,    batch: 13629/15469    Encoder_loss: 0.5985533595085144\n",
            "  epoch: 9/10,    batch: 13630/15469    Encoder_loss: 0.5940173268318176\n",
            "  epoch: 9/10,    batch: 13631/15469    Encoder_loss: 0.5163246393203735\n",
            "  epoch: 9/10,    batch: 13632/15469    Encoder_loss: 0.49179792404174805\n",
            "  epoch: 9/10,    batch: 13633/15469    Encoder_loss: 0.473177045583725\n",
            "  epoch: 9/10,    batch: 13634/15469    Encoder_loss: 0.4181368350982666\n",
            "  epoch: 9/10,    batch: 13635/15469    Encoder_loss: 0.41891106963157654\n",
            "  epoch: 9/10,    batch: 13636/15469    Encoder_loss: 0.4202568531036377\n",
            "  epoch: 9/10,    batch: 13637/15469    Encoder_loss: 0.4199158549308777\n",
            "  epoch: 9/10,    batch: 13638/15469    Encoder_loss: 0.42249518632888794\n",
            "  epoch: 9/10,    batch: 13639/15469    Encoder_loss: 0.42230916023254395\n",
            "  epoch: 9/10,    batch: 13640/15469    Encoder_loss: 0.42827120423316956\n",
            "  epoch: 9/10,    batch: 13641/15469    Encoder_loss: 0.4353586733341217\n",
            "  epoch: 9/10,    batch: 13642/15469    Encoder_loss: 0.4392278790473938\n",
            "  epoch: 9/10,    batch: 13643/15469    Encoder_loss: 0.4478992819786072\n",
            "  epoch: 9/10,    batch: 13644/15469    Encoder_loss: 0.4675436019897461\n",
            "  epoch: 9/10,    batch: 13645/15469    Encoder_loss: 0.47369837760925293\n",
            "  epoch: 9/10,    batch: 13646/15469    Encoder_loss: 0.4737817645072937\n",
            "  epoch: 9/10,    batch: 13647/15469    Encoder_loss: 0.46570977568626404\n",
            "  epoch: 9/10,    batch: 13648/15469    Encoder_loss: 0.3860498070716858\n",
            "  epoch: 9/10,    batch: 13649/15469    Encoder_loss: 0.37260934710502625\n",
            "  epoch: 9/10,    batch: 13650/15469    Encoder_loss: 0.37123239040374756\n",
            "  epoch: 9/10,    batch: 13651/15469    Encoder_loss: 0.36993223428726196\n",
            "  epoch: 9/10,    batch: 13652/15469    Encoder_loss: 0.3680937886238098\n",
            "  epoch: 9/10,    batch: 13653/15469    Encoder_loss: 0.36640849709510803\n",
            "  epoch: 9/10,    batch: 13654/15469    Encoder_loss: 0.3665623366832733\n",
            "  epoch: 9/10,    batch: 13655/15469    Encoder_loss: 0.36305803060531616\n",
            "  epoch: 9/10,    batch: 13656/15469    Encoder_loss: 0.36207112669944763\n",
            "  epoch: 9/10,    batch: 13657/15469    Encoder_loss: 0.3609299957752228\n",
            "  epoch: 9/10,    batch: 13658/15469    Encoder_loss: 0.3590725362300873\n",
            "  epoch: 9/10,    batch: 13659/15469    Encoder_loss: 0.35877174139022827\n",
            "  epoch: 9/10,    batch: 13660/15469    Encoder_loss: 0.3577929735183716\n",
            "  epoch: 9/10,    batch: 13661/15469    Encoder_loss: 0.3574236035346985\n",
            "  epoch: 9/10,    batch: 13662/15469    Encoder_loss: 0.3542410731315613\n",
            "  epoch: 9/10,    batch: 13663/15469    Encoder_loss: 0.3533812165260315\n",
            "  epoch: 9/10,    batch: 13664/15469    Encoder_loss: 0.35095492005348206\n",
            "  epoch: 9/10,    batch: 13665/15469    Encoder_loss: 0.35176146030426025\n",
            "  epoch: 9/10,    batch: 13666/15469    Encoder_loss: 0.35187873244285583\n",
            "  epoch: 9/10,    batch: 13667/15469    Encoder_loss: 0.3489728569984436\n",
            "  epoch: 9/10,    batch: 13668/15469    Encoder_loss: 0.3451703190803528\n",
            "  epoch: 9/10,    batch: 13669/15469    Encoder_loss: 0.3453623652458191\n",
            "  epoch: 9/10,    batch: 13670/15469    Encoder_loss: 0.34497296810150146\n",
            "  epoch: 9/10,    batch: 13671/15469    Encoder_loss: 0.3430808186531067\n",
            "  epoch: 9/10,    batch: 13672/15469    Encoder_loss: 0.3424960672855377\n",
            "  epoch: 9/10,    batch: 13673/15469    Encoder_loss: 0.3404385447502136\n",
            "  epoch: 9/10,    batch: 13674/15469    Encoder_loss: 0.34106725454330444\n",
            "  epoch: 9/10,    batch: 13675/15469    Encoder_loss: 0.33851927518844604\n",
            "  epoch: 9/10,    batch: 13676/15469    Encoder_loss: 0.3365531265735626\n",
            "  epoch: 9/10,    batch: 13677/15469    Encoder_loss: 0.337642103433609\n",
            "  epoch: 9/10,    batch: 13678/15469    Encoder_loss: 0.33607226610183716\n",
            "  epoch: 9/10,    batch: 13679/15469    Encoder_loss: 0.333560973405838\n",
            "  epoch: 9/10,    batch: 13680/15469    Encoder_loss: 0.3336145281791687\n",
            "  epoch: 9/10,    batch: 13681/15469    Encoder_loss: 0.33296889066696167\n",
            "  epoch: 9/10,    batch: 13682/15469    Encoder_loss: 0.331427663564682\n",
            "  epoch: 9/10,    batch: 13683/15469    Encoder_loss: 0.3292486071586609\n",
            "  epoch: 9/10,    batch: 13684/15469    Encoder_loss: 0.3283815383911133\n",
            "  epoch: 9/10,    batch: 13685/15469    Encoder_loss: 0.32689833641052246\n",
            "  epoch: 9/10,    batch: 13686/15469    Encoder_loss: 0.32973793148994446\n",
            "  epoch: 9/10,    batch: 13687/15469    Encoder_loss: 0.32622039318084717\n",
            "  epoch: 9/10,    batch: 13688/15469    Encoder_loss: 0.3278143107891083\n",
            "  epoch: 9/10,    batch: 13689/15469    Encoder_loss: 0.31759113073349\n",
            "  epoch: 9/10,    batch: 13690/15469    Encoder_loss: 0.2902560830116272\n",
            "  epoch: 9/10,    batch: 13691/15469    Encoder_loss: 0.29090559482574463\n",
            "  epoch: 9/10,    batch: 13692/15469    Encoder_loss: 0.2893452048301697\n",
            "  epoch: 9/10,    batch: 13693/15469    Encoder_loss: 0.2874610126018524\n",
            "  epoch: 9/10,    batch: 13694/15469    Encoder_loss: 0.286629855632782\n",
            "  epoch: 9/10,    batch: 13695/15469    Encoder_loss: 0.28728121519088745\n",
            "  epoch: 9/10,    batch: 13696/15469    Encoder_loss: 0.2892352044582367\n",
            "  epoch: 9/10,    batch: 13697/15469    Encoder_loss: 0.28763478994369507\n",
            "  epoch: 9/10,    batch: 13698/15469    Encoder_loss: 0.2868273854255676\n",
            "  epoch: 9/10,    batch: 13699/15469    Encoder_loss: 0.28727564215660095\n",
            "  epoch: 9/10,    batch: 13700/15469    Encoder_loss: 0.2865084111690521\n",
            "  epoch: 9/10,    batch: 13701/15469    Encoder_loss: 0.28692546486854553\n",
            "  epoch: 9/10,    batch: 13702/15469    Encoder_loss: 0.2862012982368469\n",
            "  epoch: 9/10,    batch: 13703/15469    Encoder_loss: 0.28476324677467346\n",
            "  epoch: 9/10,    batch: 13704/15469    Encoder_loss: 0.2833794057369232\n",
            "  epoch: 9/10,    batch: 13705/15469    Encoder_loss: 0.2841259837150574\n",
            "  epoch: 9/10,    batch: 13706/15469    Encoder_loss: 0.2838855981826782\n",
            "  epoch: 9/10,    batch: 13707/15469    Encoder_loss: 0.28423380851745605\n",
            "  epoch: 9/10,    batch: 13708/15469    Encoder_loss: 0.2841108441352844\n",
            "  epoch: 9/10,    batch: 13709/15469    Encoder_loss: 0.28253883123397827\n",
            "  epoch: 9/10,    batch: 13710/15469    Encoder_loss: 0.2825607359409332\n",
            "  epoch: 9/10,    batch: 13711/15469    Encoder_loss: 0.28208380937576294\n",
            "  epoch: 9/10,    batch: 13712/15469    Encoder_loss: 0.2827959656715393\n",
            "  epoch: 9/10,    batch: 13713/15469    Encoder_loss: 0.2810837924480438\n",
            "  epoch: 9/10,    batch: 13714/15469    Encoder_loss: 0.2828029990196228\n",
            "  epoch: 9/10,    batch: 13715/15469    Encoder_loss: 0.2817133963108063\n",
            "  epoch: 9/10,    batch: 13716/15469    Encoder_loss: 0.28255215287208557\n",
            "  epoch: 9/10,    batch: 13717/15469    Encoder_loss: 0.28244587779045105\n",
            "  epoch: 9/10,    batch: 13718/15469    Encoder_loss: 0.2807299792766571\n",
            "  epoch: 9/10,    batch: 13719/15469    Encoder_loss: 0.28012150526046753\n",
            "  epoch: 9/10,    batch: 13720/15469    Encoder_loss: 0.28130537271499634\n",
            "  epoch: 9/10,    batch: 13721/15469    Encoder_loss: 0.2806727886199951\n",
            "  epoch: 9/10,    batch: 13722/15469    Encoder_loss: 0.2815565764904022\n",
            "  epoch: 9/10,    batch: 13723/15469    Encoder_loss: 0.2789711058139801\n",
            "  epoch: 9/10,    batch: 13724/15469    Encoder_loss: 0.27917423844337463\n",
            "  epoch: 9/10,    batch: 13725/15469    Encoder_loss: 0.2794855535030365\n",
            "  epoch: 9/10,    batch: 13726/15469    Encoder_loss: 0.27829208970069885\n",
            "  epoch: 9/10,    batch: 13727/15469    Encoder_loss: 0.2802005410194397\n",
            "  epoch: 9/10,    batch: 13728/15469    Encoder_loss: 0.27911102771759033\n",
            "  epoch: 9/10,    batch: 13729/15469    Encoder_loss: 0.4025490880012512\n",
            "  epoch: 9/10,    batch: 13730/15469    Encoder_loss: 0.6537681818008423\n",
            "  epoch: 9/10,    batch: 13731/15469    Encoder_loss: 0.7298396229743958\n",
            "  epoch: 9/10,    batch: 13732/15469    Encoder_loss: 0.5588548183441162\n",
            "  epoch: 9/10,    batch: 13733/15469    Encoder_loss: 0.4648623466491699\n",
            "  epoch: 9/10,    batch: 13734/15469    Encoder_loss: 0.4655885696411133\n",
            "  epoch: 9/10,    batch: 13735/15469    Encoder_loss: 0.46557316184043884\n",
            "  epoch: 9/10,    batch: 13736/15469    Encoder_loss: 0.3990134596824646\n",
            "  epoch: 9/10,    batch: 13737/15469    Encoder_loss: 0.3520103991031647\n",
            "  epoch: 9/10,    batch: 13738/15469    Encoder_loss: 0.3486478924751282\n",
            "  epoch: 9/10,    batch: 13739/15469    Encoder_loss: 0.28436219692230225\n",
            "  epoch: 9/10,    batch: 13740/15469    Encoder_loss: 0.30803409218788147\n",
            "  epoch: 9/10,    batch: 13741/15469    Encoder_loss: 0.3627431094646454\n",
            "  epoch: 9/10,    batch: 13742/15469    Encoder_loss: 0.3645276427268982\n",
            "  epoch: 9/10,    batch: 13743/15469    Encoder_loss: 0.4218035340309143\n",
            "  epoch: 9/10,    batch: 13744/15469    Encoder_loss: 0.4649001657962799\n",
            "  epoch: 9/10,    batch: 13745/15469    Encoder_loss: 0.47066530585289\n",
            "  epoch: 9/10,    batch: 13746/15469    Encoder_loss: 0.4797733724117279\n",
            "  epoch: 9/10,    batch: 13747/15469    Encoder_loss: 0.4858741760253906\n",
            "  epoch: 9/10,    batch: 13748/15469    Encoder_loss: 0.48688411712646484\n",
            "  epoch: 9/10,    batch: 13749/15469    Encoder_loss: 0.485309362411499\n",
            "  epoch: 9/10,    batch: 13750/15469    Encoder_loss: 0.4830179214477539\n",
            "  epoch: 9/10,    batch: 13751/15469    Encoder_loss: 0.4838978350162506\n",
            "  epoch: 9/10,    batch: 13752/15469    Encoder_loss: 0.4823470115661621\n",
            "  epoch: 9/10,    batch: 13753/15469    Encoder_loss: 0.4809626638889313\n",
            "  epoch: 9/10,    batch: 13754/15469    Encoder_loss: 0.4809471666812897\n",
            "  epoch: 9/10,    batch: 13755/15469    Encoder_loss: 0.4817441403865814\n",
            "  epoch: 9/10,    batch: 13756/15469    Encoder_loss: 0.482138067483902\n",
            "  epoch: 9/10,    batch: 13757/15469    Encoder_loss: 0.4822469651699066\n",
            "  epoch: 9/10,    batch: 13758/15469    Encoder_loss: 0.4841379225254059\n",
            "  epoch: 9/10,    batch: 13759/15469    Encoder_loss: 0.4849250912666321\n",
            "  epoch: 9/10,    batch: 13760/15469    Encoder_loss: 0.4869176745414734\n",
            "  epoch: 9/10,    batch: 13761/15469    Encoder_loss: 0.48595118522644043\n",
            "  epoch: 9/10,    batch: 13762/15469    Encoder_loss: 0.4868684411048889\n",
            "  epoch: 9/10,    batch: 13763/15469    Encoder_loss: 0.4902794361114502\n",
            "  epoch: 9/10,    batch: 13764/15469    Encoder_loss: 0.48839759826660156\n",
            "  epoch: 9/10,    batch: 13765/15469    Encoder_loss: 0.49006742238998413\n",
            "  epoch: 9/10,    batch: 13766/15469    Encoder_loss: 0.4903489947319031\n",
            "  epoch: 9/10,    batch: 13767/15469    Encoder_loss: 0.4926013946533203\n",
            "  epoch: 9/10,    batch: 13768/15469    Encoder_loss: 0.4911661744117737\n",
            "  epoch: 9/10,    batch: 13769/15469    Encoder_loss: 0.49244800209999084\n",
            "  epoch: 9/10,    batch: 13770/15469    Encoder_loss: 0.49415460228919983\n",
            "  epoch: 9/10,    batch: 13771/15469    Encoder_loss: 0.4956018328666687\n",
            "  epoch: 9/10,    batch: 13772/15469    Encoder_loss: 0.49581974744796753\n",
            "  epoch: 9/10,    batch: 13773/15469    Encoder_loss: 0.49635663628578186\n",
            "  epoch: 9/10,    batch: 13774/15469    Encoder_loss: 0.498710572719574\n",
            "  epoch: 9/10,    batch: 13775/15469    Encoder_loss: 0.4983660578727722\n",
            "  epoch: 9/10,    batch: 13776/15469    Encoder_loss: 0.49954643845558167\n",
            "  epoch: 9/10,    batch: 13777/15469    Encoder_loss: 0.5069735050201416\n",
            "  epoch: 9/10,    batch: 13778/15469    Encoder_loss: 0.5298267602920532\n",
            "  epoch: 9/10,    batch: 13779/15469    Encoder_loss: 0.5556212067604065\n",
            "  epoch: 9/10,    batch: 13780/15469    Encoder_loss: 0.5386937856674194\n",
            "  epoch: 9/10,    batch: 13781/15469    Encoder_loss: 0.5254998207092285\n",
            "  epoch: 9/10,    batch: 13782/15469    Encoder_loss: 0.5162073969841003\n",
            "  epoch: 9/10,    batch: 13783/15469    Encoder_loss: 0.451559454202652\n",
            "  epoch: 9/10,    batch: 13784/15469    Encoder_loss: 0.44937267899513245\n",
            "  epoch: 9/10,    batch: 13785/15469    Encoder_loss: 0.4495413899421692\n",
            "  epoch: 9/10,    batch: 13786/15469    Encoder_loss: 0.45023196935653687\n",
            "  epoch: 9/10,    batch: 13787/15469    Encoder_loss: 0.45070385932922363\n",
            "  epoch: 9/10,    batch: 13788/15469    Encoder_loss: 0.44985032081604004\n",
            "  epoch: 9/10,    batch: 13789/15469    Encoder_loss: 0.450700581073761\n",
            "  epoch: 9/10,    batch: 13790/15469    Encoder_loss: 0.45099586248397827\n",
            "  epoch: 9/10,    batch: 13791/15469    Encoder_loss: 0.449847936630249\n",
            "  epoch: 9/10,    batch: 13792/15469    Encoder_loss: 0.4522036612033844\n",
            "  epoch: 9/10,    batch: 13793/15469    Encoder_loss: 0.45131856203079224\n",
            "  epoch: 9/10,    batch: 13794/15469    Encoder_loss: 0.4504670202732086\n",
            "  epoch: 9/10,    batch: 13795/15469    Encoder_loss: 0.45124226808547974\n",
            "  epoch: 9/10,    batch: 13796/15469    Encoder_loss: 0.4500872492790222\n",
            "  epoch: 9/10,    batch: 13797/15469    Encoder_loss: 0.4498138427734375\n",
            "  epoch: 9/10,    batch: 13798/15469    Encoder_loss: 0.45158690214157104\n",
            "  epoch: 9/10,    batch: 13799/15469    Encoder_loss: 0.4498782753944397\n",
            "  epoch: 9/10,    batch: 13800/15469    Encoder_loss: 0.45148611068725586\n",
            "  epoch: 9/10,    batch: 13801/15469    Encoder_loss: 0.4514981210231781\n",
            "  epoch: 9/10,    batch: 13802/15469    Encoder_loss: 0.45212095975875854\n",
            "  epoch: 9/10,    batch: 13803/15469    Encoder_loss: 0.45303454995155334\n",
            "  epoch: 9/10,    batch: 13804/15469    Encoder_loss: 0.4521922469139099\n",
            "  epoch: 9/10,    batch: 13805/15469    Encoder_loss: 0.45217084884643555\n",
            "  epoch: 9/10,    batch: 13806/15469    Encoder_loss: 0.4517245292663574\n",
            "  epoch: 9/10,    batch: 13807/15469    Encoder_loss: 0.4515676498413086\n",
            "  epoch: 9/10,    batch: 13808/15469    Encoder_loss: 0.4530543386936188\n",
            "  epoch: 9/10,    batch: 13809/15469    Encoder_loss: 0.45389899611473083\n",
            "  epoch: 9/10,    batch: 13810/15469    Encoder_loss: 0.4556148648262024\n",
            "  epoch: 9/10,    batch: 13811/15469    Encoder_loss: 0.44456857442855835\n",
            "  epoch: 9/10,    batch: 13812/15469    Encoder_loss: 0.3603271543979645\n",
            "  epoch: 9/10,    batch: 13813/15469    Encoder_loss: 0.3512658476829529\n",
            "  epoch: 9/10,    batch: 13814/15469    Encoder_loss: 0.3486550748348236\n",
            "  epoch: 9/10,    batch: 13815/15469    Encoder_loss: 0.34804844856262207\n",
            "  epoch: 9/10,    batch: 13816/15469    Encoder_loss: 0.34701937437057495\n",
            "  epoch: 9/10,    batch: 13817/15469    Encoder_loss: 0.34687119722366333\n",
            "  epoch: 9/10,    batch: 13818/15469    Encoder_loss: 0.34560611844062805\n",
            "  epoch: 9/10,    batch: 13819/15469    Encoder_loss: 0.3421112596988678\n",
            "  epoch: 9/10,    batch: 13820/15469    Encoder_loss: 0.34198230504989624\n",
            "  epoch: 9/10,    batch: 13821/15469    Encoder_loss: 0.3435361385345459\n",
            "  epoch: 9/10,    batch: 13822/15469    Encoder_loss: 0.34156131744384766\n",
            "  epoch: 9/10,    batch: 13823/15469    Encoder_loss: 0.33924800157546997\n",
            "  epoch: 9/10,    batch: 13824/15469    Encoder_loss: 0.33942911028862\n",
            "  epoch: 9/10,    batch: 13825/15469    Encoder_loss: 0.3373151421546936\n",
            "  epoch: 9/10,    batch: 13826/15469    Encoder_loss: 0.33776623010635376\n",
            "  epoch: 9/10,    batch: 13827/15469    Encoder_loss: 0.33752256631851196\n",
            "  epoch: 9/10,    batch: 13828/15469    Encoder_loss: 0.3364909291267395\n",
            "  epoch: 9/10,    batch: 13829/15469    Encoder_loss: 0.3359656035900116\n",
            "  epoch: 9/10,    batch: 13830/15469    Encoder_loss: 0.3340591490268707\n",
            "  epoch: 9/10,    batch: 13831/15469    Encoder_loss: 0.3331949710845947\n",
            "  epoch: 9/10,    batch: 13832/15469    Encoder_loss: 0.33087316155433655\n",
            "  epoch: 9/10,    batch: 13833/15469    Encoder_loss: 0.33221349120140076\n",
            "  epoch: 9/10,    batch: 13834/15469    Encoder_loss: 0.33214256167411804\n",
            "  epoch: 9/10,    batch: 13835/15469    Encoder_loss: 0.3303256034851074\n",
            "  epoch: 9/10,    batch: 13836/15469    Encoder_loss: 0.32888057827949524\n",
            "  epoch: 9/10,    batch: 13837/15469    Encoder_loss: 0.32752615213394165\n",
            "  epoch: 9/10,    batch: 13838/15469    Encoder_loss: 0.32738763093948364\n",
            "  epoch: 9/10,    batch: 13839/15469    Encoder_loss: 0.32827216386795044\n",
            "  epoch: 9/10,    batch: 13840/15469    Encoder_loss: 0.3268011212348938\n",
            "  epoch: 9/10,    batch: 13841/15469    Encoder_loss: 0.3254553973674774\n",
            "  epoch: 9/10,    batch: 13842/15469    Encoder_loss: 0.3241991102695465\n",
            "  epoch: 9/10,    batch: 13843/15469    Encoder_loss: 0.32342010736465454\n",
            "  epoch: 9/10,    batch: 13844/15469    Encoder_loss: 0.3213731646537781\n",
            "  epoch: 9/10,    batch: 13845/15469    Encoder_loss: 0.32042157649993896\n",
            "  epoch: 9/10,    batch: 13846/15469    Encoder_loss: 0.3193743824958801\n",
            "  epoch: 9/10,    batch: 13847/15469    Encoder_loss: 0.3202943205833435\n",
            "  epoch: 9/10,    batch: 13848/15469    Encoder_loss: 0.31969961524009705\n",
            "  epoch: 9/10,    batch: 13849/15469    Encoder_loss: 0.39057105779647827\n",
            "  epoch: 9/10,    batch: 13850/15469    Encoder_loss: 0.4030691981315613\n",
            "  epoch: 9/10,    batch: 13851/15469    Encoder_loss: 0.400579035282135\n",
            "  epoch: 9/10,    batch: 13852/15469    Encoder_loss: 0.47040683031082153\n",
            "  epoch: 9/10,    batch: 13853/15469    Encoder_loss: 0.47041136026382446\n",
            "  epoch: 9/10,    batch: 13854/15469    Encoder_loss: 0.46548232436180115\n",
            "  epoch: 9/10,    batch: 13855/15469    Encoder_loss: 0.4675622880458832\n",
            "  epoch: 9/10,    batch: 13856/15469    Encoder_loss: 0.46727076172828674\n",
            "  epoch: 9/10,    batch: 13857/15469    Encoder_loss: 0.46616777777671814\n",
            "  epoch: 9/10,    batch: 13858/15469    Encoder_loss: 0.4668039381504059\n",
            "  epoch: 9/10,    batch: 13859/15469    Encoder_loss: 0.4674364924430847\n",
            "  epoch: 9/10,    batch: 13860/15469    Encoder_loss: 0.4684218466281891\n",
            "  epoch: 9/10,    batch: 13861/15469    Encoder_loss: 0.4699835181236267\n",
            "  epoch: 9/10,    batch: 13862/15469    Encoder_loss: 0.4690782129764557\n",
            "  epoch: 9/10,    batch: 13863/15469    Encoder_loss: 0.4717943072319031\n",
            "  epoch: 9/10,    batch: 13864/15469    Encoder_loss: 0.47168469429016113\n",
            "  epoch: 9/10,    batch: 13865/15469    Encoder_loss: 0.47327500581741333\n",
            "  epoch: 9/10,    batch: 13866/15469    Encoder_loss: 0.475335031747818\n",
            "  epoch: 9/10,    batch: 13867/15469    Encoder_loss: 0.4747588038444519\n",
            "  epoch: 9/10,    batch: 13868/15469    Encoder_loss: 0.47564348578453064\n",
            "  epoch: 9/10,    batch: 13869/15469    Encoder_loss: 0.4767382740974426\n",
            "  epoch: 9/10,    batch: 13870/15469    Encoder_loss: 0.47749319672584534\n",
            "  epoch: 9/10,    batch: 13871/15469    Encoder_loss: 0.47974729537963867\n",
            "  epoch: 9/10,    batch: 13872/15469    Encoder_loss: 0.48180803656578064\n",
            "  epoch: 9/10,    batch: 13873/15469    Encoder_loss: 0.4828895628452301\n",
            "  epoch: 9/10,    batch: 13874/15469    Encoder_loss: 0.4830479025840759\n",
            "  epoch: 9/10,    batch: 13875/15469    Encoder_loss: 0.4845612943172455\n",
            "  epoch: 9/10,    batch: 13876/15469    Encoder_loss: 0.4870816171169281\n",
            "  epoch: 9/10,    batch: 13877/15469    Encoder_loss: 0.48728376626968384\n",
            "  epoch: 9/10,    batch: 13878/15469    Encoder_loss: 0.4892660081386566\n",
            "  epoch: 9/10,    batch: 13879/15469    Encoder_loss: 0.4900660514831543\n",
            "  epoch: 9/10,    batch: 13880/15469    Encoder_loss: 0.49190181493759155\n",
            "  epoch: 9/10,    batch: 13881/15469    Encoder_loss: 0.49157387018203735\n",
            "  epoch: 9/10,    batch: 13882/15469    Encoder_loss: 0.4911988377571106\n",
            "  epoch: 9/10,    batch: 13883/15469    Encoder_loss: 0.4936220645904541\n",
            "  epoch: 9/10,    batch: 13884/15469    Encoder_loss: 0.4948294758796692\n",
            "  epoch: 9/10,    batch: 13885/15469    Encoder_loss: 0.4977748394012451\n",
            "  epoch: 9/10,    batch: 13886/15469    Encoder_loss: 0.51662278175354\n",
            "  epoch: 9/10,    batch: 13887/15469    Encoder_loss: 0.54332035779953\n",
            "  epoch: 9/10,    batch: 13888/15469    Encoder_loss: 0.5415266752243042\n",
            "  epoch: 9/10,    batch: 13889/15469    Encoder_loss: 0.523513913154602\n",
            "  epoch: 9/10,    batch: 13890/15469    Encoder_loss: 0.5228443741798401\n",
            "  epoch: 9/10,    batch: 13891/15469    Encoder_loss: 0.4564787447452545\n",
            "  epoch: 9/10,    batch: 13892/15469    Encoder_loss: 0.4496675431728363\n",
            "  epoch: 9/10,    batch: 13893/15469    Encoder_loss: 0.4634614586830139\n",
            "  epoch: 9/10,    batch: 13894/15469    Encoder_loss: 0.6307059526443481\n",
            "  epoch: 9/10,    batch: 13895/15469    Encoder_loss: 0.8656651377677917\n",
            "  epoch: 9/10,    batch: 13896/15469    Encoder_loss: 0.8880723118782043\n",
            "  epoch: 9/10,    batch: 13897/15469    Encoder_loss: 0.6916174292564392\n",
            "  epoch: 9/10,    batch: 13898/15469    Encoder_loss: 0.6374275088310242\n",
            "  epoch: 9/10,    batch: 13899/15469    Encoder_loss: 0.6363533139228821\n",
            "  epoch: 9/10,    batch: 13900/15469    Encoder_loss: 0.6330057978630066\n",
            "  epoch: 9/10,    batch: 13901/15469    Encoder_loss: 0.5581911206245422\n",
            "  epoch: 9/10,    batch: 13902/15469    Encoder_loss: 0.5274516344070435\n",
            "  epoch: 9/10,    batch: 13903/15469    Encoder_loss: 0.5102552175521851\n",
            "  epoch: 9/10,    batch: 13904/15469    Encoder_loss: 0.45426493883132935\n",
            "  epoch: 9/10,    batch: 13905/15469    Encoder_loss: 0.45276135206222534\n",
            "  epoch: 9/10,    batch: 13906/15469    Encoder_loss: 0.4524509906768799\n",
            "  epoch: 9/10,    batch: 13907/15469    Encoder_loss: 0.4536711573600769\n",
            "  epoch: 9/10,    batch: 13908/15469    Encoder_loss: 0.453652948141098\n",
            "  epoch: 9/10,    batch: 13909/15469    Encoder_loss: 0.4541633129119873\n",
            "  epoch: 9/10,    batch: 13910/15469    Encoder_loss: 0.4562987685203552\n",
            "  epoch: 9/10,    batch: 13911/15469    Encoder_loss: 0.4629780352115631\n",
            "  epoch: 9/10,    batch: 13912/15469    Encoder_loss: 0.4699643552303314\n",
            "  epoch: 9/10,    batch: 13913/15469    Encoder_loss: 0.47004127502441406\n",
            "  epoch: 9/10,    batch: 13914/15469    Encoder_loss: 0.46942153573036194\n",
            "  epoch: 9/10,    batch: 13915/15469    Encoder_loss: 0.4701057970523834\n",
            "  epoch: 9/10,    batch: 13916/15469    Encoder_loss: 0.46896877884864807\n",
            "  epoch: 9/10,    batch: 13917/15469    Encoder_loss: 0.46920663118362427\n",
            "  epoch: 9/10,    batch: 13918/15469    Encoder_loss: 0.47314268350601196\n",
            "  epoch: 9/10,    batch: 13919/15469    Encoder_loss: 0.4197842478752136\n",
            "  epoch: 9/10,    batch: 13920/15469    Encoder_loss: 0.36765438318252563\n",
            "  epoch: 9/10,    batch: 13921/15469    Encoder_loss: 0.3655913174152374\n",
            "  epoch: 9/10,    batch: 13922/15469    Encoder_loss: 0.36320173740386963\n",
            "  epoch: 9/10,    batch: 13923/15469    Encoder_loss: 0.3627869486808777\n",
            "  epoch: 9/10,    batch: 13924/15469    Encoder_loss: 0.3603079915046692\n",
            "  epoch: 9/10,    batch: 13925/15469    Encoder_loss: 0.36072009801864624\n",
            "  epoch: 9/10,    batch: 13926/15469    Encoder_loss: 0.35920262336730957\n",
            "  epoch: 9/10,    batch: 13927/15469    Encoder_loss: 0.35691338777542114\n",
            "  epoch: 9/10,    batch: 13928/15469    Encoder_loss: 0.35672515630722046\n",
            "  epoch: 9/10,    batch: 13929/15469    Encoder_loss: 0.3537563681602478\n",
            "  epoch: 9/10,    batch: 13930/15469    Encoder_loss: 0.35366106033325195\n",
            "  epoch: 9/10,    batch: 13931/15469    Encoder_loss: 0.3522343635559082\n",
            "  epoch: 9/10,    batch: 13932/15469    Encoder_loss: 0.35135984420776367\n",
            "  epoch: 9/10,    batch: 13933/15469    Encoder_loss: 0.3500466048717499\n",
            "  epoch: 9/10,    batch: 13934/15469    Encoder_loss: 0.3485074043273926\n",
            "  epoch: 9/10,    batch: 13935/15469    Encoder_loss: 0.34782466292381287\n",
            "  epoch: 9/10,    batch: 13936/15469    Encoder_loss: 0.3470149636268616\n",
            "  epoch: 9/10,    batch: 13937/15469    Encoder_loss: 0.3448391258716583\n",
            "  epoch: 9/10,    batch: 13938/15469    Encoder_loss: 0.3445260226726532\n",
            "  epoch: 9/10,    batch: 13939/15469    Encoder_loss: 0.341092050075531\n",
            "  epoch: 9/10,    batch: 13940/15469    Encoder_loss: 0.3407573103904724\n",
            "  epoch: 9/10,    batch: 13941/15469    Encoder_loss: 0.34029319882392883\n",
            "  epoch: 9/10,    batch: 13942/15469    Encoder_loss: 0.3381248116493225\n",
            "  epoch: 9/10,    batch: 13943/15469    Encoder_loss: 0.3396983742713928\n",
            "  epoch: 9/10,    batch: 13944/15469    Encoder_loss: 0.33646902441978455\n",
            "  epoch: 9/10,    batch: 13945/15469    Encoder_loss: 0.3352603316307068\n",
            "  epoch: 9/10,    batch: 13946/15469    Encoder_loss: 0.33478066325187683\n",
            "  epoch: 9/10,    batch: 13947/15469    Encoder_loss: 0.3323363661766052\n",
            "  epoch: 9/10,    batch: 13948/15469    Encoder_loss: 0.3324667811393738\n",
            "  epoch: 9/10,    batch: 13949/15469    Encoder_loss: 0.33081623911857605\n",
            "  epoch: 9/10,    batch: 13950/15469    Encoder_loss: 0.3297431170940399\n",
            "  epoch: 9/10,    batch: 13951/15469    Encoder_loss: 0.3289338946342468\n",
            "  epoch: 9/10,    batch: 13952/15469    Encoder_loss: 0.3285287618637085\n",
            "  epoch: 9/10,    batch: 13953/15469    Encoder_loss: 0.32632821798324585\n",
            "  epoch: 9/10,    batch: 13954/15469    Encoder_loss: 0.3251819610595703\n",
            "  epoch: 9/10,    batch: 13955/15469    Encoder_loss: 0.3247342109680176\n",
            "  epoch: 9/10,    batch: 13956/15469    Encoder_loss: 0.3245312571525574\n",
            "  epoch: 9/10,    batch: 13957/15469    Encoder_loss: 0.3232211172580719\n",
            "  epoch: 9/10,    batch: 13958/15469    Encoder_loss: 0.3238375186920166\n",
            "  epoch: 9/10,    batch: 13959/15469    Encoder_loss: 0.32052984833717346\n",
            "  epoch: 9/10,    batch: 13960/15469    Encoder_loss: 0.32850852608680725\n",
            "  epoch: 9/10,    batch: 13961/15469    Encoder_loss: 0.28514522314071655\n",
            "  epoch: 9/10,    batch: 13962/15469    Encoder_loss: 0.2847820222377777\n",
            "  epoch: 9/10,    batch: 13963/15469    Encoder_loss: 0.28399306535720825\n",
            "  epoch: 9/10,    batch: 13964/15469    Encoder_loss: 0.2841392457485199\n",
            "  epoch: 9/10,    batch: 13965/15469    Encoder_loss: 0.28367263078689575\n",
            "  epoch: 9/10,    batch: 13966/15469    Encoder_loss: 0.28410884737968445\n",
            "  epoch: 9/10,    batch: 13967/15469    Encoder_loss: 0.28466126322746277\n",
            "  epoch: 9/10,    batch: 13968/15469    Encoder_loss: 0.28433170914649963\n",
            "  epoch: 9/10,    batch: 13969/15469    Encoder_loss: 0.2831510901451111\n",
            "  epoch: 9/10,    batch: 13970/15469    Encoder_loss: 0.2825421094894409\n",
            "  epoch: 9/10,    batch: 13971/15469    Encoder_loss: 0.282580703496933\n",
            "  epoch: 9/10,    batch: 13972/15469    Encoder_loss: 0.2823706269264221\n",
            "  epoch: 9/10,    batch: 13973/15469    Encoder_loss: 0.28159481287002563\n",
            "  epoch: 9/10,    batch: 13974/15469    Encoder_loss: 0.28010091185569763\n",
            "  epoch: 9/10,    batch: 13975/15469    Encoder_loss: 0.2809596359729767\n",
            "  epoch: 9/10,    batch: 13976/15469    Encoder_loss: 0.2787013649940491\n",
            "  epoch: 9/10,    batch: 13977/15469    Encoder_loss: 0.2793698012828827\n",
            "  epoch: 9/10,    batch: 13978/15469    Encoder_loss: 0.27869221568107605\n",
            "  epoch: 9/10,    batch: 13979/15469    Encoder_loss: 0.27921566367149353\n",
            "  epoch: 9/10,    batch: 13980/15469    Encoder_loss: 0.27888259291648865\n",
            "  epoch: 9/10,    batch: 13981/15469    Encoder_loss: 0.2788962721824646\n",
            "  epoch: 9/10,    batch: 13982/15469    Encoder_loss: 0.2790926396846771\n",
            "  epoch: 9/10,    batch: 13983/15469    Encoder_loss: 0.2777614891529083\n",
            "  epoch: 9/10,    batch: 13984/15469    Encoder_loss: 0.27789413928985596\n",
            "  epoch: 9/10,    batch: 13985/15469    Encoder_loss: 0.2771909236907959\n",
            "  epoch: 9/10,    batch: 13986/15469    Encoder_loss: 0.27797865867614746\n",
            "  epoch: 9/10,    batch: 13987/15469    Encoder_loss: 0.27845197916030884\n",
            "  epoch: 9/10,    batch: 13988/15469    Encoder_loss: 0.27845919132232666\n",
            "  epoch: 9/10,    batch: 13989/15469    Encoder_loss: 0.2795988619327545\n",
            "  epoch: 9/10,    batch: 13990/15469    Encoder_loss: 0.27732422947883606\n",
            "  epoch: 9/10,    batch: 13991/15469    Encoder_loss: 0.27605873346328735\n",
            "  epoch: 9/10,    batch: 13992/15469    Encoder_loss: 0.2763817012310028\n",
            "  epoch: 9/10,    batch: 13993/15469    Encoder_loss: 0.2762383818626404\n",
            "  epoch: 9/10,    batch: 13994/15469    Encoder_loss: 0.276675283908844\n",
            "  epoch: 9/10,    batch: 13995/15469    Encoder_loss: 0.27630913257598877\n",
            "  epoch: 9/10,    batch: 13996/15469    Encoder_loss: 0.275791198015213\n",
            "  epoch: 9/10,    batch: 13997/15469    Encoder_loss: 0.27498000860214233\n",
            "  epoch: 9/10,    batch: 13998/15469    Encoder_loss: 0.27685800194740295\n",
            "  epoch: 9/10,    batch: 13999/15469    Encoder_loss: 0.27549660205841064\n",
            "  epoch: 9/10,    batch: 14000/15469    Encoder_loss: 0.2747751474380493\n",
            "  epoch: 9/10,    batch: 14001/15469    Encoder_loss: 0.27480456233024597\n",
            "  epoch: 9/10,    batch: 14002/15469    Encoder_loss: 0.287691205739975\n",
            "  epoch: 9/10,    batch: 14003/15469    Encoder_loss: 0.4454072117805481\n",
            "  epoch: 9/10,    batch: 14004/15469    Encoder_loss: 0.684100866317749\n",
            "  epoch: 9/10,    batch: 14005/15469    Encoder_loss: 0.7173372507095337\n",
            "  epoch: 9/10,    batch: 14006/15469    Encoder_loss: 0.5724269151687622\n",
            "  epoch: 9/10,    batch: 14007/15469    Encoder_loss: 0.5459527969360352\n",
            "  epoch: 9/10,    batch: 14008/15469    Encoder_loss: 0.5503677129745483\n",
            "  epoch: 9/10,    batch: 14009/15469    Encoder_loss: 0.6408917307853699\n",
            "  epoch: 9/10,    batch: 14010/15469    Encoder_loss: 0.5647525787353516\n",
            "  epoch: 9/10,    batch: 14011/15469    Encoder_loss: 0.5385354161262512\n",
            "  epoch: 9/10,    batch: 14012/15469    Encoder_loss: 0.5268553495407104\n",
            "  epoch: 9/10,    batch: 14013/15469    Encoder_loss: 0.4660140573978424\n",
            "  epoch: 9/10,    batch: 14014/15469    Encoder_loss: 0.46299299597740173\n",
            "  epoch: 9/10,    batch: 14015/15469    Encoder_loss: 0.4650375247001648\n",
            "  epoch: 9/10,    batch: 14016/15469    Encoder_loss: 0.4676888585090637\n",
            "  epoch: 9/10,    batch: 14017/15469    Encoder_loss: 0.4668348729610443\n",
            "  epoch: 9/10,    batch: 14018/15469    Encoder_loss: 0.46770596504211426\n",
            "  epoch: 9/10,    batch: 14019/15469    Encoder_loss: 0.47152161598205566\n",
            "  epoch: 9/10,    batch: 14020/15469    Encoder_loss: 0.47956085205078125\n",
            "  epoch: 9/10,    batch: 14021/15469    Encoder_loss: 0.4849154055118561\n",
            "  epoch: 9/10,    batch: 14022/15469    Encoder_loss: 0.48705077171325684\n",
            "  epoch: 9/10,    batch: 14023/15469    Encoder_loss: 0.48551422357559204\n",
            "  epoch: 9/10,    batch: 14024/15469    Encoder_loss: 0.4859853684902191\n",
            "  epoch: 9/10,    batch: 14025/15469    Encoder_loss: 0.4853573739528656\n",
            "  epoch: 9/10,    batch: 14026/15469    Encoder_loss: 0.48634910583496094\n",
            "  epoch: 9/10,    batch: 14027/15469    Encoder_loss: 0.4856855869293213\n",
            "  epoch: 9/10,    batch: 14028/15469    Encoder_loss: 0.48718756437301636\n",
            "  epoch: 9/10,    batch: 14029/15469    Encoder_loss: 0.4860862195491791\n",
            "  epoch: 9/10,    batch: 14030/15469    Encoder_loss: 0.4856920838356018\n",
            "  epoch: 9/10,    batch: 14031/15469    Encoder_loss: 0.4876326620578766\n",
            "  epoch: 9/10,    batch: 14032/15469    Encoder_loss: 0.48911386728286743\n",
            "  epoch: 9/10,    batch: 14033/15469    Encoder_loss: 0.489528089761734\n",
            "  epoch: 9/10,    batch: 14034/15469    Encoder_loss: 0.490439236164093\n",
            "  epoch: 9/10,    batch: 14035/15469    Encoder_loss: 0.49215736985206604\n",
            "  epoch: 9/10,    batch: 14036/15469    Encoder_loss: 0.492199569940567\n",
            "  epoch: 9/10,    batch: 14037/15469    Encoder_loss: 0.49293574690818787\n",
            "  epoch: 9/10,    batch: 14038/15469    Encoder_loss: 0.4947383999824524\n",
            "  epoch: 9/10,    batch: 14039/15469    Encoder_loss: 0.495155394077301\n",
            "  epoch: 9/10,    batch: 14040/15469    Encoder_loss: 0.4960649907588959\n",
            "  epoch: 9/10,    batch: 14041/15469    Encoder_loss: 0.4957122802734375\n",
            "  epoch: 9/10,    batch: 14042/15469    Encoder_loss: 0.4976440370082855\n",
            "  epoch: 9/10,    batch: 14043/15469    Encoder_loss: 0.5136074423789978\n",
            "  epoch: 9/10,    batch: 14044/15469    Encoder_loss: 0.5515983700752258\n",
            "  epoch: 9/10,    batch: 14045/15469    Encoder_loss: 0.5301569104194641\n",
            "  epoch: 9/10,    batch: 14046/15469    Encoder_loss: 0.5246118307113647\n",
            "  epoch: 9/10,    batch: 14047/15469    Encoder_loss: 0.5033012628555298\n",
            "  epoch: 9/10,    batch: 14048/15469    Encoder_loss: 0.4492575526237488\n",
            "  epoch: 9/10,    batch: 14049/15469    Encoder_loss: 0.44953399896621704\n",
            "  epoch: 9/10,    batch: 14050/15469    Encoder_loss: 0.4489969313144684\n",
            "  epoch: 9/10,    batch: 14051/15469    Encoder_loss: 0.44930943846702576\n",
            "  epoch: 9/10,    batch: 14052/15469    Encoder_loss: 0.4489474892616272\n",
            "  epoch: 9/10,    batch: 14053/15469    Encoder_loss: 0.4502187669277191\n",
            "  epoch: 9/10,    batch: 14054/15469    Encoder_loss: 0.44875913858413696\n",
            "  epoch: 9/10,    batch: 14055/15469    Encoder_loss: 0.44848328828811646\n",
            "  epoch: 9/10,    batch: 14056/15469    Encoder_loss: 0.44874173402786255\n",
            "  epoch: 9/10,    batch: 14057/15469    Encoder_loss: 0.4493682384490967\n",
            "  epoch: 9/10,    batch: 14058/15469    Encoder_loss: 0.450404554605484\n",
            "  epoch: 9/10,    batch: 14059/15469    Encoder_loss: 0.4503353536128998\n",
            "  epoch: 9/10,    batch: 14060/15469    Encoder_loss: 0.44863277673721313\n",
            "  epoch: 9/10,    batch: 14061/15469    Encoder_loss: 0.4489685893058777\n",
            "  epoch: 9/10,    batch: 14062/15469    Encoder_loss: 0.4500688910484314\n",
            "  epoch: 9/10,    batch: 14063/15469    Encoder_loss: 0.4500278830528259\n",
            "  epoch: 9/10,    batch: 14064/15469    Encoder_loss: 0.4484913945198059\n",
            "  epoch: 9/10,    batch: 14065/15469    Encoder_loss: 0.44935861229896545\n",
            "  epoch: 9/10,    batch: 14066/15469    Encoder_loss: 0.4506099820137024\n",
            "  epoch: 9/10,    batch: 14067/15469    Encoder_loss: 0.4523165225982666\n",
            "  epoch: 9/10,    batch: 14068/15469    Encoder_loss: 0.45319366455078125\n",
            "  epoch: 9/10,    batch: 14069/15469    Encoder_loss: 0.45166459679603577\n",
            "  epoch: 9/10,    batch: 14070/15469    Encoder_loss: 0.4519099295139313\n",
            "  epoch: 9/10,    batch: 14071/15469    Encoder_loss: 0.4515379071235657\n",
            "  epoch: 9/10,    batch: 14072/15469    Encoder_loss: 0.45129117369651794\n",
            "  epoch: 9/10,    batch: 14073/15469    Encoder_loss: 0.45226263999938965\n",
            "  epoch: 9/10,    batch: 14074/15469    Encoder_loss: 0.4534754157066345\n",
            "  epoch: 9/10,    batch: 14075/15469    Encoder_loss: 0.45643144845962524\n",
            "  epoch: 9/10,    batch: 14076/15469    Encoder_loss: 0.4077931046485901\n",
            "  epoch: 9/10,    batch: 14077/15469    Encoder_loss: 0.3498144745826721\n",
            "  epoch: 9/10,    batch: 14078/15469    Encoder_loss: 0.3495401442050934\n",
            "  epoch: 9/10,    batch: 14079/15469    Encoder_loss: 0.3491276204586029\n",
            "  epoch: 9/10,    batch: 14080/15469    Encoder_loss: 0.3462809920310974\n",
            "  epoch: 9/10,    batch: 14081/15469    Encoder_loss: 0.3469783663749695\n",
            "  epoch: 9/10,    batch: 14082/15469    Encoder_loss: 0.34540873765945435\n",
            "  epoch: 9/10,    batch: 14083/15469    Encoder_loss: 0.34295225143432617\n",
            "  epoch: 9/10,    batch: 14084/15469    Encoder_loss: 0.34129929542541504\n",
            "  epoch: 9/10,    batch: 14085/15469    Encoder_loss: 0.3418741524219513\n",
            "  epoch: 9/10,    batch: 14086/15469    Encoder_loss: 0.3414138853549957\n",
            "  epoch: 9/10,    batch: 14087/15469    Encoder_loss: 0.3390730023384094\n",
            "  epoch: 9/10,    batch: 14088/15469    Encoder_loss: 0.33900025486946106\n",
            "  epoch: 9/10,    batch: 14089/15469    Encoder_loss: 0.3389740288257599\n",
            "  epoch: 9/10,    batch: 14090/15469    Encoder_loss: 0.3374605178833008\n",
            "  epoch: 9/10,    batch: 14091/15469    Encoder_loss: 0.33652248978614807\n",
            "  epoch: 9/10,    batch: 14092/15469    Encoder_loss: 0.3355633020401001\n",
            "  epoch: 9/10,    batch: 14093/15469    Encoder_loss: 0.3339148163795471\n",
            "  epoch: 9/10,    batch: 14094/15469    Encoder_loss: 0.33304306864738464\n",
            "  epoch: 9/10,    batch: 14095/15469    Encoder_loss: 0.33161455392837524\n",
            "  epoch: 9/10,    batch: 14096/15469    Encoder_loss: 0.33104968070983887\n",
            "  epoch: 9/10,    batch: 14097/15469    Encoder_loss: 0.3302555978298187\n",
            "  epoch: 9/10,    batch: 14098/15469    Encoder_loss: 0.3294622600078583\n",
            "  epoch: 9/10,    batch: 14099/15469    Encoder_loss: 0.32993781566619873\n",
            "  epoch: 9/10,    batch: 14100/15469    Encoder_loss: 0.3288404643535614\n",
            "  epoch: 9/10,    batch: 14101/15469    Encoder_loss: 0.3277042508125305\n",
            "  epoch: 9/10,    batch: 14102/15469    Encoder_loss: 0.32673409581184387\n",
            "  epoch: 9/10,    batch: 14103/15469    Encoder_loss: 0.32511061429977417\n",
            "  epoch: 9/10,    batch: 14104/15469    Encoder_loss: 0.3257918953895569\n",
            "  epoch: 9/10,    batch: 14105/15469    Encoder_loss: 0.3229920268058777\n",
            "  epoch: 9/10,    batch: 14106/15469    Encoder_loss: 0.3229382634162903\n",
            "  epoch: 9/10,    batch: 14107/15469    Encoder_loss: 0.32223284244537354\n",
            "  epoch: 9/10,    batch: 14108/15469    Encoder_loss: 0.3203546106815338\n",
            "  epoch: 9/10,    batch: 14109/15469    Encoder_loss: 0.320784330368042\n",
            "  epoch: 9/10,    batch: 14110/15469    Encoder_loss: 0.31945323944091797\n",
            "  epoch: 9/10,    batch: 14111/15469    Encoder_loss: 0.31869152188301086\n",
            "  epoch: 9/10,    batch: 14112/15469    Encoder_loss: 0.3187792897224426\n",
            "  epoch: 9/10,    batch: 14113/15469    Encoder_loss: 0.31879371404647827\n",
            "  epoch: 9/10,    batch: 14114/15469    Encoder_loss: 0.3170589506626129\n",
            "  epoch: 9/10,    batch: 14115/15469    Encoder_loss: 0.3177673816680908\n",
            "  epoch: 9/10,    batch: 14116/15469    Encoder_loss: 0.3153684735298157\n",
            "  epoch: 9/10,    batch: 14117/15469    Encoder_loss: 0.3251027464866638\n",
            "  epoch: 9/10,    batch: 14118/15469    Encoder_loss: 0.28439146280288696\n",
            "  epoch: 9/10,    batch: 14119/15469    Encoder_loss: 0.28048795461654663\n",
            "  epoch: 9/10,    batch: 14120/15469    Encoder_loss: 0.3270184397697449\n",
            "  epoch: 9/10,    batch: 14121/15469    Encoder_loss: 0.36424320936203003\n",
            "  epoch: 9/10,    batch: 14122/15469    Encoder_loss: 0.36511653661727905\n",
            "  epoch: 9/10,    batch: 14123/15469    Encoder_loss: 0.39279890060424805\n",
            "  epoch: 9/10,    batch: 14124/15469    Encoder_loss: 0.4670860767364502\n",
            "  epoch: 9/10,    batch: 14125/15469    Encoder_loss: 0.4654630720615387\n",
            "  epoch: 9/10,    batch: 14126/15469    Encoder_loss: 0.4662092328071594\n",
            "  epoch: 9/10,    batch: 14127/15469    Encoder_loss: 0.46775752305984497\n",
            "  epoch: 9/10,    batch: 14128/15469    Encoder_loss: 0.4672662317752838\n",
            "  epoch: 9/10,    batch: 14129/15469    Encoder_loss: 0.4664066433906555\n",
            "  epoch: 9/10,    batch: 14130/15469    Encoder_loss: 0.46800312399864197\n",
            "  epoch: 9/10,    batch: 14131/15469    Encoder_loss: 0.46719902753829956\n",
            "  epoch: 9/10,    batch: 14132/15469    Encoder_loss: 0.47053927183151245\n",
            "  epoch: 9/10,    batch: 14133/15469    Encoder_loss: 0.4693925380706787\n",
            "  epoch: 9/10,    batch: 14134/15469    Encoder_loss: 0.4701769948005676\n",
            "  epoch: 9/10,    batch: 14135/15469    Encoder_loss: 0.47045058012008667\n",
            "  epoch: 9/10,    batch: 14136/15469    Encoder_loss: 0.47153836488723755\n",
            "  epoch: 9/10,    batch: 14137/15469    Encoder_loss: 0.473680317401886\n",
            "  epoch: 9/10,    batch: 14138/15469    Encoder_loss: 0.4737797677516937\n",
            "  epoch: 9/10,    batch: 14139/15469    Encoder_loss: 0.477022260427475\n",
            "  epoch: 9/10,    batch: 14140/15469    Encoder_loss: 0.4765772819519043\n",
            "  epoch: 9/10,    batch: 14141/15469    Encoder_loss: 0.4771338999271393\n",
            "  epoch: 9/10,    batch: 14142/15469    Encoder_loss: 0.4795430302619934\n",
            "  epoch: 9/10,    batch: 14143/15469    Encoder_loss: 0.4798088073730469\n",
            "  epoch: 9/10,    batch: 14144/15469    Encoder_loss: 0.4800504744052887\n",
            "  epoch: 9/10,    batch: 14145/15469    Encoder_loss: 0.48275747895240784\n",
            "  epoch: 9/10,    batch: 14146/15469    Encoder_loss: 0.48281216621398926\n",
            "  epoch: 9/10,    batch: 14147/15469    Encoder_loss: 0.4849565923213959\n",
            "  epoch: 9/10,    batch: 14148/15469    Encoder_loss: 0.48564785718917847\n",
            "  epoch: 9/10,    batch: 14149/15469    Encoder_loss: 0.48698529601097107\n",
            "  epoch: 9/10,    batch: 14150/15469    Encoder_loss: 0.48901620507240295\n",
            "  epoch: 9/10,    batch: 14151/15469    Encoder_loss: 0.4905167520046234\n",
            "  epoch: 9/10,    batch: 14152/15469    Encoder_loss: 0.4904402196407318\n",
            "  epoch: 9/10,    batch: 14153/15469    Encoder_loss: 0.4911468029022217\n",
            "  epoch: 9/10,    batch: 14154/15469    Encoder_loss: 0.4927505850791931\n",
            "  epoch: 9/10,    batch: 14155/15469    Encoder_loss: 0.49301958084106445\n",
            "  epoch: 9/10,    batch: 14156/15469    Encoder_loss: 0.4946235716342926\n",
            "  epoch: 9/10,    batch: 14157/15469    Encoder_loss: 0.5027871131896973\n",
            "  epoch: 9/10,    batch: 14158/15469    Encoder_loss: 0.5389410257339478\n",
            "  epoch: 9/10,    batch: 14159/15469    Encoder_loss: 0.5464743971824646\n",
            "  epoch: 9/10,    batch: 14160/15469    Encoder_loss: 0.6450697183609009\n",
            "  epoch: 9/10,    batch: 14161/15469    Encoder_loss: 0.8921224474906921\n",
            "  epoch: 9/10,    batch: 14162/15469    Encoder_loss: 0.910642147064209\n",
            "  epoch: 9/10,    batch: 14163/15469    Encoder_loss: 0.7461847066879272\n",
            "  epoch: 9/10,    batch: 14164/15469    Encoder_loss: 0.6347115635871887\n",
            "  epoch: 9/10,    batch: 14165/15469    Encoder_loss: 0.6363000273704529\n",
            "  epoch: 9/10,    batch: 14166/15469    Encoder_loss: 0.6346094012260437\n",
            "  epoch: 9/10,    batch: 14167/15469    Encoder_loss: 0.578133761882782\n",
            "  epoch: 9/10,    batch: 14168/15469    Encoder_loss: 0.5224424600601196\n",
            "  epoch: 9/10,    batch: 14169/15469    Encoder_loss: 0.5229305624961853\n",
            "  epoch: 9/10,    batch: 14170/15469    Encoder_loss: 0.4588848650455475\n",
            "  epoch: 9/10,    batch: 14171/15469    Encoder_loss: 0.45115453004837036\n",
            "  epoch: 9/10,    batch: 14172/15469    Encoder_loss: 0.4516298770904541\n",
            "  epoch: 9/10,    batch: 14173/15469    Encoder_loss: 0.45307669043540955\n",
            "  epoch: 9/10,    batch: 14174/15469    Encoder_loss: 0.45150333642959595\n",
            "  epoch: 9/10,    batch: 14175/15469    Encoder_loss: 0.44986692070961\n",
            "  epoch: 9/10,    batch: 14176/15469    Encoder_loss: 0.4547320306301117\n",
            "  epoch: 9/10,    batch: 14177/15469    Encoder_loss: 0.46219825744628906\n",
            "  epoch: 9/10,    batch: 14178/15469    Encoder_loss: 0.4676300585269928\n",
            "  epoch: 9/10,    batch: 14179/15469    Encoder_loss: 0.46923547983169556\n",
            "  epoch: 9/10,    batch: 14180/15469    Encoder_loss: 0.4681430757045746\n",
            "  epoch: 9/10,    batch: 14181/15469    Encoder_loss: 0.46748846769332886\n",
            "  epoch: 9/10,    batch: 14182/15469    Encoder_loss: 0.46703729033470154\n",
            "  epoch: 9/10,    batch: 14183/15469    Encoder_loss: 0.46794670820236206\n",
            "  epoch: 9/10,    batch: 14184/15469    Encoder_loss: 0.4674929678440094\n",
            "  epoch: 9/10,    batch: 14185/15469    Encoder_loss: 0.4664429724216461\n",
            "  epoch: 9/10,    batch: 14186/15469    Encoder_loss: 0.46673840284347534\n",
            "  epoch: 9/10,    batch: 14187/15469    Encoder_loss: 0.4652922749519348\n",
            "  epoch: 9/10,    batch: 14188/15469    Encoder_loss: 0.464674711227417\n",
            "  epoch: 9/10,    batch: 14189/15469    Encoder_loss: 0.4654242992401123\n",
            "  epoch: 9/10,    batch: 14190/15469    Encoder_loss: 0.4715883731842041\n",
            "  epoch: 9/10,    batch: 14191/15469    Encoder_loss: 0.39152464270591736\n",
            "  epoch: 9/10,    batch: 14192/15469    Encoder_loss: 0.3633560240268707\n",
            "  epoch: 9/10,    batch: 14193/15469    Encoder_loss: 0.36337924003601074\n",
            "  epoch: 9/10,    batch: 14194/15469    Encoder_loss: 0.36214637756347656\n",
            "  epoch: 9/10,    batch: 14195/15469    Encoder_loss: 0.36222514510154724\n",
            "  epoch: 9/10,    batch: 14196/15469    Encoder_loss: 0.3591790795326233\n",
            "  epoch: 9/10,    batch: 14197/15469    Encoder_loss: 0.3582666218280792\n",
            "  epoch: 9/10,    batch: 14198/15469    Encoder_loss: 0.35679155588150024\n",
            "  epoch: 9/10,    batch: 14199/15469    Encoder_loss: 0.3552733063697815\n",
            "  epoch: 9/10,    batch: 14200/15469    Encoder_loss: 0.35237714648246765\n",
            "  epoch: 9/10,    batch: 14201/15469    Encoder_loss: 0.3515986502170563\n",
            "  epoch: 9/10,    batch: 14202/15469    Encoder_loss: 0.3520124554634094\n",
            "  epoch: 9/10,    batch: 14203/15469    Encoder_loss: 0.3504507541656494\n",
            "  epoch: 9/10,    batch: 14204/15469    Encoder_loss: 0.349453330039978\n",
            "  epoch: 9/10,    batch: 14205/15469    Encoder_loss: 0.34701448678970337\n",
            "  epoch: 9/10,    batch: 14206/15469    Encoder_loss: 0.3462340235710144\n",
            "  epoch: 9/10,    batch: 14207/15469    Encoder_loss: 0.3436482548713684\n",
            "  epoch: 9/10,    batch: 14208/15469    Encoder_loss: 0.3431103229522705\n",
            "  epoch: 9/10,    batch: 14209/15469    Encoder_loss: 0.3425796627998352\n",
            "  epoch: 9/10,    batch: 14210/15469    Encoder_loss: 0.3410656750202179\n",
            "  epoch: 9/10,    batch: 14211/15469    Encoder_loss: 0.3410179615020752\n",
            "  epoch: 9/10,    batch: 14212/15469    Encoder_loss: 0.339134156703949\n",
            "  epoch: 9/10,    batch: 14213/15469    Encoder_loss: 0.33769911527633667\n",
            "  epoch: 9/10,    batch: 14214/15469    Encoder_loss: 0.3373947739601135\n",
            "  epoch: 9/10,    batch: 14215/15469    Encoder_loss: 0.33448871970176697\n",
            "  epoch: 9/10,    batch: 14216/15469    Encoder_loss: 0.33322471380233765\n",
            "  epoch: 9/10,    batch: 14217/15469    Encoder_loss: 0.3321519196033478\n",
            "  epoch: 9/10,    batch: 14218/15469    Encoder_loss: 0.3301493525505066\n",
            "  epoch: 9/10,    batch: 14219/15469    Encoder_loss: 0.3309893012046814\n",
            "  epoch: 9/10,    batch: 14220/15469    Encoder_loss: 0.3302099108695984\n",
            "  epoch: 9/10,    batch: 14221/15469    Encoder_loss: 0.3270857632160187\n",
            "  epoch: 9/10,    batch: 14222/15469    Encoder_loss: 0.3259194493293762\n",
            "  epoch: 9/10,    batch: 14223/15469    Encoder_loss: 0.32445892691612244\n",
            "  epoch: 9/10,    batch: 14224/15469    Encoder_loss: 0.32377853989601135\n",
            "  epoch: 9/10,    batch: 14225/15469    Encoder_loss: 0.3232516944408417\n",
            "  epoch: 9/10,    batch: 14226/15469    Encoder_loss: 0.32055050134658813\n",
            "  epoch: 9/10,    batch: 14227/15469    Encoder_loss: 0.32054492831230164\n",
            "  epoch: 9/10,    batch: 14228/15469    Encoder_loss: 0.3200073540210724\n",
            "  epoch: 9/10,    batch: 14229/15469    Encoder_loss: 0.3188914954662323\n",
            "  epoch: 9/10,    batch: 14230/15469    Encoder_loss: 0.3201073408126831\n",
            "  epoch: 9/10,    batch: 14231/15469    Encoder_loss: 0.316781610250473\n",
            "  epoch: 9/10,    batch: 14232/15469    Encoder_loss: 0.32582956552505493\n",
            "  epoch: 9/10,    batch: 14233/15469    Encoder_loss: 0.2822478413581848\n",
            "  epoch: 9/10,    batch: 14234/15469    Encoder_loss: 0.2810843288898468\n",
            "  epoch: 9/10,    batch: 14235/15469    Encoder_loss: 0.28145498037338257\n",
            "  epoch: 9/10,    batch: 14236/15469    Encoder_loss: 0.28092044591903687\n",
            "  epoch: 9/10,    batch: 14237/15469    Encoder_loss: 0.2812247574329376\n",
            "  epoch: 9/10,    batch: 14238/15469    Encoder_loss: 0.2805838882923126\n",
            "  epoch: 9/10,    batch: 14239/15469    Encoder_loss: 0.2811645567417145\n",
            "  epoch: 9/10,    batch: 14240/15469    Encoder_loss: 0.2806718945503235\n",
            "  epoch: 9/10,    batch: 14241/15469    Encoder_loss: 0.27985504269599915\n",
            "  epoch: 9/10,    batch: 14242/15469    Encoder_loss: 0.27917686104774475\n",
            "  epoch: 9/10,    batch: 14243/15469    Encoder_loss: 0.27880072593688965\n",
            "  epoch: 9/10,    batch: 14244/15469    Encoder_loss: 0.2789159119129181\n",
            "  epoch: 9/10,    batch: 14245/15469    Encoder_loss: 0.27939388155937195\n",
            "  epoch: 9/10,    batch: 14246/15469    Encoder_loss: 0.2796890139579773\n",
            "  epoch: 9/10,    batch: 14247/15469    Encoder_loss: 0.2774674594402313\n",
            "  epoch: 9/10,    batch: 14248/15469    Encoder_loss: 0.2770814597606659\n",
            "  epoch: 9/10,    batch: 14249/15469    Encoder_loss: 0.2771068513393402\n",
            "  epoch: 9/10,    batch: 14250/15469    Encoder_loss: 0.2773002088069916\n",
            "  epoch: 9/10,    batch: 14251/15469    Encoder_loss: 0.27560704946517944\n",
            "  epoch: 9/10,    batch: 14252/15469    Encoder_loss: 0.27580833435058594\n",
            "  epoch: 9/10,    batch: 14253/15469    Encoder_loss: 0.2766439616680145\n",
            "  epoch: 9/10,    batch: 14254/15469    Encoder_loss: 0.2760590612888336\n",
            "  epoch: 9/10,    batch: 14255/15469    Encoder_loss: 0.27644988894462585\n",
            "  epoch: 9/10,    batch: 14256/15469    Encoder_loss: 0.27613934874534607\n",
            "  epoch: 9/10,    batch: 14257/15469    Encoder_loss: 0.2762480676174164\n",
            "  epoch: 9/10,    batch: 14258/15469    Encoder_loss: 0.277259886264801\n",
            "  epoch: 9/10,    batch: 14259/15469    Encoder_loss: 0.2757091522216797\n",
            "  epoch: 9/10,    batch: 14260/15469    Encoder_loss: 0.27583447098731995\n",
            "  epoch: 9/10,    batch: 14261/15469    Encoder_loss: 0.2748503088951111\n",
            "  epoch: 9/10,    batch: 14262/15469    Encoder_loss: 0.2755621671676636\n",
            "  epoch: 9/10,    batch: 14263/15469    Encoder_loss: 0.2750372886657715\n",
            "  epoch: 9/10,    batch: 14264/15469    Encoder_loss: 0.274878591299057\n",
            "  epoch: 9/10,    batch: 14265/15469    Encoder_loss: 0.2760573625564575\n",
            "  epoch: 9/10,    batch: 14266/15469    Encoder_loss: 0.2750336229801178\n",
            "  epoch: 9/10,    batch: 14267/15469    Encoder_loss: 0.27341142296791077\n",
            "  epoch: 9/10,    batch: 14268/15469    Encoder_loss: 0.273695170879364\n",
            "  epoch: 9/10,    batch: 14269/15469    Encoder_loss: 0.27465465664863586\n",
            "  epoch: 9/10,    batch: 14270/15469    Encoder_loss: 0.2752326428890228\n",
            "  epoch: 9/10,    batch: 14271/15469    Encoder_loss: 0.2760556638240814\n",
            "  epoch: 9/10,    batch: 14272/15469    Encoder_loss: 0.3561263978481293\n",
            "  epoch: 9/10,    batch: 14273/15469    Encoder_loss: 0.5206130146980286\n",
            "  epoch: 9/10,    batch: 14274/15469    Encoder_loss: 0.7639744281768799\n",
            "  epoch: 9/10,    batch: 14275/15469    Encoder_loss: 0.8817680478096008\n",
            "  epoch: 9/10,    batch: 14276/15469    Encoder_loss: 0.6971685290336609\n",
            "  epoch: 9/10,    batch: 14277/15469    Encoder_loss: 0.6473954916000366\n",
            "  epoch: 9/10,    batch: 14278/15469    Encoder_loss: 0.6489277482032776\n",
            "  epoch: 9/10,    batch: 14279/15469    Encoder_loss: 0.6452440023422241\n",
            "  epoch: 9/10,    batch: 14280/15469    Encoder_loss: 0.56611168384552\n",
            "  epoch: 9/10,    batch: 14281/15469    Encoder_loss: 0.5401509404182434\n",
            "  epoch: 9/10,    batch: 14282/15469    Encoder_loss: 0.5257834196090698\n",
            "  epoch: 9/10,    batch: 14283/15469    Encoder_loss: 0.46768587827682495\n",
            "  epoch: 9/10,    batch: 14284/15469    Encoder_loss: 0.4676593840122223\n",
            "  epoch: 9/10,    batch: 14285/15469    Encoder_loss: 0.4676912724971771\n",
            "  epoch: 9/10,    batch: 14286/15469    Encoder_loss: 0.46945324540138245\n",
            "  epoch: 9/10,    batch: 14287/15469    Encoder_loss: 0.46867308020591736\n",
            "  epoch: 9/10,    batch: 14288/15469    Encoder_loss: 0.4712827205657959\n",
            "  epoch: 9/10,    batch: 14289/15469    Encoder_loss: 0.4770643711090088\n",
            "  epoch: 9/10,    batch: 14290/15469    Encoder_loss: 0.4870685636997223\n",
            "  epoch: 9/10,    batch: 14291/15469    Encoder_loss: 0.49269580841064453\n",
            "  epoch: 9/10,    batch: 14292/15469    Encoder_loss: 0.4933614134788513\n",
            "  epoch: 9/10,    batch: 14293/15469    Encoder_loss: 0.4920448362827301\n",
            "  epoch: 9/10,    batch: 14294/15469    Encoder_loss: 0.49122172594070435\n",
            "  epoch: 9/10,    batch: 14295/15469    Encoder_loss: 0.4906120300292969\n",
            "  epoch: 9/10,    batch: 14296/15469    Encoder_loss: 0.4904383718967438\n",
            "  epoch: 9/10,    batch: 14297/15469    Encoder_loss: 0.49202102422714233\n",
            "  epoch: 9/10,    batch: 14298/15469    Encoder_loss: 0.4927922189235687\n",
            "  epoch: 9/10,    batch: 14299/15469    Encoder_loss: 0.49355965852737427\n",
            "  epoch: 9/10,    batch: 14300/15469    Encoder_loss: 0.49157387018203735\n",
            "  epoch: 9/10,    batch: 14301/15469    Encoder_loss: 0.4924580454826355\n",
            "  epoch: 9/10,    batch: 14302/15469    Encoder_loss: 0.4948795735836029\n",
            "  epoch: 9/10,    batch: 14303/15469    Encoder_loss: 0.4938270151615143\n",
            "  epoch: 9/10,    batch: 14304/15469    Encoder_loss: 0.4940185546875\n",
            "  epoch: 9/10,    batch: 14305/15469    Encoder_loss: 0.4957481920719147\n",
            "  epoch: 9/10,    batch: 14306/15469    Encoder_loss: 0.4972686171531677\n",
            "  epoch: 9/10,    batch: 14307/15469    Encoder_loss: 0.4972161650657654\n",
            "  epoch: 9/10,    batch: 14308/15469    Encoder_loss: 0.5000796318054199\n",
            "  epoch: 9/10,    batch: 14309/15469    Encoder_loss: 0.5047435760498047\n",
            "  epoch: 9/10,    batch: 14310/15469    Encoder_loss: 0.530901312828064\n",
            "  epoch: 9/10,    batch: 14311/15469    Encoder_loss: 0.5625918507575989\n",
            "  epoch: 9/10,    batch: 14312/15469    Encoder_loss: 0.525123655796051\n",
            "  epoch: 9/10,    batch: 14313/15469    Encoder_loss: 0.5329837799072266\n",
            "  epoch: 9/10,    batch: 14314/15469    Encoder_loss: 0.4839251935482025\n",
            "  epoch: 9/10,    batch: 14315/15469    Encoder_loss: 0.44946956634521484\n",
            "  epoch: 9/10,    batch: 14316/15469    Encoder_loss: 0.44949522614479065\n",
            "  epoch: 9/10,    batch: 14317/15469    Encoder_loss: 0.45053449273109436\n",
            "  epoch: 9/10,    batch: 14318/15469    Encoder_loss: 0.4498325288295746\n",
            "  epoch: 9/10,    batch: 14319/15469    Encoder_loss: 0.45075803995132446\n",
            "  epoch: 9/10,    batch: 14320/15469    Encoder_loss: 0.4514578878879547\n",
            "  epoch: 9/10,    batch: 14321/15469    Encoder_loss: 0.4511544704437256\n",
            "  epoch: 9/10,    batch: 14322/15469    Encoder_loss: 0.4503768980503082\n",
            "  epoch: 9/10,    batch: 14323/15469    Encoder_loss: 0.4480174481868744\n",
            "  epoch: 9/10,    batch: 14324/15469    Encoder_loss: 0.44949665665626526\n",
            "  epoch: 9/10,    batch: 14325/15469    Encoder_loss: 0.4493328630924225\n",
            "  epoch: 9/10,    batch: 14326/15469    Encoder_loss: 0.44854506850242615\n",
            "  epoch: 9/10,    batch: 14327/15469    Encoder_loss: 0.44879111647605896\n",
            "  epoch: 9/10,    batch: 14328/15469    Encoder_loss: 0.45193028450012207\n",
            "  epoch: 9/10,    batch: 14329/15469    Encoder_loss: 0.44975146651268005\n",
            "  epoch: 9/10,    batch: 14330/15469    Encoder_loss: 0.4509584307670593\n",
            "  epoch: 9/10,    batch: 14331/15469    Encoder_loss: 0.450259268283844\n",
            "  epoch: 9/10,    batch: 14332/15469    Encoder_loss: 0.44967547059059143\n",
            "  epoch: 9/10,    batch: 14333/15469    Encoder_loss: 0.44966840744018555\n",
            "  epoch: 9/10,    batch: 14334/15469    Encoder_loss: 0.4497770667076111\n",
            "  epoch: 9/10,    batch: 14335/15469    Encoder_loss: 0.44991534948349\n",
            "  epoch: 9/10,    batch: 14336/15469    Encoder_loss: 0.44933146238327026\n",
            "  epoch: 9/10,    batch: 14337/15469    Encoder_loss: 0.4509882926940918\n",
            "  epoch: 9/10,    batch: 14338/15469    Encoder_loss: 0.4503532350063324\n",
            "  epoch: 9/10,    batch: 14339/15469    Encoder_loss: 0.4508458971977234\n",
            "  epoch: 9/10,    batch: 14340/15469    Encoder_loss: 0.4524613320827484\n",
            "  epoch: 9/10,    batch: 14341/15469    Encoder_loss: 0.4524175226688385\n",
            "  epoch: 9/10,    batch: 14342/15469    Encoder_loss: 0.45708274841308594\n",
            "  epoch: 9/10,    batch: 14343/15469    Encoder_loss: 0.3485282063484192\n",
            "  epoch: 9/10,    batch: 14344/15469    Encoder_loss: 0.27068188786506653\n",
            "  epoch: 9/10,    batch: 14345/15469    Encoder_loss: 0.26820704340934753\n",
            "  epoch: 9/10,    batch: 14346/15469    Encoder_loss: 0.26856881380081177\n",
            "  epoch: 9/10,    batch: 14347/15469    Encoder_loss: 0.2666473984718323\n",
            "  epoch: 9/10,    batch: 14348/15469    Encoder_loss: 0.26654744148254395\n",
            "  epoch: 9/10,    batch: 14349/15469    Encoder_loss: 0.2658253312110901\n",
            "  epoch: 9/10,    batch: 14350/15469    Encoder_loss: 0.264864981174469\n",
            "  epoch: 9/10,    batch: 14351/15469    Encoder_loss: 0.2642061710357666\n",
            "  epoch: 9/10,    batch: 14352/15469    Encoder_loss: 0.2634451985359192\n",
            "  epoch: 9/10,    batch: 14353/15469    Encoder_loss: 0.574634313583374\n",
            "  epoch: 9/10,    batch: 14354/15469    Encoder_loss: 1.1057225465774536\n",
            "  epoch: 9/10,    batch: 14355/15469    Encoder_loss: 0.6524864435195923\n",
            "  epoch: 9/10,    batch: 14356/15469    Encoder_loss: 0.6170798540115356\n",
            "  epoch: 9/10,    batch: 14357/15469    Encoder_loss: 0.4274984300136566\n",
            "  epoch: 9/10,    batch: 14358/15469    Encoder_loss: 0.30158621072769165\n",
            "  epoch: 9/10,    batch: 14359/15469    Encoder_loss: 0.2697947323322296\n",
            "  epoch: 9/10,    batch: 14360/15469    Encoder_loss: 0.2656747102737427\n",
            "  epoch: 9/10,    batch: 14361/15469    Encoder_loss: 0.27105897665023804\n",
            "  epoch: 9/10,    batch: 14362/15469    Encoder_loss: 0.2873600721359253\n",
            "  epoch: 9/10,    batch: 14363/15469    Encoder_loss: 0.2867368757724762\n",
            "  epoch: 9/10,    batch: 14364/15469    Encoder_loss: 0.2832476794719696\n",
            "  epoch: 9/10,    batch: 14365/15469    Encoder_loss: 0.2792099118232727\n",
            "  epoch: 9/10,    batch: 14366/15469    Encoder_loss: 0.2772229015827179\n",
            "  epoch: 9/10,    batch: 14367/15469    Encoder_loss: 0.2766024172306061\n",
            "  epoch: 9/10,    batch: 14368/15469    Encoder_loss: 0.2749783992767334\n",
            "  epoch: 9/10,    batch: 14369/15469    Encoder_loss: 0.27319034934043884\n",
            "  epoch: 9/10,    batch: 14370/15469    Encoder_loss: 0.2744554281234741\n",
            "  epoch: 9/10,    batch: 14371/15469    Encoder_loss: 0.2726871371269226\n",
            "  epoch: 9/10,    batch: 14372/15469    Encoder_loss: 0.27154541015625\n",
            "  epoch: 9/10,    batch: 14373/15469    Encoder_loss: 0.2710898518562317\n",
            "  epoch: 9/10,    batch: 14374/15469    Encoder_loss: 0.26924747228622437\n",
            "  epoch: 9/10,    batch: 14375/15469    Encoder_loss: 0.2677713930606842\n",
            "  epoch: 9/10,    batch: 14376/15469    Encoder_loss: 0.26838919520378113\n",
            "  epoch: 9/10,    batch: 14377/15469    Encoder_loss: 0.26645535230636597\n",
            "  epoch: 9/10,    batch: 14378/15469    Encoder_loss: 0.2640778422355652\n",
            "  epoch: 9/10,    batch: 14379/15469    Encoder_loss: 0.2631836533546448\n",
            "  epoch: 9/10,    batch: 14380/15469    Encoder_loss: 0.26073509454727173\n",
            "  epoch: 9/10,    batch: 14381/15469    Encoder_loss: 0.26090750098228455\n",
            "  epoch: 9/10,    batch: 14382/15469    Encoder_loss: 0.2589736580848694\n",
            "  epoch: 9/10,    batch: 14383/15469    Encoder_loss: 0.2585127353668213\n",
            "  epoch: 9/10,    batch: 14384/15469    Encoder_loss: 0.25713980197906494\n",
            "  epoch: 9/10,    batch: 14385/15469    Encoder_loss: 0.2550191283226013\n",
            "  epoch: 9/10,    batch: 14386/15469    Encoder_loss: 0.2558901906013489\n",
            "  epoch: 9/10,    batch: 14387/15469    Encoder_loss: 0.25327223539352417\n",
            "  epoch: 9/10,    batch: 14388/15469    Encoder_loss: 0.2540232539176941\n",
            "  epoch: 9/10,    batch: 14389/15469    Encoder_loss: 0.2521606385707855\n",
            "  epoch: 9/10,    batch: 14390/15469    Encoder_loss: 0.25274601578712463\n",
            "  epoch: 9/10,    batch: 14391/15469    Encoder_loss: 0.2518538534641266\n",
            "  epoch: 9/10,    batch: 14392/15469    Encoder_loss: 0.2520481050014496\n",
            "  epoch: 9/10,    batch: 14393/15469    Encoder_loss: 0.2515227496623993\n",
            "  epoch: 9/10,    batch: 14394/15469    Encoder_loss: 0.2517680525779724\n",
            "  epoch: 9/10,    batch: 14395/15469    Encoder_loss: 0.2490846812725067\n",
            "  epoch: 9/10,    batch: 14396/15469    Encoder_loss: 0.24983751773834229\n",
            "  epoch: 9/10,    batch: 14397/15469    Encoder_loss: 0.2504143714904785\n",
            "  epoch: 9/10,    batch: 14398/15469    Encoder_loss: 0.24845221638679504\n",
            "  epoch: 9/10,    batch: 14399/15469    Encoder_loss: 0.24892762303352356\n",
            "  epoch: 9/10,    batch: 14400/15469    Encoder_loss: 0.24966609477996826\n",
            "  epoch: 9/10,    batch: 14401/15469    Encoder_loss: 0.24866309762001038\n",
            "  epoch: 9/10,    batch: 14402/15469    Encoder_loss: 0.2486496865749359\n",
            "  epoch: 9/10,    batch: 14403/15469    Encoder_loss: 0.24783547222614288\n",
            "  epoch: 9/10,    batch: 14404/15469    Encoder_loss: 0.2497476041316986\n",
            "  epoch: 9/10,    batch: 14405/15469    Encoder_loss: 0.2498789131641388\n",
            "  epoch: 9/10,    batch: 14406/15469    Encoder_loss: 0.25028955936431885\n",
            "  epoch: 9/10,    batch: 14407/15469    Encoder_loss: 0.25074318051338196\n",
            "  epoch: 9/10,    batch: 14408/15469    Encoder_loss: 0.2502621114253998\n",
            "  epoch: 9/10,    batch: 14409/15469    Encoder_loss: 0.2506442368030548\n",
            "  epoch: 9/10,    batch: 14410/15469    Encoder_loss: 0.2515444755554199\n",
            "  epoch: 9/10,    batch: 14411/15469    Encoder_loss: 0.25238820910453796\n",
            "  epoch: 9/10,    batch: 14412/15469    Encoder_loss: 0.3110969662666321\n",
            "  epoch: 9/10,    batch: 14413/15469    Encoder_loss: 0.42039284110069275\n",
            "  epoch: 9/10,    batch: 14414/15469    Encoder_loss: 0.5527135133743286\n",
            "  epoch: 9/10,    batch: 14415/15469    Encoder_loss: 0.625501275062561\n",
            "  epoch: 9/10,    batch: 14416/15469    Encoder_loss: 0.6270760297775269\n",
            "  epoch: 9/10,    batch: 14417/15469    Encoder_loss: 0.6305783987045288\n",
            "  epoch: 9/10,    batch: 14418/15469    Encoder_loss: 0.6352270245552063\n",
            "  epoch: 9/10,    batch: 14419/15469    Encoder_loss: 0.6382910013198853\n",
            "  epoch: 9/10,    batch: 14420/15469    Encoder_loss: 0.6441423296928406\n",
            "  epoch: 9/10,    batch: 14421/15469    Encoder_loss: 0.6480271220207214\n",
            "  epoch: 9/10,    batch: 14422/15469    Encoder_loss: 0.6543323397636414\n",
            "  epoch: 9/10,    batch: 14423/15469    Encoder_loss: 0.6599118113517761\n",
            "  epoch: 9/10,    batch: 14424/15469    Encoder_loss: 0.6652603149414062\n",
            "  epoch: 9/10,    batch: 14425/15469    Encoder_loss: 0.6700245141983032\n",
            "  epoch: 9/10,    batch: 14426/15469    Encoder_loss: 0.6757753491401672\n",
            "  epoch: 9/10,    batch: 14427/15469    Encoder_loss: 0.6792615652084351\n",
            "  epoch: 9/10,    batch: 14428/15469    Encoder_loss: 0.6848044395446777\n",
            "  epoch: 9/10,    batch: 14429/15469    Encoder_loss: 0.69206702709198\n",
            "  epoch: 9/10,    batch: 14430/15469    Encoder_loss: 0.6965733170509338\n",
            "  epoch: 9/10,    batch: 14431/15469    Encoder_loss: 0.7461848258972168\n",
            "  epoch: 9/10,    batch: 14432/15469    Encoder_loss: 0.7842504382133484\n",
            "  epoch: 9/10,    batch: 14433/15469    Encoder_loss: 0.698749303817749\n",
            "  epoch: 9/10,    batch: 14434/15469    Encoder_loss: 0.6051656007766724\n",
            "  epoch: 9/10,    batch: 14435/15469    Encoder_loss: 0.6068063974380493\n",
            "  epoch: 9/10,    batch: 14436/15469    Encoder_loss: 0.6065455675125122\n",
            "  epoch: 9/10,    batch: 14437/15469    Encoder_loss: 0.6086657643318176\n",
            "  epoch: 9/10,    batch: 14438/15469    Encoder_loss: 0.6093829870223999\n",
            "  epoch: 9/10,    batch: 14439/15469    Encoder_loss: 0.6110461354255676\n",
            "  epoch: 9/10,    batch: 14440/15469    Encoder_loss: 0.612072229385376\n",
            "  epoch: 9/10,    batch: 14441/15469    Encoder_loss: 0.6137711405754089\n",
            "  epoch: 9/10,    batch: 14442/15469    Encoder_loss: 0.6149656176567078\n",
            "  epoch: 9/10,    batch: 14443/15469    Encoder_loss: 0.6147066354751587\n",
            "  epoch: 9/10,    batch: 14444/15469    Encoder_loss: 0.6188417673110962\n",
            "  epoch: 9/10,    batch: 14445/15469    Encoder_loss: 0.617124080657959\n",
            "  epoch: 9/10,    batch: 14446/15469    Encoder_loss: 0.6192091107368469\n",
            "  epoch: 9/10,    batch: 14447/15469    Encoder_loss: 0.6241804957389832\n",
            "  epoch: 9/10,    batch: 14448/15469    Encoder_loss: 0.4397490918636322\n",
            "  epoch: 9/10,    batch: 14449/15469    Encoder_loss: 0.41722315549850464\n",
            "  epoch: 9/10,    batch: 14450/15469    Encoder_loss: 0.411714643239975\n",
            "  epoch: 9/10,    batch: 14451/15469    Encoder_loss: 0.408053457736969\n",
            "  epoch: 9/10,    batch: 14452/15469    Encoder_loss: 0.40316399931907654\n",
            "  epoch: 9/10,    batch: 14453/15469    Encoder_loss: 0.40015944838523865\n",
            "  epoch: 9/10,    batch: 14454/15469    Encoder_loss: 0.3961782455444336\n",
            "  epoch: 9/10,    batch: 14455/15469    Encoder_loss: 0.3930356204509735\n",
            "  epoch: 9/10,    batch: 14456/15469    Encoder_loss: 0.3888482451438904\n",
            "  epoch: 9/10,    batch: 14457/15469    Encoder_loss: 0.38555824756622314\n",
            "  epoch: 9/10,    batch: 14458/15469    Encoder_loss: 0.3827599287033081\n",
            "  epoch: 9/10,    batch: 14459/15469    Encoder_loss: 0.37848371267318726\n",
            "  epoch: 9/10,    batch: 14460/15469    Encoder_loss: 0.3762473165988922\n",
            "  epoch: 9/10,    batch: 14461/15469    Encoder_loss: 0.3711893558502197\n",
            "  epoch: 9/10,    batch: 14462/15469    Encoder_loss: 0.36807912588119507\n",
            "  epoch: 9/10,    batch: 14463/15469    Encoder_loss: 0.36431020498275757\n",
            "  epoch: 9/10,    batch: 14464/15469    Encoder_loss: 0.3603854179382324\n",
            "  epoch: 9/10,    batch: 14465/15469    Encoder_loss: 0.3574097454547882\n",
            "  epoch: 9/10,    batch: 14466/15469    Encoder_loss: 0.3543868064880371\n",
            "  epoch: 9/10,    batch: 14467/15469    Encoder_loss: 0.35221558809280396\n",
            "  epoch: 9/10,    batch: 14468/15469    Encoder_loss: 0.355402410030365\n",
            "  epoch: 9/10,    batch: 14469/15469    Encoder_loss: 0.2821866571903229\n",
            "  epoch: 9/10,    batch: 14470/15469    Encoder_loss: 0.2806645631790161\n",
            "  epoch: 9/10,    batch: 14471/15469    Encoder_loss: 0.2774185538291931\n",
            "  epoch: 9/10,    batch: 14472/15469    Encoder_loss: 0.27607256174087524\n",
            "  epoch: 9/10,    batch: 14473/15469    Encoder_loss: 0.27509981393814087\n",
            "  epoch: 9/10,    batch: 14474/15469    Encoder_loss: 0.2735099494457245\n",
            "  epoch: 9/10,    batch: 14475/15469    Encoder_loss: 0.2742033004760742\n",
            "  epoch: 9/10,    batch: 14476/15469    Encoder_loss: 0.2730025053024292\n",
            "  epoch: 9/10,    batch: 14477/15469    Encoder_loss: 0.27227795124053955\n",
            "  epoch: 9/10,    batch: 14478/15469    Encoder_loss: 0.2704084515571594\n",
            "  epoch: 9/10,    batch: 14479/15469    Encoder_loss: 0.26776769757270813\n",
            "  epoch: 9/10,    batch: 14480/15469    Encoder_loss: 0.2675008177757263\n",
            "  epoch: 9/10,    batch: 14481/15469    Encoder_loss: 0.2656182646751404\n",
            "  epoch: 9/10,    batch: 14482/15469    Encoder_loss: 0.26597949862480164\n",
            "  epoch: 9/10,    batch: 14483/15469    Encoder_loss: 0.26480188965797424\n",
            "  epoch: 9/10,    batch: 14484/15469    Encoder_loss: 0.2646149694919586\n",
            "  epoch: 9/10,    batch: 14485/15469    Encoder_loss: 0.2627705931663513\n",
            "  epoch: 9/10,    batch: 14486/15469    Encoder_loss: 0.2614809572696686\n",
            "  epoch: 9/10,    batch: 14487/15469    Encoder_loss: 0.2618134915828705\n",
            "  epoch: 9/10,    batch: 14488/15469    Encoder_loss: 0.5533608198165894\n",
            "  epoch: 9/10,    batch: 14489/15469    Encoder_loss: 1.088655710220337\n",
            "  epoch: 9/10,    batch: 14490/15469    Encoder_loss: 0.6455434560775757\n",
            "  epoch: 9/10,    batch: 14491/15469    Encoder_loss: 0.6125974655151367\n",
            "  epoch: 9/10,    batch: 14492/15469    Encoder_loss: 0.42172297835350037\n",
            "  epoch: 9/10,    batch: 14493/15469    Encoder_loss: 0.29741019010543823\n",
            "  epoch: 9/10,    batch: 14494/15469    Encoder_loss: 0.26374784111976624\n",
            "  epoch: 9/10,    batch: 14495/15469    Encoder_loss: 0.2627117335796356\n",
            "  epoch: 9/10,    batch: 14496/15469    Encoder_loss: 0.26931196451187134\n",
            "  epoch: 9/10,    batch: 14497/15469    Encoder_loss: 0.28140193223953247\n",
            "  epoch: 9/10,    batch: 14498/15469    Encoder_loss: 0.27965596318244934\n",
            "  epoch: 9/10,    batch: 14499/15469    Encoder_loss: 0.2789457142353058\n",
            "  epoch: 9/10,    batch: 14500/15469    Encoder_loss: 0.2777825891971588\n",
            "  epoch: 9/10,    batch: 14501/15469    Encoder_loss: 0.2755115330219269\n",
            "  epoch: 9/10,    batch: 14502/15469    Encoder_loss: 0.2761058211326599\n",
            "  epoch: 9/10,    batch: 14503/15469    Encoder_loss: 0.2734474539756775\n",
            "  epoch: 9/10,    batch: 14504/15469    Encoder_loss: 0.2709228992462158\n",
            "  epoch: 9/10,    batch: 14505/15469    Encoder_loss: 0.2704513967037201\n",
            "  epoch: 9/10,    batch: 14506/15469    Encoder_loss: 0.2691837251186371\n",
            "  epoch: 9/10,    batch: 14507/15469    Encoder_loss: 0.26633593440055847\n",
            "  epoch: 9/10,    batch: 14508/15469    Encoder_loss: 0.2666878402233124\n",
            "  epoch: 9/10,    batch: 14509/15469    Encoder_loss: 0.26397666335105896\n",
            "  epoch: 9/10,    batch: 14510/15469    Encoder_loss: 0.26390039920806885\n",
            "  epoch: 9/10,    batch: 14511/15469    Encoder_loss: 0.2630314826965332\n",
            "  epoch: 9/10,    batch: 14512/15469    Encoder_loss: 0.2606520652770996\n",
            "  epoch: 9/10,    batch: 14513/15469    Encoder_loss: 0.25968602299690247\n",
            "  epoch: 9/10,    batch: 14514/15469    Encoder_loss: 0.25838202238082886\n",
            "  epoch: 9/10,    batch: 14515/15469    Encoder_loss: 0.2584858536720276\n",
            "  epoch: 9/10,    batch: 14516/15469    Encoder_loss: 0.25669246912002563\n",
            "  epoch: 9/10,    batch: 14517/15469    Encoder_loss: 0.25587520003318787\n",
            "  epoch: 9/10,    batch: 14518/15469    Encoder_loss: 0.25529706478118896\n",
            "  epoch: 9/10,    batch: 14519/15469    Encoder_loss: 0.2526399493217468\n",
            "  epoch: 9/10,    batch: 14520/15469    Encoder_loss: 0.25212836265563965\n",
            "  epoch: 9/10,    batch: 14521/15469    Encoder_loss: 0.2524491250514984\n",
            "  epoch: 9/10,    batch: 14522/15469    Encoder_loss: 0.24913915991783142\n",
            "  epoch: 9/10,    batch: 14523/15469    Encoder_loss: 0.24884402751922607\n",
            "  epoch: 9/10,    batch: 14524/15469    Encoder_loss: 0.24849581718444824\n",
            "  epoch: 9/10,    batch: 14525/15469    Encoder_loss: 0.24696084856987\n",
            "  epoch: 9/10,    batch: 14526/15469    Encoder_loss: 0.2468089610338211\n",
            "  epoch: 9/10,    batch: 14527/15469    Encoder_loss: 0.24510562419891357\n",
            "  epoch: 9/10,    batch: 14528/15469    Encoder_loss: 0.24444201588630676\n",
            "  epoch: 9/10,    batch: 14529/15469    Encoder_loss: 0.24394161999225616\n",
            "  epoch: 9/10,    batch: 14530/15469    Encoder_loss: 0.24369986355304718\n",
            "  epoch: 9/10,    batch: 14531/15469    Encoder_loss: 0.2421393096446991\n",
            "  epoch: 9/10,    batch: 14532/15469    Encoder_loss: 0.2421121895313263\n",
            "  epoch: 9/10,    batch: 14533/15469    Encoder_loss: 0.2428579330444336\n",
            "  epoch: 9/10,    batch: 14534/15469    Encoder_loss: 0.24229517579078674\n",
            "  epoch: 9/10,    batch: 14535/15469    Encoder_loss: 0.2431933879852295\n",
            "  epoch: 9/10,    batch: 14536/15469    Encoder_loss: 0.24406282603740692\n",
            "  epoch: 9/10,    batch: 14537/15469    Encoder_loss: 0.2418638914823532\n",
            "  epoch: 9/10,    batch: 14538/15469    Encoder_loss: 0.24339130520820618\n",
            "  epoch: 9/10,    batch: 14539/15469    Encoder_loss: 0.2441978007555008\n",
            "  epoch: 9/10,    batch: 14540/15469    Encoder_loss: 0.24505344033241272\n",
            "  epoch: 9/10,    batch: 14541/15469    Encoder_loss: 0.24465370178222656\n",
            "  epoch: 9/10,    batch: 14542/15469    Encoder_loss: 0.24345490336418152\n",
            "  epoch: 9/10,    batch: 14543/15469    Encoder_loss: 0.24461832642555237\n",
            "  epoch: 9/10,    batch: 14544/15469    Encoder_loss: 0.24612250924110413\n",
            "  epoch: 9/10,    batch: 14545/15469    Encoder_loss: 0.24519316852092743\n",
            "  epoch: 9/10,    batch: 14546/15469    Encoder_loss: 0.2464783638715744\n",
            "  epoch: 9/10,    batch: 14547/15469    Encoder_loss: 0.24538886547088623\n",
            "  epoch: 9/10,    batch: 14548/15469    Encoder_loss: 0.3664374351501465\n",
            "  epoch: 9/10,    batch: 14549/15469    Encoder_loss: 0.41567397117614746\n",
            "  epoch: 9/10,    batch: 14550/15469    Encoder_loss: 0.5917019844055176\n",
            "  epoch: 9/10,    batch: 14551/15469    Encoder_loss: 0.6216022372245789\n",
            "  epoch: 9/10,    batch: 14552/15469    Encoder_loss: 0.6232314109802246\n",
            "  epoch: 9/10,    batch: 14553/15469    Encoder_loss: 0.6267508268356323\n",
            "  epoch: 9/10,    batch: 14554/15469    Encoder_loss: 0.6314544677734375\n",
            "  epoch: 9/10,    batch: 14555/15469    Encoder_loss: 0.6351515650749207\n",
            "  epoch: 9/10,    batch: 14556/15469    Encoder_loss: 0.6405867338180542\n",
            "  epoch: 9/10,    batch: 14557/15469    Encoder_loss: 0.6460614204406738\n",
            "  epoch: 9/10,    batch: 14558/15469    Encoder_loss: 0.6500696539878845\n",
            "  epoch: 9/10,    batch: 14559/15469    Encoder_loss: 0.6565448641777039\n",
            "  epoch: 9/10,    batch: 14560/15469    Encoder_loss: 0.6620491743087769\n",
            "  epoch: 9/10,    batch: 14561/15469    Encoder_loss: 0.6691150665283203\n",
            "  epoch: 9/10,    batch: 14562/15469    Encoder_loss: 0.6717515587806702\n",
            "  epoch: 9/10,    batch: 14563/15469    Encoder_loss: 0.6779690384864807\n",
            "  epoch: 9/10,    batch: 14564/15469    Encoder_loss: 0.6827579736709595\n",
            "  epoch: 9/10,    batch: 14565/15469    Encoder_loss: 0.6893102526664734\n",
            "  epoch: 9/10,    batch: 14566/15469    Encoder_loss: 0.7006179094314575\n",
            "  epoch: 9/10,    batch: 14567/15469    Encoder_loss: 0.7898774743080139\n",
            "  epoch: 9/10,    batch: 14568/15469    Encoder_loss: 0.7526694536209106\n",
            "  epoch: 9/10,    batch: 14569/15469    Encoder_loss: 0.6184820532798767\n",
            "  epoch: 9/10,    batch: 14570/15469    Encoder_loss: 0.6008334159851074\n",
            "  epoch: 9/10,    batch: 14571/15469    Encoder_loss: 0.601521372795105\n",
            "  epoch: 9/10,    batch: 14572/15469    Encoder_loss: 0.6019518971443176\n",
            "  epoch: 9/10,    batch: 14573/15469    Encoder_loss: 0.6033116579055786\n",
            "  epoch: 9/10,    batch: 14574/15469    Encoder_loss: 0.603601336479187\n",
            "  epoch: 9/10,    batch: 14575/15469    Encoder_loss: 0.6052929162979126\n",
            "  epoch: 9/10,    batch: 14576/15469    Encoder_loss: 0.6082606315612793\n",
            "  epoch: 9/10,    batch: 14577/15469    Encoder_loss: 0.6101788282394409\n",
            "  epoch: 9/10,    batch: 14578/15469    Encoder_loss: 0.6099539995193481\n",
            "  epoch: 9/10,    batch: 14579/15469    Encoder_loss: 0.6113988757133484\n",
            "  epoch: 9/10,    batch: 14580/15469    Encoder_loss: 0.6133252382278442\n",
            "  epoch: 9/10,    batch: 14581/15469    Encoder_loss: 0.6156941056251526\n",
            "  epoch: 9/10,    batch: 14582/15469    Encoder_loss: 0.6173268556594849\n",
            "  epoch: 9/10,    batch: 14583/15469    Encoder_loss: 0.5276402235031128\n",
            "  epoch: 9/10,    batch: 14584/15469    Encoder_loss: 0.41124624013900757\n",
            "  epoch: 9/10,    batch: 14585/15469    Encoder_loss: 0.40797990560531616\n",
            "  epoch: 9/10,    batch: 14586/15469    Encoder_loss: 0.40295857191085815\n",
            "  epoch: 9/10,    batch: 14587/15469    Encoder_loss: 0.39793291687965393\n",
            "  epoch: 9/10,    batch: 14588/15469    Encoder_loss: 0.39384061098098755\n",
            "  epoch: 9/10,    batch: 14589/15469    Encoder_loss: 0.38968899846076965\n",
            "  epoch: 9/10,    batch: 14590/15469    Encoder_loss: 0.38671401143074036\n",
            "  epoch: 9/10,    batch: 14591/15469    Encoder_loss: 0.38426724076271057\n",
            "  epoch: 9/10,    batch: 14592/15469    Encoder_loss: 0.3803347051143646\n",
            "  epoch: 9/10,    batch: 14593/15469    Encoder_loss: 0.37569981813430786\n",
            "  epoch: 9/10,    batch: 14594/15469    Encoder_loss: 0.3727717399597168\n",
            "  epoch: 9/10,    batch: 14595/15469    Encoder_loss: 0.36776551604270935\n",
            "  epoch: 9/10,    batch: 14596/15469    Encoder_loss: 0.3665556311607361\n",
            "  epoch: 9/10,    batch: 14597/15469    Encoder_loss: 0.3614307641983032\n",
            "  epoch: 9/10,    batch: 14598/15469    Encoder_loss: 0.35906511545181274\n",
            "  epoch: 9/10,    batch: 14599/15469    Encoder_loss: 0.3554740250110626\n",
            "  epoch: 9/10,    batch: 14600/15469    Encoder_loss: 0.35150572657585144\n",
            "  epoch: 9/10,    batch: 14601/15469    Encoder_loss: 0.34887662529945374\n",
            "  epoch: 9/10,    batch: 14602/15469    Encoder_loss: 0.3474336266517639\n",
            "  epoch: 9/10,    batch: 14603/15469    Encoder_loss: 0.3511367440223694\n",
            "  epoch: 9/10,    batch: 14604/15469    Encoder_loss: 0.2878552973270416\n",
            "  epoch: 9/10,    batch: 14605/15469    Encoder_loss: 0.27181172370910645\n",
            "  epoch: 9/10,    batch: 14606/15469    Encoder_loss: 0.27190864086151123\n",
            "  epoch: 9/10,    batch: 14607/15469    Encoder_loss: 0.271226704120636\n",
            "  epoch: 9/10,    batch: 14608/15469    Encoder_loss: 0.2708629071712494\n",
            "  epoch: 9/10,    batch: 14609/15469    Encoder_loss: 0.26941993832588196\n",
            "  epoch: 9/10,    batch: 14610/15469    Encoder_loss: 0.26709187030792236\n",
            "  epoch: 9/10,    batch: 14611/15469    Encoder_loss: 0.2674891948699951\n",
            "  epoch: 9/10,    batch: 14612/15469    Encoder_loss: 0.26576054096221924\n",
            "  epoch: 9/10,    batch: 14613/15469    Encoder_loss: 0.26555198431015015\n",
            "  epoch: 9/10,    batch: 14614/15469    Encoder_loss: 0.26552921533584595\n",
            "  epoch: 9/10,    batch: 14615/15469    Encoder_loss: 0.26525646448135376\n",
            "  epoch: 9/10,    batch: 14616/15469    Encoder_loss: 0.26425793766975403\n",
            "  epoch: 9/10,    batch: 14617/15469    Encoder_loss: 0.2642294466495514\n",
            "  epoch: 9/10,    batch: 14618/15469    Encoder_loss: 0.2622397541999817\n",
            "  epoch: 9/10,    batch: 14619/15469    Encoder_loss: 0.26136690378189087\n",
            "  epoch: 9/10,    batch: 14620/15469    Encoder_loss: 0.26143014430999756\n",
            "  epoch: 9/10,    batch: 14621/15469    Encoder_loss: 0.26146000623703003\n",
            "  epoch: 9/10,    batch: 14622/15469    Encoder_loss: 0.26096290349960327\n",
            "  epoch: 9/10,    batch: 14623/15469    Encoder_loss: 0.2579968571662903\n",
            "  epoch: 9/10,    batch: 14624/15469    Encoder_loss: 0.2578863203525543\n",
            "  epoch: 9/10,    batch: 14625/15469    Encoder_loss: 0.6355961561203003\n",
            "  epoch: 9/10,    batch: 14626/15469    Encoder_loss: 1.0600814819335938\n",
            "  epoch: 9/10,    batch: 14627/15469    Encoder_loss: 0.6349759101867676\n",
            "  epoch: 9/10,    batch: 14628/15469    Encoder_loss: 0.596226692199707\n",
            "  epoch: 9/10,    batch: 14629/15469    Encoder_loss: 0.4133242964744568\n",
            "  epoch: 9/10,    batch: 14630/15469    Encoder_loss: 0.2835260033607483\n",
            "  epoch: 9/10,    batch: 14631/15469    Encoder_loss: 0.2613425850868225\n",
            "  epoch: 9/10,    batch: 14632/15469    Encoder_loss: 0.26123470067977905\n",
            "  epoch: 9/10,    batch: 14633/15469    Encoder_loss: 0.2674958407878876\n",
            "  epoch: 9/10,    batch: 14634/15469    Encoder_loss: 0.28743618726730347\n",
            "  epoch: 9/10,    batch: 14635/15469    Encoder_loss: 0.2788791060447693\n",
            "  epoch: 9/10,    batch: 14636/15469    Encoder_loss: 0.2728881239891052\n",
            "  epoch: 9/10,    batch: 14637/15469    Encoder_loss: 0.2704790234565735\n",
            "  epoch: 9/10,    batch: 14638/15469    Encoder_loss: 0.2696310877799988\n",
            "  epoch: 9/10,    batch: 14639/15469    Encoder_loss: 0.26766371726989746\n",
            "  epoch: 9/10,    batch: 14640/15469    Encoder_loss: 0.2674858272075653\n",
            "  epoch: 9/10,    batch: 14641/15469    Encoder_loss: 0.26728859543800354\n",
            "  epoch: 9/10,    batch: 14642/15469    Encoder_loss: 0.26325029134750366\n",
            "  epoch: 9/10,    batch: 14643/15469    Encoder_loss: 0.26249390840530396\n",
            "  epoch: 9/10,    batch: 14644/15469    Encoder_loss: 0.2625965476036072\n",
            "  epoch: 9/10,    batch: 14645/15469    Encoder_loss: 0.26017865538597107\n",
            "  epoch: 9/10,    batch: 14646/15469    Encoder_loss: 0.25656333565711975\n",
            "  epoch: 9/10,    batch: 14647/15469    Encoder_loss: 0.25943922996520996\n",
            "  epoch: 9/10,    batch: 14648/15469    Encoder_loss: 0.2571009397506714\n",
            "  epoch: 9/10,    batch: 14649/15469    Encoder_loss: 0.2552799582481384\n",
            "  epoch: 9/10,    batch: 14650/15469    Encoder_loss: 0.2545146048069\n",
            "  epoch: 9/10,    batch: 14651/15469    Encoder_loss: 0.2528052031993866\n",
            "  epoch: 9/10,    batch: 14652/15469    Encoder_loss: 0.2516471743583679\n",
            "  epoch: 9/10,    batch: 14653/15469    Encoder_loss: 0.25227171182632446\n",
            "  epoch: 9/10,    batch: 14654/15469    Encoder_loss: 0.2491898089647293\n",
            "  epoch: 9/10,    batch: 14655/15469    Encoder_loss: 0.24755620956420898\n",
            "  epoch: 9/10,    batch: 14656/15469    Encoder_loss: 0.24750594794750214\n",
            "  epoch: 9/10,    batch: 14657/15469    Encoder_loss: 0.24542102217674255\n",
            "  epoch: 9/10,    batch: 14658/15469    Encoder_loss: 0.24452416598796844\n",
            "  epoch: 9/10,    batch: 14659/15469    Encoder_loss: 0.24451549351215363\n",
            "  epoch: 9/10,    batch: 14660/15469    Encoder_loss: 0.24357476830482483\n",
            "  epoch: 9/10,    batch: 14661/15469    Encoder_loss: 0.2430216372013092\n",
            "  epoch: 9/10,    batch: 14662/15469    Encoder_loss: 0.2421640157699585\n",
            "  epoch: 9/10,    batch: 14663/15469    Encoder_loss: 0.24100390076637268\n",
            "  epoch: 9/10,    batch: 14664/15469    Encoder_loss: 0.24170240759849548\n",
            "  epoch: 9/10,    batch: 14665/15469    Encoder_loss: 0.24132347106933594\n",
            "  epoch: 9/10,    batch: 14666/15469    Encoder_loss: 0.241298109292984\n",
            "  epoch: 9/10,    batch: 14667/15469    Encoder_loss: 0.23975121974945068\n",
            "  epoch: 9/10,    batch: 14668/15469    Encoder_loss: 0.2395593672990799\n",
            "  epoch: 9/10,    batch: 14669/15469    Encoder_loss: 0.2384314239025116\n",
            "  epoch: 9/10,    batch: 14670/15469    Encoder_loss: 0.24006135761737823\n",
            "  epoch: 9/10,    batch: 14671/15469    Encoder_loss: 0.2397041767835617\n",
            "  epoch: 9/10,    batch: 14672/15469    Encoder_loss: 0.23978666961193085\n",
            "  epoch: 9/10,    batch: 14673/15469    Encoder_loss: 0.23944830894470215\n",
            "  epoch: 9/10,    batch: 14674/15469    Encoder_loss: 0.23941174149513245\n",
            "  epoch: 9/10,    batch: 14675/15469    Encoder_loss: 0.24035806953907013\n",
            "  epoch: 9/10,    batch: 14676/15469    Encoder_loss: 0.24129506945610046\n",
            "  epoch: 9/10,    batch: 14677/15469    Encoder_loss: 0.24153201282024384\n",
            "  epoch: 9/10,    batch: 14678/15469    Encoder_loss: 0.2411898672580719\n",
            "  epoch: 9/10,    batch: 14679/15469    Encoder_loss: 0.24105405807495117\n",
            "  epoch: 9/10,    batch: 14680/15469    Encoder_loss: 0.2427995651960373\n",
            "  epoch: 9/10,    batch: 14681/15469    Encoder_loss: 0.2411806583404541\n",
            "  epoch: 9/10,    batch: 14682/15469    Encoder_loss: 0.2435695230960846\n",
            "  epoch: 9/10,    batch: 14683/15469    Encoder_loss: 0.30854469537734985\n",
            "  epoch: 9/10,    batch: 14684/15469    Encoder_loss: 0.4297659993171692\n",
            "  epoch: 9/10,    batch: 14685/15469    Encoder_loss: 0.614115297794342\n",
            "  epoch: 9/10,    batch: 14686/15469    Encoder_loss: 0.6184110045433044\n",
            "  epoch: 9/10,    batch: 14687/15469    Encoder_loss: 0.620684027671814\n",
            "  epoch: 9/10,    batch: 14688/15469    Encoder_loss: 0.6220670938491821\n",
            "  epoch: 9/10,    batch: 14689/15469    Encoder_loss: 0.6257184743881226\n",
            "  epoch: 9/10,    batch: 14690/15469    Encoder_loss: 0.6296367645263672\n",
            "  epoch: 9/10,    batch: 14691/15469    Encoder_loss: 0.6334900856018066\n",
            "  epoch: 9/10,    batch: 14692/15469    Encoder_loss: 0.6395187377929688\n",
            "  epoch: 9/10,    batch: 14693/15469    Encoder_loss: 0.6466236114501953\n",
            "  epoch: 9/10,    batch: 14694/15469    Encoder_loss: 0.6503679156303406\n",
            "  epoch: 9/10,    batch: 14695/15469    Encoder_loss: 0.6557458639144897\n",
            "  epoch: 9/10,    batch: 14696/15469    Encoder_loss: 0.6613943576812744\n",
            "  epoch: 9/10,    batch: 14697/15469    Encoder_loss: 0.6671496033668518\n",
            "  epoch: 9/10,    batch: 14698/15469    Encoder_loss: 0.6743489503860474\n",
            "  epoch: 9/10,    batch: 14699/15469    Encoder_loss: 0.6774765849113464\n",
            "  epoch: 9/10,    batch: 14700/15469    Encoder_loss: 0.6801601648330688\n",
            "  epoch: 9/10,    batch: 14701/15469    Encoder_loss: 0.6944495439529419\n",
            "  epoch: 9/10,    batch: 14702/15469    Encoder_loss: 0.7576371431350708\n",
            "  epoch: 9/10,    batch: 14703/15469    Encoder_loss: 0.7807202935218811\n",
            "  epoch: 9/10,    batch: 14704/15469    Encoder_loss: 0.7045231461524963\n",
            "  epoch: 9/10,    batch: 14705/15469    Encoder_loss: 0.5947639346122742\n",
            "  epoch: 9/10,    batch: 14706/15469    Encoder_loss: 0.5962502360343933\n",
            "  epoch: 9/10,    batch: 14707/15469    Encoder_loss: 0.5980120897293091\n",
            "  epoch: 9/10,    batch: 14708/15469    Encoder_loss: 0.5998099446296692\n",
            "  epoch: 9/10,    batch: 14709/15469    Encoder_loss: 0.6002038717269897\n",
            "  epoch: 9/10,    batch: 14710/15469    Encoder_loss: 0.6038763523101807\n",
            "  epoch: 9/10,    batch: 14711/15469    Encoder_loss: 0.6009839773178101\n",
            "  epoch: 9/10,    batch: 14712/15469    Encoder_loss: 0.6032601594924927\n",
            "  epoch: 9/10,    batch: 14713/15469    Encoder_loss: 0.605582058429718\n",
            "  epoch: 9/10,    batch: 14714/15469    Encoder_loss: 0.6063910722732544\n",
            "  epoch: 9/10,    batch: 14715/15469    Encoder_loss: 0.6089977025985718\n",
            "  epoch: 9/10,    batch: 14716/15469    Encoder_loss: 0.6095737218856812\n",
            "  epoch: 9/10,    batch: 14717/15469    Encoder_loss: 0.6111138463020325\n",
            "  epoch: 9/10,    batch: 14718/15469    Encoder_loss: 0.6114568710327148\n",
            "  epoch: 9/10,    batch: 14719/15469    Encoder_loss: 0.4240531027317047\n",
            "  epoch: 9/10,    batch: 14720/15469    Encoder_loss: 0.40716448426246643\n",
            "  epoch: 9/10,    batch: 14721/15469    Encoder_loss: 0.40403470396995544\n",
            "  epoch: 9/10,    batch: 14722/15469    Encoder_loss: 0.3986881375312805\n",
            "  epoch: 9/10,    batch: 14723/15469    Encoder_loss: 0.3954770267009735\n",
            "  epoch: 9/10,    batch: 14724/15469    Encoder_loss: 0.3917829096317291\n",
            "  epoch: 9/10,    batch: 14725/15469    Encoder_loss: 0.38800865411758423\n",
            "  epoch: 9/10,    batch: 14726/15469    Encoder_loss: 0.38545745611190796\n",
            "  epoch: 9/10,    batch: 14727/15469    Encoder_loss: 0.381660521030426\n",
            "  epoch: 9/10,    batch: 14728/15469    Encoder_loss: 0.3773811459541321\n",
            "  epoch: 9/10,    batch: 14729/15469    Encoder_loss: 0.37372887134552\n",
            "  epoch: 9/10,    batch: 14730/15469    Encoder_loss: 0.37041348218917847\n",
            "  epoch: 9/10,    batch: 14731/15469    Encoder_loss: 0.36641237139701843\n",
            "  epoch: 9/10,    batch: 14732/15469    Encoder_loss: 0.36323994398117065\n",
            "  epoch: 9/10,    batch: 14733/15469    Encoder_loss: 0.36187344789505005\n",
            "  epoch: 9/10,    batch: 14734/15469    Encoder_loss: 0.354573130607605\n",
            "  epoch: 9/10,    batch: 14735/15469    Encoder_loss: 0.35157814621925354\n",
            "  epoch: 9/10,    batch: 14736/15469    Encoder_loss: 0.34795066714286804\n",
            "  epoch: 9/10,    batch: 14737/15469    Encoder_loss: 0.346640020608902\n",
            "  epoch: 9/10,    batch: 14738/15469    Encoder_loss: 0.3447464108467102\n",
            "  epoch: 9/10,    batch: 14739/15469    Encoder_loss: 0.32963401079177856\n",
            "  epoch: 9/10,    batch: 14740/15469    Encoder_loss: 0.27403873205184937\n",
            "  epoch: 9/10,    batch: 14741/15469    Encoder_loss: 0.2710041105747223\n",
            "  epoch: 9/10,    batch: 14742/15469    Encoder_loss: 0.27135875821113586\n",
            "  epoch: 9/10,    batch: 14743/15469    Encoder_loss: 0.27215927839279175\n",
            "  epoch: 9/10,    batch: 14744/15469    Encoder_loss: 0.26986637711524963\n",
            "  epoch: 9/10,    batch: 14745/15469    Encoder_loss: 0.26842182874679565\n",
            "  epoch: 9/10,    batch: 14746/15469    Encoder_loss: 0.2690357565879822\n",
            "  epoch: 9/10,    batch: 14747/15469    Encoder_loss: 0.26795563101768494\n",
            "  epoch: 9/10,    batch: 14748/15469    Encoder_loss: 0.26732444763183594\n",
            "  epoch: 9/10,    batch: 14749/15469    Encoder_loss: 0.264839231967926\n",
            "  epoch: 9/10,    batch: 14750/15469    Encoder_loss: 0.26669198274612427\n",
            "  epoch: 9/10,    batch: 14751/15469    Encoder_loss: 0.26576390862464905\n",
            "  epoch: 9/10,    batch: 14752/15469    Encoder_loss: 0.26505640149116516\n",
            "  epoch: 9/10,    batch: 14753/15469    Encoder_loss: 0.26333826780319214\n",
            "  epoch: 9/10,    batch: 14754/15469    Encoder_loss: 0.26375702023506165\n",
            "  epoch: 9/10,    batch: 14755/15469    Encoder_loss: 0.2625175714492798\n",
            "  epoch: 9/10,    batch: 14756/15469    Encoder_loss: 0.26146751642227173\n",
            "  epoch: 9/10,    batch: 14757/15469    Encoder_loss: 0.2613310217857361\n",
            "  epoch: 9/10,    batch: 14758/15469    Encoder_loss: 0.26039808988571167\n",
            "  epoch: 9/10,    batch: 14759/15469    Encoder_loss: 0.26073887944221497\n",
            "  epoch: 9/10,    batch: 14760/15469    Encoder_loss: 0.6425917148590088\n",
            "  epoch: 9/10,    batch: 14761/15469    Encoder_loss: 1.0621649026870728\n",
            "  epoch: 9/10,    batch: 14762/15469    Encoder_loss: 0.6381062269210815\n",
            "  epoch: 9/10,    batch: 14763/15469    Encoder_loss: 0.597149133682251\n",
            "  epoch: 9/10,    batch: 14764/15469    Encoder_loss: 0.4148714542388916\n",
            "  epoch: 9/10,    batch: 14765/15469    Encoder_loss: 0.28246384859085083\n",
            "  epoch: 9/10,    batch: 14766/15469    Encoder_loss: 0.26363450288772583\n",
            "  epoch: 9/10,    batch: 14767/15469    Encoder_loss: 0.2638254165649414\n",
            "  epoch: 9/10,    batch: 14768/15469    Encoder_loss: 0.26984909176826477\n",
            "  epoch: 9/10,    batch: 14769/15469    Encoder_loss: 0.28909337520599365\n",
            "  epoch: 9/10,    batch: 14770/15469    Encoder_loss: 0.28159448504447937\n",
            "  epoch: 9/10,    batch: 14771/15469    Encoder_loss: 0.2748202681541443\n",
            "  epoch: 9/10,    batch: 14772/15469    Encoder_loss: 0.2706649899482727\n",
            "  epoch: 9/10,    batch: 14773/15469    Encoder_loss: 0.268489807844162\n",
            "  epoch: 9/10,    batch: 14774/15469    Encoder_loss: 0.2670225501060486\n",
            "  epoch: 9/10,    batch: 14775/15469    Encoder_loss: 0.26366543769836426\n",
            "  epoch: 9/10,    batch: 14776/15469    Encoder_loss: 0.26298844814300537\n",
            "  epoch: 9/10,    batch: 14777/15469    Encoder_loss: 0.2621021568775177\n",
            "  epoch: 9/10,    batch: 14778/15469    Encoder_loss: 0.2614295184612274\n",
            "  epoch: 9/10,    batch: 14779/15469    Encoder_loss: 0.2593501806259155\n",
            "  epoch: 9/10,    batch: 14780/15469    Encoder_loss: 0.2578076124191284\n",
            "  epoch: 9/10,    batch: 14781/15469    Encoder_loss: 0.25679880380630493\n",
            "  epoch: 9/10,    batch: 14782/15469    Encoder_loss: 0.2560001313686371\n",
            "  epoch: 9/10,    batch: 14783/15469    Encoder_loss: 0.25499629974365234\n",
            "  epoch: 9/10,    batch: 14784/15469    Encoder_loss: 0.25476258993148804\n",
            "  epoch: 9/10,    batch: 14785/15469    Encoder_loss: 0.253033310174942\n",
            "  epoch: 9/10,    batch: 14786/15469    Encoder_loss: 0.2517893314361572\n",
            "  epoch: 9/10,    batch: 14787/15469    Encoder_loss: 0.25104424357414246\n",
            "  epoch: 9/10,    batch: 14788/15469    Encoder_loss: 0.2489977329969406\n",
            "  epoch: 9/10,    batch: 14789/15469    Encoder_loss: 0.2480408400297165\n",
            "  epoch: 9/10,    batch: 14790/15469    Encoder_loss: 0.2453971654176712\n",
            "  epoch: 9/10,    batch: 14791/15469    Encoder_loss: 0.246771901845932\n",
            "  epoch: 9/10,    batch: 14792/15469    Encoder_loss: 0.24475102126598358\n",
            "  epoch: 9/10,    batch: 14793/15469    Encoder_loss: 0.24353483319282532\n",
            "  epoch: 9/10,    batch: 14794/15469    Encoder_loss: 0.2412228286266327\n",
            "  epoch: 9/10,    batch: 14795/15469    Encoder_loss: 0.24200141429901123\n",
            "  epoch: 9/10,    batch: 14796/15469    Encoder_loss: 0.24202510714530945\n",
            "  epoch: 9/10,    batch: 14797/15469    Encoder_loss: 0.24148602783679962\n",
            "  epoch: 9/10,    batch: 14798/15469    Encoder_loss: 0.23946459591388702\n",
            "  epoch: 9/10,    batch: 14799/15469    Encoder_loss: 0.24016928672790527\n",
            "  epoch: 9/10,    batch: 14800/15469    Encoder_loss: 0.23970572650432587\n",
            "  epoch: 9/10,    batch: 14801/15469    Encoder_loss: 0.238015815615654\n",
            "  epoch: 9/10,    batch: 14802/15469    Encoder_loss: 0.23822149634361267\n",
            "  epoch: 9/10,    batch: 14803/15469    Encoder_loss: 0.2381516396999359\n",
            "  epoch: 9/10,    batch: 14804/15469    Encoder_loss: 0.23812592029571533\n",
            "  epoch: 9/10,    batch: 14805/15469    Encoder_loss: 0.23647069931030273\n",
            "  epoch: 9/10,    batch: 14806/15469    Encoder_loss: 0.23542344570159912\n",
            "  epoch: 9/10,    batch: 14807/15469    Encoder_loss: 0.2363051176071167\n",
            "  epoch: 9/10,    batch: 14808/15469    Encoder_loss: 0.23662729561328888\n",
            "  epoch: 9/10,    batch: 14809/15469    Encoder_loss: 0.23546965420246124\n",
            "  epoch: 9/10,    batch: 14810/15469    Encoder_loss: 0.23687033355236053\n",
            "  epoch: 9/10,    batch: 14811/15469    Encoder_loss: 0.2371605932712555\n",
            "  epoch: 9/10,    batch: 14812/15469    Encoder_loss: 0.23755836486816406\n",
            "  epoch: 9/10,    batch: 14813/15469    Encoder_loss: 0.2375405728816986\n",
            "  epoch: 9/10,    batch: 14814/15469    Encoder_loss: 0.23742926120758057\n",
            "  epoch: 9/10,    batch: 14815/15469    Encoder_loss: 0.237584188580513\n",
            "  epoch: 9/10,    batch: 14816/15469    Encoder_loss: 0.23772390186786652\n",
            "  epoch: 9/10,    batch: 14817/15469    Encoder_loss: 0.2387377917766571\n",
            "  epoch: 9/10,    batch: 14818/15469    Encoder_loss: 0.23881788551807404\n",
            "  epoch: 9/10,    batch: 14819/15469    Encoder_loss: 0.27702730894088745\n",
            "  epoch: 9/10,    batch: 14820/15469    Encoder_loss: 0.41707825660705566\n",
            "  epoch: 9/10,    batch: 14821/15469    Encoder_loss: 0.6037101149559021\n",
            "  epoch: 9/10,    batch: 14822/15469    Encoder_loss: 0.6150538921356201\n",
            "  epoch: 9/10,    batch: 14823/15469    Encoder_loss: 0.6151547431945801\n",
            "  epoch: 9/10,    batch: 14824/15469    Encoder_loss: 0.6186662912368774\n",
            "  epoch: 9/10,    batch: 14825/15469    Encoder_loss: 0.6225570440292358\n",
            "  epoch: 9/10,    batch: 14826/15469    Encoder_loss: 0.6265872716903687\n",
            "  epoch: 9/10,    batch: 14827/15469    Encoder_loss: 0.6311976909637451\n",
            "  epoch: 9/10,    batch: 14828/15469    Encoder_loss: 0.6365727782249451\n",
            "  epoch: 9/10,    batch: 14829/15469    Encoder_loss: 0.642428994178772\n",
            "  epoch: 9/10,    batch: 14830/15469    Encoder_loss: 0.6471955180168152\n",
            "  epoch: 9/10,    batch: 14831/15469    Encoder_loss: 0.6533719897270203\n",
            "  epoch: 9/10,    batch: 14832/15469    Encoder_loss: 0.6595355272293091\n",
            "  epoch: 9/10,    batch: 14833/15469    Encoder_loss: 0.6650243997573853\n",
            "  epoch: 9/10,    batch: 14834/15469    Encoder_loss: 0.6704176664352417\n",
            "  epoch: 9/10,    batch: 14835/15469    Encoder_loss: 0.6754442453384399\n",
            "  epoch: 9/10,    batch: 14836/15469    Encoder_loss: 0.6798564195632935\n",
            "  epoch: 9/10,    batch: 14837/15469    Encoder_loss: 0.6853048205375671\n",
            "  epoch: 9/10,    batch: 14838/15469    Encoder_loss: 0.7425486445426941\n",
            "  epoch: 9/10,    batch: 14839/15469    Encoder_loss: 0.7799548506736755\n",
            "  epoch: 9/10,    batch: 14840/15469    Encoder_loss: 0.7088714838027954\n",
            "  epoch: 9/10,    batch: 14841/15469    Encoder_loss: 0.5950270295143127\n",
            "  epoch: 9/10,    batch: 14842/15469    Encoder_loss: 0.5968053936958313\n",
            "  epoch: 9/10,    batch: 14843/15469    Encoder_loss: 0.597038745880127\n",
            "  epoch: 9/10,    batch: 14844/15469    Encoder_loss: 0.5987751483917236\n",
            "  epoch: 9/10,    batch: 14845/15469    Encoder_loss: 0.6005077958106995\n",
            "  epoch: 9/10,    batch: 14846/15469    Encoder_loss: 0.6014745831489563\n",
            "  epoch: 9/10,    batch: 14847/15469    Encoder_loss: 0.6026996374130249\n",
            "  epoch: 9/10,    batch: 14848/15469    Encoder_loss: 0.6043792366981506\n",
            "  epoch: 9/10,    batch: 14849/15469    Encoder_loss: 0.60570228099823\n",
            "  epoch: 9/10,    batch: 14850/15469    Encoder_loss: 0.6064997911453247\n",
            "  epoch: 9/10,    batch: 14851/15469    Encoder_loss: 0.6077199578285217\n",
            "  epoch: 9/10,    batch: 14852/15469    Encoder_loss: 0.6110836863517761\n",
            "  epoch: 9/10,    batch: 14853/15469    Encoder_loss: 0.609123170375824\n",
            "  epoch: 9/10,    batch: 14854/15469    Encoder_loss: 0.6161063313484192\n",
            "  epoch: 9/10,    batch: 14855/15469    Encoder_loss: 0.4460584223270416\n",
            "  epoch: 9/10,    batch: 14856/15469    Encoder_loss: 0.40423330664634705\n",
            "  epoch: 9/10,    batch: 14857/15469    Encoder_loss: 0.4002133011817932\n",
            "  epoch: 9/10,    batch: 14858/15469    Encoder_loss: 0.3976260721683502\n",
            "  epoch: 9/10,    batch: 14859/15469    Encoder_loss: 0.39368706941604614\n",
            "  epoch: 9/10,    batch: 14860/15469    Encoder_loss: 0.389996200799942\n",
            "  epoch: 9/10,    batch: 14861/15469    Encoder_loss: 0.3867954909801483\n",
            "  epoch: 9/10,    batch: 14862/15469    Encoder_loss: 0.38179048895835876\n",
            "  epoch: 9/10,    batch: 14863/15469    Encoder_loss: 0.37858185172080994\n",
            "  epoch: 9/10,    batch: 14864/15469    Encoder_loss: 0.3765929043292999\n",
            "  epoch: 9/10,    batch: 14865/15469    Encoder_loss: 0.3702566921710968\n",
            "  epoch: 9/10,    batch: 14866/15469    Encoder_loss: 0.36911073327064514\n",
            "  epoch: 9/10,    batch: 14867/15469    Encoder_loss: 0.36432182788848877\n",
            "  epoch: 9/10,    batch: 14868/15469    Encoder_loss: 0.36195144057273865\n",
            "  epoch: 9/10,    batch: 14869/15469    Encoder_loss: 0.35786303877830505\n",
            "  epoch: 9/10,    batch: 14870/15469    Encoder_loss: 0.35423654317855835\n",
            "  epoch: 9/10,    batch: 14871/15469    Encoder_loss: 0.3507631719112396\n",
            "  epoch: 9/10,    batch: 14872/15469    Encoder_loss: 0.34799036383628845\n",
            "  epoch: 9/10,    batch: 14873/15469    Encoder_loss: 0.34564894437789917\n",
            "  epoch: 9/10,    batch: 14874/15469    Encoder_loss: 0.34500351548194885\n",
            "  epoch: 9/10,    batch: 14875/15469    Encoder_loss: 0.34388256072998047\n",
            "  epoch: 9/10,    batch: 14876/15469    Encoder_loss: 0.27195820212364197\n",
            "  epoch: 9/10,    batch: 14877/15469    Encoder_loss: 0.27004116773605347\n",
            "  epoch: 9/10,    batch: 14878/15469    Encoder_loss: 0.26906654238700867\n",
            "  epoch: 9/10,    batch: 14879/15469    Encoder_loss: 0.2682848274707794\n",
            "  epoch: 9/10,    batch: 14880/15469    Encoder_loss: 0.2667950391769409\n",
            "  epoch: 9/10,    batch: 14881/15469    Encoder_loss: 0.2678107023239136\n",
            "  epoch: 9/10,    batch: 14882/15469    Encoder_loss: 0.2666507363319397\n",
            "  epoch: 9/10,    batch: 14883/15469    Encoder_loss: 0.2657236158847809\n",
            "  epoch: 9/10,    batch: 14884/15469    Encoder_loss: 0.26608067750930786\n",
            "  epoch: 9/10,    batch: 14885/15469    Encoder_loss: 0.26428014039993286\n",
            "  epoch: 9/10,    batch: 14886/15469    Encoder_loss: 0.26374536752700806\n",
            "  epoch: 9/10,    batch: 14887/15469    Encoder_loss: 0.2640785276889801\n",
            "  epoch: 9/10,    batch: 14888/15469    Encoder_loss: 0.2637760639190674\n",
            "  epoch: 9/10,    batch: 14889/15469    Encoder_loss: 0.2626797556877136\n",
            "  epoch: 9/10,    batch: 14890/15469    Encoder_loss: 0.26204848289489746\n",
            "  epoch: 9/10,    batch: 14891/15469    Encoder_loss: 0.26164692640304565\n",
            "  epoch: 9/10,    batch: 14892/15469    Encoder_loss: 0.2625153958797455\n",
            "  epoch: 9/10,    batch: 14893/15469    Encoder_loss: 0.26114994287490845\n",
            "  epoch: 9/10,    batch: 14894/15469    Encoder_loss: 0.26091426610946655\n",
            "  epoch: 9/10,    batch: 14895/15469    Encoder_loss: 0.26025182008743286\n",
            "  epoch: 9/10,    batch: 14896/15469    Encoder_loss: 0.26053157448768616\n",
            "  epoch: 9/10,    batch: 14897/15469    Encoder_loss: 0.728936493396759\n",
            "  epoch: 9/10,    batch: 14898/15469    Encoder_loss: 1.0000107288360596\n",
            "  epoch: 9/10,    batch: 14899/15469    Encoder_loss: 0.6305726766586304\n",
            "  epoch: 9/10,    batch: 14900/15469    Encoder_loss: 0.57567298412323\n",
            "  epoch: 9/10,    batch: 14901/15469    Encoder_loss: 0.40398797392845154\n",
            "  epoch: 9/10,    batch: 14902/15469    Encoder_loss: 0.26930591464042664\n",
            "  epoch: 9/10,    batch: 14903/15469    Encoder_loss: 0.26131850481033325\n",
            "  epoch: 9/10,    batch: 14904/15469    Encoder_loss: 0.25906801223754883\n",
            "  epoch: 9/10,    batch: 14905/15469    Encoder_loss: 0.2602345943450928\n",
            "  epoch: 9/10,    batch: 14906/15469    Encoder_loss: 0.2710070013999939\n",
            "  epoch: 9/10,    batch: 14907/15469    Encoder_loss: 0.26762309670448303\n",
            "  epoch: 9/10,    batch: 14908/15469    Encoder_loss: 0.26530468463897705\n",
            "  epoch: 9/10,    batch: 14909/15469    Encoder_loss: 0.2627129554748535\n",
            "  epoch: 9/10,    batch: 14910/15469    Encoder_loss: 0.2605033218860626\n",
            "  epoch: 9/10,    batch: 14911/15469    Encoder_loss: 0.25864726305007935\n",
            "  epoch: 9/10,    batch: 14912/15469    Encoder_loss: 0.2584156394004822\n",
            "  epoch: 9/10,    batch: 14913/15469    Encoder_loss: 0.2575353980064392\n",
            "  epoch: 9/10,    batch: 14914/15469    Encoder_loss: 0.2563017010688782\n",
            "  epoch: 9/10,    batch: 14915/15469    Encoder_loss: 0.2551822066307068\n",
            "  epoch: 9/10,    batch: 14916/15469    Encoder_loss: 0.25349196791648865\n",
            "  epoch: 9/10,    batch: 14917/15469    Encoder_loss: 0.2530997097492218\n",
            "  epoch: 9/10,    batch: 14918/15469    Encoder_loss: 0.25191015005111694\n",
            "  epoch: 9/10,    batch: 14919/15469    Encoder_loss: 0.2506450414657593\n",
            "  epoch: 9/10,    batch: 14920/15469    Encoder_loss: 0.24969887733459473\n",
            "  epoch: 9/10,    batch: 14921/15469    Encoder_loss: 0.24937160313129425\n",
            "  epoch: 9/10,    batch: 14922/15469    Encoder_loss: 0.2485814392566681\n",
            "  epoch: 9/10,    batch: 14923/15469    Encoder_loss: 0.24664406478405\n",
            "  epoch: 9/10,    batch: 14924/15469    Encoder_loss: 0.2461872547864914\n",
            "  epoch: 9/10,    batch: 14925/15469    Encoder_loss: 0.24370579421520233\n",
            "  epoch: 9/10,    batch: 14926/15469    Encoder_loss: 0.24322454631328583\n",
            "  epoch: 9/10,    batch: 14927/15469    Encoder_loss: 0.24194283783435822\n",
            "  epoch: 9/10,    batch: 14928/15469    Encoder_loss: 0.24191100895404816\n",
            "  epoch: 9/10,    batch: 14929/15469    Encoder_loss: 0.24096839129924774\n",
            "  epoch: 9/10,    batch: 14930/15469    Encoder_loss: 0.2402825653553009\n",
            "  epoch: 9/10,    batch: 14931/15469    Encoder_loss: 0.24090281128883362\n",
            "  epoch: 9/10,    batch: 14932/15469    Encoder_loss: 0.23994572460651398\n",
            "  epoch: 9/10,    batch: 14933/15469    Encoder_loss: 0.23867085576057434\n",
            "  epoch: 9/10,    batch: 14934/15469    Encoder_loss: 0.23840077221393585\n",
            "  epoch: 9/10,    batch: 14935/15469    Encoder_loss: 0.23743796348571777\n",
            "  epoch: 9/10,    batch: 14936/15469    Encoder_loss: 0.2374924123287201\n",
            "  epoch: 9/10,    batch: 14937/15469    Encoder_loss: 0.23683634400367737\n",
            "  epoch: 9/10,    batch: 14938/15469    Encoder_loss: 0.23615506291389465\n",
            "  epoch: 9/10,    batch: 14939/15469    Encoder_loss: 0.2364659309387207\n",
            "  epoch: 9/10,    batch: 14940/15469    Encoder_loss: 0.23733356595039368\n",
            "  epoch: 9/10,    batch: 14941/15469    Encoder_loss: 0.23662500083446503\n",
            "  epoch: 9/10,    batch: 14942/15469    Encoder_loss: 0.236135795712471\n",
            "  epoch: 9/10,    batch: 14943/15469    Encoder_loss: 0.23586933314800262\n",
            "  epoch: 9/10,    batch: 14944/15469    Encoder_loss: 0.23709836602210999\n",
            "  epoch: 9/10,    batch: 14945/15469    Encoder_loss: 0.23683468997478485\n",
            "  epoch: 9/10,    batch: 14946/15469    Encoder_loss: 0.2375175505876541\n",
            "  epoch: 9/10,    batch: 14947/15469    Encoder_loss: 0.2381315529346466\n",
            "  epoch: 9/10,    batch: 14948/15469    Encoder_loss: 0.23737196624279022\n",
            "  epoch: 9/10,    batch: 14949/15469    Encoder_loss: 0.23733189702033997\n",
            "  epoch: 9/10,    batch: 14950/15469    Encoder_loss: 0.23940038681030273\n",
            "  epoch: 9/10,    batch: 14951/15469    Encoder_loss: 0.23810860514640808\n",
            "  epoch: 9/10,    batch: 14952/15469    Encoder_loss: 0.23902305960655212\n",
            "  epoch: 9/10,    batch: 14953/15469    Encoder_loss: 0.2500271797180176\n",
            "  epoch: 9/10,    batch: 14954/15469    Encoder_loss: 0.4033277630805969\n",
            "  epoch: 9/10,    batch: 14955/15469    Encoder_loss: 0.41033315658569336\n",
            "  epoch: 9/10,    batch: 14956/15469    Encoder_loss: 0.5976969003677368\n",
            "  epoch: 9/10,    batch: 14957/15469    Encoder_loss: 0.6176811456680298\n",
            "  epoch: 9/10,    batch: 14958/15469    Encoder_loss: 0.6188289523124695\n",
            "  epoch: 9/10,    batch: 14959/15469    Encoder_loss: 0.6213040351867676\n",
            "  epoch: 9/10,    batch: 14960/15469    Encoder_loss: 0.6257165670394897\n",
            "  epoch: 9/10,    batch: 14961/15469    Encoder_loss: 0.6301388144493103\n",
            "  epoch: 9/10,    batch: 14962/15469    Encoder_loss: 0.6338287591934204\n",
            "  epoch: 9/10,    batch: 14963/15469    Encoder_loss: 0.639369785785675\n",
            "  epoch: 9/10,    batch: 14964/15469    Encoder_loss: 0.6464048624038696\n",
            "  epoch: 9/10,    batch: 14965/15469    Encoder_loss: 0.6520877480506897\n",
            "  epoch: 9/10,    batch: 14966/15469    Encoder_loss: 0.6566666960716248\n",
            "  epoch: 9/10,    batch: 14967/15469    Encoder_loss: 0.6621715426445007\n",
            "  epoch: 9/10,    batch: 14968/15469    Encoder_loss: 0.666951060295105\n",
            "  epoch: 9/10,    batch: 14969/15469    Encoder_loss: 0.672353208065033\n",
            "  epoch: 9/10,    batch: 14970/15469    Encoder_loss: 0.6784865260124207\n",
            "  epoch: 9/10,    batch: 14971/15469    Encoder_loss: 0.6834241151809692\n",
            "  epoch: 9/10,    batch: 14972/15469    Encoder_loss: 0.6957575082778931\n",
            "  epoch: 9/10,    batch: 14973/15469    Encoder_loss: 0.7824231386184692\n",
            "  epoch: 9/10,    batch: 14974/15469    Encoder_loss: 0.7403672933578491\n",
            "  epoch: 9/10,    batch: 14975/15469    Encoder_loss: 0.6033182144165039\n",
            "  epoch: 9/10,    batch: 14976/15469    Encoder_loss: 0.5939924716949463\n",
            "  epoch: 9/10,    batch: 14977/15469    Encoder_loss: 0.5960178971290588\n",
            "  epoch: 9/10,    batch: 14978/15469    Encoder_loss: 0.5978338122367859\n",
            "  epoch: 9/10,    batch: 14979/15469    Encoder_loss: 0.599227786064148\n",
            "  epoch: 9/10,    batch: 14980/15469    Encoder_loss: 0.5991048812866211\n",
            "  epoch: 9/10,    batch: 14981/15469    Encoder_loss: 0.5999678373336792\n",
            "  epoch: 9/10,    batch: 14982/15469    Encoder_loss: 0.6005231738090515\n",
            "  epoch: 9/10,    batch: 14983/15469    Encoder_loss: 0.6021896004676819\n",
            "  epoch: 9/10,    batch: 14984/15469    Encoder_loss: 0.6039251089096069\n",
            "  epoch: 9/10,    batch: 14985/15469    Encoder_loss: 0.6064034700393677\n",
            "  epoch: 9/10,    batch: 14986/15469    Encoder_loss: 0.6069298386573792\n",
            "  epoch: 9/10,    batch: 14987/15469    Encoder_loss: 0.6090379953384399\n",
            "  epoch: 9/10,    batch: 14988/15469    Encoder_loss: 0.612834095954895\n",
            "  epoch: 9/10,    batch: 14989/15469    Encoder_loss: 0.49762946367263794\n",
            "  epoch: 9/10,    batch: 14990/15469    Encoder_loss: 0.40447497367858887\n",
            "  epoch: 9/10,    batch: 14991/15469    Encoder_loss: 0.4020150303840637\n",
            "  epoch: 9/10,    batch: 14992/15469    Encoder_loss: 0.39745670557022095\n",
            "  epoch: 9/10,    batch: 14993/15469    Encoder_loss: 0.3945532441139221\n",
            "  epoch: 9/10,    batch: 14994/15469    Encoder_loss: 0.3913581967353821\n",
            "  epoch: 9/10,    batch: 14995/15469    Encoder_loss: 0.38810673356056213\n",
            "  epoch: 9/10,    batch: 14996/15469    Encoder_loss: 0.3829597532749176\n",
            "  epoch: 9/10,    batch: 14997/15469    Encoder_loss: 0.38088351488113403\n",
            "  epoch: 9/10,    batch: 14998/15469    Encoder_loss: 0.3775559663772583\n",
            "  epoch: 9/10,    batch: 14999/15469    Encoder_loss: 0.37513285875320435\n",
            "  epoch: 9/10,    batch: 15000/15469    Encoder_loss: 0.3706136643886566\n",
            "  epoch: 9/10,    batch: 15001/15469    Encoder_loss: 0.3682619035243988\n",
            "  epoch: 9/10,    batch: 15002/15469    Encoder_loss: 0.36459702253341675\n",
            "  epoch: 9/10,    batch: 15003/15469    Encoder_loss: 0.36123353242874146\n",
            "  epoch: 9/10,    batch: 15004/15469    Encoder_loss: 0.3586465120315552\n",
            "  epoch: 9/10,    batch: 15005/15469    Encoder_loss: 0.3560512363910675\n",
            "  epoch: 9/10,    batch: 15006/15469    Encoder_loss: 0.3524772822856903\n",
            "  epoch: 9/10,    batch: 15007/15469    Encoder_loss: 0.35015952587127686\n",
            "  epoch: 9/10,    batch: 15008/15469    Encoder_loss: 0.3471405506134033\n",
            "  epoch: 9/10,    batch: 15009/15469    Encoder_loss: 0.3496706485748291\n",
            "  epoch: 9/10,    batch: 15010/15469    Encoder_loss: 0.2920995056629181\n",
            "  epoch: 9/10,    batch: 15011/15469    Encoder_loss: 0.27477625012397766\n",
            "  epoch: 9/10,    batch: 15012/15469    Encoder_loss: 0.2740626633167267\n",
            "  epoch: 9/10,    batch: 15013/15469    Encoder_loss: 0.27351149916648865\n",
            "  epoch: 9/10,    batch: 15014/15469    Encoder_loss: 0.2736801505088806\n",
            "  epoch: 9/10,    batch: 15015/15469    Encoder_loss: 0.2721572816371918\n",
            "  epoch: 9/10,    batch: 15016/15469    Encoder_loss: 0.27157387137413025\n",
            "  epoch: 9/10,    batch: 15017/15469    Encoder_loss: 0.26927369832992554\n",
            "  epoch: 9/10,    batch: 15018/15469    Encoder_loss: 0.26919201016426086\n",
            "  epoch: 9/10,    batch: 15019/15469    Encoder_loss: 0.26956719160079956\n",
            "  epoch: 9/10,    batch: 15020/15469    Encoder_loss: 0.2688823938369751\n",
            "  epoch: 9/10,    batch: 15021/15469    Encoder_loss: 0.26916635036468506\n",
            "  epoch: 9/10,    batch: 15022/15469    Encoder_loss: 0.26685500144958496\n",
            "  epoch: 9/10,    batch: 15023/15469    Encoder_loss: 0.2686949372291565\n",
            "  epoch: 9/10,    batch: 15024/15469    Encoder_loss: 0.2658163011074066\n",
            "  epoch: 9/10,    batch: 15025/15469    Encoder_loss: 0.26470211148262024\n",
            "  epoch: 9/10,    batch: 15026/15469    Encoder_loss: 0.2645423114299774\n",
            "  epoch: 9/10,    batch: 15027/15469    Encoder_loss: 0.26256635785102844\n",
            "  epoch: 9/10,    batch: 15028/15469    Encoder_loss: 0.2634316086769104\n",
            "  epoch: 9/10,    batch: 15029/15469    Encoder_loss: 0.2633267641067505\n",
            "  epoch: 9/10,    batch: 15030/15469    Encoder_loss: 0.6177777051925659\n",
            "  epoch: 9/10,    batch: 15031/15469    Encoder_loss: 1.062096118927002\n",
            "  epoch: 9/10,    batch: 15032/15469    Encoder_loss: 0.6392576694488525\n",
            "  epoch: 9/10,    batch: 15033/15469    Encoder_loss: 0.6021674871444702\n",
            "  epoch: 9/10,    batch: 15034/15469    Encoder_loss: 0.4201432764530182\n",
            "  epoch: 9/10,    batch: 15035/15469    Encoder_loss: 0.2876986861228943\n",
            "  epoch: 9/10,    batch: 15036/15469    Encoder_loss: 0.26546087861061096\n",
            "  epoch: 9/10,    batch: 15037/15469    Encoder_loss: 0.2655673325061798\n",
            "  epoch: 9/10,    batch: 15038/15469    Encoder_loss: 0.262246698141098\n",
            "  epoch: 9/10,    batch: 15039/15469    Encoder_loss: 0.2710828185081482\n",
            "  epoch: 9/10,    batch: 15040/15469    Encoder_loss: 0.26976290345191956\n",
            "  epoch: 9/10,    batch: 15041/15469    Encoder_loss: 0.2701972424983978\n",
            "  epoch: 9/10,    batch: 15042/15469    Encoder_loss: 0.2654920518398285\n",
            "  epoch: 9/10,    batch: 15043/15469    Encoder_loss: 0.2623492181301117\n",
            "  epoch: 9/10,    batch: 15044/15469    Encoder_loss: 0.26105302572250366\n",
            "  epoch: 9/10,    batch: 15045/15469    Encoder_loss: 0.26001086831092834\n",
            "  epoch: 9/10,    batch: 15046/15469    Encoder_loss: 0.2585532069206238\n",
            "  epoch: 9/10,    batch: 15047/15469    Encoder_loss: 0.2581292986869812\n",
            "  epoch: 9/10,    batch: 15048/15469    Encoder_loss: 0.2567310035228729\n",
            "  epoch: 9/10,    batch: 15049/15469    Encoder_loss: 0.2565421164035797\n",
            "  epoch: 9/10,    batch: 15050/15469    Encoder_loss: 0.2546662390232086\n",
            "  epoch: 9/10,    batch: 15051/15469    Encoder_loss: 0.255246639251709\n",
            "  epoch: 9/10,    batch: 15052/15469    Encoder_loss: 0.2531607747077942\n",
            "  epoch: 9/10,    batch: 15053/15469    Encoder_loss: 0.2511526048183441\n",
            "  epoch: 9/10,    batch: 15054/15469    Encoder_loss: 0.2516171336174011\n",
            "  epoch: 9/10,    batch: 15055/15469    Encoder_loss: 0.24903079867362976\n",
            "  epoch: 9/10,    batch: 15056/15469    Encoder_loss: 0.24761657416820526\n",
            "  epoch: 9/10,    batch: 15057/15469    Encoder_loss: 0.24791833758354187\n",
            "  epoch: 9/10,    batch: 15058/15469    Encoder_loss: 0.2460426390171051\n",
            "  epoch: 9/10,    batch: 15059/15469    Encoder_loss: 0.2435045689344406\n",
            "  epoch: 9/10,    batch: 15060/15469    Encoder_loss: 0.24277839064598083\n",
            "  epoch: 9/10,    batch: 15061/15469    Encoder_loss: 0.24161605536937714\n",
            "  epoch: 9/10,    batch: 15062/15469    Encoder_loss: 0.24240127205848694\n",
            "  epoch: 9/10,    batch: 15063/15469    Encoder_loss: 0.24206408858299255\n",
            "  epoch: 9/10,    batch: 15064/15469    Encoder_loss: 0.2404920756816864\n",
            "  epoch: 9/10,    batch: 15065/15469    Encoder_loss: 0.24002453684806824\n",
            "  epoch: 9/10,    batch: 15066/15469    Encoder_loss: 0.2392815798521042\n",
            "  epoch: 9/10,    batch: 15067/15469    Encoder_loss: 0.23829737305641174\n",
            "  epoch: 9/10,    batch: 15068/15469    Encoder_loss: 0.2383473813533783\n",
            "  epoch: 9/10,    batch: 15069/15469    Encoder_loss: 0.23684731125831604\n",
            "  epoch: 9/10,    batch: 15070/15469    Encoder_loss: 0.23685285449028015\n",
            "  epoch: 9/10,    batch: 15071/15469    Encoder_loss: 0.23547203838825226\n",
            "  epoch: 9/10,    batch: 15072/15469    Encoder_loss: 0.23724237084388733\n",
            "  epoch: 9/10,    batch: 15073/15469    Encoder_loss: 0.23583519458770752\n",
            "  epoch: 9/10,    batch: 15074/15469    Encoder_loss: 0.23502680659294128\n",
            "  epoch: 9/10,    batch: 15075/15469    Encoder_loss: 0.2365899533033371\n",
            "  epoch: 9/10,    batch: 15076/15469    Encoder_loss: 0.23579701781272888\n",
            "  epoch: 9/10,    batch: 15077/15469    Encoder_loss: 0.23671214282512665\n",
            "  epoch: 9/10,    batch: 15078/15469    Encoder_loss: 0.23703768849372864\n",
            "  epoch: 9/10,    batch: 15079/15469    Encoder_loss: 0.23618996143341064\n",
            "  epoch: 9/10,    batch: 15080/15469    Encoder_loss: 0.23612597584724426\n",
            "  epoch: 9/10,    batch: 15081/15469    Encoder_loss: 0.23792722821235657\n",
            "  epoch: 9/10,    batch: 15082/15469    Encoder_loss: 0.23718877136707306\n",
            "  epoch: 9/10,    batch: 15083/15469    Encoder_loss: 0.23729881644248962\n",
            "  epoch: 9/10,    batch: 15084/15469    Encoder_loss: 0.2382030040025711\n",
            "  epoch: 9/10,    batch: 15085/15469    Encoder_loss: 0.2392020970582962\n",
            "  epoch: 9/10,    batch: 15086/15469    Encoder_loss: 0.23950040340423584\n",
            "  epoch: 9/10,    batch: 15087/15469    Encoder_loss: 0.2397468388080597\n",
            "  epoch: 9/10,    batch: 15088/15469    Encoder_loss: 0.2843683362007141\n",
            "  epoch: 9/10,    batch: 15089/15469    Encoder_loss: 0.4107690751552582\n",
            "  epoch: 9/10,    batch: 15090/15469    Encoder_loss: 0.5238078832626343\n",
            "  epoch: 9/10,    batch: 15091/15469    Encoder_loss: 0.6142627000808716\n",
            "  epoch: 9/10,    batch: 15092/15469    Encoder_loss: 0.6189141273498535\n",
            "  epoch: 9/10,    batch: 15093/15469    Encoder_loss: 0.6196616291999817\n",
            "  epoch: 9/10,    batch: 15094/15469    Encoder_loss: 0.6233655214309692\n",
            "  epoch: 9/10,    batch: 15095/15469    Encoder_loss: 0.6269901394844055\n",
            "  epoch: 9/10,    batch: 15096/15469    Encoder_loss: 0.6322051882743835\n",
            "  epoch: 9/10,    batch: 15097/15469    Encoder_loss: 0.6378375291824341\n",
            "  epoch: 9/10,    batch: 15098/15469    Encoder_loss: 0.6433283686637878\n",
            "  epoch: 9/10,    batch: 15099/15469    Encoder_loss: 0.6480579376220703\n",
            "  epoch: 9/10,    batch: 15100/15469    Encoder_loss: 0.6548478007316589\n",
            "  epoch: 9/10,    batch: 15101/15469    Encoder_loss: 0.6591721773147583\n",
            "  epoch: 9/10,    batch: 15102/15469    Encoder_loss: 0.6645739674568176\n",
            "  epoch: 9/10,    batch: 15103/15469    Encoder_loss: 0.6707507371902466\n",
            "  epoch: 9/10,    batch: 15104/15469    Encoder_loss: 0.6742222309112549\n",
            "  epoch: 9/10,    batch: 15105/15469    Encoder_loss: 0.679713249206543\n",
            "  epoch: 9/10,    batch: 15106/15469    Encoder_loss: 0.6865414381027222\n",
            "  epoch: 9/10,    batch: 15107/15469    Encoder_loss: 0.7320213913917542\n",
            "  epoch: 9/10,    batch: 15108/15469    Encoder_loss: 0.7759045362472534\n",
            "  epoch: 9/10,    batch: 15109/15469    Encoder_loss: 0.6935654282569885\n",
            "  epoch: 9/10,    batch: 15110/15469    Encoder_loss: 0.5951049327850342\n",
            "  epoch: 9/10,    batch: 15111/15469    Encoder_loss: 0.596495509147644\n",
            "  epoch: 9/10,    batch: 15112/15469    Encoder_loss: 0.5955361127853394\n",
            "  epoch: 9/10,    batch: 15113/15469    Encoder_loss: 0.599075973033905\n",
            "  epoch: 9/10,    batch: 15114/15469    Encoder_loss: 0.5980124473571777\n",
            "  epoch: 9/10,    batch: 15115/15469    Encoder_loss: 0.6007146835327148\n",
            "  epoch: 9/10,    batch: 15116/15469    Encoder_loss: 0.602250874042511\n",
            "  epoch: 9/10,    batch: 15117/15469    Encoder_loss: 0.6039968729019165\n",
            "  epoch: 9/10,    batch: 15118/15469    Encoder_loss: 0.6058064699172974\n",
            "  epoch: 9/10,    batch: 15119/15469    Encoder_loss: 0.6064386367797852\n",
            "  epoch: 9/10,    batch: 15120/15469    Encoder_loss: 0.6091500520706177\n",
            "  epoch: 9/10,    batch: 15121/15469    Encoder_loss: 0.6105867624282837\n",
            "  epoch: 9/10,    batch: 15122/15469    Encoder_loss: 0.6129236817359924\n",
            "  epoch: 9/10,    batch: 15123/15469    Encoder_loss: 0.6194284558296204\n",
            "  epoch: 9/10,    batch: 15124/15469    Encoder_loss: 0.47322505712509155\n",
            "  epoch: 9/10,    batch: 15125/15469    Encoder_loss: 0.4098661541938782\n",
            "  epoch: 9/10,    batch: 15126/15469    Encoder_loss: 0.40771540999412537\n",
            "  epoch: 9/10,    batch: 15127/15469    Encoder_loss: 0.4042568802833557\n",
            "  epoch: 9/10,    batch: 15128/15469    Encoder_loss: 0.4000960886478424\n",
            "  epoch: 9/10,    batch: 15129/15469    Encoder_loss: 0.398398756980896\n",
            "  epoch: 9/10,    batch: 15130/15469    Encoder_loss: 0.39250993728637695\n",
            "  epoch: 9/10,    batch: 15131/15469    Encoder_loss: 0.389096736907959\n",
            "  epoch: 9/10,    batch: 15132/15469    Encoder_loss: 0.38558611273765564\n",
            "  epoch: 9/10,    batch: 15133/15469    Encoder_loss: 0.38192829489707947\n",
            "  epoch: 9/10,    batch: 15134/15469    Encoder_loss: 0.37763428688049316\n",
            "  epoch: 9/10,    batch: 15135/15469    Encoder_loss: 0.3751963973045349\n",
            "  epoch: 9/10,    batch: 15136/15469    Encoder_loss: 0.37155669927597046\n",
            "  epoch: 9/10,    batch: 15137/15469    Encoder_loss: 0.37110310792922974\n",
            "  epoch: 9/10,    batch: 15138/15469    Encoder_loss: 0.36525315046310425\n",
            "  epoch: 9/10,    batch: 15139/15469    Encoder_loss: 0.361417293548584\n",
            "  epoch: 9/10,    batch: 15140/15469    Encoder_loss: 0.3585537075996399\n",
            "  epoch: 9/10,    batch: 15141/15469    Encoder_loss: 0.35544008016586304\n",
            "  epoch: 9/10,    batch: 15142/15469    Encoder_loss: 0.3529677391052246\n",
            "  epoch: 9/10,    batch: 15143/15469    Encoder_loss: 0.350445419549942\n",
            "  epoch: 9/10,    batch: 15144/15469    Encoder_loss: 0.3543632924556732\n",
            "  epoch: 9/10,    batch: 15145/15469    Encoder_loss: 0.29013311862945557\n",
            "  epoch: 9/10,    batch: 15146/15469    Encoder_loss: 0.2785492241382599\n",
            "  epoch: 9/10,    batch: 15147/15469    Encoder_loss: 0.27699169516563416\n",
            "  epoch: 9/10,    batch: 15148/15469    Encoder_loss: 0.27664533257484436\n",
            "  epoch: 9/10,    batch: 15149/15469    Encoder_loss: 0.27511411905288696\n",
            "  epoch: 9/10,    batch: 15150/15469    Encoder_loss: 0.27422404289245605\n",
            "  epoch: 9/10,    batch: 15151/15469    Encoder_loss: 0.27400344610214233\n",
            "  epoch: 9/10,    batch: 15152/15469    Encoder_loss: 0.2722412645816803\n",
            "  epoch: 9/10,    batch: 15153/15469    Encoder_loss: 0.2717050313949585\n",
            "  epoch: 9/10,    batch: 15154/15469    Encoder_loss: 0.2724238932132721\n",
            "  epoch: 9/10,    batch: 15155/15469    Encoder_loss: 0.2721306085586548\n",
            "  epoch: 9/10,    batch: 15156/15469    Encoder_loss: 0.2711937427520752\n",
            "  epoch: 9/10,    batch: 15157/15469    Encoder_loss: 0.27139806747436523\n",
            "  epoch: 9/10,    batch: 15158/15469    Encoder_loss: 0.26997706294059753\n",
            "  epoch: 9/10,    batch: 15159/15469    Encoder_loss: 0.26998159289360046\n",
            "  epoch: 9/10,    batch: 15160/15469    Encoder_loss: 0.2679494321346283\n",
            "  epoch: 9/10,    batch: 15161/15469    Encoder_loss: 0.2675064206123352\n",
            "  epoch: 9/10,    batch: 15162/15469    Encoder_loss: 0.267851322889328\n",
            "  epoch: 9/10,    batch: 15163/15469    Encoder_loss: 0.26625707745552063\n",
            "  epoch: 9/10,    batch: 15164/15469    Encoder_loss: 0.26495617628097534\n",
            "  epoch: 9/10,    batch: 15165/15469    Encoder_loss: 0.6111204624176025\n",
            "  epoch: 9/10,    batch: 15166/15469    Encoder_loss: 1.079389214515686\n",
            "  epoch: 9/10,    batch: 15167/15469    Encoder_loss: 0.6455591320991516\n",
            "  epoch: 9/10,    batch: 15168/15469    Encoder_loss: 0.611842155456543\n",
            "  epoch: 9/10,    batch: 15169/15469    Encoder_loss: 0.4258951246738434\n",
            "  epoch: 9/10,    batch: 15170/15469    Encoder_loss: 0.2949265241622925\n",
            "  epoch: 9/10,    batch: 15171/15469    Encoder_loss: 0.268220990896225\n",
            "  epoch: 9/10,    batch: 15172/15469    Encoder_loss: 0.2674406170845032\n",
            "  epoch: 9/10,    batch: 15173/15469    Encoder_loss: 0.2640001177787781\n",
            "  epoch: 9/10,    batch: 15174/15469    Encoder_loss: 0.2723178267478943\n",
            "  epoch: 9/10,    batch: 15175/15469    Encoder_loss: 0.27254781126976013\n",
            "  epoch: 9/10,    batch: 15176/15469    Encoder_loss: 0.27019235491752625\n",
            "  epoch: 9/10,    batch: 15177/15469    Encoder_loss: 0.266894668340683\n",
            "  epoch: 9/10,    batch: 15178/15469    Encoder_loss: 0.265417218208313\n",
            "  epoch: 9/10,    batch: 15179/15469    Encoder_loss: 0.26475560665130615\n",
            "  epoch: 9/10,    batch: 15180/15469    Encoder_loss: 0.2622995972633362\n",
            "  epoch: 9/10,    batch: 15181/15469    Encoder_loss: 0.2602202296257019\n",
            "  epoch: 9/10,    batch: 15182/15469    Encoder_loss: 0.25987598299980164\n",
            "  epoch: 9/10,    batch: 15183/15469    Encoder_loss: 0.25917044281959534\n",
            "  epoch: 9/10,    batch: 15184/15469    Encoder_loss: 0.2575417160987854\n",
            "  epoch: 9/10,    batch: 15185/15469    Encoder_loss: 0.25654512643814087\n",
            "  epoch: 9/10,    batch: 15186/15469    Encoder_loss: 0.25596386194229126\n",
            "  epoch: 9/10,    batch: 15187/15469    Encoder_loss: 0.25566989183425903\n",
            "  epoch: 9/10,    batch: 15188/15469    Encoder_loss: 0.25342100858688354\n",
            "  epoch: 9/10,    batch: 15189/15469    Encoder_loss: 0.2517506182193756\n",
            "  epoch: 9/10,    batch: 15190/15469    Encoder_loss: 0.251947283744812\n",
            "  epoch: 9/10,    batch: 15191/15469    Encoder_loss: 0.24880942702293396\n",
            "  epoch: 9/10,    batch: 15192/15469    Encoder_loss: 0.24915647506713867\n",
            "  epoch: 9/10,    batch: 15193/15469    Encoder_loss: 0.2495848387479782\n",
            "  epoch: 9/10,    batch: 15194/15469    Encoder_loss: 0.24673131108283997\n",
            "  epoch: 9/10,    batch: 15195/15469    Encoder_loss: 0.24566785991191864\n",
            "  epoch: 9/10,    batch: 15196/15469    Encoder_loss: 0.24503394961357117\n",
            "  epoch: 9/10,    batch: 15197/15469    Encoder_loss: 0.24504995346069336\n",
            "  epoch: 9/10,    batch: 15198/15469    Encoder_loss: 0.24370387196540833\n",
            "  epoch: 9/10,    batch: 15199/15469    Encoder_loss: 0.2427748441696167\n",
            "  epoch: 9/10,    batch: 15200/15469    Encoder_loss: 0.24222120642662048\n",
            "  epoch: 9/10,    batch: 15201/15469    Encoder_loss: 0.24170264601707458\n",
            "  epoch: 9/10,    batch: 15202/15469    Encoder_loss: 0.2411380559206009\n",
            "  epoch: 9/10,    batch: 15203/15469    Encoder_loss: 0.24053242802619934\n",
            "  epoch: 9/10,    batch: 15204/15469    Encoder_loss: 0.2408209592103958\n",
            "  epoch: 9/10,    batch: 15205/15469    Encoder_loss: 0.23986926674842834\n",
            "  epoch: 9/10,    batch: 15206/15469    Encoder_loss: 0.23992130160331726\n",
            "  epoch: 9/10,    batch: 15207/15469    Encoder_loss: 0.23938331007957458\n",
            "  epoch: 9/10,    batch: 15208/15469    Encoder_loss: 0.23930910229682922\n",
            "  epoch: 9/10,    batch: 15209/15469    Encoder_loss: 0.2408723533153534\n",
            "  epoch: 9/10,    batch: 15210/15469    Encoder_loss: 0.24057777225971222\n",
            "  epoch: 9/10,    batch: 15211/15469    Encoder_loss: 0.24145810306072235\n",
            "  epoch: 9/10,    batch: 15212/15469    Encoder_loss: 0.24124133586883545\n",
            "  epoch: 9/10,    batch: 15213/15469    Encoder_loss: 0.24128516018390656\n",
            "  epoch: 9/10,    batch: 15214/15469    Encoder_loss: 0.24190182983875275\n",
            "  epoch: 9/10,    batch: 15215/15469    Encoder_loss: 0.24240295588970184\n",
            "  epoch: 9/10,    batch: 15216/15469    Encoder_loss: 0.24372923374176025\n",
            "  epoch: 9/10,    batch: 15217/15469    Encoder_loss: 0.2451024055480957\n",
            "  epoch: 9/10,    batch: 15218/15469    Encoder_loss: 0.24616870284080505\n",
            "  epoch: 9/10,    batch: 15219/15469    Encoder_loss: 0.24643412232398987\n",
            "  epoch: 9/10,    batch: 15220/15469    Encoder_loss: 0.24669460952281952\n",
            "  epoch: 9/10,    batch: 15221/15469    Encoder_loss: 0.24731874465942383\n",
            "  epoch: 9/10,    batch: 15222/15469    Encoder_loss: 0.3800918757915497\n",
            "  epoch: 9/10,    batch: 15223/15469    Encoder_loss: 0.4154375195503235\n",
            "  epoch: 9/10,    batch: 15224/15469    Encoder_loss: 0.5568190813064575\n",
            "  epoch: 9/10,    batch: 15225/15469    Encoder_loss: 0.6223880052566528\n",
            "  epoch: 9/10,    batch: 15226/15469    Encoder_loss: 0.6278573870658875\n",
            "  epoch: 9/10,    batch: 15227/15469    Encoder_loss: 0.6325216293334961\n",
            "  epoch: 9/10,    batch: 15228/15469    Encoder_loss: 0.6366103887557983\n",
            "  epoch: 9/10,    batch: 15229/15469    Encoder_loss: 0.6404569745063782\n",
            "  epoch: 9/10,    batch: 15230/15469    Encoder_loss: 0.6467331647872925\n",
            "  epoch: 9/10,    batch: 15231/15469    Encoder_loss: 0.652418315410614\n",
            "  epoch: 9/10,    batch: 15232/15469    Encoder_loss: 0.6572907567024231\n",
            "  epoch: 9/10,    batch: 15233/15469    Encoder_loss: 0.6620528697967529\n",
            "  epoch: 9/10,    batch: 15234/15469    Encoder_loss: 0.6651701331138611\n",
            "  epoch: 9/10,    batch: 15235/15469    Encoder_loss: 0.6728924512863159\n",
            "  epoch: 9/10,    batch: 15236/15469    Encoder_loss: 0.6788864731788635\n",
            "  epoch: 9/10,    batch: 15237/15469    Encoder_loss: 0.6831632852554321\n",
            "  epoch: 9/10,    batch: 15238/15469    Encoder_loss: 0.6894296407699585\n",
            "  epoch: 9/10,    batch: 15239/15469    Encoder_loss: 0.6953689455986023\n",
            "  epoch: 9/10,    batch: 15240/15469    Encoder_loss: 0.7022829055786133\n",
            "  epoch: 9/10,    batch: 15241/15469    Encoder_loss: 0.7783023715019226\n",
            "  epoch: 9/10,    batch: 15242/15469    Encoder_loss: 0.7624036073684692\n",
            "  epoch: 9/10,    batch: 15243/15469    Encoder_loss: 0.6364137530326843\n",
            "  epoch: 9/10,    batch: 15244/15469    Encoder_loss: 0.6110442876815796\n",
            "  epoch: 9/10,    batch: 15245/15469    Encoder_loss: 0.6113367080688477\n",
            "  epoch: 9/10,    batch: 15246/15469    Encoder_loss: 0.6106671094894409\n",
            "  epoch: 9/10,    batch: 15247/15469    Encoder_loss: 0.6180054545402527\n",
            "  epoch: 9/10,    batch: 15248/15469    Encoder_loss: 0.6166626811027527\n",
            "  epoch: 9/10,    batch: 15249/15469    Encoder_loss: 0.6189227104187012\n",
            "  epoch: 9/10,    batch: 15250/15469    Encoder_loss: 0.6201510429382324\n",
            "  epoch: 9/10,    batch: 15251/15469    Encoder_loss: 0.622130274772644\n",
            "  epoch: 9/10,    batch: 15252/15469    Encoder_loss: 0.6263923645019531\n",
            "  epoch: 9/10,    batch: 15253/15469    Encoder_loss: 0.6236816048622131\n",
            "  epoch: 9/10,    batch: 15254/15469    Encoder_loss: 0.6284229755401611\n",
            "  epoch: 9/10,    batch: 15255/15469    Encoder_loss: 0.6295157074928284\n",
            "  epoch: 9/10,    batch: 15256/15469    Encoder_loss: 0.6308558583259583\n",
            "  epoch: 9/10,    batch: 15257/15469    Encoder_loss: 0.6108810305595398\n",
            "  epoch: 9/10,    batch: 15258/15469    Encoder_loss: 0.43482574820518494\n",
            "  epoch: 9/10,    batch: 15259/15469    Encoder_loss: 0.4217258095741272\n",
            "  epoch: 9/10,    batch: 15260/15469    Encoder_loss: 0.4203135371208191\n",
            "  epoch: 9/10,    batch: 15261/15469    Encoder_loss: 0.415463924407959\n",
            "  epoch: 9/10,    batch: 15262/15469    Encoder_loss: 0.41401880979537964\n",
            "  epoch: 9/10,    batch: 15263/15469    Encoder_loss: 0.4072757363319397\n",
            "  epoch: 9/10,    batch: 15264/15469    Encoder_loss: 0.4088802933692932\n",
            "  epoch: 9/10,    batch: 15265/15469    Encoder_loss: 0.40356069803237915\n",
            "  epoch: 9/10,    batch: 15266/15469    Encoder_loss: 0.40401092171669006\n",
            "  epoch: 9/10,    batch: 15267/15469    Encoder_loss: 0.4016243815422058\n",
            "  epoch: 9/10,    batch: 15268/15469    Encoder_loss: 0.4015887379646301\n",
            "  epoch: 9/10,    batch: 15269/15469    Encoder_loss: 0.3921113908290863\n",
            "  epoch: 9/10,    batch: 15270/15469    Encoder_loss: 0.38747626543045044\n",
            "  epoch: 9/10,    batch: 15271/15469    Encoder_loss: 0.3882935047149658\n",
            "  epoch: 9/10,    batch: 15272/15469    Encoder_loss: 0.3844636082649231\n",
            "  epoch: 9/10,    batch: 15273/15469    Encoder_loss: 0.3843921422958374\n",
            "  epoch: 9/10,    batch: 15274/15469    Encoder_loss: 0.38147610425949097\n",
            "  epoch: 9/10,    batch: 15275/15469    Encoder_loss: 0.37427958846092224\n",
            "  epoch: 9/10,    batch: 15276/15469    Encoder_loss: 0.3713047504425049\n",
            "  epoch: 9/10,    batch: 15277/15469    Encoder_loss: 0.372589111328125\n",
            "  epoch: 9/10,    batch: 15278/15469    Encoder_loss: 0.3573271930217743\n",
            "  epoch: 9/10,    batch: 15279/15469    Encoder_loss: 0.3002031147480011\n",
            "  epoch: 9/10,    batch: 15280/15469    Encoder_loss: 0.30004626512527466\n",
            "  epoch: 9/10,    batch: 15281/15469    Encoder_loss: 0.3019942045211792\n",
            "  epoch: 9/10,    batch: 15282/15469    Encoder_loss: 0.30229073762893677\n",
            "  epoch: 9/10,    batch: 15283/15469    Encoder_loss: 0.3002726137638092\n",
            "  epoch: 9/10,    batch: 15284/15469    Encoder_loss: 0.2991965115070343\n",
            "  epoch: 9/10,    batch: 15285/15469    Encoder_loss: 0.29540517926216125\n",
            "  epoch: 9/10,    batch: 15286/15469    Encoder_loss: 0.3006836473941803\n",
            "  epoch: 9/10,    batch: 15287/15469    Encoder_loss: 0.29855918884277344\n",
            "  epoch: 9/10,    batch: 15288/15469    Encoder_loss: 0.2974480390548706\n",
            "  epoch: 9/10,    batch: 15289/15469    Encoder_loss: 0.29532110691070557\n",
            "  epoch: 9/10,    batch: 15290/15469    Encoder_loss: 0.2940906584262848\n",
            "  epoch: 9/10,    batch: 15291/15469    Encoder_loss: 0.29549121856689453\n",
            "  epoch: 9/10,    batch: 15292/15469    Encoder_loss: 0.29260677099227905\n",
            "  epoch: 9/10,    batch: 15293/15469    Encoder_loss: 0.2883037328720093\n",
            "  epoch: 9/10,    batch: 15294/15469    Encoder_loss: 0.2913571298122406\n",
            "  epoch: 9/10,    batch: 15295/15469    Encoder_loss: 0.2934624254703522\n",
            "  epoch: 9/10,    batch: 15296/15469    Encoder_loss: 0.29453369975090027\n",
            "  epoch: 9/10,    batch: 15297/15469    Encoder_loss: 0.2942163050174713\n",
            "  epoch: 9/10,    batch: 15298/15469    Encoder_loss: 0.5244614481925964\n",
            "  epoch: 9/10,    batch: 15299/15469    Encoder_loss: 1.1382752656936646\n",
            "  epoch: 9/10,    batch: 15300/15469    Encoder_loss: 0.6944930553436279\n",
            "  epoch: 9/10,    batch: 15301/15469    Encoder_loss: 0.659274697303772\n",
            "  epoch: 9/10,    batch: 15302/15469    Encoder_loss: 0.46900853514671326\n",
            "  epoch: 9/10,    batch: 15303/15469    Encoder_loss: 0.34622249007225037\n",
            "  epoch: 9/10,    batch: 15304/15469    Encoder_loss: 0.29708921909332275\n",
            "  epoch: 9/10,    batch: 15305/15469    Encoder_loss: 0.2943367063999176\n",
            "  epoch: 9/10,    batch: 15306/15469    Encoder_loss: 0.29278719425201416\n",
            "  epoch: 9/10,    batch: 15307/15469    Encoder_loss: 0.3131045997142792\n",
            "  epoch: 9/10,    batch: 15308/15469    Encoder_loss: 0.3149675726890564\n",
            "  epoch: 9/10,    batch: 15309/15469    Encoder_loss: 0.30379071831703186\n",
            "  epoch: 9/10,    batch: 15310/15469    Encoder_loss: 0.3011355400085449\n",
            "  epoch: 9/10,    batch: 15311/15469    Encoder_loss: 0.2980315089225769\n",
            "  epoch: 9/10,    batch: 15312/15469    Encoder_loss: 0.29381564259529114\n",
            "  epoch: 9/10,    batch: 15313/15469    Encoder_loss: 0.2959856390953064\n",
            "  epoch: 9/10,    batch: 15314/15469    Encoder_loss: 0.28987592458724976\n",
            "  epoch: 9/10,    batch: 15315/15469    Encoder_loss: 0.2888977527618408\n",
            "  epoch: 9/10,    batch: 15316/15469    Encoder_loss: 0.2870447635650635\n",
            "  epoch: 9/10,    batch: 15317/15469    Encoder_loss: 0.28506386280059814\n",
            "  epoch: 9/10,    batch: 15318/15469    Encoder_loss: 0.2863328158855438\n",
            "  epoch: 9/10,    batch: 15319/15469    Encoder_loss: 0.2915606200695038\n",
            "  epoch: 9/10,    batch: 15320/15469    Encoder_loss: 0.2888476848602295\n",
            "  epoch: 9/10,    batch: 15321/15469    Encoder_loss: 0.28320568799972534\n",
            "  epoch: 9/10,    batch: 15322/15469    Encoder_loss: 0.28264400362968445\n",
            "  epoch: 9/10,    batch: 15323/15469    Encoder_loss: 0.2834182679653168\n",
            "  epoch: 9/10,    batch: 15324/15469    Encoder_loss: 0.285746693611145\n",
            "  epoch: 9/10,    batch: 15325/15469    Encoder_loss: 0.2850778102874756\n",
            "  epoch: 9/10,    batch: 15326/15469    Encoder_loss: 0.2806365191936493\n",
            "  epoch: 9/10,    batch: 15327/15469    Encoder_loss: 0.27855879068374634\n",
            "  epoch: 9/10,    batch: 15328/15469    Encoder_loss: 0.27631527185440063\n",
            "  epoch: 9/10,    batch: 15329/15469    Encoder_loss: 0.2760685682296753\n",
            "  epoch: 9/10,    batch: 15330/15469    Encoder_loss: 0.2749823033809662\n",
            "  epoch: 9/10,    batch: 15331/15469    Encoder_loss: 0.27833235263824463\n",
            "  epoch: 9/10,    batch: 15332/15469    Encoder_loss: 0.2743625342845917\n",
            "  epoch: 9/10,    batch: 15333/15469    Encoder_loss: 0.27513429522514343\n",
            "  epoch: 9/10,    batch: 15334/15469    Encoder_loss: 0.27740010619163513\n",
            "  epoch: 9/10,    batch: 15335/15469    Encoder_loss: 0.2742632031440735\n",
            "  epoch: 9/10,    batch: 15336/15469    Encoder_loss: 0.2733778953552246\n",
            "  epoch: 9/10,    batch: 15337/15469    Encoder_loss: 0.27016323804855347\n",
            "  epoch: 9/10,    batch: 15338/15469    Encoder_loss: 0.27221471071243286\n",
            "  epoch: 9/10,    batch: 15339/15469    Encoder_loss: 0.27229493856430054\n",
            "  epoch: 9/10,    batch: 15340/15469    Encoder_loss: 0.2734675705432892\n",
            "  epoch: 9/10,    batch: 15341/15469    Encoder_loss: 0.2728951871395111\n",
            "  epoch: 9/10,    batch: 15342/15469    Encoder_loss: 0.2708147466182709\n",
            "  epoch: 9/10,    batch: 15343/15469    Encoder_loss: 0.2751094698905945\n",
            "  epoch: 9/10,    batch: 15344/15469    Encoder_loss: 0.2725856602191925\n",
            "  epoch: 9/10,    batch: 15345/15469    Encoder_loss: 0.27374154329299927\n",
            "  epoch: 9/10,    batch: 15346/15469    Encoder_loss: 0.27387213706970215\n",
            "  epoch: 9/10,    batch: 15347/15469    Encoder_loss: 0.27542218565940857\n",
            "  epoch: 9/10,    batch: 15348/15469    Encoder_loss: 0.27328091859817505\n",
            "  epoch: 9/10,    batch: 15349/15469    Encoder_loss: 0.2772602140903473\n",
            "  epoch: 9/10,    batch: 15350/15469    Encoder_loss: 0.27899810671806335\n",
            "  epoch: 9/10,    batch: 15351/15469    Encoder_loss: 0.2760125398635864\n",
            "  epoch: 9/10,    batch: 15352/15469    Encoder_loss: 0.27493324875831604\n",
            "  epoch: 9/10,    batch: 15353/15469    Encoder_loss: 0.2791617810726166\n",
            "  epoch: 9/10,    batch: 15354/15469    Encoder_loss: 0.2753344178199768\n",
            "  epoch: 9/10,    batch: 15355/15469    Encoder_loss: 0.34146609902381897\n",
            "  epoch: 9/10,    batch: 15356/15469    Encoder_loss: 0.44809800386428833\n",
            "  epoch: 9/10,    batch: 15357/15469    Encoder_loss: 0.47503799200057983\n",
            "  epoch: 9/10,    batch: 15358/15469    Encoder_loss: 0.6514602899551392\n",
            "  epoch: 9/10,    batch: 15359/15469    Encoder_loss: 0.6566828489303589\n",
            "  epoch: 9/10,    batch: 15360/15469    Encoder_loss: 0.6613168716430664\n",
            "  epoch: 9/10,    batch: 15361/15469    Encoder_loss: 0.6687125563621521\n",
            "  epoch: 9/10,    batch: 15362/15469    Encoder_loss: 0.6736848950386047\n",
            "  epoch: 9/10,    batch: 15363/15469    Encoder_loss: 0.6785837411880493\n",
            "  epoch: 9/10,    batch: 15364/15469    Encoder_loss: 0.6843284368515015\n",
            "  epoch: 9/10,    batch: 15365/15469    Encoder_loss: 0.686903715133667\n",
            "  epoch: 9/10,    batch: 15366/15469    Encoder_loss: 0.6915149092674255\n",
            "  epoch: 9/10,    batch: 15367/15469    Encoder_loss: 0.6966465711593628\n",
            "  epoch: 9/10,    batch: 15368/15469    Encoder_loss: 0.702563464641571\n",
            "  epoch: 9/10,    batch: 15369/15469    Encoder_loss: 0.7100130915641785\n",
            "  epoch: 9/10,    batch: 15370/15469    Encoder_loss: 0.712640643119812\n",
            "  epoch: 9/10,    batch: 15371/15469    Encoder_loss: 0.7145693898200989\n",
            "  epoch: 9/10,    batch: 15372/15469    Encoder_loss: 0.7211356163024902\n",
            "  epoch: 9/10,    batch: 15373/15469    Encoder_loss: 0.7274676561355591\n",
            "  epoch: 9/10,    batch: 15374/15469    Encoder_loss: 0.7667043209075928\n",
            "  epoch: 9/10,    batch: 15375/15469    Encoder_loss: 0.8208239674568176\n",
            "  epoch: 9/10,    batch: 15376/15469    Encoder_loss: 0.7514639496803284\n",
            "  epoch: 9/10,    batch: 15377/15469    Encoder_loss: 0.6371115446090698\n",
            "  epoch: 9/10,    batch: 15378/15469    Encoder_loss: 0.6415659189224243\n",
            "  epoch: 9/10,    batch: 15379/15469    Encoder_loss: 0.6370481252670288\n",
            "  epoch: 9/10,    batch: 15380/15469    Encoder_loss: 0.635424017906189\n",
            "  epoch: 9/10,    batch: 15381/15469    Encoder_loss: 0.6408165693283081\n",
            "  epoch: 9/10,    batch: 15382/15469    Encoder_loss: 0.6437946557998657\n",
            "  epoch: 9/10,    batch: 15383/15469    Encoder_loss: 0.6477499008178711\n",
            "  epoch: 9/10,    batch: 15384/15469    Encoder_loss: 0.646372377872467\n",
            "  epoch: 9/10,    batch: 15385/15469    Encoder_loss: 0.6468408703804016\n",
            "  epoch: 9/10,    batch: 15386/15469    Encoder_loss: 0.6418450474739075\n",
            "  epoch: 9/10,    batch: 15387/15469    Encoder_loss: 0.6470131278038025\n",
            "  epoch: 9/10,    batch: 15388/15469    Encoder_loss: 0.6490370035171509\n",
            "  epoch: 9/10,    batch: 15389/15469    Encoder_loss: 0.652481198310852\n",
            "  epoch: 9/10,    batch: 15390/15469    Encoder_loss: 0.6614861488342285\n",
            "  epoch: 9/10,    batch: 15391/15469    Encoder_loss: 0.4864915907382965\n",
            "  epoch: 9/10,    batch: 15392/15469    Encoder_loss: 0.45024019479751587\n",
            "  epoch: 9/10,    batch: 15393/15469    Encoder_loss: 0.4426043629646301\n",
            "  epoch: 9/10,    batch: 15394/15469    Encoder_loss: 0.43693843483924866\n",
            "  epoch: 9/10,    batch: 15395/15469    Encoder_loss: 0.4359878599643707\n",
            "  epoch: 9/10,    batch: 15396/15469    Encoder_loss: 0.4336603283882141\n",
            "  epoch: 9/10,    batch: 15397/15469    Encoder_loss: 0.43107160925865173\n",
            "  epoch: 9/10,    batch: 15398/15469    Encoder_loss: 0.4289482533931732\n",
            "  epoch: 9/10,    batch: 15399/15469    Encoder_loss: 0.422264963388443\n",
            "  epoch: 9/10,    batch: 15400/15469    Encoder_loss: 0.4184601902961731\n",
            "  epoch: 9/10,    batch: 15401/15469    Encoder_loss: 0.4171329140663147\n",
            "  epoch: 9/10,    batch: 15402/15469    Encoder_loss: 0.41202569007873535\n",
            "  epoch: 9/10,    batch: 15403/15469    Encoder_loss: 0.4068385660648346\n",
            "  epoch: 9/10,    batch: 15404/15469    Encoder_loss: 0.40412479639053345\n",
            "  epoch: 9/10,    batch: 15405/15469    Encoder_loss: 0.4001087248325348\n",
            "  epoch: 9/10,    batch: 15406/15469    Encoder_loss: 0.39547058939933777\n",
            "  epoch: 9/10,    batch: 15407/15469    Encoder_loss: 0.3891521096229553\n",
            "  epoch: 9/10,    batch: 15408/15469    Encoder_loss: 0.3903560936450958\n",
            "  epoch: 9/10,    batch: 15409/15469    Encoder_loss: 0.3931580185890198\n",
            "  epoch: 9/10,    batch: 15410/15469    Encoder_loss: 0.3900275230407715\n",
            "  epoch: 9/10,    batch: 15411/15469    Encoder_loss: 0.3920845687389374\n",
            "  epoch: 9/10,    batch: 15412/15469    Encoder_loss: 0.31222018599510193\n",
            "  epoch: 9/10,    batch: 15413/15469    Encoder_loss: 0.3130697011947632\n",
            "  epoch: 9/10,    batch: 15414/15469    Encoder_loss: 0.3161413073539734\n",
            "  epoch: 9/10,    batch: 15415/15469    Encoder_loss: 0.3151629865169525\n",
            "  epoch: 9/10,    batch: 15416/15469    Encoder_loss: 0.3141580820083618\n",
            "  epoch: 9/10,    batch: 15417/15469    Encoder_loss: 0.3129933476448059\n",
            "  epoch: 9/10,    batch: 15418/15469    Encoder_loss: 0.31152409315109253\n",
            "  epoch: 9/10,    batch: 15419/15469    Encoder_loss: 0.3106474280357361\n",
            "  epoch: 9/10,    batch: 15420/15469    Encoder_loss: 0.3109816908836365\n",
            "  epoch: 9/10,    batch: 15421/15469    Encoder_loss: 0.31043189764022827\n",
            "  epoch: 9/10,    batch: 15422/15469    Encoder_loss: 0.3096139132976532\n",
            "  epoch: 9/10,    batch: 15423/15469    Encoder_loss: 0.30970555543899536\n",
            "  epoch: 9/10,    batch: 15424/15469    Encoder_loss: 0.30551213026046753\n",
            "  epoch: 9/10,    batch: 15425/15469    Encoder_loss: 0.30470409989356995\n",
            "  epoch: 9/10,    batch: 15426/15469    Encoder_loss: 0.30804339051246643\n",
            "  epoch: 9/10,    batch: 15427/15469    Encoder_loss: 0.30789700150489807\n",
            "  epoch: 9/10,    batch: 15428/15469    Encoder_loss: 0.3028346300125122\n",
            "  epoch: 9/10,    batch: 15429/15469    Encoder_loss: 0.30760449171066284\n",
            "  epoch: 9/10,    batch: 15430/15469    Encoder_loss: 0.30520308017730713\n",
            "  epoch: 9/10,    batch: 15431/15469    Encoder_loss: 0.4636024534702301\n",
            "  epoch: 9/10,    batch: 15432/15469    Encoder_loss: 1.125860333442688\n",
            "  epoch: 9/10,    batch: 15433/15469    Encoder_loss: 0.7540898323059082\n",
            "  epoch: 9/10,    batch: 15434/15469    Encoder_loss: 0.675032913684845\n",
            "  epoch: 9/10,    batch: 15435/15469    Encoder_loss: 0.49684658646583557\n",
            "  epoch: 9/10,    batch: 15436/15469    Encoder_loss: 0.37714844942092896\n",
            "  epoch: 9/10,    batch: 15437/15469    Encoder_loss: 0.3085058331489563\n",
            "  epoch: 9/10,    batch: 15438/15469    Encoder_loss: 0.31095296144485474\n",
            "  epoch: 9/10,    batch: 15439/15469    Encoder_loss: 0.30742552876472473\n",
            "  epoch: 9/10,    batch: 15440/15469    Encoder_loss: 0.31989651918411255\n",
            "  epoch: 9/10,    batch: 15441/15469    Encoder_loss: 0.3178953230381012\n",
            "  epoch: 9/10,    batch: 15442/15469    Encoder_loss: 0.31259745359420776\n",
            "  epoch: 9/10,    batch: 15443/15469    Encoder_loss: 0.30777400732040405\n",
            "  epoch: 9/10,    batch: 15444/15469    Encoder_loss: 0.3044721484184265\n",
            "  epoch: 9/10,    batch: 15445/15469    Encoder_loss: 0.30071794986724854\n",
            "  epoch: 9/10,    batch: 15446/15469    Encoder_loss: 0.30246952176094055\n",
            "  epoch: 9/10,    batch: 15447/15469    Encoder_loss: 0.2988720238208771\n",
            "  epoch: 9/10,    batch: 15448/15469    Encoder_loss: 0.2956926226615906\n",
            "  epoch: 9/10,    batch: 15449/15469    Encoder_loss: 0.30100297927856445\n",
            "  epoch: 9/10,    batch: 15450/15469    Encoder_loss: 0.29741841554641724\n",
            "  epoch: 9/10,    batch: 15451/15469    Encoder_loss: 0.29663997888565063\n",
            "  epoch: 9/10,    batch: 15452/15469    Encoder_loss: 0.2973253130912781\n",
            "  epoch: 9/10,    batch: 15453/15469    Encoder_loss: 0.2922644019126892\n",
            "  epoch: 9/10,    batch: 15454/15469    Encoder_loss: 0.29193729162216187\n",
            "  epoch: 9/10,    batch: 15455/15469    Encoder_loss: 0.29083067178726196\n",
            "  epoch: 9/10,    batch: 15456/15469    Encoder_loss: 0.2926919460296631\n",
            "  epoch: 9/10,    batch: 15457/15469    Encoder_loss: 0.2906149923801422\n",
            "  epoch: 9/10,    batch: 15458/15469    Encoder_loss: 0.28957778215408325\n",
            "  epoch: 9/10,    batch: 15459/15469    Encoder_loss: 0.2854147255420685\n",
            "  epoch: 9/10,    batch: 15460/15469    Encoder_loss: 0.28275156021118164\n",
            "  epoch: 9/10,    batch: 15461/15469    Encoder_loss: 0.27864721417427063\n",
            "  epoch: 9/10,    batch: 15462/15469    Encoder_loss: 0.2850903868675232\n",
            "  epoch: 9/10,    batch: 15463/15469    Encoder_loss: 0.2775362432003021\n",
            "  epoch: 9/10,    batch: 15464/15469    Encoder_loss: 0.28295621275901794\n",
            "  epoch: 9/10,    batch: 15465/15469    Encoder_loss: 0.2805722653865814\n",
            "  epoch: 9/10,    batch: 15466/15469    Encoder_loss: 0.28227970004081726\n",
            "  epoch: 9/10,    batch: 15467/15469    Encoder_loss: 0.28075525164604187\n",
            "  epoch: 9/10,    batch: 15468/15469    Encoder_loss: 0.2808271646499634\n",
            "epoch: 9/10,  Tot_epoch_loss: 7968.11572265625\n",
            "  adding: encoder_checkpoints/ (stored 0%)\n",
            "  adding: encoder_checkpoints/encoder_epoch5.h5 (deflated 10%)\n",
            "  adding: encoder_checkpoints/encoder_epoch8.h5 (deflated 10%)\n",
            "  adding: encoder_checkpoints/encoder_epoch6.h5 (deflated 10%)\n",
            "  adding: encoder_checkpoints/encoder_epoch1.h5 (deflated 10%)\n",
            "  adding: encoder_checkpoints/encoder_epoch3.h5 (deflated 10%)\n",
            "  adding: encoder_checkpoints/encoder_epoch2.h5 (deflated 11%)\n",
            "  adding: encoder_checkpoints/encoder_epoch7.h5 (deflated 10%)\n",
            "  adding: encoder_checkpoints/encoder_epoch0.h5 (deflated 11%)\n",
            "  adding: encoder_checkpoints/encoder_epoch4.h5 (deflated 10%)\n",
            "  adding: encoder_checkpoints/encoder_epoch9.h5 (deflated 10%)\n"
          ]
        }
      ],
      "source": [
        "# init encoder\n",
        "encoder = init_encoder(in_dim=(WINDOW_SIZE, SAMPLE_SIZE), out_dim=(WINDOW_SIZE, LATENT_VAR_SIZE))\n",
        "encoder_training(encoder, generator, train_dataset, LATENT_VAR_SIZE, \"./drive/MyDrive/ml-applications/TimeSeriesAnomalyDetection_project/saved_encoder_checkpoints_swat\", num_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xndp1iQ-17uK"
      },
      "source": [
        "# Anomaly Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLKMChTJAINk"
      },
      "outputs": [],
      "source": [
        "def reconstruction_loss(time_sequences, fake_sequences, discriminator, _lambda):\n",
        "  '''\n",
        "  This function acts as both recontruction loss function when mapping a time sequence to\n",
        "  a latent variable 'z' and as anomaly score.\n",
        "\n",
        "  This loss is the sum of 2 losses:\n",
        "      - residual loss: the sum of the absolute value of the components of the difference\n",
        "          between a real time sequence and a generated one by the generator\n",
        "\n",
        "      - discrimination loss: the sum of the absolute value of the components of the difference\n",
        "          between the outputs of the LSTM layer of the discriminator when the inputs\n",
        "          are a real time sequence and a generated one\n",
        "  '''\n",
        "  residual_loss = tf.reduce_sum(abs(time_sequences - fake_sequences))\n",
        "\n",
        "  interm_layer = discriminator.layers[0]  # LSTM layer\n",
        "\n",
        "  features_real = interm_layer(time_sequences)\n",
        "  features_fake = interm_layer(fake_sequences)\n",
        "\n",
        "  discrimination_loss = tf.reduce_sum(abs(features_real-features_fake))\n",
        "\n",
        "  total_loss = (1-_lambda)*residual_loss + _lambda*discrimination_loss\n",
        "\n",
        "  return total_loss\n",
        "\n",
        "def anomaly_score(test_data, generator, discriminator, encoder, _lambda):\n",
        "  '''\n",
        "  Outputs a list of losses, each loss corresponding to a time window\n",
        "  '''\n",
        "\n",
        "  # IMPORTANT: DURING ANOMALY DETECTION, SET BATCH_SIZE=1!!!!!!!!!!!!!!!!!!\n",
        "  # i.e., a batch contains a single window of timesteps\n",
        "\n",
        "  i = 0\n",
        "  loss_list = []\n",
        "  for _, batch in enumerate(test_data):\n",
        "    print(f\"batch: {i} out of {len(test_data)}\")\n",
        "    i+=1\n",
        "\n",
        "    # map time sequences to latent space\n",
        "    latent_var = encoder(batch[0]) # batch size is 1, so batch[0] points to the window\n",
        "\n",
        "    # reconstruct time sequences from latent space\n",
        "    reconstr_sequences = generator(latent_var)\n",
        "\n",
        "    # compute reconstruction loss\n",
        "    loss = reconstruction_loss(batch[0], reconstr_sequences, discriminator, _lambda)\n",
        "\n",
        "    loss_list.append(loss)\n",
        "  return loss_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fp-XAX5fdXZ"
      },
      "source": [
        "# Perform inference\n",
        "Either use the generator and discriminator at the state you have them now (if you just trained them), or load a previously saved checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPrZlASiPSLu",
        "outputId": "d5cfb991-f558-4727-fa38-2e81ae2ac15c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ./checkpoints_2023-08-16_00:13:33.763024.zip\n",
            "   creating: ./checkpoints/\n",
            "   creating: ./checkpoints/generator/\n",
            "  inflating: ./checkpoints/generator/gen_epoch3.h5  \n",
            "  inflating: ./checkpoints/generator/gen_epoch9.h5  \n",
            "  inflating: ./checkpoints/generator/gen_epoch1.h5  \n",
            "  inflating: ./checkpoints/generator/gen_epoch2.h5  \n",
            "  inflating: ./checkpoints/generator/gen_epoch5.h5  \n",
            "  inflating: ./checkpoints/generator/gen_epoch4.h5  \n",
            "  inflating: ./checkpoints/generator/gen_epoch0.h5  \n",
            "  inflating: ./checkpoints/generator/gen_epoch7.h5  \n",
            "  inflating: ./checkpoints/generator/gen_epoch8.h5  \n",
            "  inflating: ./checkpoints/generator/gen_epoch6.h5  \n",
            "   creating: ./checkpoints/discriminator/\n",
            "  inflating: ./checkpoints/discriminator/discr_epoch2.h5  \n",
            "  inflating: ./checkpoints/discriminator/discr_epoch8.h5  \n",
            "  inflating: ./checkpoints/discriminator/discr_epoch4.h5  \n",
            "  inflating: ./checkpoints/discriminator/discr_epoch9.h5  \n",
            "  inflating: ./checkpoints/discriminator/discr_epoch6.h5  \n",
            "  inflating: ./checkpoints/discriminator/discr_epoch0.h5  \n",
            "  inflating: ./checkpoints/discriminator/discr_epoch5.h5  \n",
            "  inflating: ./checkpoints/discriminator/discr_epoch3.h5  \n",
            "  inflating: ./checkpoints/discriminator/discr_epoch7.h5  \n",
            "  inflating: ./checkpoints/discriminator/discr_epoch1.h5  \n",
            "Archive:  ./encoder_checkpoints_2023-08-16_18:21:27.077528.zip\n",
            "   creating: ./encoder_checkpoints/\n",
            "  inflating: ./encoder_checkpoints/encoder_epoch5.h5  \n",
            "  inflating: ./encoder_checkpoints/encoder_epoch8.h5  \n",
            "  inflating: ./encoder_checkpoints/encoder_epoch6.h5  \n",
            "  inflating: ./encoder_checkpoints/encoder_epoch1.h5  \n",
            "  inflating: ./encoder_checkpoints/encoder_epoch3.h5  \n",
            "  inflating: ./encoder_checkpoints/encoder_epoch2.h5  \n",
            "  inflating: ./encoder_checkpoints/encoder_epoch7.h5  \n",
            "  inflating: ./encoder_checkpoints/encoder_epoch0.h5  \n",
            "  inflating: ./encoder_checkpoints/encoder_epoch4.h5  \n",
            "  inflating: ./encoder_checkpoints/encoder_epoch9.h5  \n"
          ]
        }
      ],
      "source": [
        "generator = init_generator(in_dim=(WINDOW_SIZE, LATENT_VAR_SIZE), out_dim=(WINDOW_SIZE, SAMPLE_SIZE))\n",
        "discriminator = init_discriminator(in_dim=(WINDOW_SIZE, SAMPLE_SIZE), out_dim=1)\n",
        "encoder = init_encoder(in_dim=(WINDOW_SIZE, SAMPLE_SIZE), out_dim=(WINDOW_SIZE, LATENT_VAR_SIZE))\n",
        "\n",
        "# download and uzip latest generator and discriminator checkpoints\n",
        "!cp ./drive/MyDrive/ml-applications/TimeSeriesAnomalyDetection_project/saved_checkpoints_swat/checkpoints_2023-08-16_00:13:33.763024.zip .\n",
        "!unzip ./checkpoints_2023-08-16_00:13:33.763024.zip -d .\n",
        "\n",
        "# download and unzip latest encoder checkpoint\n",
        "!cp ./drive/MyDrive/ml-applications/TimeSeriesAnomalyDetection_project/saved_encoder_checkpoints_swat/encoder_checkpoints_2023-08-16_18:21:27.077528.zip .\n",
        "!unzip ./encoder_checkpoints_2023-08-16_18:21:27.077528.zip -d .\n",
        "\n",
        "epoch = 9#19\n",
        "generator.load_weights(f\"./checkpoints/generator/gen_epoch{epoch}.h5\")\n",
        "discriminator.load_weights(f\"./checkpoints/discriminator/discr_epoch{epoch}.h5\")\n",
        "\n",
        "epoch_enc = 9#19 # SET THIS TO 19 AFTER FULLY TRAINING THE ENCODER!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "encoder.load_weights(f\"./encoder_checkpoints/encoder_epoch{epoch_enc}.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK7kf_ErATDk",
        "outputId": "af76cfa1-eca0-49ef-8992-60317625668a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "batch: 39991 out of 44991\n",
            "batch: 39992 out of 44991\n",
            "batch: 39993 out of 44991\n",
            "batch: 39994 out of 44991\n",
            "batch: 39995 out of 44991\n",
            "batch: 39996 out of 44991\n",
            "batch: 39997 out of 44991\n",
            "batch: 39998 out of 44991\n",
            "batch: 39999 out of 44991\n",
            "batch: 40000 out of 44991\n",
            "batch: 40001 out of 44991\n",
            "batch: 40002 out of 44991\n",
            "batch: 40003 out of 44991\n",
            "batch: 40004 out of 44991\n",
            "batch: 40005 out of 44991\n",
            "batch: 40006 out of 44991\n",
            "batch: 40007 out of 44991\n",
            "batch: 40008 out of 44991\n",
            "batch: 40009 out of 44991\n",
            "batch: 40010 out of 44991\n",
            "batch: 40011 out of 44991\n",
            "batch: 40012 out of 44991\n",
            "batch: 40013 out of 44991\n",
            "batch: 40014 out of 44991\n",
            "batch: 40015 out of 44991\n",
            "batch: 40016 out of 44991\n",
            "batch: 40017 out of 44991\n",
            "batch: 40018 out of 44991\n",
            "batch: 40019 out of 44991\n",
            "batch: 40020 out of 44991\n",
            "batch: 40021 out of 44991\n",
            "batch: 40022 out of 44991\n",
            "batch: 40023 out of 44991\n",
            "batch: 40024 out of 44991\n",
            "batch: 40025 out of 44991\n",
            "batch: 40026 out of 44991\n",
            "batch: 40027 out of 44991\n",
            "batch: 40028 out of 44991\n",
            "batch: 40029 out of 44991\n",
            "batch: 40030 out of 44991\n",
            "batch: 40031 out of 44991\n",
            "batch: 40032 out of 44991\n",
            "batch: 40033 out of 44991\n",
            "batch: 40034 out of 44991\n",
            "batch: 40035 out of 44991\n",
            "batch: 40036 out of 44991\n",
            "batch: 40037 out of 44991\n",
            "batch: 40038 out of 44991\n",
            "batch: 40039 out of 44991\n",
            "batch: 40040 out of 44991\n",
            "batch: 40041 out of 44991\n",
            "batch: 40042 out of 44991\n",
            "batch: 40043 out of 44991\n",
            "batch: 40044 out of 44991\n",
            "batch: 40045 out of 44991\n",
            "batch: 40046 out of 44991\n",
            "batch: 40047 out of 44991\n",
            "batch: 40048 out of 44991\n",
            "batch: 40049 out of 44991\n",
            "batch: 40050 out of 44991\n",
            "batch: 40051 out of 44991\n",
            "batch: 40052 out of 44991\n",
            "batch: 40053 out of 44991\n",
            "batch: 40054 out of 44991\n",
            "batch: 40055 out of 44991\n",
            "batch: 40056 out of 44991\n",
            "batch: 40057 out of 44991\n",
            "batch: 40058 out of 44991\n",
            "batch: 40059 out of 44991\n",
            "batch: 40060 out of 44991\n",
            "batch: 40061 out of 44991\n",
            "batch: 40062 out of 44991\n",
            "batch: 40063 out of 44991\n",
            "batch: 40064 out of 44991\n",
            "batch: 40065 out of 44991\n",
            "batch: 40066 out of 44991\n",
            "batch: 40067 out of 44991\n",
            "batch: 40068 out of 44991\n",
            "batch: 40069 out of 44991\n",
            "batch: 40070 out of 44991\n",
            "batch: 40071 out of 44991\n",
            "batch: 40072 out of 44991\n",
            "batch: 40073 out of 44991\n",
            "batch: 40074 out of 44991\n",
            "batch: 40075 out of 44991\n",
            "batch: 40076 out of 44991\n",
            "batch: 40077 out of 44991\n",
            "batch: 40078 out of 44991\n",
            "batch: 40079 out of 44991\n",
            "batch: 40080 out of 44991\n",
            "batch: 40081 out of 44991\n",
            "batch: 40082 out of 44991\n",
            "batch: 40083 out of 44991\n",
            "batch: 40084 out of 44991\n",
            "batch: 40085 out of 44991\n",
            "batch: 40086 out of 44991\n",
            "batch: 40087 out of 44991\n",
            "batch: 40088 out of 44991\n",
            "batch: 40089 out of 44991\n",
            "batch: 40090 out of 44991\n",
            "batch: 40091 out of 44991\n",
            "batch: 40092 out of 44991\n",
            "batch: 40093 out of 44991\n",
            "batch: 40094 out of 44991\n",
            "batch: 40095 out of 44991\n",
            "batch: 40096 out of 44991\n",
            "batch: 40097 out of 44991\n",
            "batch: 40098 out of 44991\n",
            "batch: 40099 out of 44991\n",
            "batch: 40100 out of 44991\n",
            "batch: 40101 out of 44991\n",
            "batch: 40102 out of 44991\n",
            "batch: 40103 out of 44991\n",
            "batch: 40104 out of 44991\n",
            "batch: 40105 out of 44991\n",
            "batch: 40106 out of 44991\n",
            "batch: 40107 out of 44991\n",
            "batch: 40108 out of 44991\n",
            "batch: 40109 out of 44991\n",
            "batch: 40110 out of 44991\n",
            "batch: 40111 out of 44991\n",
            "batch: 40112 out of 44991\n",
            "batch: 40113 out of 44991\n",
            "batch: 40114 out of 44991\n",
            "batch: 40115 out of 44991\n",
            "batch: 40116 out of 44991\n",
            "batch: 40117 out of 44991\n",
            "batch: 40118 out of 44991\n",
            "batch: 40119 out of 44991\n",
            "batch: 40120 out of 44991\n",
            "batch: 40121 out of 44991\n",
            "batch: 40122 out of 44991\n",
            "batch: 40123 out of 44991\n",
            "batch: 40124 out of 44991\n",
            "batch: 40125 out of 44991\n",
            "batch: 40126 out of 44991\n",
            "batch: 40127 out of 44991\n",
            "batch: 40128 out of 44991\n",
            "batch: 40129 out of 44991\n",
            "batch: 40130 out of 44991\n",
            "batch: 40131 out of 44991\n",
            "batch: 40132 out of 44991\n",
            "batch: 40133 out of 44991\n",
            "batch: 40134 out of 44991\n",
            "batch: 40135 out of 44991\n",
            "batch: 40136 out of 44991\n",
            "batch: 40137 out of 44991\n",
            "batch: 40138 out of 44991\n",
            "batch: 40139 out of 44991\n",
            "batch: 40140 out of 44991\n",
            "batch: 40141 out of 44991\n",
            "batch: 40142 out of 44991\n",
            "batch: 40143 out of 44991\n",
            "batch: 40144 out of 44991\n",
            "batch: 40145 out of 44991\n",
            "batch: 40146 out of 44991\n",
            "batch: 40147 out of 44991\n",
            "batch: 40148 out of 44991\n",
            "batch: 40149 out of 44991\n",
            "batch: 40150 out of 44991\n",
            "batch: 40151 out of 44991\n",
            "batch: 40152 out of 44991\n",
            "batch: 40153 out of 44991\n",
            "batch: 40154 out of 44991\n",
            "batch: 40155 out of 44991\n",
            "batch: 40156 out of 44991\n",
            "batch: 40157 out of 44991\n",
            "batch: 40158 out of 44991\n",
            "batch: 40159 out of 44991\n",
            "batch: 40160 out of 44991\n",
            "batch: 40161 out of 44991\n",
            "batch: 40162 out of 44991\n",
            "batch: 40163 out of 44991\n",
            "batch: 40164 out of 44991\n",
            "batch: 40165 out of 44991\n",
            "batch: 40166 out of 44991\n",
            "batch: 40167 out of 44991\n",
            "batch: 40168 out of 44991\n",
            "batch: 40169 out of 44991\n",
            "batch: 40170 out of 44991\n",
            "batch: 40171 out of 44991\n",
            "batch: 40172 out of 44991\n",
            "batch: 40173 out of 44991\n",
            "batch: 40174 out of 44991\n",
            "batch: 40175 out of 44991\n",
            "batch: 40176 out of 44991\n",
            "batch: 40177 out of 44991\n",
            "batch: 40178 out of 44991\n",
            "batch: 40179 out of 44991\n",
            "batch: 40180 out of 44991\n",
            "batch: 40181 out of 44991\n",
            "batch: 40182 out of 44991\n",
            "batch: 40183 out of 44991\n",
            "batch: 40184 out of 44991\n",
            "batch: 40185 out of 44991\n",
            "batch: 40186 out of 44991\n",
            "batch: 40187 out of 44991\n",
            "batch: 40188 out of 44991\n",
            "batch: 40189 out of 44991\n",
            "batch: 40190 out of 44991\n",
            "batch: 40191 out of 44991\n",
            "batch: 40192 out of 44991\n",
            "batch: 40193 out of 44991\n",
            "batch: 40194 out of 44991\n",
            "batch: 40195 out of 44991\n",
            "batch: 40196 out of 44991\n",
            "batch: 40197 out of 44991\n",
            "batch: 40198 out of 44991\n",
            "batch: 40199 out of 44991\n",
            "batch: 40200 out of 44991\n",
            "batch: 40201 out of 44991\n",
            "batch: 40202 out of 44991\n",
            "batch: 40203 out of 44991\n",
            "batch: 40204 out of 44991\n",
            "batch: 40205 out of 44991\n",
            "batch: 40206 out of 44991\n",
            "batch: 40207 out of 44991\n",
            "batch: 40208 out of 44991\n",
            "batch: 40209 out of 44991\n",
            "batch: 40210 out of 44991\n",
            "batch: 40211 out of 44991\n",
            "batch: 40212 out of 44991\n",
            "batch: 40213 out of 44991\n",
            "batch: 40214 out of 44991\n",
            "batch: 40215 out of 44991\n",
            "batch: 40216 out of 44991\n",
            "batch: 40217 out of 44991\n",
            "batch: 40218 out of 44991\n",
            "batch: 40219 out of 44991\n",
            "batch: 40220 out of 44991\n",
            "batch: 40221 out of 44991\n",
            "batch: 40222 out of 44991\n",
            "batch: 40223 out of 44991\n",
            "batch: 40224 out of 44991\n",
            "batch: 40225 out of 44991\n",
            "batch: 40226 out of 44991\n",
            "batch: 40227 out of 44991\n",
            "batch: 40228 out of 44991\n",
            "batch: 40229 out of 44991\n",
            "batch: 40230 out of 44991\n",
            "batch: 40231 out of 44991\n",
            "batch: 40232 out of 44991\n",
            "batch: 40233 out of 44991\n",
            "batch: 40234 out of 44991\n",
            "batch: 40235 out of 44991\n",
            "batch: 40236 out of 44991\n",
            "batch: 40237 out of 44991\n",
            "batch: 40238 out of 44991\n",
            "batch: 40239 out of 44991\n",
            "batch: 40240 out of 44991\n",
            "batch: 40241 out of 44991\n",
            "batch: 40242 out of 44991\n",
            "batch: 40243 out of 44991\n",
            "batch: 40244 out of 44991\n",
            "batch: 40245 out of 44991\n",
            "batch: 40246 out of 44991\n",
            "batch: 40247 out of 44991\n",
            "batch: 40248 out of 44991\n",
            "batch: 40249 out of 44991\n",
            "batch: 40250 out of 44991\n",
            "batch: 40251 out of 44991\n",
            "batch: 40252 out of 44991\n",
            "batch: 40253 out of 44991\n",
            "batch: 40254 out of 44991\n",
            "batch: 40255 out of 44991\n",
            "batch: 40256 out of 44991\n",
            "batch: 40257 out of 44991\n",
            "batch: 40258 out of 44991\n",
            "batch: 40259 out of 44991\n",
            "batch: 40260 out of 44991\n",
            "batch: 40261 out of 44991\n",
            "batch: 40262 out of 44991\n",
            "batch: 40263 out of 44991\n",
            "batch: 40264 out of 44991\n",
            "batch: 40265 out of 44991\n",
            "batch: 40266 out of 44991\n",
            "batch: 40267 out of 44991\n",
            "batch: 40268 out of 44991\n",
            "batch: 40269 out of 44991\n",
            "batch: 40270 out of 44991\n",
            "batch: 40271 out of 44991\n",
            "batch: 40272 out of 44991\n",
            "batch: 40273 out of 44991\n",
            "batch: 40274 out of 44991\n",
            "batch: 40275 out of 44991\n",
            "batch: 40276 out of 44991\n",
            "batch: 40277 out of 44991\n",
            "batch: 40278 out of 44991\n",
            "batch: 40279 out of 44991\n",
            "batch: 40280 out of 44991\n",
            "batch: 40281 out of 44991\n",
            "batch: 40282 out of 44991\n",
            "batch: 40283 out of 44991\n",
            "batch: 40284 out of 44991\n",
            "batch: 40285 out of 44991\n",
            "batch: 40286 out of 44991\n",
            "batch: 40287 out of 44991\n",
            "batch: 40288 out of 44991\n",
            "batch: 40289 out of 44991\n",
            "batch: 40290 out of 44991\n",
            "batch: 40291 out of 44991\n",
            "batch: 40292 out of 44991\n",
            "batch: 40293 out of 44991\n",
            "batch: 40294 out of 44991\n",
            "batch: 40295 out of 44991\n",
            "batch: 40296 out of 44991\n",
            "batch: 40297 out of 44991\n",
            "batch: 40298 out of 44991\n",
            "batch: 40299 out of 44991\n",
            "batch: 40300 out of 44991\n",
            "batch: 40301 out of 44991\n",
            "batch: 40302 out of 44991\n",
            "batch: 40303 out of 44991\n",
            "batch: 40304 out of 44991\n",
            "batch: 40305 out of 44991\n",
            "batch: 40306 out of 44991\n",
            "batch: 40307 out of 44991\n",
            "batch: 40308 out of 44991\n",
            "batch: 40309 out of 44991\n",
            "batch: 40310 out of 44991\n",
            "batch: 40311 out of 44991\n",
            "batch: 40312 out of 44991\n",
            "batch: 40313 out of 44991\n",
            "batch: 40314 out of 44991\n",
            "batch: 40315 out of 44991\n",
            "batch: 40316 out of 44991\n",
            "batch: 40317 out of 44991\n",
            "batch: 40318 out of 44991\n",
            "batch: 40319 out of 44991\n",
            "batch: 40320 out of 44991\n",
            "batch: 40321 out of 44991\n",
            "batch: 40322 out of 44991\n",
            "batch: 40323 out of 44991\n",
            "batch: 40324 out of 44991\n",
            "batch: 40325 out of 44991\n",
            "batch: 40326 out of 44991\n",
            "batch: 40327 out of 44991\n",
            "batch: 40328 out of 44991\n",
            "batch: 40329 out of 44991\n",
            "batch: 40330 out of 44991\n",
            "batch: 40331 out of 44991\n",
            "batch: 40332 out of 44991\n",
            "batch: 40333 out of 44991\n",
            "batch: 40334 out of 44991\n",
            "batch: 40335 out of 44991\n",
            "batch: 40336 out of 44991\n",
            "batch: 40337 out of 44991\n",
            "batch: 40338 out of 44991\n",
            "batch: 40339 out of 44991\n",
            "batch: 40340 out of 44991\n",
            "batch: 40341 out of 44991\n",
            "batch: 40342 out of 44991\n",
            "batch: 40343 out of 44991\n",
            "batch: 40344 out of 44991\n",
            "batch: 40345 out of 44991\n",
            "batch: 40346 out of 44991\n",
            "batch: 40347 out of 44991\n",
            "batch: 40348 out of 44991\n",
            "batch: 40349 out of 44991\n",
            "batch: 40350 out of 44991\n",
            "batch: 40351 out of 44991\n",
            "batch: 40352 out of 44991\n",
            "batch: 40353 out of 44991\n",
            "batch: 40354 out of 44991\n",
            "batch: 40355 out of 44991\n",
            "batch: 40356 out of 44991\n",
            "batch: 40357 out of 44991\n",
            "batch: 40358 out of 44991\n",
            "batch: 40359 out of 44991\n",
            "batch: 40360 out of 44991\n",
            "batch: 40361 out of 44991\n",
            "batch: 40362 out of 44991\n",
            "batch: 40363 out of 44991\n",
            "batch: 40364 out of 44991\n",
            "batch: 40365 out of 44991\n",
            "batch: 40366 out of 44991\n",
            "batch: 40367 out of 44991\n",
            "batch: 40368 out of 44991\n",
            "batch: 40369 out of 44991\n",
            "batch: 40370 out of 44991\n",
            "batch: 40371 out of 44991\n",
            "batch: 40372 out of 44991\n",
            "batch: 40373 out of 44991\n",
            "batch: 40374 out of 44991\n",
            "batch: 40375 out of 44991\n",
            "batch: 40376 out of 44991\n",
            "batch: 40377 out of 44991\n",
            "batch: 40378 out of 44991\n",
            "batch: 40379 out of 44991\n",
            "batch: 40380 out of 44991\n",
            "batch: 40381 out of 44991\n",
            "batch: 40382 out of 44991\n",
            "batch: 40383 out of 44991\n",
            "batch: 40384 out of 44991\n",
            "batch: 40385 out of 44991\n",
            "batch: 40386 out of 44991\n",
            "batch: 40387 out of 44991\n",
            "batch: 40388 out of 44991\n",
            "batch: 40389 out of 44991\n",
            "batch: 40390 out of 44991\n",
            "batch: 40391 out of 44991\n",
            "batch: 40392 out of 44991\n",
            "batch: 40393 out of 44991\n",
            "batch: 40394 out of 44991\n",
            "batch: 40395 out of 44991\n",
            "batch: 40396 out of 44991\n",
            "batch: 40397 out of 44991\n",
            "batch: 40398 out of 44991\n",
            "batch: 40399 out of 44991\n",
            "batch: 40400 out of 44991\n",
            "batch: 40401 out of 44991\n",
            "batch: 40402 out of 44991\n",
            "batch: 40403 out of 44991\n",
            "batch: 40404 out of 44991\n",
            "batch: 40405 out of 44991\n",
            "batch: 40406 out of 44991\n",
            "batch: 40407 out of 44991\n",
            "batch: 40408 out of 44991\n",
            "batch: 40409 out of 44991\n",
            "batch: 40410 out of 44991\n",
            "batch: 40411 out of 44991\n",
            "batch: 40412 out of 44991\n",
            "batch: 40413 out of 44991\n",
            "batch: 40414 out of 44991\n",
            "batch: 40415 out of 44991\n",
            "batch: 40416 out of 44991\n",
            "batch: 40417 out of 44991\n",
            "batch: 40418 out of 44991\n",
            "batch: 40419 out of 44991\n",
            "batch: 40420 out of 44991\n",
            "batch: 40421 out of 44991\n",
            "batch: 40422 out of 44991\n",
            "batch: 40423 out of 44991\n",
            "batch: 40424 out of 44991\n",
            "batch: 40425 out of 44991\n",
            "batch: 40426 out of 44991\n",
            "batch: 40427 out of 44991\n",
            "batch: 40428 out of 44991\n",
            "batch: 40429 out of 44991\n",
            "batch: 40430 out of 44991\n",
            "batch: 40431 out of 44991\n",
            "batch: 40432 out of 44991\n",
            "batch: 40433 out of 44991\n",
            "batch: 40434 out of 44991\n",
            "batch: 40435 out of 44991\n",
            "batch: 40436 out of 44991\n",
            "batch: 40437 out of 44991\n",
            "batch: 40438 out of 44991\n",
            "batch: 40439 out of 44991\n",
            "batch: 40440 out of 44991\n",
            "batch: 40441 out of 44991\n",
            "batch: 40442 out of 44991\n",
            "batch: 40443 out of 44991\n",
            "batch: 40444 out of 44991\n",
            "batch: 40445 out of 44991\n",
            "batch: 40446 out of 44991\n",
            "batch: 40447 out of 44991\n",
            "batch: 40448 out of 44991\n",
            "batch: 40449 out of 44991\n",
            "batch: 40450 out of 44991\n",
            "batch: 40451 out of 44991\n",
            "batch: 40452 out of 44991\n",
            "batch: 40453 out of 44991\n",
            "batch: 40454 out of 44991\n",
            "batch: 40455 out of 44991\n",
            "batch: 40456 out of 44991\n",
            "batch: 40457 out of 44991\n",
            "batch: 40458 out of 44991\n",
            "batch: 40459 out of 44991\n",
            "batch: 40460 out of 44991\n",
            "batch: 40461 out of 44991\n",
            "batch: 40462 out of 44991\n",
            "batch: 40463 out of 44991\n",
            "batch: 40464 out of 44991\n",
            "batch: 40465 out of 44991\n",
            "batch: 40466 out of 44991\n",
            "batch: 40467 out of 44991\n",
            "batch: 40468 out of 44991\n",
            "batch: 40469 out of 44991\n",
            "batch: 40470 out of 44991\n",
            "batch: 40471 out of 44991\n",
            "batch: 40472 out of 44991\n",
            "batch: 40473 out of 44991\n",
            "batch: 40474 out of 44991\n",
            "batch: 40475 out of 44991\n",
            "batch: 40476 out of 44991\n",
            "batch: 40477 out of 44991\n",
            "batch: 40478 out of 44991\n",
            "batch: 40479 out of 44991\n",
            "batch: 40480 out of 44991\n",
            "batch: 40481 out of 44991\n",
            "batch: 40482 out of 44991\n",
            "batch: 40483 out of 44991\n",
            "batch: 40484 out of 44991\n",
            "batch: 40485 out of 44991\n",
            "batch: 40486 out of 44991\n",
            "batch: 40487 out of 44991\n",
            "batch: 40488 out of 44991\n",
            "batch: 40489 out of 44991\n",
            "batch: 40490 out of 44991\n",
            "batch: 40491 out of 44991\n",
            "batch: 40492 out of 44991\n",
            "batch: 40493 out of 44991\n",
            "batch: 40494 out of 44991\n",
            "batch: 40495 out of 44991\n",
            "batch: 40496 out of 44991\n",
            "batch: 40497 out of 44991\n",
            "batch: 40498 out of 44991\n",
            "batch: 40499 out of 44991\n",
            "batch: 40500 out of 44991\n",
            "batch: 40501 out of 44991\n",
            "batch: 40502 out of 44991\n",
            "batch: 40503 out of 44991\n",
            "batch: 40504 out of 44991\n",
            "batch: 40505 out of 44991\n",
            "batch: 40506 out of 44991\n",
            "batch: 40507 out of 44991\n",
            "batch: 40508 out of 44991\n",
            "batch: 40509 out of 44991\n",
            "batch: 40510 out of 44991\n",
            "batch: 40511 out of 44991\n",
            "batch: 40512 out of 44991\n",
            "batch: 40513 out of 44991\n",
            "batch: 40514 out of 44991\n",
            "batch: 40515 out of 44991\n",
            "batch: 40516 out of 44991\n",
            "batch: 40517 out of 44991\n",
            "batch: 40518 out of 44991\n",
            "batch: 40519 out of 44991\n",
            "batch: 40520 out of 44991\n",
            "batch: 40521 out of 44991\n",
            "batch: 40522 out of 44991\n",
            "batch: 40523 out of 44991\n",
            "batch: 40524 out of 44991\n",
            "batch: 40525 out of 44991\n",
            "batch: 40526 out of 44991\n",
            "batch: 40527 out of 44991\n",
            "batch: 40528 out of 44991\n",
            "batch: 40529 out of 44991\n",
            "batch: 40530 out of 44991\n",
            "batch: 40531 out of 44991\n",
            "batch: 40532 out of 44991\n",
            "batch: 40533 out of 44991\n",
            "batch: 40534 out of 44991\n",
            "batch: 40535 out of 44991\n",
            "batch: 40536 out of 44991\n",
            "batch: 40537 out of 44991\n",
            "batch: 40538 out of 44991\n",
            "batch: 40539 out of 44991\n",
            "batch: 40540 out of 44991\n",
            "batch: 40541 out of 44991\n",
            "batch: 40542 out of 44991\n",
            "batch: 40543 out of 44991\n",
            "batch: 40544 out of 44991\n",
            "batch: 40545 out of 44991\n",
            "batch: 40546 out of 44991\n",
            "batch: 40547 out of 44991\n",
            "batch: 40548 out of 44991\n",
            "batch: 40549 out of 44991\n",
            "batch: 40550 out of 44991\n",
            "batch: 40551 out of 44991\n",
            "batch: 40552 out of 44991\n",
            "batch: 40553 out of 44991\n",
            "batch: 40554 out of 44991\n",
            "batch: 40555 out of 44991\n",
            "batch: 40556 out of 44991\n",
            "batch: 40557 out of 44991\n",
            "batch: 40558 out of 44991\n",
            "batch: 40559 out of 44991\n",
            "batch: 40560 out of 44991\n",
            "batch: 40561 out of 44991\n",
            "batch: 40562 out of 44991\n",
            "batch: 40563 out of 44991\n",
            "batch: 40564 out of 44991\n",
            "batch: 40565 out of 44991\n",
            "batch: 40566 out of 44991\n",
            "batch: 40567 out of 44991\n",
            "batch: 40568 out of 44991\n",
            "batch: 40569 out of 44991\n",
            "batch: 40570 out of 44991\n",
            "batch: 40571 out of 44991\n",
            "batch: 40572 out of 44991\n",
            "batch: 40573 out of 44991\n",
            "batch: 40574 out of 44991\n",
            "batch: 40575 out of 44991\n",
            "batch: 40576 out of 44991\n",
            "batch: 40577 out of 44991\n",
            "batch: 40578 out of 44991\n",
            "batch: 40579 out of 44991\n",
            "batch: 40580 out of 44991\n",
            "batch: 40581 out of 44991\n",
            "batch: 40582 out of 44991\n",
            "batch: 40583 out of 44991\n",
            "batch: 40584 out of 44991\n",
            "batch: 40585 out of 44991\n",
            "batch: 40586 out of 44991\n",
            "batch: 40587 out of 44991\n",
            "batch: 40588 out of 44991\n",
            "batch: 40589 out of 44991\n",
            "batch: 40590 out of 44991\n",
            "batch: 40591 out of 44991\n",
            "batch: 40592 out of 44991\n",
            "batch: 40593 out of 44991\n",
            "batch: 40594 out of 44991\n",
            "batch: 40595 out of 44991\n",
            "batch: 40596 out of 44991\n",
            "batch: 40597 out of 44991\n",
            "batch: 40598 out of 44991\n",
            "batch: 40599 out of 44991\n",
            "batch: 40600 out of 44991\n",
            "batch: 40601 out of 44991\n",
            "batch: 40602 out of 44991\n",
            "batch: 40603 out of 44991\n",
            "batch: 40604 out of 44991\n",
            "batch: 40605 out of 44991\n",
            "batch: 40606 out of 44991\n",
            "batch: 40607 out of 44991\n",
            "batch: 40608 out of 44991\n",
            "batch: 40609 out of 44991\n",
            "batch: 40610 out of 44991\n",
            "batch: 40611 out of 44991\n",
            "batch: 40612 out of 44991\n",
            "batch: 40613 out of 44991\n",
            "batch: 40614 out of 44991\n",
            "batch: 40615 out of 44991\n",
            "batch: 40616 out of 44991\n",
            "batch: 40617 out of 44991\n",
            "batch: 40618 out of 44991\n",
            "batch: 40619 out of 44991\n",
            "batch: 40620 out of 44991\n",
            "batch: 40621 out of 44991\n",
            "batch: 40622 out of 44991\n",
            "batch: 40623 out of 44991\n",
            "batch: 40624 out of 44991\n",
            "batch: 40625 out of 44991\n",
            "batch: 40626 out of 44991\n",
            "batch: 40627 out of 44991\n",
            "batch: 40628 out of 44991\n",
            "batch: 40629 out of 44991\n",
            "batch: 40630 out of 44991\n",
            "batch: 40631 out of 44991\n",
            "batch: 40632 out of 44991\n",
            "batch: 40633 out of 44991\n",
            "batch: 40634 out of 44991\n",
            "batch: 40635 out of 44991\n",
            "batch: 40636 out of 44991\n",
            "batch: 40637 out of 44991\n",
            "batch: 40638 out of 44991\n",
            "batch: 40639 out of 44991\n",
            "batch: 40640 out of 44991\n",
            "batch: 40641 out of 44991\n",
            "batch: 40642 out of 44991\n",
            "batch: 40643 out of 44991\n",
            "batch: 40644 out of 44991\n",
            "batch: 40645 out of 44991\n",
            "batch: 40646 out of 44991\n",
            "batch: 40647 out of 44991\n",
            "batch: 40648 out of 44991\n",
            "batch: 40649 out of 44991\n",
            "batch: 40650 out of 44991\n",
            "batch: 40651 out of 44991\n",
            "batch: 40652 out of 44991\n",
            "batch: 40653 out of 44991\n",
            "batch: 40654 out of 44991\n",
            "batch: 40655 out of 44991\n",
            "batch: 40656 out of 44991\n",
            "batch: 40657 out of 44991\n",
            "batch: 40658 out of 44991\n",
            "batch: 40659 out of 44991\n",
            "batch: 40660 out of 44991\n",
            "batch: 40661 out of 44991\n",
            "batch: 40662 out of 44991\n",
            "batch: 40663 out of 44991\n",
            "batch: 40664 out of 44991\n",
            "batch: 40665 out of 44991\n",
            "batch: 40666 out of 44991\n",
            "batch: 40667 out of 44991\n",
            "batch: 40668 out of 44991\n",
            "batch: 40669 out of 44991\n",
            "batch: 40670 out of 44991\n",
            "batch: 40671 out of 44991\n",
            "batch: 40672 out of 44991\n",
            "batch: 40673 out of 44991\n",
            "batch: 40674 out of 44991\n",
            "batch: 40675 out of 44991\n",
            "batch: 40676 out of 44991\n",
            "batch: 40677 out of 44991\n",
            "batch: 40678 out of 44991\n",
            "batch: 40679 out of 44991\n",
            "batch: 40680 out of 44991\n",
            "batch: 40681 out of 44991\n",
            "batch: 40682 out of 44991\n",
            "batch: 40683 out of 44991\n",
            "batch: 40684 out of 44991\n",
            "batch: 40685 out of 44991\n",
            "batch: 40686 out of 44991\n",
            "batch: 40687 out of 44991\n",
            "batch: 40688 out of 44991\n",
            "batch: 40689 out of 44991\n",
            "batch: 40690 out of 44991\n",
            "batch: 40691 out of 44991\n",
            "batch: 40692 out of 44991\n",
            "batch: 40693 out of 44991\n",
            "batch: 40694 out of 44991\n",
            "batch: 40695 out of 44991\n",
            "batch: 40696 out of 44991\n",
            "batch: 40697 out of 44991\n",
            "batch: 40698 out of 44991\n",
            "batch: 40699 out of 44991\n",
            "batch: 40700 out of 44991\n",
            "batch: 40701 out of 44991\n",
            "batch: 40702 out of 44991\n",
            "batch: 40703 out of 44991\n",
            "batch: 40704 out of 44991\n",
            "batch: 40705 out of 44991\n",
            "batch: 40706 out of 44991\n",
            "batch: 40707 out of 44991\n",
            "batch: 40708 out of 44991\n",
            "batch: 40709 out of 44991\n",
            "batch: 40710 out of 44991\n",
            "batch: 40711 out of 44991\n",
            "batch: 40712 out of 44991\n",
            "batch: 40713 out of 44991\n",
            "batch: 40714 out of 44991\n",
            "batch: 40715 out of 44991\n",
            "batch: 40716 out of 44991\n",
            "batch: 40717 out of 44991\n",
            "batch: 40718 out of 44991\n",
            "batch: 40719 out of 44991\n",
            "batch: 40720 out of 44991\n",
            "batch: 40721 out of 44991\n",
            "batch: 40722 out of 44991\n",
            "batch: 40723 out of 44991\n",
            "batch: 40724 out of 44991\n",
            "batch: 40725 out of 44991\n",
            "batch: 40726 out of 44991\n",
            "batch: 40727 out of 44991\n",
            "batch: 40728 out of 44991\n",
            "batch: 40729 out of 44991\n",
            "batch: 40730 out of 44991\n",
            "batch: 40731 out of 44991\n",
            "batch: 40732 out of 44991\n",
            "batch: 40733 out of 44991\n",
            "batch: 40734 out of 44991\n",
            "batch: 40735 out of 44991\n",
            "batch: 40736 out of 44991\n",
            "batch: 40737 out of 44991\n",
            "batch: 40738 out of 44991\n",
            "batch: 40739 out of 44991\n",
            "batch: 40740 out of 44991\n",
            "batch: 40741 out of 44991\n",
            "batch: 40742 out of 44991\n",
            "batch: 40743 out of 44991\n",
            "batch: 40744 out of 44991\n",
            "batch: 40745 out of 44991\n",
            "batch: 40746 out of 44991\n",
            "batch: 40747 out of 44991\n",
            "batch: 40748 out of 44991\n",
            "batch: 40749 out of 44991\n",
            "batch: 40750 out of 44991\n",
            "batch: 40751 out of 44991\n",
            "batch: 40752 out of 44991\n",
            "batch: 40753 out of 44991\n",
            "batch: 40754 out of 44991\n",
            "batch: 40755 out of 44991\n",
            "batch: 40756 out of 44991\n",
            "batch: 40757 out of 44991\n",
            "batch: 40758 out of 44991\n",
            "batch: 40759 out of 44991\n",
            "batch: 40760 out of 44991\n",
            "batch: 40761 out of 44991\n",
            "batch: 40762 out of 44991\n",
            "batch: 40763 out of 44991\n",
            "batch: 40764 out of 44991\n",
            "batch: 40765 out of 44991\n",
            "batch: 40766 out of 44991\n",
            "batch: 40767 out of 44991\n",
            "batch: 40768 out of 44991\n",
            "batch: 40769 out of 44991\n",
            "batch: 40770 out of 44991\n",
            "batch: 40771 out of 44991\n",
            "batch: 40772 out of 44991\n",
            "batch: 40773 out of 44991\n",
            "batch: 40774 out of 44991\n",
            "batch: 40775 out of 44991\n",
            "batch: 40776 out of 44991\n",
            "batch: 40777 out of 44991\n",
            "batch: 40778 out of 44991\n",
            "batch: 40779 out of 44991\n",
            "batch: 40780 out of 44991\n",
            "batch: 40781 out of 44991\n",
            "batch: 40782 out of 44991\n",
            "batch: 40783 out of 44991\n",
            "batch: 40784 out of 44991\n",
            "batch: 40785 out of 44991\n",
            "batch: 40786 out of 44991\n",
            "batch: 40787 out of 44991\n",
            "batch: 40788 out of 44991\n",
            "batch: 40789 out of 44991\n",
            "batch: 40790 out of 44991\n",
            "batch: 40791 out of 44991\n",
            "batch: 40792 out of 44991\n",
            "batch: 40793 out of 44991\n",
            "batch: 40794 out of 44991\n",
            "batch: 40795 out of 44991\n",
            "batch: 40796 out of 44991\n",
            "batch: 40797 out of 44991\n",
            "batch: 40798 out of 44991\n",
            "batch: 40799 out of 44991\n",
            "batch: 40800 out of 44991\n",
            "batch: 40801 out of 44991\n",
            "batch: 40802 out of 44991\n",
            "batch: 40803 out of 44991\n",
            "batch: 40804 out of 44991\n",
            "batch: 40805 out of 44991\n",
            "batch: 40806 out of 44991\n",
            "batch: 40807 out of 44991\n",
            "batch: 40808 out of 44991\n",
            "batch: 40809 out of 44991\n",
            "batch: 40810 out of 44991\n",
            "batch: 40811 out of 44991\n",
            "batch: 40812 out of 44991\n",
            "batch: 40813 out of 44991\n",
            "batch: 40814 out of 44991\n",
            "batch: 40815 out of 44991\n",
            "batch: 40816 out of 44991\n",
            "batch: 40817 out of 44991\n",
            "batch: 40818 out of 44991\n",
            "batch: 40819 out of 44991\n",
            "batch: 40820 out of 44991\n",
            "batch: 40821 out of 44991\n",
            "batch: 40822 out of 44991\n",
            "batch: 40823 out of 44991\n",
            "batch: 40824 out of 44991\n",
            "batch: 40825 out of 44991\n",
            "batch: 40826 out of 44991\n",
            "batch: 40827 out of 44991\n",
            "batch: 40828 out of 44991\n",
            "batch: 40829 out of 44991\n",
            "batch: 40830 out of 44991\n",
            "batch: 40831 out of 44991\n",
            "batch: 40832 out of 44991\n",
            "batch: 40833 out of 44991\n",
            "batch: 40834 out of 44991\n",
            "batch: 40835 out of 44991\n",
            "batch: 40836 out of 44991\n",
            "batch: 40837 out of 44991\n",
            "batch: 40838 out of 44991\n",
            "batch: 40839 out of 44991\n",
            "batch: 40840 out of 44991\n",
            "batch: 40841 out of 44991\n",
            "batch: 40842 out of 44991\n",
            "batch: 40843 out of 44991\n",
            "batch: 40844 out of 44991\n",
            "batch: 40845 out of 44991\n",
            "batch: 40846 out of 44991\n",
            "batch: 40847 out of 44991\n",
            "batch: 40848 out of 44991\n",
            "batch: 40849 out of 44991\n",
            "batch: 40850 out of 44991\n",
            "batch: 40851 out of 44991\n",
            "batch: 40852 out of 44991\n",
            "batch: 40853 out of 44991\n",
            "batch: 40854 out of 44991\n",
            "batch: 40855 out of 44991\n",
            "batch: 40856 out of 44991\n",
            "batch: 40857 out of 44991\n",
            "batch: 40858 out of 44991\n",
            "batch: 40859 out of 44991\n",
            "batch: 40860 out of 44991\n",
            "batch: 40861 out of 44991\n",
            "batch: 40862 out of 44991\n",
            "batch: 40863 out of 44991\n",
            "batch: 40864 out of 44991\n",
            "batch: 40865 out of 44991\n",
            "batch: 40866 out of 44991\n",
            "batch: 40867 out of 44991\n",
            "batch: 40868 out of 44991\n",
            "batch: 40869 out of 44991\n",
            "batch: 40870 out of 44991\n",
            "batch: 40871 out of 44991\n",
            "batch: 40872 out of 44991\n",
            "batch: 40873 out of 44991\n",
            "batch: 40874 out of 44991\n",
            "batch: 40875 out of 44991\n",
            "batch: 40876 out of 44991\n",
            "batch: 40877 out of 44991\n",
            "batch: 40878 out of 44991\n",
            "batch: 40879 out of 44991\n",
            "batch: 40880 out of 44991\n",
            "batch: 40881 out of 44991\n",
            "batch: 40882 out of 44991\n",
            "batch: 40883 out of 44991\n",
            "batch: 40884 out of 44991\n",
            "batch: 40885 out of 44991\n",
            "batch: 40886 out of 44991\n",
            "batch: 40887 out of 44991\n",
            "batch: 40888 out of 44991\n",
            "batch: 40889 out of 44991\n",
            "batch: 40890 out of 44991\n",
            "batch: 40891 out of 44991\n",
            "batch: 40892 out of 44991\n",
            "batch: 40893 out of 44991\n",
            "batch: 40894 out of 44991\n",
            "batch: 40895 out of 44991\n",
            "batch: 40896 out of 44991\n",
            "batch: 40897 out of 44991\n",
            "batch: 40898 out of 44991\n",
            "batch: 40899 out of 44991\n",
            "batch: 40900 out of 44991\n",
            "batch: 40901 out of 44991\n",
            "batch: 40902 out of 44991\n",
            "batch: 40903 out of 44991\n",
            "batch: 40904 out of 44991\n",
            "batch: 40905 out of 44991\n",
            "batch: 40906 out of 44991\n",
            "batch: 40907 out of 44991\n",
            "batch: 40908 out of 44991\n",
            "batch: 40909 out of 44991\n",
            "batch: 40910 out of 44991\n",
            "batch: 40911 out of 44991\n",
            "batch: 40912 out of 44991\n",
            "batch: 40913 out of 44991\n",
            "batch: 40914 out of 44991\n",
            "batch: 40915 out of 44991\n",
            "batch: 40916 out of 44991\n",
            "batch: 40917 out of 44991\n",
            "batch: 40918 out of 44991\n",
            "batch: 40919 out of 44991\n",
            "batch: 40920 out of 44991\n",
            "batch: 40921 out of 44991\n",
            "batch: 40922 out of 44991\n",
            "batch: 40923 out of 44991\n",
            "batch: 40924 out of 44991\n",
            "batch: 40925 out of 44991\n",
            "batch: 40926 out of 44991\n",
            "batch: 40927 out of 44991\n",
            "batch: 40928 out of 44991\n",
            "batch: 40929 out of 44991\n",
            "batch: 40930 out of 44991\n",
            "batch: 40931 out of 44991\n",
            "batch: 40932 out of 44991\n",
            "batch: 40933 out of 44991\n",
            "batch: 40934 out of 44991\n",
            "batch: 40935 out of 44991\n",
            "batch: 40936 out of 44991\n",
            "batch: 40937 out of 44991\n",
            "batch: 40938 out of 44991\n",
            "batch: 40939 out of 44991\n",
            "batch: 40940 out of 44991\n",
            "batch: 40941 out of 44991\n",
            "batch: 40942 out of 44991\n",
            "batch: 40943 out of 44991\n",
            "batch: 40944 out of 44991\n",
            "batch: 40945 out of 44991\n",
            "batch: 40946 out of 44991\n",
            "batch: 40947 out of 44991\n",
            "batch: 40948 out of 44991\n",
            "batch: 40949 out of 44991\n",
            "batch: 40950 out of 44991\n",
            "batch: 40951 out of 44991\n",
            "batch: 40952 out of 44991\n",
            "batch: 40953 out of 44991\n",
            "batch: 40954 out of 44991\n",
            "batch: 40955 out of 44991\n",
            "batch: 40956 out of 44991\n",
            "batch: 40957 out of 44991\n",
            "batch: 40958 out of 44991\n",
            "batch: 40959 out of 44991\n",
            "batch: 40960 out of 44991\n",
            "batch: 40961 out of 44991\n",
            "batch: 40962 out of 44991\n",
            "batch: 40963 out of 44991\n",
            "batch: 40964 out of 44991\n",
            "batch: 40965 out of 44991\n",
            "batch: 40966 out of 44991\n",
            "batch: 40967 out of 44991\n",
            "batch: 40968 out of 44991\n",
            "batch: 40969 out of 44991\n",
            "batch: 40970 out of 44991\n",
            "batch: 40971 out of 44991\n",
            "batch: 40972 out of 44991\n",
            "batch: 40973 out of 44991\n",
            "batch: 40974 out of 44991\n",
            "batch: 40975 out of 44991\n",
            "batch: 40976 out of 44991\n",
            "batch: 40977 out of 44991\n",
            "batch: 40978 out of 44991\n",
            "batch: 40979 out of 44991\n",
            "batch: 40980 out of 44991\n",
            "batch: 40981 out of 44991\n",
            "batch: 40982 out of 44991\n",
            "batch: 40983 out of 44991\n",
            "batch: 40984 out of 44991\n",
            "batch: 40985 out of 44991\n",
            "batch: 40986 out of 44991\n",
            "batch: 40987 out of 44991\n",
            "batch: 40988 out of 44991\n",
            "batch: 40989 out of 44991\n",
            "batch: 40990 out of 44991\n",
            "batch: 40991 out of 44991\n",
            "batch: 40992 out of 44991\n",
            "batch: 40993 out of 44991\n",
            "batch: 40994 out of 44991\n",
            "batch: 40995 out of 44991\n",
            "batch: 40996 out of 44991\n",
            "batch: 40997 out of 44991\n",
            "batch: 40998 out of 44991\n",
            "batch: 40999 out of 44991\n",
            "batch: 41000 out of 44991\n",
            "batch: 41001 out of 44991\n",
            "batch: 41002 out of 44991\n",
            "batch: 41003 out of 44991\n",
            "batch: 41004 out of 44991\n",
            "batch: 41005 out of 44991\n",
            "batch: 41006 out of 44991\n",
            "batch: 41007 out of 44991\n",
            "batch: 41008 out of 44991\n",
            "batch: 41009 out of 44991\n",
            "batch: 41010 out of 44991\n",
            "batch: 41011 out of 44991\n",
            "batch: 41012 out of 44991\n",
            "batch: 41013 out of 44991\n",
            "batch: 41014 out of 44991\n",
            "batch: 41015 out of 44991\n",
            "batch: 41016 out of 44991\n",
            "batch: 41017 out of 44991\n",
            "batch: 41018 out of 44991\n",
            "batch: 41019 out of 44991\n",
            "batch: 41020 out of 44991\n",
            "batch: 41021 out of 44991\n",
            "batch: 41022 out of 44991\n",
            "batch: 41023 out of 44991\n",
            "batch: 41024 out of 44991\n",
            "batch: 41025 out of 44991\n",
            "batch: 41026 out of 44991\n",
            "batch: 41027 out of 44991\n",
            "batch: 41028 out of 44991\n",
            "batch: 41029 out of 44991\n",
            "batch: 41030 out of 44991\n",
            "batch: 41031 out of 44991\n",
            "batch: 41032 out of 44991\n",
            "batch: 41033 out of 44991\n",
            "batch: 41034 out of 44991\n",
            "batch: 41035 out of 44991\n",
            "batch: 41036 out of 44991\n",
            "batch: 41037 out of 44991\n",
            "batch: 41038 out of 44991\n",
            "batch: 41039 out of 44991\n",
            "batch: 41040 out of 44991\n",
            "batch: 41041 out of 44991\n",
            "batch: 41042 out of 44991\n",
            "batch: 41043 out of 44991\n",
            "batch: 41044 out of 44991\n",
            "batch: 41045 out of 44991\n",
            "batch: 41046 out of 44991\n",
            "batch: 41047 out of 44991\n",
            "batch: 41048 out of 44991\n",
            "batch: 41049 out of 44991\n",
            "batch: 41050 out of 44991\n",
            "batch: 41051 out of 44991\n",
            "batch: 41052 out of 44991\n",
            "batch: 41053 out of 44991\n",
            "batch: 41054 out of 44991\n",
            "batch: 41055 out of 44991\n",
            "batch: 41056 out of 44991\n",
            "batch: 41057 out of 44991\n",
            "batch: 41058 out of 44991\n",
            "batch: 41059 out of 44991\n",
            "batch: 41060 out of 44991\n",
            "batch: 41061 out of 44991\n",
            "batch: 41062 out of 44991\n",
            "batch: 41063 out of 44991\n",
            "batch: 41064 out of 44991\n",
            "batch: 41065 out of 44991\n",
            "batch: 41066 out of 44991\n",
            "batch: 41067 out of 44991\n",
            "batch: 41068 out of 44991\n",
            "batch: 41069 out of 44991\n",
            "batch: 41070 out of 44991\n",
            "batch: 41071 out of 44991\n",
            "batch: 41072 out of 44991\n",
            "batch: 41073 out of 44991\n",
            "batch: 41074 out of 44991\n",
            "batch: 41075 out of 44991\n",
            "batch: 41076 out of 44991\n",
            "batch: 41077 out of 44991\n",
            "batch: 41078 out of 44991\n",
            "batch: 41079 out of 44991\n",
            "batch: 41080 out of 44991\n",
            "batch: 41081 out of 44991\n",
            "batch: 41082 out of 44991\n",
            "batch: 41083 out of 44991\n",
            "batch: 41084 out of 44991\n",
            "batch: 41085 out of 44991\n",
            "batch: 41086 out of 44991\n",
            "batch: 41087 out of 44991\n",
            "batch: 41088 out of 44991\n",
            "batch: 41089 out of 44991\n",
            "batch: 41090 out of 44991\n",
            "batch: 41091 out of 44991\n",
            "batch: 41092 out of 44991\n",
            "batch: 41093 out of 44991\n",
            "batch: 41094 out of 44991\n",
            "batch: 41095 out of 44991\n",
            "batch: 41096 out of 44991\n",
            "batch: 41097 out of 44991\n",
            "batch: 41098 out of 44991\n",
            "batch: 41099 out of 44991\n",
            "batch: 41100 out of 44991\n",
            "batch: 41101 out of 44991\n",
            "batch: 41102 out of 44991\n",
            "batch: 41103 out of 44991\n",
            "batch: 41104 out of 44991\n",
            "batch: 41105 out of 44991\n",
            "batch: 41106 out of 44991\n",
            "batch: 41107 out of 44991\n",
            "batch: 41108 out of 44991\n",
            "batch: 41109 out of 44991\n",
            "batch: 41110 out of 44991\n",
            "batch: 41111 out of 44991\n",
            "batch: 41112 out of 44991\n",
            "batch: 41113 out of 44991\n",
            "batch: 41114 out of 44991\n",
            "batch: 41115 out of 44991\n",
            "batch: 41116 out of 44991\n",
            "batch: 41117 out of 44991\n",
            "batch: 41118 out of 44991\n",
            "batch: 41119 out of 44991\n",
            "batch: 41120 out of 44991\n",
            "batch: 41121 out of 44991\n",
            "batch: 41122 out of 44991\n",
            "batch: 41123 out of 44991\n",
            "batch: 41124 out of 44991\n",
            "batch: 41125 out of 44991\n",
            "batch: 41126 out of 44991\n",
            "batch: 41127 out of 44991\n",
            "batch: 41128 out of 44991\n",
            "batch: 41129 out of 44991\n",
            "batch: 41130 out of 44991\n",
            "batch: 41131 out of 44991\n",
            "batch: 41132 out of 44991\n",
            "batch: 41133 out of 44991\n",
            "batch: 41134 out of 44991\n",
            "batch: 41135 out of 44991\n",
            "batch: 41136 out of 44991\n",
            "batch: 41137 out of 44991\n",
            "batch: 41138 out of 44991\n",
            "batch: 41139 out of 44991\n",
            "batch: 41140 out of 44991\n",
            "batch: 41141 out of 44991\n",
            "batch: 41142 out of 44991\n",
            "batch: 41143 out of 44991\n",
            "batch: 41144 out of 44991\n",
            "batch: 41145 out of 44991\n",
            "batch: 41146 out of 44991\n",
            "batch: 41147 out of 44991\n",
            "batch: 41148 out of 44991\n",
            "batch: 41149 out of 44991\n",
            "batch: 41150 out of 44991\n",
            "batch: 41151 out of 44991\n",
            "batch: 41152 out of 44991\n",
            "batch: 41153 out of 44991\n",
            "batch: 41154 out of 44991\n",
            "batch: 41155 out of 44991\n",
            "batch: 41156 out of 44991\n",
            "batch: 41157 out of 44991\n",
            "batch: 41158 out of 44991\n",
            "batch: 41159 out of 44991\n",
            "batch: 41160 out of 44991\n",
            "batch: 41161 out of 44991\n",
            "batch: 41162 out of 44991\n",
            "batch: 41163 out of 44991\n",
            "batch: 41164 out of 44991\n",
            "batch: 41165 out of 44991\n",
            "batch: 41166 out of 44991\n",
            "batch: 41167 out of 44991\n",
            "batch: 41168 out of 44991\n",
            "batch: 41169 out of 44991\n",
            "batch: 41170 out of 44991\n",
            "batch: 41171 out of 44991\n",
            "batch: 41172 out of 44991\n",
            "batch: 41173 out of 44991\n",
            "batch: 41174 out of 44991\n",
            "batch: 41175 out of 44991\n",
            "batch: 41176 out of 44991\n",
            "batch: 41177 out of 44991\n",
            "batch: 41178 out of 44991\n",
            "batch: 41179 out of 44991\n",
            "batch: 41180 out of 44991\n",
            "batch: 41181 out of 44991\n",
            "batch: 41182 out of 44991\n",
            "batch: 41183 out of 44991\n",
            "batch: 41184 out of 44991\n",
            "batch: 41185 out of 44991\n",
            "batch: 41186 out of 44991\n",
            "batch: 41187 out of 44991\n",
            "batch: 41188 out of 44991\n",
            "batch: 41189 out of 44991\n",
            "batch: 41190 out of 44991\n",
            "batch: 41191 out of 44991\n",
            "batch: 41192 out of 44991\n",
            "batch: 41193 out of 44991\n",
            "batch: 41194 out of 44991\n",
            "batch: 41195 out of 44991\n",
            "batch: 41196 out of 44991\n",
            "batch: 41197 out of 44991\n",
            "batch: 41198 out of 44991\n",
            "batch: 41199 out of 44991\n",
            "batch: 41200 out of 44991\n",
            "batch: 41201 out of 44991\n",
            "batch: 41202 out of 44991\n",
            "batch: 41203 out of 44991\n",
            "batch: 41204 out of 44991\n",
            "batch: 41205 out of 44991\n",
            "batch: 41206 out of 44991\n",
            "batch: 41207 out of 44991\n",
            "batch: 41208 out of 44991\n",
            "batch: 41209 out of 44991\n",
            "batch: 41210 out of 44991\n",
            "batch: 41211 out of 44991\n",
            "batch: 41212 out of 44991\n",
            "batch: 41213 out of 44991\n",
            "batch: 41214 out of 44991\n",
            "batch: 41215 out of 44991\n",
            "batch: 41216 out of 44991\n",
            "batch: 41217 out of 44991\n",
            "batch: 41218 out of 44991\n",
            "batch: 41219 out of 44991\n",
            "batch: 41220 out of 44991\n",
            "batch: 41221 out of 44991\n",
            "batch: 41222 out of 44991\n",
            "batch: 41223 out of 44991\n",
            "batch: 41224 out of 44991\n",
            "batch: 41225 out of 44991\n",
            "batch: 41226 out of 44991\n",
            "batch: 41227 out of 44991\n",
            "batch: 41228 out of 44991\n",
            "batch: 41229 out of 44991\n",
            "batch: 41230 out of 44991\n",
            "batch: 41231 out of 44991\n",
            "batch: 41232 out of 44991\n",
            "batch: 41233 out of 44991\n",
            "batch: 41234 out of 44991\n",
            "batch: 41235 out of 44991\n",
            "batch: 41236 out of 44991\n",
            "batch: 41237 out of 44991\n",
            "batch: 41238 out of 44991\n",
            "batch: 41239 out of 44991\n",
            "batch: 41240 out of 44991\n",
            "batch: 41241 out of 44991\n",
            "batch: 41242 out of 44991\n",
            "batch: 41243 out of 44991\n",
            "batch: 41244 out of 44991\n",
            "batch: 41245 out of 44991\n",
            "batch: 41246 out of 44991\n",
            "batch: 41247 out of 44991\n",
            "batch: 41248 out of 44991\n",
            "batch: 41249 out of 44991\n",
            "batch: 41250 out of 44991\n",
            "batch: 41251 out of 44991\n",
            "batch: 41252 out of 44991\n",
            "batch: 41253 out of 44991\n",
            "batch: 41254 out of 44991\n",
            "batch: 41255 out of 44991\n",
            "batch: 41256 out of 44991\n",
            "batch: 41257 out of 44991\n",
            "batch: 41258 out of 44991\n",
            "batch: 41259 out of 44991\n",
            "batch: 41260 out of 44991\n",
            "batch: 41261 out of 44991\n",
            "batch: 41262 out of 44991\n",
            "batch: 41263 out of 44991\n",
            "batch: 41264 out of 44991\n",
            "batch: 41265 out of 44991\n",
            "batch: 41266 out of 44991\n",
            "batch: 41267 out of 44991\n",
            "batch: 41268 out of 44991\n",
            "batch: 41269 out of 44991\n",
            "batch: 41270 out of 44991\n",
            "batch: 41271 out of 44991\n",
            "batch: 41272 out of 44991\n",
            "batch: 41273 out of 44991\n",
            "batch: 41274 out of 44991\n",
            "batch: 41275 out of 44991\n",
            "batch: 41276 out of 44991\n",
            "batch: 41277 out of 44991\n",
            "batch: 41278 out of 44991\n",
            "batch: 41279 out of 44991\n",
            "batch: 41280 out of 44991\n",
            "batch: 41281 out of 44991\n",
            "batch: 41282 out of 44991\n",
            "batch: 41283 out of 44991\n",
            "batch: 41284 out of 44991\n",
            "batch: 41285 out of 44991\n",
            "batch: 41286 out of 44991\n",
            "batch: 41287 out of 44991\n",
            "batch: 41288 out of 44991\n",
            "batch: 41289 out of 44991\n",
            "batch: 41290 out of 44991\n",
            "batch: 41291 out of 44991\n",
            "batch: 41292 out of 44991\n",
            "batch: 41293 out of 44991\n",
            "batch: 41294 out of 44991\n",
            "batch: 41295 out of 44991\n",
            "batch: 41296 out of 44991\n",
            "batch: 41297 out of 44991\n",
            "batch: 41298 out of 44991\n",
            "batch: 41299 out of 44991\n",
            "batch: 41300 out of 44991\n",
            "batch: 41301 out of 44991\n",
            "batch: 41302 out of 44991\n",
            "batch: 41303 out of 44991\n",
            "batch: 41304 out of 44991\n",
            "batch: 41305 out of 44991\n",
            "batch: 41306 out of 44991\n",
            "batch: 41307 out of 44991\n",
            "batch: 41308 out of 44991\n",
            "batch: 41309 out of 44991\n",
            "batch: 41310 out of 44991\n",
            "batch: 41311 out of 44991\n",
            "batch: 41312 out of 44991\n",
            "batch: 41313 out of 44991\n",
            "batch: 41314 out of 44991\n",
            "batch: 41315 out of 44991\n",
            "batch: 41316 out of 44991\n",
            "batch: 41317 out of 44991\n",
            "batch: 41318 out of 44991\n",
            "batch: 41319 out of 44991\n",
            "batch: 41320 out of 44991\n",
            "batch: 41321 out of 44991\n",
            "batch: 41322 out of 44991\n",
            "batch: 41323 out of 44991\n",
            "batch: 41324 out of 44991\n",
            "batch: 41325 out of 44991\n",
            "batch: 41326 out of 44991\n",
            "batch: 41327 out of 44991\n",
            "batch: 41328 out of 44991\n",
            "batch: 41329 out of 44991\n",
            "batch: 41330 out of 44991\n",
            "batch: 41331 out of 44991\n",
            "batch: 41332 out of 44991\n",
            "batch: 41333 out of 44991\n",
            "batch: 41334 out of 44991\n",
            "batch: 41335 out of 44991\n",
            "batch: 41336 out of 44991\n",
            "batch: 41337 out of 44991\n",
            "batch: 41338 out of 44991\n",
            "batch: 41339 out of 44991\n",
            "batch: 41340 out of 44991\n",
            "batch: 41341 out of 44991\n",
            "batch: 41342 out of 44991\n",
            "batch: 41343 out of 44991\n",
            "batch: 41344 out of 44991\n",
            "batch: 41345 out of 44991\n",
            "batch: 41346 out of 44991\n",
            "batch: 41347 out of 44991\n",
            "batch: 41348 out of 44991\n",
            "batch: 41349 out of 44991\n",
            "batch: 41350 out of 44991\n",
            "batch: 41351 out of 44991\n",
            "batch: 41352 out of 44991\n",
            "batch: 41353 out of 44991\n",
            "batch: 41354 out of 44991\n",
            "batch: 41355 out of 44991\n",
            "batch: 41356 out of 44991\n",
            "batch: 41357 out of 44991\n",
            "batch: 41358 out of 44991\n",
            "batch: 41359 out of 44991\n",
            "batch: 41360 out of 44991\n",
            "batch: 41361 out of 44991\n",
            "batch: 41362 out of 44991\n",
            "batch: 41363 out of 44991\n",
            "batch: 41364 out of 44991\n",
            "batch: 41365 out of 44991\n",
            "batch: 41366 out of 44991\n",
            "batch: 41367 out of 44991\n",
            "batch: 41368 out of 44991\n",
            "batch: 41369 out of 44991\n",
            "batch: 41370 out of 44991\n",
            "batch: 41371 out of 44991\n",
            "batch: 41372 out of 44991\n",
            "batch: 41373 out of 44991\n",
            "batch: 41374 out of 44991\n",
            "batch: 41375 out of 44991\n",
            "batch: 41376 out of 44991\n",
            "batch: 41377 out of 44991\n",
            "batch: 41378 out of 44991\n",
            "batch: 41379 out of 44991\n",
            "batch: 41380 out of 44991\n",
            "batch: 41381 out of 44991\n",
            "batch: 41382 out of 44991\n",
            "batch: 41383 out of 44991\n",
            "batch: 41384 out of 44991\n",
            "batch: 41385 out of 44991\n",
            "batch: 41386 out of 44991\n",
            "batch: 41387 out of 44991\n",
            "batch: 41388 out of 44991\n",
            "batch: 41389 out of 44991\n",
            "batch: 41390 out of 44991\n",
            "batch: 41391 out of 44991\n",
            "batch: 41392 out of 44991\n",
            "batch: 41393 out of 44991\n",
            "batch: 41394 out of 44991\n",
            "batch: 41395 out of 44991\n",
            "batch: 41396 out of 44991\n",
            "batch: 41397 out of 44991\n",
            "batch: 41398 out of 44991\n",
            "batch: 41399 out of 44991\n",
            "batch: 41400 out of 44991\n",
            "batch: 41401 out of 44991\n",
            "batch: 41402 out of 44991\n",
            "batch: 41403 out of 44991\n",
            "batch: 41404 out of 44991\n",
            "batch: 41405 out of 44991\n",
            "batch: 41406 out of 44991\n",
            "batch: 41407 out of 44991\n",
            "batch: 41408 out of 44991\n",
            "batch: 41409 out of 44991\n",
            "batch: 41410 out of 44991\n",
            "batch: 41411 out of 44991\n",
            "batch: 41412 out of 44991\n",
            "batch: 41413 out of 44991\n",
            "batch: 41414 out of 44991\n",
            "batch: 41415 out of 44991\n",
            "batch: 41416 out of 44991\n",
            "batch: 41417 out of 44991\n",
            "batch: 41418 out of 44991\n",
            "batch: 41419 out of 44991\n",
            "batch: 41420 out of 44991\n",
            "batch: 41421 out of 44991\n",
            "batch: 41422 out of 44991\n",
            "batch: 41423 out of 44991\n",
            "batch: 41424 out of 44991\n",
            "batch: 41425 out of 44991\n",
            "batch: 41426 out of 44991\n",
            "batch: 41427 out of 44991\n",
            "batch: 41428 out of 44991\n",
            "batch: 41429 out of 44991\n",
            "batch: 41430 out of 44991\n",
            "batch: 41431 out of 44991\n",
            "batch: 41432 out of 44991\n",
            "batch: 41433 out of 44991\n",
            "batch: 41434 out of 44991\n",
            "batch: 41435 out of 44991\n",
            "batch: 41436 out of 44991\n",
            "batch: 41437 out of 44991\n",
            "batch: 41438 out of 44991\n",
            "batch: 41439 out of 44991\n",
            "batch: 41440 out of 44991\n",
            "batch: 41441 out of 44991\n",
            "batch: 41442 out of 44991\n",
            "batch: 41443 out of 44991\n",
            "batch: 41444 out of 44991\n",
            "batch: 41445 out of 44991\n",
            "batch: 41446 out of 44991\n",
            "batch: 41447 out of 44991\n",
            "batch: 41448 out of 44991\n",
            "batch: 41449 out of 44991\n",
            "batch: 41450 out of 44991\n",
            "batch: 41451 out of 44991\n",
            "batch: 41452 out of 44991\n",
            "batch: 41453 out of 44991\n",
            "batch: 41454 out of 44991\n",
            "batch: 41455 out of 44991\n",
            "batch: 41456 out of 44991\n",
            "batch: 41457 out of 44991\n",
            "batch: 41458 out of 44991\n",
            "batch: 41459 out of 44991\n",
            "batch: 41460 out of 44991\n",
            "batch: 41461 out of 44991\n",
            "batch: 41462 out of 44991\n",
            "batch: 41463 out of 44991\n",
            "batch: 41464 out of 44991\n",
            "batch: 41465 out of 44991\n",
            "batch: 41466 out of 44991\n",
            "batch: 41467 out of 44991\n",
            "batch: 41468 out of 44991\n",
            "batch: 41469 out of 44991\n",
            "batch: 41470 out of 44991\n",
            "batch: 41471 out of 44991\n",
            "batch: 41472 out of 44991\n",
            "batch: 41473 out of 44991\n",
            "batch: 41474 out of 44991\n",
            "batch: 41475 out of 44991\n",
            "batch: 41476 out of 44991\n",
            "batch: 41477 out of 44991\n",
            "batch: 41478 out of 44991\n",
            "batch: 41479 out of 44991\n",
            "batch: 41480 out of 44991\n",
            "batch: 41481 out of 44991\n",
            "batch: 41482 out of 44991\n",
            "batch: 41483 out of 44991\n",
            "batch: 41484 out of 44991\n",
            "batch: 41485 out of 44991\n",
            "batch: 41486 out of 44991\n",
            "batch: 41487 out of 44991\n",
            "batch: 41488 out of 44991\n",
            "batch: 41489 out of 44991\n",
            "batch: 41490 out of 44991\n",
            "batch: 41491 out of 44991\n",
            "batch: 41492 out of 44991\n",
            "batch: 41493 out of 44991\n",
            "batch: 41494 out of 44991\n",
            "batch: 41495 out of 44991\n",
            "batch: 41496 out of 44991\n",
            "batch: 41497 out of 44991\n",
            "batch: 41498 out of 44991\n",
            "batch: 41499 out of 44991\n",
            "batch: 41500 out of 44991\n",
            "batch: 41501 out of 44991\n",
            "batch: 41502 out of 44991\n",
            "batch: 41503 out of 44991\n",
            "batch: 41504 out of 44991\n",
            "batch: 41505 out of 44991\n",
            "batch: 41506 out of 44991\n",
            "batch: 41507 out of 44991\n",
            "batch: 41508 out of 44991\n",
            "batch: 41509 out of 44991\n",
            "batch: 41510 out of 44991\n",
            "batch: 41511 out of 44991\n",
            "batch: 41512 out of 44991\n",
            "batch: 41513 out of 44991\n",
            "batch: 41514 out of 44991\n",
            "batch: 41515 out of 44991\n",
            "batch: 41516 out of 44991\n",
            "batch: 41517 out of 44991\n",
            "batch: 41518 out of 44991\n",
            "batch: 41519 out of 44991\n",
            "batch: 41520 out of 44991\n",
            "batch: 41521 out of 44991\n",
            "batch: 41522 out of 44991\n",
            "batch: 41523 out of 44991\n",
            "batch: 41524 out of 44991\n",
            "batch: 41525 out of 44991\n",
            "batch: 41526 out of 44991\n",
            "batch: 41527 out of 44991\n",
            "batch: 41528 out of 44991\n",
            "batch: 41529 out of 44991\n",
            "batch: 41530 out of 44991\n",
            "batch: 41531 out of 44991\n",
            "batch: 41532 out of 44991\n",
            "batch: 41533 out of 44991\n",
            "batch: 41534 out of 44991\n",
            "batch: 41535 out of 44991\n",
            "batch: 41536 out of 44991\n",
            "batch: 41537 out of 44991\n",
            "batch: 41538 out of 44991\n",
            "batch: 41539 out of 44991\n",
            "batch: 41540 out of 44991\n",
            "batch: 41541 out of 44991\n",
            "batch: 41542 out of 44991\n",
            "batch: 41543 out of 44991\n",
            "batch: 41544 out of 44991\n",
            "batch: 41545 out of 44991\n",
            "batch: 41546 out of 44991\n",
            "batch: 41547 out of 44991\n",
            "batch: 41548 out of 44991\n",
            "batch: 41549 out of 44991\n",
            "batch: 41550 out of 44991\n",
            "batch: 41551 out of 44991\n",
            "batch: 41552 out of 44991\n",
            "batch: 41553 out of 44991\n",
            "batch: 41554 out of 44991\n",
            "batch: 41555 out of 44991\n",
            "batch: 41556 out of 44991\n",
            "batch: 41557 out of 44991\n",
            "batch: 41558 out of 44991\n",
            "batch: 41559 out of 44991\n",
            "batch: 41560 out of 44991\n",
            "batch: 41561 out of 44991\n",
            "batch: 41562 out of 44991\n",
            "batch: 41563 out of 44991\n",
            "batch: 41564 out of 44991\n",
            "batch: 41565 out of 44991\n",
            "batch: 41566 out of 44991\n",
            "batch: 41567 out of 44991\n",
            "batch: 41568 out of 44991\n",
            "batch: 41569 out of 44991\n",
            "batch: 41570 out of 44991\n",
            "batch: 41571 out of 44991\n",
            "batch: 41572 out of 44991\n",
            "batch: 41573 out of 44991\n",
            "batch: 41574 out of 44991\n",
            "batch: 41575 out of 44991\n",
            "batch: 41576 out of 44991\n",
            "batch: 41577 out of 44991\n",
            "batch: 41578 out of 44991\n",
            "batch: 41579 out of 44991\n",
            "batch: 41580 out of 44991\n",
            "batch: 41581 out of 44991\n",
            "batch: 41582 out of 44991\n",
            "batch: 41583 out of 44991\n",
            "batch: 41584 out of 44991\n",
            "batch: 41585 out of 44991\n",
            "batch: 41586 out of 44991\n",
            "batch: 41587 out of 44991\n",
            "batch: 41588 out of 44991\n",
            "batch: 41589 out of 44991\n",
            "batch: 41590 out of 44991\n",
            "batch: 41591 out of 44991\n",
            "batch: 41592 out of 44991\n",
            "batch: 41593 out of 44991\n",
            "batch: 41594 out of 44991\n",
            "batch: 41595 out of 44991\n",
            "batch: 41596 out of 44991\n",
            "batch: 41597 out of 44991\n",
            "batch: 41598 out of 44991\n",
            "batch: 41599 out of 44991\n",
            "batch: 41600 out of 44991\n",
            "batch: 41601 out of 44991\n",
            "batch: 41602 out of 44991\n",
            "batch: 41603 out of 44991\n",
            "batch: 41604 out of 44991\n",
            "batch: 41605 out of 44991\n",
            "batch: 41606 out of 44991\n",
            "batch: 41607 out of 44991\n",
            "batch: 41608 out of 44991\n",
            "batch: 41609 out of 44991\n",
            "batch: 41610 out of 44991\n",
            "batch: 41611 out of 44991\n",
            "batch: 41612 out of 44991\n",
            "batch: 41613 out of 44991\n",
            "batch: 41614 out of 44991\n",
            "batch: 41615 out of 44991\n",
            "batch: 41616 out of 44991\n",
            "batch: 41617 out of 44991\n",
            "batch: 41618 out of 44991\n",
            "batch: 41619 out of 44991\n",
            "batch: 41620 out of 44991\n",
            "batch: 41621 out of 44991\n",
            "batch: 41622 out of 44991\n",
            "batch: 41623 out of 44991\n",
            "batch: 41624 out of 44991\n",
            "batch: 41625 out of 44991\n",
            "batch: 41626 out of 44991\n",
            "batch: 41627 out of 44991\n",
            "batch: 41628 out of 44991\n",
            "batch: 41629 out of 44991\n",
            "batch: 41630 out of 44991\n",
            "batch: 41631 out of 44991\n",
            "batch: 41632 out of 44991\n",
            "batch: 41633 out of 44991\n",
            "batch: 41634 out of 44991\n",
            "batch: 41635 out of 44991\n",
            "batch: 41636 out of 44991\n",
            "batch: 41637 out of 44991\n",
            "batch: 41638 out of 44991\n",
            "batch: 41639 out of 44991\n",
            "batch: 41640 out of 44991\n",
            "batch: 41641 out of 44991\n",
            "batch: 41642 out of 44991\n",
            "batch: 41643 out of 44991\n",
            "batch: 41644 out of 44991\n",
            "batch: 41645 out of 44991\n",
            "batch: 41646 out of 44991\n",
            "batch: 41647 out of 44991\n",
            "batch: 41648 out of 44991\n",
            "batch: 41649 out of 44991\n",
            "batch: 41650 out of 44991\n",
            "batch: 41651 out of 44991\n",
            "batch: 41652 out of 44991\n",
            "batch: 41653 out of 44991\n",
            "batch: 41654 out of 44991\n",
            "batch: 41655 out of 44991\n",
            "batch: 41656 out of 44991\n",
            "batch: 41657 out of 44991\n",
            "batch: 41658 out of 44991\n",
            "batch: 41659 out of 44991\n",
            "batch: 41660 out of 44991\n",
            "batch: 41661 out of 44991\n",
            "batch: 41662 out of 44991\n",
            "batch: 41663 out of 44991\n",
            "batch: 41664 out of 44991\n",
            "batch: 41665 out of 44991\n",
            "batch: 41666 out of 44991\n",
            "batch: 41667 out of 44991\n",
            "batch: 41668 out of 44991\n",
            "batch: 41669 out of 44991\n",
            "batch: 41670 out of 44991\n",
            "batch: 41671 out of 44991\n",
            "batch: 41672 out of 44991\n",
            "batch: 41673 out of 44991\n",
            "batch: 41674 out of 44991\n",
            "batch: 41675 out of 44991\n",
            "batch: 41676 out of 44991\n",
            "batch: 41677 out of 44991\n",
            "batch: 41678 out of 44991\n",
            "batch: 41679 out of 44991\n",
            "batch: 41680 out of 44991\n",
            "batch: 41681 out of 44991\n",
            "batch: 41682 out of 44991\n",
            "batch: 41683 out of 44991\n",
            "batch: 41684 out of 44991\n",
            "batch: 41685 out of 44991\n",
            "batch: 41686 out of 44991\n",
            "batch: 41687 out of 44991\n",
            "batch: 41688 out of 44991\n",
            "batch: 41689 out of 44991\n",
            "batch: 41690 out of 44991\n",
            "batch: 41691 out of 44991\n",
            "batch: 41692 out of 44991\n",
            "batch: 41693 out of 44991\n",
            "batch: 41694 out of 44991\n",
            "batch: 41695 out of 44991\n",
            "batch: 41696 out of 44991\n",
            "batch: 41697 out of 44991\n",
            "batch: 41698 out of 44991\n",
            "batch: 41699 out of 44991\n",
            "batch: 41700 out of 44991\n",
            "batch: 41701 out of 44991\n",
            "batch: 41702 out of 44991\n",
            "batch: 41703 out of 44991\n",
            "batch: 41704 out of 44991\n",
            "batch: 41705 out of 44991\n",
            "batch: 41706 out of 44991\n",
            "batch: 41707 out of 44991\n",
            "batch: 41708 out of 44991\n",
            "batch: 41709 out of 44991\n",
            "batch: 41710 out of 44991\n",
            "batch: 41711 out of 44991\n",
            "batch: 41712 out of 44991\n",
            "batch: 41713 out of 44991\n",
            "batch: 41714 out of 44991\n",
            "batch: 41715 out of 44991\n",
            "batch: 41716 out of 44991\n",
            "batch: 41717 out of 44991\n",
            "batch: 41718 out of 44991\n",
            "batch: 41719 out of 44991\n",
            "batch: 41720 out of 44991\n",
            "batch: 41721 out of 44991\n",
            "batch: 41722 out of 44991\n",
            "batch: 41723 out of 44991\n",
            "batch: 41724 out of 44991\n",
            "batch: 41725 out of 44991\n",
            "batch: 41726 out of 44991\n",
            "batch: 41727 out of 44991\n",
            "batch: 41728 out of 44991\n",
            "batch: 41729 out of 44991\n",
            "batch: 41730 out of 44991\n",
            "batch: 41731 out of 44991\n",
            "batch: 41732 out of 44991\n",
            "batch: 41733 out of 44991\n",
            "batch: 41734 out of 44991\n",
            "batch: 41735 out of 44991\n",
            "batch: 41736 out of 44991\n",
            "batch: 41737 out of 44991\n",
            "batch: 41738 out of 44991\n",
            "batch: 41739 out of 44991\n",
            "batch: 41740 out of 44991\n",
            "batch: 41741 out of 44991\n",
            "batch: 41742 out of 44991\n",
            "batch: 41743 out of 44991\n",
            "batch: 41744 out of 44991\n",
            "batch: 41745 out of 44991\n",
            "batch: 41746 out of 44991\n",
            "batch: 41747 out of 44991\n",
            "batch: 41748 out of 44991\n",
            "batch: 41749 out of 44991\n",
            "batch: 41750 out of 44991\n",
            "batch: 41751 out of 44991\n",
            "batch: 41752 out of 44991\n",
            "batch: 41753 out of 44991\n",
            "batch: 41754 out of 44991\n",
            "batch: 41755 out of 44991\n",
            "batch: 41756 out of 44991\n",
            "batch: 41757 out of 44991\n",
            "batch: 41758 out of 44991\n",
            "batch: 41759 out of 44991\n",
            "batch: 41760 out of 44991\n",
            "batch: 41761 out of 44991\n",
            "batch: 41762 out of 44991\n",
            "batch: 41763 out of 44991\n",
            "batch: 41764 out of 44991\n",
            "batch: 41765 out of 44991\n",
            "batch: 41766 out of 44991\n",
            "batch: 41767 out of 44991\n",
            "batch: 41768 out of 44991\n",
            "batch: 41769 out of 44991\n",
            "batch: 41770 out of 44991\n",
            "batch: 41771 out of 44991\n",
            "batch: 41772 out of 44991\n",
            "batch: 41773 out of 44991\n",
            "batch: 41774 out of 44991\n",
            "batch: 41775 out of 44991\n",
            "batch: 41776 out of 44991\n",
            "batch: 41777 out of 44991\n",
            "batch: 41778 out of 44991\n",
            "batch: 41779 out of 44991\n",
            "batch: 41780 out of 44991\n",
            "batch: 41781 out of 44991\n",
            "batch: 41782 out of 44991\n",
            "batch: 41783 out of 44991\n",
            "batch: 41784 out of 44991\n",
            "batch: 41785 out of 44991\n",
            "batch: 41786 out of 44991\n",
            "batch: 41787 out of 44991\n",
            "batch: 41788 out of 44991\n",
            "batch: 41789 out of 44991\n",
            "batch: 41790 out of 44991\n",
            "batch: 41791 out of 44991\n",
            "batch: 41792 out of 44991\n",
            "batch: 41793 out of 44991\n",
            "batch: 41794 out of 44991\n",
            "batch: 41795 out of 44991\n",
            "batch: 41796 out of 44991\n",
            "batch: 41797 out of 44991\n",
            "batch: 41798 out of 44991\n",
            "batch: 41799 out of 44991\n",
            "batch: 41800 out of 44991\n",
            "batch: 41801 out of 44991\n",
            "batch: 41802 out of 44991\n",
            "batch: 41803 out of 44991\n",
            "batch: 41804 out of 44991\n",
            "batch: 41805 out of 44991\n",
            "batch: 41806 out of 44991\n",
            "batch: 41807 out of 44991\n",
            "batch: 41808 out of 44991\n",
            "batch: 41809 out of 44991\n",
            "batch: 41810 out of 44991\n",
            "batch: 41811 out of 44991\n",
            "batch: 41812 out of 44991\n",
            "batch: 41813 out of 44991\n",
            "batch: 41814 out of 44991\n",
            "batch: 41815 out of 44991\n",
            "batch: 41816 out of 44991\n",
            "batch: 41817 out of 44991\n",
            "batch: 41818 out of 44991\n",
            "batch: 41819 out of 44991\n",
            "batch: 41820 out of 44991\n",
            "batch: 41821 out of 44991\n",
            "batch: 41822 out of 44991\n",
            "batch: 41823 out of 44991\n",
            "batch: 41824 out of 44991\n",
            "batch: 41825 out of 44991\n",
            "batch: 41826 out of 44991\n",
            "batch: 41827 out of 44991\n",
            "batch: 41828 out of 44991\n",
            "batch: 41829 out of 44991\n",
            "batch: 41830 out of 44991\n",
            "batch: 41831 out of 44991\n",
            "batch: 41832 out of 44991\n",
            "batch: 41833 out of 44991\n",
            "batch: 41834 out of 44991\n",
            "batch: 41835 out of 44991\n",
            "batch: 41836 out of 44991\n",
            "batch: 41837 out of 44991\n",
            "batch: 41838 out of 44991\n",
            "batch: 41839 out of 44991\n",
            "batch: 41840 out of 44991\n",
            "batch: 41841 out of 44991\n",
            "batch: 41842 out of 44991\n",
            "batch: 41843 out of 44991\n",
            "batch: 41844 out of 44991\n",
            "batch: 41845 out of 44991\n",
            "batch: 41846 out of 44991\n",
            "batch: 41847 out of 44991\n",
            "batch: 41848 out of 44991\n",
            "batch: 41849 out of 44991\n",
            "batch: 41850 out of 44991\n",
            "batch: 41851 out of 44991\n",
            "batch: 41852 out of 44991\n",
            "batch: 41853 out of 44991\n",
            "batch: 41854 out of 44991\n",
            "batch: 41855 out of 44991\n",
            "batch: 41856 out of 44991\n",
            "batch: 41857 out of 44991\n",
            "batch: 41858 out of 44991\n",
            "batch: 41859 out of 44991\n",
            "batch: 41860 out of 44991\n",
            "batch: 41861 out of 44991\n",
            "batch: 41862 out of 44991\n",
            "batch: 41863 out of 44991\n",
            "batch: 41864 out of 44991\n",
            "batch: 41865 out of 44991\n",
            "batch: 41866 out of 44991\n",
            "batch: 41867 out of 44991\n",
            "batch: 41868 out of 44991\n",
            "batch: 41869 out of 44991\n",
            "batch: 41870 out of 44991\n",
            "batch: 41871 out of 44991\n",
            "batch: 41872 out of 44991\n",
            "batch: 41873 out of 44991\n",
            "batch: 41874 out of 44991\n",
            "batch: 41875 out of 44991\n",
            "batch: 41876 out of 44991\n",
            "batch: 41877 out of 44991\n",
            "batch: 41878 out of 44991\n",
            "batch: 41879 out of 44991\n",
            "batch: 41880 out of 44991\n",
            "batch: 41881 out of 44991\n",
            "batch: 41882 out of 44991\n",
            "batch: 41883 out of 44991\n",
            "batch: 41884 out of 44991\n",
            "batch: 41885 out of 44991\n",
            "batch: 41886 out of 44991\n",
            "batch: 41887 out of 44991\n",
            "batch: 41888 out of 44991\n",
            "batch: 41889 out of 44991\n",
            "batch: 41890 out of 44991\n",
            "batch: 41891 out of 44991\n",
            "batch: 41892 out of 44991\n",
            "batch: 41893 out of 44991\n",
            "batch: 41894 out of 44991\n",
            "batch: 41895 out of 44991\n",
            "batch: 41896 out of 44991\n",
            "batch: 41897 out of 44991\n",
            "batch: 41898 out of 44991\n",
            "batch: 41899 out of 44991\n",
            "batch: 41900 out of 44991\n",
            "batch: 41901 out of 44991\n",
            "batch: 41902 out of 44991\n",
            "batch: 41903 out of 44991\n",
            "batch: 41904 out of 44991\n",
            "batch: 41905 out of 44991\n",
            "batch: 41906 out of 44991\n",
            "batch: 41907 out of 44991\n",
            "batch: 41908 out of 44991\n",
            "batch: 41909 out of 44991\n",
            "batch: 41910 out of 44991\n",
            "batch: 41911 out of 44991\n",
            "batch: 41912 out of 44991\n",
            "batch: 41913 out of 44991\n",
            "batch: 41914 out of 44991\n",
            "batch: 41915 out of 44991\n",
            "batch: 41916 out of 44991\n",
            "batch: 41917 out of 44991\n",
            "batch: 41918 out of 44991\n",
            "batch: 41919 out of 44991\n",
            "batch: 41920 out of 44991\n",
            "batch: 41921 out of 44991\n",
            "batch: 41922 out of 44991\n",
            "batch: 41923 out of 44991\n",
            "batch: 41924 out of 44991\n",
            "batch: 41925 out of 44991\n",
            "batch: 41926 out of 44991\n",
            "batch: 41927 out of 44991\n",
            "batch: 41928 out of 44991\n",
            "batch: 41929 out of 44991\n",
            "batch: 41930 out of 44991\n",
            "batch: 41931 out of 44991\n",
            "batch: 41932 out of 44991\n",
            "batch: 41933 out of 44991\n",
            "batch: 41934 out of 44991\n",
            "batch: 41935 out of 44991\n",
            "batch: 41936 out of 44991\n",
            "batch: 41937 out of 44991\n",
            "batch: 41938 out of 44991\n",
            "batch: 41939 out of 44991\n",
            "batch: 41940 out of 44991\n",
            "batch: 41941 out of 44991\n",
            "batch: 41942 out of 44991\n",
            "batch: 41943 out of 44991\n",
            "batch: 41944 out of 44991\n",
            "batch: 41945 out of 44991\n",
            "batch: 41946 out of 44991\n",
            "batch: 41947 out of 44991\n",
            "batch: 41948 out of 44991\n",
            "batch: 41949 out of 44991\n",
            "batch: 41950 out of 44991\n",
            "batch: 41951 out of 44991\n",
            "batch: 41952 out of 44991\n",
            "batch: 41953 out of 44991\n",
            "batch: 41954 out of 44991\n",
            "batch: 41955 out of 44991\n",
            "batch: 41956 out of 44991\n",
            "batch: 41957 out of 44991\n",
            "batch: 41958 out of 44991\n",
            "batch: 41959 out of 44991\n",
            "batch: 41960 out of 44991\n",
            "batch: 41961 out of 44991\n",
            "batch: 41962 out of 44991\n",
            "batch: 41963 out of 44991\n",
            "batch: 41964 out of 44991\n",
            "batch: 41965 out of 44991\n",
            "batch: 41966 out of 44991\n",
            "batch: 41967 out of 44991\n",
            "batch: 41968 out of 44991\n",
            "batch: 41969 out of 44991\n",
            "batch: 41970 out of 44991\n",
            "batch: 41971 out of 44991\n",
            "batch: 41972 out of 44991\n",
            "batch: 41973 out of 44991\n",
            "batch: 41974 out of 44991\n",
            "batch: 41975 out of 44991\n",
            "batch: 41976 out of 44991\n",
            "batch: 41977 out of 44991\n",
            "batch: 41978 out of 44991\n",
            "batch: 41979 out of 44991\n",
            "batch: 41980 out of 44991\n",
            "batch: 41981 out of 44991\n",
            "batch: 41982 out of 44991\n",
            "batch: 41983 out of 44991\n",
            "batch: 41984 out of 44991\n",
            "batch: 41985 out of 44991\n",
            "batch: 41986 out of 44991\n",
            "batch: 41987 out of 44991\n",
            "batch: 41988 out of 44991\n",
            "batch: 41989 out of 44991\n",
            "batch: 41990 out of 44991\n",
            "batch: 41991 out of 44991\n",
            "batch: 41992 out of 44991\n",
            "batch: 41993 out of 44991\n",
            "batch: 41994 out of 44991\n",
            "batch: 41995 out of 44991\n",
            "batch: 41996 out of 44991\n",
            "batch: 41997 out of 44991\n",
            "batch: 41998 out of 44991\n",
            "batch: 41999 out of 44991\n",
            "batch: 42000 out of 44991\n",
            "batch: 42001 out of 44991\n",
            "batch: 42002 out of 44991\n",
            "batch: 42003 out of 44991\n",
            "batch: 42004 out of 44991\n",
            "batch: 42005 out of 44991\n",
            "batch: 42006 out of 44991\n",
            "batch: 42007 out of 44991\n",
            "batch: 42008 out of 44991\n",
            "batch: 42009 out of 44991\n",
            "batch: 42010 out of 44991\n",
            "batch: 42011 out of 44991\n",
            "batch: 42012 out of 44991\n",
            "batch: 42013 out of 44991\n",
            "batch: 42014 out of 44991\n",
            "batch: 42015 out of 44991\n",
            "batch: 42016 out of 44991\n",
            "batch: 42017 out of 44991\n",
            "batch: 42018 out of 44991\n",
            "batch: 42019 out of 44991\n",
            "batch: 42020 out of 44991\n",
            "batch: 42021 out of 44991\n",
            "batch: 42022 out of 44991\n",
            "batch: 42023 out of 44991\n",
            "batch: 42024 out of 44991\n",
            "batch: 42025 out of 44991\n",
            "batch: 42026 out of 44991\n",
            "batch: 42027 out of 44991\n",
            "batch: 42028 out of 44991\n",
            "batch: 42029 out of 44991\n",
            "batch: 42030 out of 44991\n",
            "batch: 42031 out of 44991\n",
            "batch: 42032 out of 44991\n",
            "batch: 42033 out of 44991\n",
            "batch: 42034 out of 44991\n",
            "batch: 42035 out of 44991\n",
            "batch: 42036 out of 44991\n",
            "batch: 42037 out of 44991\n",
            "batch: 42038 out of 44991\n",
            "batch: 42039 out of 44991\n",
            "batch: 42040 out of 44991\n",
            "batch: 42041 out of 44991\n",
            "batch: 42042 out of 44991\n",
            "batch: 42043 out of 44991\n",
            "batch: 42044 out of 44991\n",
            "batch: 42045 out of 44991\n",
            "batch: 42046 out of 44991\n",
            "batch: 42047 out of 44991\n",
            "batch: 42048 out of 44991\n",
            "batch: 42049 out of 44991\n",
            "batch: 42050 out of 44991\n",
            "batch: 42051 out of 44991\n",
            "batch: 42052 out of 44991\n",
            "batch: 42053 out of 44991\n",
            "batch: 42054 out of 44991\n",
            "batch: 42055 out of 44991\n",
            "batch: 42056 out of 44991\n",
            "batch: 42057 out of 44991\n",
            "batch: 42058 out of 44991\n",
            "batch: 42059 out of 44991\n",
            "batch: 42060 out of 44991\n",
            "batch: 42061 out of 44991\n",
            "batch: 42062 out of 44991\n",
            "batch: 42063 out of 44991\n",
            "batch: 42064 out of 44991\n",
            "batch: 42065 out of 44991\n",
            "batch: 42066 out of 44991\n",
            "batch: 42067 out of 44991\n",
            "batch: 42068 out of 44991\n",
            "batch: 42069 out of 44991\n",
            "batch: 42070 out of 44991\n",
            "batch: 42071 out of 44991\n",
            "batch: 42072 out of 44991\n",
            "batch: 42073 out of 44991\n",
            "batch: 42074 out of 44991\n",
            "batch: 42075 out of 44991\n",
            "batch: 42076 out of 44991\n",
            "batch: 42077 out of 44991\n",
            "batch: 42078 out of 44991\n",
            "batch: 42079 out of 44991\n",
            "batch: 42080 out of 44991\n",
            "batch: 42081 out of 44991\n",
            "batch: 42082 out of 44991\n",
            "batch: 42083 out of 44991\n",
            "batch: 42084 out of 44991\n",
            "batch: 42085 out of 44991\n",
            "batch: 42086 out of 44991\n",
            "batch: 42087 out of 44991\n",
            "batch: 42088 out of 44991\n",
            "batch: 42089 out of 44991\n",
            "batch: 42090 out of 44991\n",
            "batch: 42091 out of 44991\n",
            "batch: 42092 out of 44991\n",
            "batch: 42093 out of 44991\n",
            "batch: 42094 out of 44991\n",
            "batch: 42095 out of 44991\n",
            "batch: 42096 out of 44991\n",
            "batch: 42097 out of 44991\n",
            "batch: 42098 out of 44991\n",
            "batch: 42099 out of 44991\n",
            "batch: 42100 out of 44991\n",
            "batch: 42101 out of 44991\n",
            "batch: 42102 out of 44991\n",
            "batch: 42103 out of 44991\n",
            "batch: 42104 out of 44991\n",
            "batch: 42105 out of 44991\n",
            "batch: 42106 out of 44991\n",
            "batch: 42107 out of 44991\n",
            "batch: 42108 out of 44991\n",
            "batch: 42109 out of 44991\n",
            "batch: 42110 out of 44991\n",
            "batch: 42111 out of 44991\n",
            "batch: 42112 out of 44991\n",
            "batch: 42113 out of 44991\n",
            "batch: 42114 out of 44991\n",
            "batch: 42115 out of 44991\n",
            "batch: 42116 out of 44991\n",
            "batch: 42117 out of 44991\n",
            "batch: 42118 out of 44991\n",
            "batch: 42119 out of 44991\n",
            "batch: 42120 out of 44991\n",
            "batch: 42121 out of 44991\n",
            "batch: 42122 out of 44991\n",
            "batch: 42123 out of 44991\n",
            "batch: 42124 out of 44991\n",
            "batch: 42125 out of 44991\n",
            "batch: 42126 out of 44991\n",
            "batch: 42127 out of 44991\n",
            "batch: 42128 out of 44991\n",
            "batch: 42129 out of 44991\n",
            "batch: 42130 out of 44991\n",
            "batch: 42131 out of 44991\n",
            "batch: 42132 out of 44991\n",
            "batch: 42133 out of 44991\n",
            "batch: 42134 out of 44991\n",
            "batch: 42135 out of 44991\n",
            "batch: 42136 out of 44991\n",
            "batch: 42137 out of 44991\n",
            "batch: 42138 out of 44991\n",
            "batch: 42139 out of 44991\n",
            "batch: 42140 out of 44991\n",
            "batch: 42141 out of 44991\n",
            "batch: 42142 out of 44991\n",
            "batch: 42143 out of 44991\n",
            "batch: 42144 out of 44991\n",
            "batch: 42145 out of 44991\n",
            "batch: 42146 out of 44991\n",
            "batch: 42147 out of 44991\n",
            "batch: 42148 out of 44991\n",
            "batch: 42149 out of 44991\n",
            "batch: 42150 out of 44991\n",
            "batch: 42151 out of 44991\n",
            "batch: 42152 out of 44991\n",
            "batch: 42153 out of 44991\n",
            "batch: 42154 out of 44991\n",
            "batch: 42155 out of 44991\n",
            "batch: 42156 out of 44991\n",
            "batch: 42157 out of 44991\n",
            "batch: 42158 out of 44991\n",
            "batch: 42159 out of 44991\n",
            "batch: 42160 out of 44991\n",
            "batch: 42161 out of 44991\n",
            "batch: 42162 out of 44991\n",
            "batch: 42163 out of 44991\n",
            "batch: 42164 out of 44991\n",
            "batch: 42165 out of 44991\n",
            "batch: 42166 out of 44991\n",
            "batch: 42167 out of 44991\n",
            "batch: 42168 out of 44991\n",
            "batch: 42169 out of 44991\n",
            "batch: 42170 out of 44991\n",
            "batch: 42171 out of 44991\n",
            "batch: 42172 out of 44991\n",
            "batch: 42173 out of 44991\n",
            "batch: 42174 out of 44991\n",
            "batch: 42175 out of 44991\n",
            "batch: 42176 out of 44991\n",
            "batch: 42177 out of 44991\n",
            "batch: 42178 out of 44991\n",
            "batch: 42179 out of 44991\n",
            "batch: 42180 out of 44991\n",
            "batch: 42181 out of 44991\n",
            "batch: 42182 out of 44991\n",
            "batch: 42183 out of 44991\n",
            "batch: 42184 out of 44991\n",
            "batch: 42185 out of 44991\n",
            "batch: 42186 out of 44991\n",
            "batch: 42187 out of 44991\n",
            "batch: 42188 out of 44991\n",
            "batch: 42189 out of 44991\n",
            "batch: 42190 out of 44991\n",
            "batch: 42191 out of 44991\n",
            "batch: 42192 out of 44991\n",
            "batch: 42193 out of 44991\n",
            "batch: 42194 out of 44991\n",
            "batch: 42195 out of 44991\n",
            "batch: 42196 out of 44991\n",
            "batch: 42197 out of 44991\n",
            "batch: 42198 out of 44991\n",
            "batch: 42199 out of 44991\n",
            "batch: 42200 out of 44991\n",
            "batch: 42201 out of 44991\n",
            "batch: 42202 out of 44991\n",
            "batch: 42203 out of 44991\n",
            "batch: 42204 out of 44991\n",
            "batch: 42205 out of 44991\n",
            "batch: 42206 out of 44991\n",
            "batch: 42207 out of 44991\n",
            "batch: 42208 out of 44991\n",
            "batch: 42209 out of 44991\n",
            "batch: 42210 out of 44991\n",
            "batch: 42211 out of 44991\n",
            "batch: 42212 out of 44991\n",
            "batch: 42213 out of 44991\n",
            "batch: 42214 out of 44991\n",
            "batch: 42215 out of 44991\n",
            "batch: 42216 out of 44991\n",
            "batch: 42217 out of 44991\n",
            "batch: 42218 out of 44991\n",
            "batch: 42219 out of 44991\n",
            "batch: 42220 out of 44991\n",
            "batch: 42221 out of 44991\n",
            "batch: 42222 out of 44991\n",
            "batch: 42223 out of 44991\n",
            "batch: 42224 out of 44991\n",
            "batch: 42225 out of 44991\n",
            "batch: 42226 out of 44991\n",
            "batch: 42227 out of 44991\n",
            "batch: 42228 out of 44991\n",
            "batch: 42229 out of 44991\n",
            "batch: 42230 out of 44991\n",
            "batch: 42231 out of 44991\n",
            "batch: 42232 out of 44991\n",
            "batch: 42233 out of 44991\n",
            "batch: 42234 out of 44991\n",
            "batch: 42235 out of 44991\n",
            "batch: 42236 out of 44991\n",
            "batch: 42237 out of 44991\n",
            "batch: 42238 out of 44991\n",
            "batch: 42239 out of 44991\n",
            "batch: 42240 out of 44991\n",
            "batch: 42241 out of 44991\n",
            "batch: 42242 out of 44991\n",
            "batch: 42243 out of 44991\n",
            "batch: 42244 out of 44991\n",
            "batch: 42245 out of 44991\n",
            "batch: 42246 out of 44991\n",
            "batch: 42247 out of 44991\n",
            "batch: 42248 out of 44991\n",
            "batch: 42249 out of 44991\n",
            "batch: 42250 out of 44991\n",
            "batch: 42251 out of 44991\n",
            "batch: 42252 out of 44991\n",
            "batch: 42253 out of 44991\n",
            "batch: 42254 out of 44991\n",
            "batch: 42255 out of 44991\n",
            "batch: 42256 out of 44991\n",
            "batch: 42257 out of 44991\n",
            "batch: 42258 out of 44991\n",
            "batch: 42259 out of 44991\n",
            "batch: 42260 out of 44991\n",
            "batch: 42261 out of 44991\n",
            "batch: 42262 out of 44991\n",
            "batch: 42263 out of 44991\n",
            "batch: 42264 out of 44991\n",
            "batch: 42265 out of 44991\n",
            "batch: 42266 out of 44991\n",
            "batch: 42267 out of 44991\n",
            "batch: 42268 out of 44991\n",
            "batch: 42269 out of 44991\n",
            "batch: 42270 out of 44991\n",
            "batch: 42271 out of 44991\n",
            "batch: 42272 out of 44991\n",
            "batch: 42273 out of 44991\n",
            "batch: 42274 out of 44991\n",
            "batch: 42275 out of 44991\n",
            "batch: 42276 out of 44991\n",
            "batch: 42277 out of 44991\n",
            "batch: 42278 out of 44991\n",
            "batch: 42279 out of 44991\n",
            "batch: 42280 out of 44991\n",
            "batch: 42281 out of 44991\n",
            "batch: 42282 out of 44991\n",
            "batch: 42283 out of 44991\n",
            "batch: 42284 out of 44991\n",
            "batch: 42285 out of 44991\n",
            "batch: 42286 out of 44991\n",
            "batch: 42287 out of 44991\n",
            "batch: 42288 out of 44991\n",
            "batch: 42289 out of 44991\n",
            "batch: 42290 out of 44991\n",
            "batch: 42291 out of 44991\n",
            "batch: 42292 out of 44991\n",
            "batch: 42293 out of 44991\n",
            "batch: 42294 out of 44991\n",
            "batch: 42295 out of 44991\n",
            "batch: 42296 out of 44991\n",
            "batch: 42297 out of 44991\n",
            "batch: 42298 out of 44991\n",
            "batch: 42299 out of 44991\n",
            "batch: 42300 out of 44991\n",
            "batch: 42301 out of 44991\n",
            "batch: 42302 out of 44991\n",
            "batch: 42303 out of 44991\n",
            "batch: 42304 out of 44991\n",
            "batch: 42305 out of 44991\n",
            "batch: 42306 out of 44991\n",
            "batch: 42307 out of 44991\n",
            "batch: 42308 out of 44991\n",
            "batch: 42309 out of 44991\n",
            "batch: 42310 out of 44991\n",
            "batch: 42311 out of 44991\n",
            "batch: 42312 out of 44991\n",
            "batch: 42313 out of 44991\n",
            "batch: 42314 out of 44991\n",
            "batch: 42315 out of 44991\n",
            "batch: 42316 out of 44991\n",
            "batch: 42317 out of 44991\n",
            "batch: 42318 out of 44991\n",
            "batch: 42319 out of 44991\n",
            "batch: 42320 out of 44991\n",
            "batch: 42321 out of 44991\n",
            "batch: 42322 out of 44991\n",
            "batch: 42323 out of 44991\n",
            "batch: 42324 out of 44991\n",
            "batch: 42325 out of 44991\n",
            "batch: 42326 out of 44991\n",
            "batch: 42327 out of 44991\n",
            "batch: 42328 out of 44991\n",
            "batch: 42329 out of 44991\n",
            "batch: 42330 out of 44991\n",
            "batch: 42331 out of 44991\n",
            "batch: 42332 out of 44991\n",
            "batch: 42333 out of 44991\n",
            "batch: 42334 out of 44991\n",
            "batch: 42335 out of 44991\n",
            "batch: 42336 out of 44991\n",
            "batch: 42337 out of 44991\n",
            "batch: 42338 out of 44991\n",
            "batch: 42339 out of 44991\n",
            "batch: 42340 out of 44991\n",
            "batch: 42341 out of 44991\n",
            "batch: 42342 out of 44991\n",
            "batch: 42343 out of 44991\n",
            "batch: 42344 out of 44991\n",
            "batch: 42345 out of 44991\n",
            "batch: 42346 out of 44991\n",
            "batch: 42347 out of 44991\n",
            "batch: 42348 out of 44991\n",
            "batch: 42349 out of 44991\n",
            "batch: 42350 out of 44991\n",
            "batch: 42351 out of 44991\n",
            "batch: 42352 out of 44991\n",
            "batch: 42353 out of 44991\n",
            "batch: 42354 out of 44991\n",
            "batch: 42355 out of 44991\n",
            "batch: 42356 out of 44991\n",
            "batch: 42357 out of 44991\n",
            "batch: 42358 out of 44991\n",
            "batch: 42359 out of 44991\n",
            "batch: 42360 out of 44991\n",
            "batch: 42361 out of 44991\n",
            "batch: 42362 out of 44991\n",
            "batch: 42363 out of 44991\n",
            "batch: 42364 out of 44991\n",
            "batch: 42365 out of 44991\n",
            "batch: 42366 out of 44991\n",
            "batch: 42367 out of 44991\n",
            "batch: 42368 out of 44991\n",
            "batch: 42369 out of 44991\n",
            "batch: 42370 out of 44991\n",
            "batch: 42371 out of 44991\n",
            "batch: 42372 out of 44991\n",
            "batch: 42373 out of 44991\n",
            "batch: 42374 out of 44991\n",
            "batch: 42375 out of 44991\n",
            "batch: 42376 out of 44991\n",
            "batch: 42377 out of 44991\n",
            "batch: 42378 out of 44991\n",
            "batch: 42379 out of 44991\n",
            "batch: 42380 out of 44991\n",
            "batch: 42381 out of 44991\n",
            "batch: 42382 out of 44991\n",
            "batch: 42383 out of 44991\n",
            "batch: 42384 out of 44991\n",
            "batch: 42385 out of 44991\n",
            "batch: 42386 out of 44991\n",
            "batch: 42387 out of 44991\n",
            "batch: 42388 out of 44991\n",
            "batch: 42389 out of 44991\n",
            "batch: 42390 out of 44991\n",
            "batch: 42391 out of 44991\n",
            "batch: 42392 out of 44991\n",
            "batch: 42393 out of 44991\n",
            "batch: 42394 out of 44991\n",
            "batch: 42395 out of 44991\n",
            "batch: 42396 out of 44991\n",
            "batch: 42397 out of 44991\n",
            "batch: 42398 out of 44991\n",
            "batch: 42399 out of 44991\n",
            "batch: 42400 out of 44991\n",
            "batch: 42401 out of 44991\n",
            "batch: 42402 out of 44991\n",
            "batch: 42403 out of 44991\n",
            "batch: 42404 out of 44991\n",
            "batch: 42405 out of 44991\n",
            "batch: 42406 out of 44991\n",
            "batch: 42407 out of 44991\n",
            "batch: 42408 out of 44991\n",
            "batch: 42409 out of 44991\n",
            "batch: 42410 out of 44991\n",
            "batch: 42411 out of 44991\n",
            "batch: 42412 out of 44991\n",
            "batch: 42413 out of 44991\n",
            "batch: 42414 out of 44991\n",
            "batch: 42415 out of 44991\n",
            "batch: 42416 out of 44991\n",
            "batch: 42417 out of 44991\n",
            "batch: 42418 out of 44991\n",
            "batch: 42419 out of 44991\n",
            "batch: 42420 out of 44991\n",
            "batch: 42421 out of 44991\n",
            "batch: 42422 out of 44991\n",
            "batch: 42423 out of 44991\n",
            "batch: 42424 out of 44991\n",
            "batch: 42425 out of 44991\n",
            "batch: 42426 out of 44991\n",
            "batch: 42427 out of 44991\n",
            "batch: 42428 out of 44991\n",
            "batch: 42429 out of 44991\n",
            "batch: 42430 out of 44991\n",
            "batch: 42431 out of 44991\n",
            "batch: 42432 out of 44991\n",
            "batch: 42433 out of 44991\n",
            "batch: 42434 out of 44991\n",
            "batch: 42435 out of 44991\n",
            "batch: 42436 out of 44991\n",
            "batch: 42437 out of 44991\n",
            "batch: 42438 out of 44991\n",
            "batch: 42439 out of 44991\n",
            "batch: 42440 out of 44991\n",
            "batch: 42441 out of 44991\n",
            "batch: 42442 out of 44991\n",
            "batch: 42443 out of 44991\n",
            "batch: 42444 out of 44991\n",
            "batch: 42445 out of 44991\n",
            "batch: 42446 out of 44991\n",
            "batch: 42447 out of 44991\n",
            "batch: 42448 out of 44991\n",
            "batch: 42449 out of 44991\n",
            "batch: 42450 out of 44991\n",
            "batch: 42451 out of 44991\n",
            "batch: 42452 out of 44991\n",
            "batch: 42453 out of 44991\n",
            "batch: 42454 out of 44991\n",
            "batch: 42455 out of 44991\n",
            "batch: 42456 out of 44991\n",
            "batch: 42457 out of 44991\n",
            "batch: 42458 out of 44991\n",
            "batch: 42459 out of 44991\n",
            "batch: 42460 out of 44991\n",
            "batch: 42461 out of 44991\n",
            "batch: 42462 out of 44991\n",
            "batch: 42463 out of 44991\n",
            "batch: 42464 out of 44991\n",
            "batch: 42465 out of 44991\n",
            "batch: 42466 out of 44991\n",
            "batch: 42467 out of 44991\n",
            "batch: 42468 out of 44991\n",
            "batch: 42469 out of 44991\n",
            "batch: 42470 out of 44991\n",
            "batch: 42471 out of 44991\n",
            "batch: 42472 out of 44991\n",
            "batch: 42473 out of 44991\n",
            "batch: 42474 out of 44991\n",
            "batch: 42475 out of 44991\n",
            "batch: 42476 out of 44991\n",
            "batch: 42477 out of 44991\n",
            "batch: 42478 out of 44991\n",
            "batch: 42479 out of 44991\n",
            "batch: 42480 out of 44991\n",
            "batch: 42481 out of 44991\n",
            "batch: 42482 out of 44991\n",
            "batch: 42483 out of 44991\n",
            "batch: 42484 out of 44991\n",
            "batch: 42485 out of 44991\n",
            "batch: 42486 out of 44991\n",
            "batch: 42487 out of 44991\n",
            "batch: 42488 out of 44991\n",
            "batch: 42489 out of 44991\n",
            "batch: 42490 out of 44991\n",
            "batch: 42491 out of 44991\n",
            "batch: 42492 out of 44991\n",
            "batch: 42493 out of 44991\n",
            "batch: 42494 out of 44991\n",
            "batch: 42495 out of 44991\n",
            "batch: 42496 out of 44991\n",
            "batch: 42497 out of 44991\n",
            "batch: 42498 out of 44991\n",
            "batch: 42499 out of 44991\n",
            "batch: 42500 out of 44991\n",
            "batch: 42501 out of 44991\n",
            "batch: 42502 out of 44991\n",
            "batch: 42503 out of 44991\n",
            "batch: 42504 out of 44991\n",
            "batch: 42505 out of 44991\n",
            "batch: 42506 out of 44991\n",
            "batch: 42507 out of 44991\n",
            "batch: 42508 out of 44991\n",
            "batch: 42509 out of 44991\n",
            "batch: 42510 out of 44991\n",
            "batch: 42511 out of 44991\n",
            "batch: 42512 out of 44991\n",
            "batch: 42513 out of 44991\n",
            "batch: 42514 out of 44991\n",
            "batch: 42515 out of 44991\n",
            "batch: 42516 out of 44991\n",
            "batch: 42517 out of 44991\n",
            "batch: 42518 out of 44991\n",
            "batch: 42519 out of 44991\n",
            "batch: 42520 out of 44991\n",
            "batch: 42521 out of 44991\n",
            "batch: 42522 out of 44991\n",
            "batch: 42523 out of 44991\n",
            "batch: 42524 out of 44991\n",
            "batch: 42525 out of 44991\n",
            "batch: 42526 out of 44991\n",
            "batch: 42527 out of 44991\n",
            "batch: 42528 out of 44991\n",
            "batch: 42529 out of 44991\n",
            "batch: 42530 out of 44991\n",
            "batch: 42531 out of 44991\n",
            "batch: 42532 out of 44991\n",
            "batch: 42533 out of 44991\n",
            "batch: 42534 out of 44991\n",
            "batch: 42535 out of 44991\n",
            "batch: 42536 out of 44991\n",
            "batch: 42537 out of 44991\n",
            "batch: 42538 out of 44991\n",
            "batch: 42539 out of 44991\n",
            "batch: 42540 out of 44991\n",
            "batch: 42541 out of 44991\n",
            "batch: 42542 out of 44991\n",
            "batch: 42543 out of 44991\n",
            "batch: 42544 out of 44991\n",
            "batch: 42545 out of 44991\n",
            "batch: 42546 out of 44991\n",
            "batch: 42547 out of 44991\n",
            "batch: 42548 out of 44991\n",
            "batch: 42549 out of 44991\n",
            "batch: 42550 out of 44991\n",
            "batch: 42551 out of 44991\n",
            "batch: 42552 out of 44991\n",
            "batch: 42553 out of 44991\n",
            "batch: 42554 out of 44991\n",
            "batch: 42555 out of 44991\n",
            "batch: 42556 out of 44991\n",
            "batch: 42557 out of 44991\n",
            "batch: 42558 out of 44991\n",
            "batch: 42559 out of 44991\n",
            "batch: 42560 out of 44991\n",
            "batch: 42561 out of 44991\n",
            "batch: 42562 out of 44991\n",
            "batch: 42563 out of 44991\n",
            "batch: 42564 out of 44991\n",
            "batch: 42565 out of 44991\n",
            "batch: 42566 out of 44991\n",
            "batch: 42567 out of 44991\n",
            "batch: 42568 out of 44991\n",
            "batch: 42569 out of 44991\n",
            "batch: 42570 out of 44991\n",
            "batch: 42571 out of 44991\n",
            "batch: 42572 out of 44991\n",
            "batch: 42573 out of 44991\n",
            "batch: 42574 out of 44991\n",
            "batch: 42575 out of 44991\n",
            "batch: 42576 out of 44991\n",
            "batch: 42577 out of 44991\n",
            "batch: 42578 out of 44991\n",
            "batch: 42579 out of 44991\n",
            "batch: 42580 out of 44991\n",
            "batch: 42581 out of 44991\n",
            "batch: 42582 out of 44991\n",
            "batch: 42583 out of 44991\n",
            "batch: 42584 out of 44991\n",
            "batch: 42585 out of 44991\n",
            "batch: 42586 out of 44991\n",
            "batch: 42587 out of 44991\n",
            "batch: 42588 out of 44991\n",
            "batch: 42589 out of 44991\n",
            "batch: 42590 out of 44991\n",
            "batch: 42591 out of 44991\n",
            "batch: 42592 out of 44991\n",
            "batch: 42593 out of 44991\n",
            "batch: 42594 out of 44991\n",
            "batch: 42595 out of 44991\n",
            "batch: 42596 out of 44991\n",
            "batch: 42597 out of 44991\n",
            "batch: 42598 out of 44991\n",
            "batch: 42599 out of 44991\n",
            "batch: 42600 out of 44991\n",
            "batch: 42601 out of 44991\n",
            "batch: 42602 out of 44991\n",
            "batch: 42603 out of 44991\n",
            "batch: 42604 out of 44991\n",
            "batch: 42605 out of 44991\n",
            "batch: 42606 out of 44991\n",
            "batch: 42607 out of 44991\n",
            "batch: 42608 out of 44991\n",
            "batch: 42609 out of 44991\n",
            "batch: 42610 out of 44991\n",
            "batch: 42611 out of 44991\n",
            "batch: 42612 out of 44991\n",
            "batch: 42613 out of 44991\n",
            "batch: 42614 out of 44991\n",
            "batch: 42615 out of 44991\n",
            "batch: 42616 out of 44991\n",
            "batch: 42617 out of 44991\n",
            "batch: 42618 out of 44991\n",
            "batch: 42619 out of 44991\n",
            "batch: 42620 out of 44991\n",
            "batch: 42621 out of 44991\n",
            "batch: 42622 out of 44991\n",
            "batch: 42623 out of 44991\n",
            "batch: 42624 out of 44991\n",
            "batch: 42625 out of 44991\n",
            "batch: 42626 out of 44991\n",
            "batch: 42627 out of 44991\n",
            "batch: 42628 out of 44991\n",
            "batch: 42629 out of 44991\n",
            "batch: 42630 out of 44991\n",
            "batch: 42631 out of 44991\n",
            "batch: 42632 out of 44991\n",
            "batch: 42633 out of 44991\n",
            "batch: 42634 out of 44991\n",
            "batch: 42635 out of 44991\n",
            "batch: 42636 out of 44991\n",
            "batch: 42637 out of 44991\n",
            "batch: 42638 out of 44991\n",
            "batch: 42639 out of 44991\n",
            "batch: 42640 out of 44991\n",
            "batch: 42641 out of 44991\n",
            "batch: 42642 out of 44991\n",
            "batch: 42643 out of 44991\n",
            "batch: 42644 out of 44991\n",
            "batch: 42645 out of 44991\n",
            "batch: 42646 out of 44991\n",
            "batch: 42647 out of 44991\n",
            "batch: 42648 out of 44991\n",
            "batch: 42649 out of 44991\n",
            "batch: 42650 out of 44991\n",
            "batch: 42651 out of 44991\n",
            "batch: 42652 out of 44991\n",
            "batch: 42653 out of 44991\n",
            "batch: 42654 out of 44991\n",
            "batch: 42655 out of 44991\n",
            "batch: 42656 out of 44991\n",
            "batch: 42657 out of 44991\n",
            "batch: 42658 out of 44991\n",
            "batch: 42659 out of 44991\n",
            "batch: 42660 out of 44991\n",
            "batch: 42661 out of 44991\n",
            "batch: 42662 out of 44991\n",
            "batch: 42663 out of 44991\n",
            "batch: 42664 out of 44991\n",
            "batch: 42665 out of 44991\n",
            "batch: 42666 out of 44991\n",
            "batch: 42667 out of 44991\n",
            "batch: 42668 out of 44991\n",
            "batch: 42669 out of 44991\n",
            "batch: 42670 out of 44991\n",
            "batch: 42671 out of 44991\n",
            "batch: 42672 out of 44991\n",
            "batch: 42673 out of 44991\n",
            "batch: 42674 out of 44991\n",
            "batch: 42675 out of 44991\n",
            "batch: 42676 out of 44991\n",
            "batch: 42677 out of 44991\n",
            "batch: 42678 out of 44991\n",
            "batch: 42679 out of 44991\n",
            "batch: 42680 out of 44991\n",
            "batch: 42681 out of 44991\n",
            "batch: 42682 out of 44991\n",
            "batch: 42683 out of 44991\n",
            "batch: 42684 out of 44991\n",
            "batch: 42685 out of 44991\n",
            "batch: 42686 out of 44991\n",
            "batch: 42687 out of 44991\n",
            "batch: 42688 out of 44991\n",
            "batch: 42689 out of 44991\n",
            "batch: 42690 out of 44991\n",
            "batch: 42691 out of 44991\n",
            "batch: 42692 out of 44991\n",
            "batch: 42693 out of 44991\n",
            "batch: 42694 out of 44991\n",
            "batch: 42695 out of 44991\n",
            "batch: 42696 out of 44991\n",
            "batch: 42697 out of 44991\n",
            "batch: 42698 out of 44991\n",
            "batch: 42699 out of 44991\n",
            "batch: 42700 out of 44991\n",
            "batch: 42701 out of 44991\n",
            "batch: 42702 out of 44991\n",
            "batch: 42703 out of 44991\n",
            "batch: 42704 out of 44991\n",
            "batch: 42705 out of 44991\n",
            "batch: 42706 out of 44991\n",
            "batch: 42707 out of 44991\n",
            "batch: 42708 out of 44991\n",
            "batch: 42709 out of 44991\n",
            "batch: 42710 out of 44991\n",
            "batch: 42711 out of 44991\n",
            "batch: 42712 out of 44991\n",
            "batch: 42713 out of 44991\n",
            "batch: 42714 out of 44991\n",
            "batch: 42715 out of 44991\n",
            "batch: 42716 out of 44991\n",
            "batch: 42717 out of 44991\n",
            "batch: 42718 out of 44991\n",
            "batch: 42719 out of 44991\n",
            "batch: 42720 out of 44991\n",
            "batch: 42721 out of 44991\n",
            "batch: 42722 out of 44991\n",
            "batch: 42723 out of 44991\n",
            "batch: 42724 out of 44991\n",
            "batch: 42725 out of 44991\n",
            "batch: 42726 out of 44991\n",
            "batch: 42727 out of 44991\n",
            "batch: 42728 out of 44991\n",
            "batch: 42729 out of 44991\n",
            "batch: 42730 out of 44991\n",
            "batch: 42731 out of 44991\n",
            "batch: 42732 out of 44991\n",
            "batch: 42733 out of 44991\n",
            "batch: 42734 out of 44991\n",
            "batch: 42735 out of 44991\n",
            "batch: 42736 out of 44991\n",
            "batch: 42737 out of 44991\n",
            "batch: 42738 out of 44991\n",
            "batch: 42739 out of 44991\n",
            "batch: 42740 out of 44991\n",
            "batch: 42741 out of 44991\n",
            "batch: 42742 out of 44991\n",
            "batch: 42743 out of 44991\n",
            "batch: 42744 out of 44991\n",
            "batch: 42745 out of 44991\n",
            "batch: 42746 out of 44991\n",
            "batch: 42747 out of 44991\n",
            "batch: 42748 out of 44991\n",
            "batch: 42749 out of 44991\n",
            "batch: 42750 out of 44991\n",
            "batch: 42751 out of 44991\n",
            "batch: 42752 out of 44991\n",
            "batch: 42753 out of 44991\n",
            "batch: 42754 out of 44991\n",
            "batch: 42755 out of 44991\n",
            "batch: 42756 out of 44991\n",
            "batch: 42757 out of 44991\n",
            "batch: 42758 out of 44991\n",
            "batch: 42759 out of 44991\n",
            "batch: 42760 out of 44991\n",
            "batch: 42761 out of 44991\n",
            "batch: 42762 out of 44991\n",
            "batch: 42763 out of 44991\n",
            "batch: 42764 out of 44991\n",
            "batch: 42765 out of 44991\n",
            "batch: 42766 out of 44991\n",
            "batch: 42767 out of 44991\n",
            "batch: 42768 out of 44991\n",
            "batch: 42769 out of 44991\n",
            "batch: 42770 out of 44991\n",
            "batch: 42771 out of 44991\n",
            "batch: 42772 out of 44991\n",
            "batch: 42773 out of 44991\n",
            "batch: 42774 out of 44991\n",
            "batch: 42775 out of 44991\n",
            "batch: 42776 out of 44991\n",
            "batch: 42777 out of 44991\n",
            "batch: 42778 out of 44991\n",
            "batch: 42779 out of 44991\n",
            "batch: 42780 out of 44991\n",
            "batch: 42781 out of 44991\n",
            "batch: 42782 out of 44991\n",
            "batch: 42783 out of 44991\n",
            "batch: 42784 out of 44991\n",
            "batch: 42785 out of 44991\n",
            "batch: 42786 out of 44991\n",
            "batch: 42787 out of 44991\n",
            "batch: 42788 out of 44991\n",
            "batch: 42789 out of 44991\n",
            "batch: 42790 out of 44991\n",
            "batch: 42791 out of 44991\n",
            "batch: 42792 out of 44991\n",
            "batch: 42793 out of 44991\n",
            "batch: 42794 out of 44991\n",
            "batch: 42795 out of 44991\n",
            "batch: 42796 out of 44991\n",
            "batch: 42797 out of 44991\n",
            "batch: 42798 out of 44991\n",
            "batch: 42799 out of 44991\n",
            "batch: 42800 out of 44991\n",
            "batch: 42801 out of 44991\n",
            "batch: 42802 out of 44991\n",
            "batch: 42803 out of 44991\n",
            "batch: 42804 out of 44991\n",
            "batch: 42805 out of 44991\n",
            "batch: 42806 out of 44991\n",
            "batch: 42807 out of 44991\n",
            "batch: 42808 out of 44991\n",
            "batch: 42809 out of 44991\n",
            "batch: 42810 out of 44991\n",
            "batch: 42811 out of 44991\n",
            "batch: 42812 out of 44991\n",
            "batch: 42813 out of 44991\n",
            "batch: 42814 out of 44991\n",
            "batch: 42815 out of 44991\n",
            "batch: 42816 out of 44991\n",
            "batch: 42817 out of 44991\n",
            "batch: 42818 out of 44991\n",
            "batch: 42819 out of 44991\n",
            "batch: 42820 out of 44991\n",
            "batch: 42821 out of 44991\n",
            "batch: 42822 out of 44991\n",
            "batch: 42823 out of 44991\n",
            "batch: 42824 out of 44991\n",
            "batch: 42825 out of 44991\n",
            "batch: 42826 out of 44991\n",
            "batch: 42827 out of 44991\n",
            "batch: 42828 out of 44991\n",
            "batch: 42829 out of 44991\n",
            "batch: 42830 out of 44991\n",
            "batch: 42831 out of 44991\n",
            "batch: 42832 out of 44991\n",
            "batch: 42833 out of 44991\n",
            "batch: 42834 out of 44991\n",
            "batch: 42835 out of 44991\n",
            "batch: 42836 out of 44991\n",
            "batch: 42837 out of 44991\n",
            "batch: 42838 out of 44991\n",
            "batch: 42839 out of 44991\n",
            "batch: 42840 out of 44991\n",
            "batch: 42841 out of 44991\n",
            "batch: 42842 out of 44991\n",
            "batch: 42843 out of 44991\n",
            "batch: 42844 out of 44991\n",
            "batch: 42845 out of 44991\n",
            "batch: 42846 out of 44991\n",
            "batch: 42847 out of 44991\n",
            "batch: 42848 out of 44991\n",
            "batch: 42849 out of 44991\n",
            "batch: 42850 out of 44991\n",
            "batch: 42851 out of 44991\n",
            "batch: 42852 out of 44991\n",
            "batch: 42853 out of 44991\n",
            "batch: 42854 out of 44991\n",
            "batch: 42855 out of 44991\n",
            "batch: 42856 out of 44991\n",
            "batch: 42857 out of 44991\n",
            "batch: 42858 out of 44991\n",
            "batch: 42859 out of 44991\n",
            "batch: 42860 out of 44991\n",
            "batch: 42861 out of 44991\n",
            "batch: 42862 out of 44991\n",
            "batch: 42863 out of 44991\n",
            "batch: 42864 out of 44991\n",
            "batch: 42865 out of 44991\n",
            "batch: 42866 out of 44991\n",
            "batch: 42867 out of 44991\n",
            "batch: 42868 out of 44991\n",
            "batch: 42869 out of 44991\n",
            "batch: 42870 out of 44991\n",
            "batch: 42871 out of 44991\n",
            "batch: 42872 out of 44991\n",
            "batch: 42873 out of 44991\n",
            "batch: 42874 out of 44991\n",
            "batch: 42875 out of 44991\n",
            "batch: 42876 out of 44991\n",
            "batch: 42877 out of 44991\n",
            "batch: 42878 out of 44991\n",
            "batch: 42879 out of 44991\n",
            "batch: 42880 out of 44991\n",
            "batch: 42881 out of 44991\n",
            "batch: 42882 out of 44991\n",
            "batch: 42883 out of 44991\n",
            "batch: 42884 out of 44991\n",
            "batch: 42885 out of 44991\n",
            "batch: 42886 out of 44991\n",
            "batch: 42887 out of 44991\n",
            "batch: 42888 out of 44991\n",
            "batch: 42889 out of 44991\n",
            "batch: 42890 out of 44991\n",
            "batch: 42891 out of 44991\n",
            "batch: 42892 out of 44991\n",
            "batch: 42893 out of 44991\n",
            "batch: 42894 out of 44991\n",
            "batch: 42895 out of 44991\n",
            "batch: 42896 out of 44991\n",
            "batch: 42897 out of 44991\n",
            "batch: 42898 out of 44991\n",
            "batch: 42899 out of 44991\n",
            "batch: 42900 out of 44991\n",
            "batch: 42901 out of 44991\n",
            "batch: 42902 out of 44991\n",
            "batch: 42903 out of 44991\n",
            "batch: 42904 out of 44991\n",
            "batch: 42905 out of 44991\n",
            "batch: 42906 out of 44991\n",
            "batch: 42907 out of 44991\n",
            "batch: 42908 out of 44991\n",
            "batch: 42909 out of 44991\n",
            "batch: 42910 out of 44991\n",
            "batch: 42911 out of 44991\n",
            "batch: 42912 out of 44991\n",
            "batch: 42913 out of 44991\n",
            "batch: 42914 out of 44991\n",
            "batch: 42915 out of 44991\n",
            "batch: 42916 out of 44991\n",
            "batch: 42917 out of 44991\n",
            "batch: 42918 out of 44991\n",
            "batch: 42919 out of 44991\n",
            "batch: 42920 out of 44991\n",
            "batch: 42921 out of 44991\n",
            "batch: 42922 out of 44991\n",
            "batch: 42923 out of 44991\n",
            "batch: 42924 out of 44991\n",
            "batch: 42925 out of 44991\n",
            "batch: 42926 out of 44991\n",
            "batch: 42927 out of 44991\n",
            "batch: 42928 out of 44991\n",
            "batch: 42929 out of 44991\n",
            "batch: 42930 out of 44991\n",
            "batch: 42931 out of 44991\n",
            "batch: 42932 out of 44991\n",
            "batch: 42933 out of 44991\n",
            "batch: 42934 out of 44991\n",
            "batch: 42935 out of 44991\n",
            "batch: 42936 out of 44991\n",
            "batch: 42937 out of 44991\n",
            "batch: 42938 out of 44991\n",
            "batch: 42939 out of 44991\n",
            "batch: 42940 out of 44991\n",
            "batch: 42941 out of 44991\n",
            "batch: 42942 out of 44991\n",
            "batch: 42943 out of 44991\n",
            "batch: 42944 out of 44991\n",
            "batch: 42945 out of 44991\n",
            "batch: 42946 out of 44991\n",
            "batch: 42947 out of 44991\n",
            "batch: 42948 out of 44991\n",
            "batch: 42949 out of 44991\n",
            "batch: 42950 out of 44991\n",
            "batch: 42951 out of 44991\n",
            "batch: 42952 out of 44991\n",
            "batch: 42953 out of 44991\n",
            "batch: 42954 out of 44991\n",
            "batch: 42955 out of 44991\n",
            "batch: 42956 out of 44991\n",
            "batch: 42957 out of 44991\n",
            "batch: 42958 out of 44991\n",
            "batch: 42959 out of 44991\n",
            "batch: 42960 out of 44991\n",
            "batch: 42961 out of 44991\n",
            "batch: 42962 out of 44991\n",
            "batch: 42963 out of 44991\n",
            "batch: 42964 out of 44991\n",
            "batch: 42965 out of 44991\n",
            "batch: 42966 out of 44991\n",
            "batch: 42967 out of 44991\n",
            "batch: 42968 out of 44991\n",
            "batch: 42969 out of 44991\n",
            "batch: 42970 out of 44991\n",
            "batch: 42971 out of 44991\n",
            "batch: 42972 out of 44991\n",
            "batch: 42973 out of 44991\n",
            "batch: 42974 out of 44991\n",
            "batch: 42975 out of 44991\n",
            "batch: 42976 out of 44991\n",
            "batch: 42977 out of 44991\n",
            "batch: 42978 out of 44991\n",
            "batch: 42979 out of 44991\n",
            "batch: 42980 out of 44991\n",
            "batch: 42981 out of 44991\n",
            "batch: 42982 out of 44991\n",
            "batch: 42983 out of 44991\n",
            "batch: 42984 out of 44991\n",
            "batch: 42985 out of 44991\n",
            "batch: 42986 out of 44991\n",
            "batch: 42987 out of 44991\n",
            "batch: 42988 out of 44991\n",
            "batch: 42989 out of 44991\n",
            "batch: 42990 out of 44991\n",
            "batch: 42991 out of 44991\n",
            "batch: 42992 out of 44991\n",
            "batch: 42993 out of 44991\n",
            "batch: 42994 out of 44991\n",
            "batch: 42995 out of 44991\n",
            "batch: 42996 out of 44991\n",
            "batch: 42997 out of 44991\n",
            "batch: 42998 out of 44991\n",
            "batch: 42999 out of 44991\n",
            "batch: 43000 out of 44991\n",
            "batch: 43001 out of 44991\n",
            "batch: 43002 out of 44991\n",
            "batch: 43003 out of 44991\n",
            "batch: 43004 out of 44991\n",
            "batch: 43005 out of 44991\n",
            "batch: 43006 out of 44991\n",
            "batch: 43007 out of 44991\n",
            "batch: 43008 out of 44991\n",
            "batch: 43009 out of 44991\n",
            "batch: 43010 out of 44991\n",
            "batch: 43011 out of 44991\n",
            "batch: 43012 out of 44991\n",
            "batch: 43013 out of 44991\n",
            "batch: 43014 out of 44991\n",
            "batch: 43015 out of 44991\n",
            "batch: 43016 out of 44991\n",
            "batch: 43017 out of 44991\n",
            "batch: 43018 out of 44991\n",
            "batch: 43019 out of 44991\n",
            "batch: 43020 out of 44991\n",
            "batch: 43021 out of 44991\n",
            "batch: 43022 out of 44991\n",
            "batch: 43023 out of 44991\n",
            "batch: 43024 out of 44991\n",
            "batch: 43025 out of 44991\n",
            "batch: 43026 out of 44991\n",
            "batch: 43027 out of 44991\n",
            "batch: 43028 out of 44991\n",
            "batch: 43029 out of 44991\n",
            "batch: 43030 out of 44991\n",
            "batch: 43031 out of 44991\n",
            "batch: 43032 out of 44991\n",
            "batch: 43033 out of 44991\n",
            "batch: 43034 out of 44991\n",
            "batch: 43035 out of 44991\n",
            "batch: 43036 out of 44991\n",
            "batch: 43037 out of 44991\n",
            "batch: 43038 out of 44991\n",
            "batch: 43039 out of 44991\n",
            "batch: 43040 out of 44991\n",
            "batch: 43041 out of 44991\n",
            "batch: 43042 out of 44991\n",
            "batch: 43043 out of 44991\n",
            "batch: 43044 out of 44991\n",
            "batch: 43045 out of 44991\n",
            "batch: 43046 out of 44991\n",
            "batch: 43047 out of 44991\n",
            "batch: 43048 out of 44991\n",
            "batch: 43049 out of 44991\n",
            "batch: 43050 out of 44991\n",
            "batch: 43051 out of 44991\n",
            "batch: 43052 out of 44991\n",
            "batch: 43053 out of 44991\n",
            "batch: 43054 out of 44991\n",
            "batch: 43055 out of 44991\n",
            "batch: 43056 out of 44991\n",
            "batch: 43057 out of 44991\n",
            "batch: 43058 out of 44991\n",
            "batch: 43059 out of 44991\n",
            "batch: 43060 out of 44991\n",
            "batch: 43061 out of 44991\n",
            "batch: 43062 out of 44991\n",
            "batch: 43063 out of 44991\n",
            "batch: 43064 out of 44991\n",
            "batch: 43065 out of 44991\n",
            "batch: 43066 out of 44991\n",
            "batch: 43067 out of 44991\n",
            "batch: 43068 out of 44991\n",
            "batch: 43069 out of 44991\n",
            "batch: 43070 out of 44991\n",
            "batch: 43071 out of 44991\n",
            "batch: 43072 out of 44991\n",
            "batch: 43073 out of 44991\n",
            "batch: 43074 out of 44991\n",
            "batch: 43075 out of 44991\n",
            "batch: 43076 out of 44991\n",
            "batch: 43077 out of 44991\n",
            "batch: 43078 out of 44991\n",
            "batch: 43079 out of 44991\n",
            "batch: 43080 out of 44991\n",
            "batch: 43081 out of 44991\n",
            "batch: 43082 out of 44991\n",
            "batch: 43083 out of 44991\n",
            "batch: 43084 out of 44991\n",
            "batch: 43085 out of 44991\n",
            "batch: 43086 out of 44991\n",
            "batch: 43087 out of 44991\n",
            "batch: 43088 out of 44991\n",
            "batch: 43089 out of 44991\n",
            "batch: 43090 out of 44991\n",
            "batch: 43091 out of 44991\n",
            "batch: 43092 out of 44991\n",
            "batch: 43093 out of 44991\n",
            "batch: 43094 out of 44991\n",
            "batch: 43095 out of 44991\n",
            "batch: 43096 out of 44991\n",
            "batch: 43097 out of 44991\n",
            "batch: 43098 out of 44991\n",
            "batch: 43099 out of 44991\n",
            "batch: 43100 out of 44991\n",
            "batch: 43101 out of 44991\n",
            "batch: 43102 out of 44991\n",
            "batch: 43103 out of 44991\n",
            "batch: 43104 out of 44991\n",
            "batch: 43105 out of 44991\n",
            "batch: 43106 out of 44991\n",
            "batch: 43107 out of 44991\n",
            "batch: 43108 out of 44991\n",
            "batch: 43109 out of 44991\n",
            "batch: 43110 out of 44991\n",
            "batch: 43111 out of 44991\n",
            "batch: 43112 out of 44991\n",
            "batch: 43113 out of 44991\n",
            "batch: 43114 out of 44991\n",
            "batch: 43115 out of 44991\n",
            "batch: 43116 out of 44991\n",
            "batch: 43117 out of 44991\n",
            "batch: 43118 out of 44991\n",
            "batch: 43119 out of 44991\n",
            "batch: 43120 out of 44991\n",
            "batch: 43121 out of 44991\n",
            "batch: 43122 out of 44991\n",
            "batch: 43123 out of 44991\n",
            "batch: 43124 out of 44991\n",
            "batch: 43125 out of 44991\n",
            "batch: 43126 out of 44991\n",
            "batch: 43127 out of 44991\n",
            "batch: 43128 out of 44991\n",
            "batch: 43129 out of 44991\n",
            "batch: 43130 out of 44991\n",
            "batch: 43131 out of 44991\n",
            "batch: 43132 out of 44991\n",
            "batch: 43133 out of 44991\n",
            "batch: 43134 out of 44991\n",
            "batch: 43135 out of 44991\n",
            "batch: 43136 out of 44991\n",
            "batch: 43137 out of 44991\n",
            "batch: 43138 out of 44991\n",
            "batch: 43139 out of 44991\n",
            "batch: 43140 out of 44991\n",
            "batch: 43141 out of 44991\n",
            "batch: 43142 out of 44991\n",
            "batch: 43143 out of 44991\n",
            "batch: 43144 out of 44991\n",
            "batch: 43145 out of 44991\n",
            "batch: 43146 out of 44991\n",
            "batch: 43147 out of 44991\n",
            "batch: 43148 out of 44991\n",
            "batch: 43149 out of 44991\n",
            "batch: 43150 out of 44991\n",
            "batch: 43151 out of 44991\n",
            "batch: 43152 out of 44991\n",
            "batch: 43153 out of 44991\n",
            "batch: 43154 out of 44991\n",
            "batch: 43155 out of 44991\n",
            "batch: 43156 out of 44991\n",
            "batch: 43157 out of 44991\n",
            "batch: 43158 out of 44991\n",
            "batch: 43159 out of 44991\n",
            "batch: 43160 out of 44991\n",
            "batch: 43161 out of 44991\n",
            "batch: 43162 out of 44991\n",
            "batch: 43163 out of 44991\n",
            "batch: 43164 out of 44991\n",
            "batch: 43165 out of 44991\n",
            "batch: 43166 out of 44991\n",
            "batch: 43167 out of 44991\n",
            "batch: 43168 out of 44991\n",
            "batch: 43169 out of 44991\n",
            "batch: 43170 out of 44991\n",
            "batch: 43171 out of 44991\n",
            "batch: 43172 out of 44991\n",
            "batch: 43173 out of 44991\n",
            "batch: 43174 out of 44991\n",
            "batch: 43175 out of 44991\n",
            "batch: 43176 out of 44991\n",
            "batch: 43177 out of 44991\n",
            "batch: 43178 out of 44991\n",
            "batch: 43179 out of 44991\n",
            "batch: 43180 out of 44991\n",
            "batch: 43181 out of 44991\n",
            "batch: 43182 out of 44991\n",
            "batch: 43183 out of 44991\n",
            "batch: 43184 out of 44991\n",
            "batch: 43185 out of 44991\n",
            "batch: 43186 out of 44991\n",
            "batch: 43187 out of 44991\n",
            "batch: 43188 out of 44991\n",
            "batch: 43189 out of 44991\n",
            "batch: 43190 out of 44991\n",
            "batch: 43191 out of 44991\n",
            "batch: 43192 out of 44991\n",
            "batch: 43193 out of 44991\n",
            "batch: 43194 out of 44991\n",
            "batch: 43195 out of 44991\n",
            "batch: 43196 out of 44991\n",
            "batch: 43197 out of 44991\n",
            "batch: 43198 out of 44991\n",
            "batch: 43199 out of 44991\n",
            "batch: 43200 out of 44991\n",
            "batch: 43201 out of 44991\n",
            "batch: 43202 out of 44991\n",
            "batch: 43203 out of 44991\n",
            "batch: 43204 out of 44991\n",
            "batch: 43205 out of 44991\n",
            "batch: 43206 out of 44991\n",
            "batch: 43207 out of 44991\n",
            "batch: 43208 out of 44991\n",
            "batch: 43209 out of 44991\n",
            "batch: 43210 out of 44991\n",
            "batch: 43211 out of 44991\n",
            "batch: 43212 out of 44991\n",
            "batch: 43213 out of 44991\n",
            "batch: 43214 out of 44991\n",
            "batch: 43215 out of 44991\n",
            "batch: 43216 out of 44991\n",
            "batch: 43217 out of 44991\n",
            "batch: 43218 out of 44991\n",
            "batch: 43219 out of 44991\n",
            "batch: 43220 out of 44991\n",
            "batch: 43221 out of 44991\n",
            "batch: 43222 out of 44991\n",
            "batch: 43223 out of 44991\n",
            "batch: 43224 out of 44991\n",
            "batch: 43225 out of 44991\n",
            "batch: 43226 out of 44991\n",
            "batch: 43227 out of 44991\n",
            "batch: 43228 out of 44991\n",
            "batch: 43229 out of 44991\n",
            "batch: 43230 out of 44991\n",
            "batch: 43231 out of 44991\n",
            "batch: 43232 out of 44991\n",
            "batch: 43233 out of 44991\n",
            "batch: 43234 out of 44991\n",
            "batch: 43235 out of 44991\n",
            "batch: 43236 out of 44991\n",
            "batch: 43237 out of 44991\n",
            "batch: 43238 out of 44991\n",
            "batch: 43239 out of 44991\n",
            "batch: 43240 out of 44991\n",
            "batch: 43241 out of 44991\n",
            "batch: 43242 out of 44991\n",
            "batch: 43243 out of 44991\n",
            "batch: 43244 out of 44991\n",
            "batch: 43245 out of 44991\n",
            "batch: 43246 out of 44991\n",
            "batch: 43247 out of 44991\n",
            "batch: 43248 out of 44991\n",
            "batch: 43249 out of 44991\n",
            "batch: 43250 out of 44991\n",
            "batch: 43251 out of 44991\n",
            "batch: 43252 out of 44991\n",
            "batch: 43253 out of 44991\n",
            "batch: 43254 out of 44991\n",
            "batch: 43255 out of 44991\n",
            "batch: 43256 out of 44991\n",
            "batch: 43257 out of 44991\n",
            "batch: 43258 out of 44991\n",
            "batch: 43259 out of 44991\n",
            "batch: 43260 out of 44991\n",
            "batch: 43261 out of 44991\n",
            "batch: 43262 out of 44991\n",
            "batch: 43263 out of 44991\n",
            "batch: 43264 out of 44991\n",
            "batch: 43265 out of 44991\n",
            "batch: 43266 out of 44991\n",
            "batch: 43267 out of 44991\n",
            "batch: 43268 out of 44991\n",
            "batch: 43269 out of 44991\n",
            "batch: 43270 out of 44991\n",
            "batch: 43271 out of 44991\n",
            "batch: 43272 out of 44991\n",
            "batch: 43273 out of 44991\n",
            "batch: 43274 out of 44991\n",
            "batch: 43275 out of 44991\n",
            "batch: 43276 out of 44991\n",
            "batch: 43277 out of 44991\n",
            "batch: 43278 out of 44991\n",
            "batch: 43279 out of 44991\n",
            "batch: 43280 out of 44991\n",
            "batch: 43281 out of 44991\n",
            "batch: 43282 out of 44991\n",
            "batch: 43283 out of 44991\n",
            "batch: 43284 out of 44991\n",
            "batch: 43285 out of 44991\n",
            "batch: 43286 out of 44991\n",
            "batch: 43287 out of 44991\n",
            "batch: 43288 out of 44991\n",
            "batch: 43289 out of 44991\n",
            "batch: 43290 out of 44991\n",
            "batch: 43291 out of 44991\n",
            "batch: 43292 out of 44991\n",
            "batch: 43293 out of 44991\n",
            "batch: 43294 out of 44991\n",
            "batch: 43295 out of 44991\n",
            "batch: 43296 out of 44991\n",
            "batch: 43297 out of 44991\n",
            "batch: 43298 out of 44991\n",
            "batch: 43299 out of 44991\n",
            "batch: 43300 out of 44991\n",
            "batch: 43301 out of 44991\n",
            "batch: 43302 out of 44991\n",
            "batch: 43303 out of 44991\n",
            "batch: 43304 out of 44991\n",
            "batch: 43305 out of 44991\n",
            "batch: 43306 out of 44991\n",
            "batch: 43307 out of 44991\n",
            "batch: 43308 out of 44991\n",
            "batch: 43309 out of 44991\n",
            "batch: 43310 out of 44991\n",
            "batch: 43311 out of 44991\n",
            "batch: 43312 out of 44991\n",
            "batch: 43313 out of 44991\n",
            "batch: 43314 out of 44991\n",
            "batch: 43315 out of 44991\n",
            "batch: 43316 out of 44991\n",
            "batch: 43317 out of 44991\n",
            "batch: 43318 out of 44991\n",
            "batch: 43319 out of 44991\n",
            "batch: 43320 out of 44991\n",
            "batch: 43321 out of 44991\n",
            "batch: 43322 out of 44991\n",
            "batch: 43323 out of 44991\n",
            "batch: 43324 out of 44991\n",
            "batch: 43325 out of 44991\n",
            "batch: 43326 out of 44991\n",
            "batch: 43327 out of 44991\n",
            "batch: 43328 out of 44991\n",
            "batch: 43329 out of 44991\n",
            "batch: 43330 out of 44991\n",
            "batch: 43331 out of 44991\n",
            "batch: 43332 out of 44991\n",
            "batch: 43333 out of 44991\n",
            "batch: 43334 out of 44991\n",
            "batch: 43335 out of 44991\n",
            "batch: 43336 out of 44991\n",
            "batch: 43337 out of 44991\n",
            "batch: 43338 out of 44991\n",
            "batch: 43339 out of 44991\n",
            "batch: 43340 out of 44991\n",
            "batch: 43341 out of 44991\n",
            "batch: 43342 out of 44991\n",
            "batch: 43343 out of 44991\n",
            "batch: 43344 out of 44991\n",
            "batch: 43345 out of 44991\n",
            "batch: 43346 out of 44991\n",
            "batch: 43347 out of 44991\n",
            "batch: 43348 out of 44991\n",
            "batch: 43349 out of 44991\n",
            "batch: 43350 out of 44991\n",
            "batch: 43351 out of 44991\n",
            "batch: 43352 out of 44991\n",
            "batch: 43353 out of 44991\n",
            "batch: 43354 out of 44991\n",
            "batch: 43355 out of 44991\n",
            "batch: 43356 out of 44991\n",
            "batch: 43357 out of 44991\n",
            "batch: 43358 out of 44991\n",
            "batch: 43359 out of 44991\n",
            "batch: 43360 out of 44991\n",
            "batch: 43361 out of 44991\n",
            "batch: 43362 out of 44991\n",
            "batch: 43363 out of 44991\n",
            "batch: 43364 out of 44991\n",
            "batch: 43365 out of 44991\n",
            "batch: 43366 out of 44991\n",
            "batch: 43367 out of 44991\n",
            "batch: 43368 out of 44991\n",
            "batch: 43369 out of 44991\n",
            "batch: 43370 out of 44991\n",
            "batch: 43371 out of 44991\n",
            "batch: 43372 out of 44991\n",
            "batch: 43373 out of 44991\n",
            "batch: 43374 out of 44991\n",
            "batch: 43375 out of 44991\n",
            "batch: 43376 out of 44991\n",
            "batch: 43377 out of 44991\n",
            "batch: 43378 out of 44991\n",
            "batch: 43379 out of 44991\n",
            "batch: 43380 out of 44991\n",
            "batch: 43381 out of 44991\n",
            "batch: 43382 out of 44991\n",
            "batch: 43383 out of 44991\n",
            "batch: 43384 out of 44991\n",
            "batch: 43385 out of 44991\n",
            "batch: 43386 out of 44991\n",
            "batch: 43387 out of 44991\n",
            "batch: 43388 out of 44991\n",
            "batch: 43389 out of 44991\n",
            "batch: 43390 out of 44991\n",
            "batch: 43391 out of 44991\n",
            "batch: 43392 out of 44991\n",
            "batch: 43393 out of 44991\n",
            "batch: 43394 out of 44991\n",
            "batch: 43395 out of 44991\n",
            "batch: 43396 out of 44991\n",
            "batch: 43397 out of 44991\n",
            "batch: 43398 out of 44991\n",
            "batch: 43399 out of 44991\n",
            "batch: 43400 out of 44991\n",
            "batch: 43401 out of 44991\n",
            "batch: 43402 out of 44991\n",
            "batch: 43403 out of 44991\n",
            "batch: 43404 out of 44991\n",
            "batch: 43405 out of 44991\n",
            "batch: 43406 out of 44991\n",
            "batch: 43407 out of 44991\n",
            "batch: 43408 out of 44991\n",
            "batch: 43409 out of 44991\n",
            "batch: 43410 out of 44991\n",
            "batch: 43411 out of 44991\n",
            "batch: 43412 out of 44991\n",
            "batch: 43413 out of 44991\n",
            "batch: 43414 out of 44991\n",
            "batch: 43415 out of 44991\n",
            "batch: 43416 out of 44991\n",
            "batch: 43417 out of 44991\n",
            "batch: 43418 out of 44991\n",
            "batch: 43419 out of 44991\n",
            "batch: 43420 out of 44991\n",
            "batch: 43421 out of 44991\n",
            "batch: 43422 out of 44991\n",
            "batch: 43423 out of 44991\n",
            "batch: 43424 out of 44991\n",
            "batch: 43425 out of 44991\n",
            "batch: 43426 out of 44991\n",
            "batch: 43427 out of 44991\n",
            "batch: 43428 out of 44991\n",
            "batch: 43429 out of 44991\n",
            "batch: 43430 out of 44991\n",
            "batch: 43431 out of 44991\n",
            "batch: 43432 out of 44991\n",
            "batch: 43433 out of 44991\n",
            "batch: 43434 out of 44991\n",
            "batch: 43435 out of 44991\n",
            "batch: 43436 out of 44991\n",
            "batch: 43437 out of 44991\n",
            "batch: 43438 out of 44991\n",
            "batch: 43439 out of 44991\n",
            "batch: 43440 out of 44991\n",
            "batch: 43441 out of 44991\n",
            "batch: 43442 out of 44991\n",
            "batch: 43443 out of 44991\n",
            "batch: 43444 out of 44991\n",
            "batch: 43445 out of 44991\n",
            "batch: 43446 out of 44991\n",
            "batch: 43447 out of 44991\n",
            "batch: 43448 out of 44991\n",
            "batch: 43449 out of 44991\n",
            "batch: 43450 out of 44991\n",
            "batch: 43451 out of 44991\n",
            "batch: 43452 out of 44991\n",
            "batch: 43453 out of 44991\n",
            "batch: 43454 out of 44991\n",
            "batch: 43455 out of 44991\n",
            "batch: 43456 out of 44991\n",
            "batch: 43457 out of 44991\n",
            "batch: 43458 out of 44991\n",
            "batch: 43459 out of 44991\n",
            "batch: 43460 out of 44991\n",
            "batch: 43461 out of 44991\n",
            "batch: 43462 out of 44991\n",
            "batch: 43463 out of 44991\n",
            "batch: 43464 out of 44991\n",
            "batch: 43465 out of 44991\n",
            "batch: 43466 out of 44991\n",
            "batch: 43467 out of 44991\n",
            "batch: 43468 out of 44991\n",
            "batch: 43469 out of 44991\n",
            "batch: 43470 out of 44991\n",
            "batch: 43471 out of 44991\n",
            "batch: 43472 out of 44991\n",
            "batch: 43473 out of 44991\n",
            "batch: 43474 out of 44991\n",
            "batch: 43475 out of 44991\n",
            "batch: 43476 out of 44991\n",
            "batch: 43477 out of 44991\n",
            "batch: 43478 out of 44991\n",
            "batch: 43479 out of 44991\n",
            "batch: 43480 out of 44991\n",
            "batch: 43481 out of 44991\n",
            "batch: 43482 out of 44991\n",
            "batch: 43483 out of 44991\n",
            "batch: 43484 out of 44991\n",
            "batch: 43485 out of 44991\n",
            "batch: 43486 out of 44991\n",
            "batch: 43487 out of 44991\n",
            "batch: 43488 out of 44991\n",
            "batch: 43489 out of 44991\n",
            "batch: 43490 out of 44991\n",
            "batch: 43491 out of 44991\n",
            "batch: 43492 out of 44991\n",
            "batch: 43493 out of 44991\n",
            "batch: 43494 out of 44991\n",
            "batch: 43495 out of 44991\n",
            "batch: 43496 out of 44991\n",
            "batch: 43497 out of 44991\n",
            "batch: 43498 out of 44991\n",
            "batch: 43499 out of 44991\n",
            "batch: 43500 out of 44991\n",
            "batch: 43501 out of 44991\n",
            "batch: 43502 out of 44991\n",
            "batch: 43503 out of 44991\n",
            "batch: 43504 out of 44991\n",
            "batch: 43505 out of 44991\n",
            "batch: 43506 out of 44991\n",
            "batch: 43507 out of 44991\n",
            "batch: 43508 out of 44991\n",
            "batch: 43509 out of 44991\n",
            "batch: 43510 out of 44991\n",
            "batch: 43511 out of 44991\n",
            "batch: 43512 out of 44991\n",
            "batch: 43513 out of 44991\n",
            "batch: 43514 out of 44991\n",
            "batch: 43515 out of 44991\n",
            "batch: 43516 out of 44991\n",
            "batch: 43517 out of 44991\n",
            "batch: 43518 out of 44991\n",
            "batch: 43519 out of 44991\n",
            "batch: 43520 out of 44991\n",
            "batch: 43521 out of 44991\n",
            "batch: 43522 out of 44991\n",
            "batch: 43523 out of 44991\n",
            "batch: 43524 out of 44991\n",
            "batch: 43525 out of 44991\n",
            "batch: 43526 out of 44991\n",
            "batch: 43527 out of 44991\n",
            "batch: 43528 out of 44991\n",
            "batch: 43529 out of 44991\n",
            "batch: 43530 out of 44991\n",
            "batch: 43531 out of 44991\n",
            "batch: 43532 out of 44991\n",
            "batch: 43533 out of 44991\n",
            "batch: 43534 out of 44991\n",
            "batch: 43535 out of 44991\n",
            "batch: 43536 out of 44991\n",
            "batch: 43537 out of 44991\n",
            "batch: 43538 out of 44991\n",
            "batch: 43539 out of 44991\n",
            "batch: 43540 out of 44991\n",
            "batch: 43541 out of 44991\n",
            "batch: 43542 out of 44991\n",
            "batch: 43543 out of 44991\n",
            "batch: 43544 out of 44991\n",
            "batch: 43545 out of 44991\n",
            "batch: 43546 out of 44991\n",
            "batch: 43547 out of 44991\n",
            "batch: 43548 out of 44991\n",
            "batch: 43549 out of 44991\n",
            "batch: 43550 out of 44991\n",
            "batch: 43551 out of 44991\n",
            "batch: 43552 out of 44991\n",
            "batch: 43553 out of 44991\n",
            "batch: 43554 out of 44991\n",
            "batch: 43555 out of 44991\n",
            "batch: 43556 out of 44991\n",
            "batch: 43557 out of 44991\n",
            "batch: 43558 out of 44991\n",
            "batch: 43559 out of 44991\n",
            "batch: 43560 out of 44991\n",
            "batch: 43561 out of 44991\n",
            "batch: 43562 out of 44991\n",
            "batch: 43563 out of 44991\n",
            "batch: 43564 out of 44991\n",
            "batch: 43565 out of 44991\n",
            "batch: 43566 out of 44991\n",
            "batch: 43567 out of 44991\n",
            "batch: 43568 out of 44991\n",
            "batch: 43569 out of 44991\n",
            "batch: 43570 out of 44991\n",
            "batch: 43571 out of 44991\n",
            "batch: 43572 out of 44991\n",
            "batch: 43573 out of 44991\n",
            "batch: 43574 out of 44991\n",
            "batch: 43575 out of 44991\n",
            "batch: 43576 out of 44991\n",
            "batch: 43577 out of 44991\n",
            "batch: 43578 out of 44991\n",
            "batch: 43579 out of 44991\n",
            "batch: 43580 out of 44991\n",
            "batch: 43581 out of 44991\n",
            "batch: 43582 out of 44991\n",
            "batch: 43583 out of 44991\n",
            "batch: 43584 out of 44991\n",
            "batch: 43585 out of 44991\n",
            "batch: 43586 out of 44991\n",
            "batch: 43587 out of 44991\n",
            "batch: 43588 out of 44991\n",
            "batch: 43589 out of 44991\n",
            "batch: 43590 out of 44991\n",
            "batch: 43591 out of 44991\n",
            "batch: 43592 out of 44991\n",
            "batch: 43593 out of 44991\n",
            "batch: 43594 out of 44991\n",
            "batch: 43595 out of 44991\n",
            "batch: 43596 out of 44991\n",
            "batch: 43597 out of 44991\n",
            "batch: 43598 out of 44991\n",
            "batch: 43599 out of 44991\n",
            "batch: 43600 out of 44991\n",
            "batch: 43601 out of 44991\n",
            "batch: 43602 out of 44991\n",
            "batch: 43603 out of 44991\n",
            "batch: 43604 out of 44991\n",
            "batch: 43605 out of 44991\n",
            "batch: 43606 out of 44991\n",
            "batch: 43607 out of 44991\n",
            "batch: 43608 out of 44991\n",
            "batch: 43609 out of 44991\n",
            "batch: 43610 out of 44991\n",
            "batch: 43611 out of 44991\n",
            "batch: 43612 out of 44991\n",
            "batch: 43613 out of 44991\n",
            "batch: 43614 out of 44991\n",
            "batch: 43615 out of 44991\n",
            "batch: 43616 out of 44991\n",
            "batch: 43617 out of 44991\n",
            "batch: 43618 out of 44991\n",
            "batch: 43619 out of 44991\n",
            "batch: 43620 out of 44991\n",
            "batch: 43621 out of 44991\n",
            "batch: 43622 out of 44991\n",
            "batch: 43623 out of 44991\n",
            "batch: 43624 out of 44991\n",
            "batch: 43625 out of 44991\n",
            "batch: 43626 out of 44991\n",
            "batch: 43627 out of 44991\n",
            "batch: 43628 out of 44991\n",
            "batch: 43629 out of 44991\n",
            "batch: 43630 out of 44991\n",
            "batch: 43631 out of 44991\n",
            "batch: 43632 out of 44991\n",
            "batch: 43633 out of 44991\n",
            "batch: 43634 out of 44991\n",
            "batch: 43635 out of 44991\n",
            "batch: 43636 out of 44991\n",
            "batch: 43637 out of 44991\n",
            "batch: 43638 out of 44991\n",
            "batch: 43639 out of 44991\n",
            "batch: 43640 out of 44991\n",
            "batch: 43641 out of 44991\n",
            "batch: 43642 out of 44991\n",
            "batch: 43643 out of 44991\n",
            "batch: 43644 out of 44991\n",
            "batch: 43645 out of 44991\n",
            "batch: 43646 out of 44991\n",
            "batch: 43647 out of 44991\n",
            "batch: 43648 out of 44991\n",
            "batch: 43649 out of 44991\n",
            "batch: 43650 out of 44991\n",
            "batch: 43651 out of 44991\n",
            "batch: 43652 out of 44991\n",
            "batch: 43653 out of 44991\n",
            "batch: 43654 out of 44991\n",
            "batch: 43655 out of 44991\n",
            "batch: 43656 out of 44991\n",
            "batch: 43657 out of 44991\n",
            "batch: 43658 out of 44991\n",
            "batch: 43659 out of 44991\n",
            "batch: 43660 out of 44991\n",
            "batch: 43661 out of 44991\n",
            "batch: 43662 out of 44991\n",
            "batch: 43663 out of 44991\n",
            "batch: 43664 out of 44991\n",
            "batch: 43665 out of 44991\n",
            "batch: 43666 out of 44991\n",
            "batch: 43667 out of 44991\n",
            "batch: 43668 out of 44991\n",
            "batch: 43669 out of 44991\n",
            "batch: 43670 out of 44991\n",
            "batch: 43671 out of 44991\n",
            "batch: 43672 out of 44991\n",
            "batch: 43673 out of 44991\n",
            "batch: 43674 out of 44991\n",
            "batch: 43675 out of 44991\n",
            "batch: 43676 out of 44991\n",
            "batch: 43677 out of 44991\n",
            "batch: 43678 out of 44991\n",
            "batch: 43679 out of 44991\n",
            "batch: 43680 out of 44991\n",
            "batch: 43681 out of 44991\n",
            "batch: 43682 out of 44991\n",
            "batch: 43683 out of 44991\n",
            "batch: 43684 out of 44991\n",
            "batch: 43685 out of 44991\n",
            "batch: 43686 out of 44991\n",
            "batch: 43687 out of 44991\n",
            "batch: 43688 out of 44991\n",
            "batch: 43689 out of 44991\n",
            "batch: 43690 out of 44991\n",
            "batch: 43691 out of 44991\n",
            "batch: 43692 out of 44991\n",
            "batch: 43693 out of 44991\n",
            "batch: 43694 out of 44991\n",
            "batch: 43695 out of 44991\n",
            "batch: 43696 out of 44991\n",
            "batch: 43697 out of 44991\n",
            "batch: 43698 out of 44991\n",
            "batch: 43699 out of 44991\n",
            "batch: 43700 out of 44991\n",
            "batch: 43701 out of 44991\n",
            "batch: 43702 out of 44991\n",
            "batch: 43703 out of 44991\n",
            "batch: 43704 out of 44991\n",
            "batch: 43705 out of 44991\n",
            "batch: 43706 out of 44991\n",
            "batch: 43707 out of 44991\n",
            "batch: 43708 out of 44991\n",
            "batch: 43709 out of 44991\n",
            "batch: 43710 out of 44991\n",
            "batch: 43711 out of 44991\n",
            "batch: 43712 out of 44991\n",
            "batch: 43713 out of 44991\n",
            "batch: 43714 out of 44991\n",
            "batch: 43715 out of 44991\n",
            "batch: 43716 out of 44991\n",
            "batch: 43717 out of 44991\n",
            "batch: 43718 out of 44991\n",
            "batch: 43719 out of 44991\n",
            "batch: 43720 out of 44991\n",
            "batch: 43721 out of 44991\n",
            "batch: 43722 out of 44991\n",
            "batch: 43723 out of 44991\n",
            "batch: 43724 out of 44991\n",
            "batch: 43725 out of 44991\n",
            "batch: 43726 out of 44991\n",
            "batch: 43727 out of 44991\n",
            "batch: 43728 out of 44991\n",
            "batch: 43729 out of 44991\n",
            "batch: 43730 out of 44991\n",
            "batch: 43731 out of 44991\n",
            "batch: 43732 out of 44991\n",
            "batch: 43733 out of 44991\n",
            "batch: 43734 out of 44991\n",
            "batch: 43735 out of 44991\n",
            "batch: 43736 out of 44991\n",
            "batch: 43737 out of 44991\n",
            "batch: 43738 out of 44991\n",
            "batch: 43739 out of 44991\n",
            "batch: 43740 out of 44991\n",
            "batch: 43741 out of 44991\n",
            "batch: 43742 out of 44991\n",
            "batch: 43743 out of 44991\n",
            "batch: 43744 out of 44991\n",
            "batch: 43745 out of 44991\n",
            "batch: 43746 out of 44991\n",
            "batch: 43747 out of 44991\n",
            "batch: 43748 out of 44991\n",
            "batch: 43749 out of 44991\n",
            "batch: 43750 out of 44991\n",
            "batch: 43751 out of 44991\n",
            "batch: 43752 out of 44991\n",
            "batch: 43753 out of 44991\n",
            "batch: 43754 out of 44991\n",
            "batch: 43755 out of 44991\n",
            "batch: 43756 out of 44991\n",
            "batch: 43757 out of 44991\n",
            "batch: 43758 out of 44991\n",
            "batch: 43759 out of 44991\n",
            "batch: 43760 out of 44991\n",
            "batch: 43761 out of 44991\n",
            "batch: 43762 out of 44991\n",
            "batch: 43763 out of 44991\n",
            "batch: 43764 out of 44991\n",
            "batch: 43765 out of 44991\n",
            "batch: 43766 out of 44991\n",
            "batch: 43767 out of 44991\n",
            "batch: 43768 out of 44991\n",
            "batch: 43769 out of 44991\n",
            "batch: 43770 out of 44991\n",
            "batch: 43771 out of 44991\n",
            "batch: 43772 out of 44991\n",
            "batch: 43773 out of 44991\n",
            "batch: 43774 out of 44991\n",
            "batch: 43775 out of 44991\n",
            "batch: 43776 out of 44991\n",
            "batch: 43777 out of 44991\n",
            "batch: 43778 out of 44991\n",
            "batch: 43779 out of 44991\n",
            "batch: 43780 out of 44991\n",
            "batch: 43781 out of 44991\n",
            "batch: 43782 out of 44991\n",
            "batch: 43783 out of 44991\n",
            "batch: 43784 out of 44991\n",
            "batch: 43785 out of 44991\n",
            "batch: 43786 out of 44991\n",
            "batch: 43787 out of 44991\n",
            "batch: 43788 out of 44991\n",
            "batch: 43789 out of 44991\n",
            "batch: 43790 out of 44991\n",
            "batch: 43791 out of 44991\n",
            "batch: 43792 out of 44991\n",
            "batch: 43793 out of 44991\n",
            "batch: 43794 out of 44991\n",
            "batch: 43795 out of 44991\n",
            "batch: 43796 out of 44991\n",
            "batch: 43797 out of 44991\n",
            "batch: 43798 out of 44991\n",
            "batch: 43799 out of 44991\n",
            "batch: 43800 out of 44991\n",
            "batch: 43801 out of 44991\n",
            "batch: 43802 out of 44991\n",
            "batch: 43803 out of 44991\n",
            "batch: 43804 out of 44991\n",
            "batch: 43805 out of 44991\n",
            "batch: 43806 out of 44991\n",
            "batch: 43807 out of 44991\n",
            "batch: 43808 out of 44991\n",
            "batch: 43809 out of 44991\n",
            "batch: 43810 out of 44991\n",
            "batch: 43811 out of 44991\n",
            "batch: 43812 out of 44991\n",
            "batch: 43813 out of 44991\n",
            "batch: 43814 out of 44991\n",
            "batch: 43815 out of 44991\n",
            "batch: 43816 out of 44991\n",
            "batch: 43817 out of 44991\n",
            "batch: 43818 out of 44991\n",
            "batch: 43819 out of 44991\n",
            "batch: 43820 out of 44991\n",
            "batch: 43821 out of 44991\n",
            "batch: 43822 out of 44991\n",
            "batch: 43823 out of 44991\n",
            "batch: 43824 out of 44991\n",
            "batch: 43825 out of 44991\n",
            "batch: 43826 out of 44991\n",
            "batch: 43827 out of 44991\n",
            "batch: 43828 out of 44991\n",
            "batch: 43829 out of 44991\n",
            "batch: 43830 out of 44991\n",
            "batch: 43831 out of 44991\n",
            "batch: 43832 out of 44991\n",
            "batch: 43833 out of 44991\n",
            "batch: 43834 out of 44991\n",
            "batch: 43835 out of 44991\n",
            "batch: 43836 out of 44991\n",
            "batch: 43837 out of 44991\n",
            "batch: 43838 out of 44991\n",
            "batch: 43839 out of 44991\n",
            "batch: 43840 out of 44991\n",
            "batch: 43841 out of 44991\n",
            "batch: 43842 out of 44991\n",
            "batch: 43843 out of 44991\n",
            "batch: 43844 out of 44991\n",
            "batch: 43845 out of 44991\n",
            "batch: 43846 out of 44991\n",
            "batch: 43847 out of 44991\n",
            "batch: 43848 out of 44991\n",
            "batch: 43849 out of 44991\n",
            "batch: 43850 out of 44991\n",
            "batch: 43851 out of 44991\n",
            "batch: 43852 out of 44991\n",
            "batch: 43853 out of 44991\n",
            "batch: 43854 out of 44991\n",
            "batch: 43855 out of 44991\n",
            "batch: 43856 out of 44991\n",
            "batch: 43857 out of 44991\n",
            "batch: 43858 out of 44991\n",
            "batch: 43859 out of 44991\n",
            "batch: 43860 out of 44991\n",
            "batch: 43861 out of 44991\n",
            "batch: 43862 out of 44991\n",
            "batch: 43863 out of 44991\n",
            "batch: 43864 out of 44991\n",
            "batch: 43865 out of 44991\n",
            "batch: 43866 out of 44991\n",
            "batch: 43867 out of 44991\n",
            "batch: 43868 out of 44991\n",
            "batch: 43869 out of 44991\n",
            "batch: 43870 out of 44991\n",
            "batch: 43871 out of 44991\n",
            "batch: 43872 out of 44991\n",
            "batch: 43873 out of 44991\n",
            "batch: 43874 out of 44991\n",
            "batch: 43875 out of 44991\n",
            "batch: 43876 out of 44991\n",
            "batch: 43877 out of 44991\n",
            "batch: 43878 out of 44991\n",
            "batch: 43879 out of 44991\n",
            "batch: 43880 out of 44991\n",
            "batch: 43881 out of 44991\n",
            "batch: 43882 out of 44991\n",
            "batch: 43883 out of 44991\n",
            "batch: 43884 out of 44991\n",
            "batch: 43885 out of 44991\n",
            "batch: 43886 out of 44991\n",
            "batch: 43887 out of 44991\n",
            "batch: 43888 out of 44991\n",
            "batch: 43889 out of 44991\n",
            "batch: 43890 out of 44991\n",
            "batch: 43891 out of 44991\n",
            "batch: 43892 out of 44991\n",
            "batch: 43893 out of 44991\n",
            "batch: 43894 out of 44991\n",
            "batch: 43895 out of 44991\n",
            "batch: 43896 out of 44991\n",
            "batch: 43897 out of 44991\n",
            "batch: 43898 out of 44991\n",
            "batch: 43899 out of 44991\n",
            "batch: 43900 out of 44991\n",
            "batch: 43901 out of 44991\n",
            "batch: 43902 out of 44991\n",
            "batch: 43903 out of 44991\n",
            "batch: 43904 out of 44991\n",
            "batch: 43905 out of 44991\n",
            "batch: 43906 out of 44991\n",
            "batch: 43907 out of 44991\n",
            "batch: 43908 out of 44991\n",
            "batch: 43909 out of 44991\n",
            "batch: 43910 out of 44991\n",
            "batch: 43911 out of 44991\n",
            "batch: 43912 out of 44991\n",
            "batch: 43913 out of 44991\n",
            "batch: 43914 out of 44991\n",
            "batch: 43915 out of 44991\n",
            "batch: 43916 out of 44991\n",
            "batch: 43917 out of 44991\n",
            "batch: 43918 out of 44991\n",
            "batch: 43919 out of 44991\n",
            "batch: 43920 out of 44991\n",
            "batch: 43921 out of 44991\n",
            "batch: 43922 out of 44991\n",
            "batch: 43923 out of 44991\n",
            "batch: 43924 out of 44991\n",
            "batch: 43925 out of 44991\n",
            "batch: 43926 out of 44991\n",
            "batch: 43927 out of 44991\n",
            "batch: 43928 out of 44991\n",
            "batch: 43929 out of 44991\n",
            "batch: 43930 out of 44991\n",
            "batch: 43931 out of 44991\n",
            "batch: 43932 out of 44991\n",
            "batch: 43933 out of 44991\n",
            "batch: 43934 out of 44991\n",
            "batch: 43935 out of 44991\n",
            "batch: 43936 out of 44991\n",
            "batch: 43937 out of 44991\n",
            "batch: 43938 out of 44991\n",
            "batch: 43939 out of 44991\n",
            "batch: 43940 out of 44991\n",
            "batch: 43941 out of 44991\n",
            "batch: 43942 out of 44991\n",
            "batch: 43943 out of 44991\n",
            "batch: 43944 out of 44991\n",
            "batch: 43945 out of 44991\n",
            "batch: 43946 out of 44991\n",
            "batch: 43947 out of 44991\n",
            "batch: 43948 out of 44991\n",
            "batch: 43949 out of 44991\n",
            "batch: 43950 out of 44991\n",
            "batch: 43951 out of 44991\n",
            "batch: 43952 out of 44991\n",
            "batch: 43953 out of 44991\n",
            "batch: 43954 out of 44991\n",
            "batch: 43955 out of 44991\n",
            "batch: 43956 out of 44991\n",
            "batch: 43957 out of 44991\n",
            "batch: 43958 out of 44991\n",
            "batch: 43959 out of 44991\n",
            "batch: 43960 out of 44991\n",
            "batch: 43961 out of 44991\n",
            "batch: 43962 out of 44991\n",
            "batch: 43963 out of 44991\n",
            "batch: 43964 out of 44991\n",
            "batch: 43965 out of 44991\n",
            "batch: 43966 out of 44991\n",
            "batch: 43967 out of 44991\n",
            "batch: 43968 out of 44991\n",
            "batch: 43969 out of 44991\n",
            "batch: 43970 out of 44991\n",
            "batch: 43971 out of 44991\n",
            "batch: 43972 out of 44991\n",
            "batch: 43973 out of 44991\n",
            "batch: 43974 out of 44991\n",
            "batch: 43975 out of 44991\n",
            "batch: 43976 out of 44991\n",
            "batch: 43977 out of 44991\n",
            "batch: 43978 out of 44991\n",
            "batch: 43979 out of 44991\n",
            "batch: 43980 out of 44991\n",
            "batch: 43981 out of 44991\n",
            "batch: 43982 out of 44991\n",
            "batch: 43983 out of 44991\n",
            "batch: 43984 out of 44991\n",
            "batch: 43985 out of 44991\n",
            "batch: 43986 out of 44991\n",
            "batch: 43987 out of 44991\n",
            "batch: 43988 out of 44991\n",
            "batch: 43989 out of 44991\n",
            "batch: 43990 out of 44991\n",
            "batch: 43991 out of 44991\n",
            "batch: 43992 out of 44991\n",
            "batch: 43993 out of 44991\n",
            "batch: 43994 out of 44991\n",
            "batch: 43995 out of 44991\n",
            "batch: 43996 out of 44991\n",
            "batch: 43997 out of 44991\n",
            "batch: 43998 out of 44991\n",
            "batch: 43999 out of 44991\n",
            "batch: 44000 out of 44991\n",
            "batch: 44001 out of 44991\n",
            "batch: 44002 out of 44991\n",
            "batch: 44003 out of 44991\n",
            "batch: 44004 out of 44991\n",
            "batch: 44005 out of 44991\n",
            "batch: 44006 out of 44991\n",
            "batch: 44007 out of 44991\n",
            "batch: 44008 out of 44991\n",
            "batch: 44009 out of 44991\n",
            "batch: 44010 out of 44991\n",
            "batch: 44011 out of 44991\n",
            "batch: 44012 out of 44991\n",
            "batch: 44013 out of 44991\n",
            "batch: 44014 out of 44991\n",
            "batch: 44015 out of 44991\n",
            "batch: 44016 out of 44991\n",
            "batch: 44017 out of 44991\n",
            "batch: 44018 out of 44991\n",
            "batch: 44019 out of 44991\n",
            "batch: 44020 out of 44991\n",
            "batch: 44021 out of 44991\n",
            "batch: 44022 out of 44991\n",
            "batch: 44023 out of 44991\n",
            "batch: 44024 out of 44991\n",
            "batch: 44025 out of 44991\n",
            "batch: 44026 out of 44991\n",
            "batch: 44027 out of 44991\n",
            "batch: 44028 out of 44991\n",
            "batch: 44029 out of 44991\n",
            "batch: 44030 out of 44991\n",
            "batch: 44031 out of 44991\n",
            "batch: 44032 out of 44991\n",
            "batch: 44033 out of 44991\n",
            "batch: 44034 out of 44991\n",
            "batch: 44035 out of 44991\n",
            "batch: 44036 out of 44991\n",
            "batch: 44037 out of 44991\n",
            "batch: 44038 out of 44991\n",
            "batch: 44039 out of 44991\n",
            "batch: 44040 out of 44991\n",
            "batch: 44041 out of 44991\n",
            "batch: 44042 out of 44991\n",
            "batch: 44043 out of 44991\n",
            "batch: 44044 out of 44991\n",
            "batch: 44045 out of 44991\n",
            "batch: 44046 out of 44991\n",
            "batch: 44047 out of 44991\n",
            "batch: 44048 out of 44991\n",
            "batch: 44049 out of 44991\n",
            "batch: 44050 out of 44991\n",
            "batch: 44051 out of 44991\n",
            "batch: 44052 out of 44991\n",
            "batch: 44053 out of 44991\n",
            "batch: 44054 out of 44991\n",
            "batch: 44055 out of 44991\n",
            "batch: 44056 out of 44991\n",
            "batch: 44057 out of 44991\n",
            "batch: 44058 out of 44991\n",
            "batch: 44059 out of 44991\n",
            "batch: 44060 out of 44991\n",
            "batch: 44061 out of 44991\n",
            "batch: 44062 out of 44991\n",
            "batch: 44063 out of 44991\n",
            "batch: 44064 out of 44991\n",
            "batch: 44065 out of 44991\n",
            "batch: 44066 out of 44991\n",
            "batch: 44067 out of 44991\n",
            "batch: 44068 out of 44991\n",
            "batch: 44069 out of 44991\n",
            "batch: 44070 out of 44991\n",
            "batch: 44071 out of 44991\n",
            "batch: 44072 out of 44991\n",
            "batch: 44073 out of 44991\n",
            "batch: 44074 out of 44991\n",
            "batch: 44075 out of 44991\n",
            "batch: 44076 out of 44991\n",
            "batch: 44077 out of 44991\n",
            "batch: 44078 out of 44991\n",
            "batch: 44079 out of 44991\n",
            "batch: 44080 out of 44991\n",
            "batch: 44081 out of 44991\n",
            "batch: 44082 out of 44991\n",
            "batch: 44083 out of 44991\n",
            "batch: 44084 out of 44991\n",
            "batch: 44085 out of 44991\n",
            "batch: 44086 out of 44991\n",
            "batch: 44087 out of 44991\n",
            "batch: 44088 out of 44991\n",
            "batch: 44089 out of 44991\n",
            "batch: 44090 out of 44991\n",
            "batch: 44091 out of 44991\n",
            "batch: 44092 out of 44991\n",
            "batch: 44093 out of 44991\n",
            "batch: 44094 out of 44991\n",
            "batch: 44095 out of 44991\n",
            "batch: 44096 out of 44991\n",
            "batch: 44097 out of 44991\n",
            "batch: 44098 out of 44991\n",
            "batch: 44099 out of 44991\n",
            "batch: 44100 out of 44991\n",
            "batch: 44101 out of 44991\n",
            "batch: 44102 out of 44991\n",
            "batch: 44103 out of 44991\n",
            "batch: 44104 out of 44991\n",
            "batch: 44105 out of 44991\n",
            "batch: 44106 out of 44991\n",
            "batch: 44107 out of 44991\n",
            "batch: 44108 out of 44991\n",
            "batch: 44109 out of 44991\n",
            "batch: 44110 out of 44991\n",
            "batch: 44111 out of 44991\n",
            "batch: 44112 out of 44991\n",
            "batch: 44113 out of 44991\n",
            "batch: 44114 out of 44991\n",
            "batch: 44115 out of 44991\n",
            "batch: 44116 out of 44991\n",
            "batch: 44117 out of 44991\n",
            "batch: 44118 out of 44991\n",
            "batch: 44119 out of 44991\n",
            "batch: 44120 out of 44991\n",
            "batch: 44121 out of 44991\n",
            "batch: 44122 out of 44991\n",
            "batch: 44123 out of 44991\n",
            "batch: 44124 out of 44991\n",
            "batch: 44125 out of 44991\n",
            "batch: 44126 out of 44991\n",
            "batch: 44127 out of 44991\n",
            "batch: 44128 out of 44991\n",
            "batch: 44129 out of 44991\n",
            "batch: 44130 out of 44991\n",
            "batch: 44131 out of 44991\n",
            "batch: 44132 out of 44991\n",
            "batch: 44133 out of 44991\n",
            "batch: 44134 out of 44991\n",
            "batch: 44135 out of 44991\n",
            "batch: 44136 out of 44991\n",
            "batch: 44137 out of 44991\n",
            "batch: 44138 out of 44991\n",
            "batch: 44139 out of 44991\n",
            "batch: 44140 out of 44991\n",
            "batch: 44141 out of 44991\n",
            "batch: 44142 out of 44991\n",
            "batch: 44143 out of 44991\n",
            "batch: 44144 out of 44991\n",
            "batch: 44145 out of 44991\n",
            "batch: 44146 out of 44991\n",
            "batch: 44147 out of 44991\n",
            "batch: 44148 out of 44991\n",
            "batch: 44149 out of 44991\n",
            "batch: 44150 out of 44991\n",
            "batch: 44151 out of 44991\n",
            "batch: 44152 out of 44991\n",
            "batch: 44153 out of 44991\n",
            "batch: 44154 out of 44991\n",
            "batch: 44155 out of 44991\n",
            "batch: 44156 out of 44991\n",
            "batch: 44157 out of 44991\n",
            "batch: 44158 out of 44991\n",
            "batch: 44159 out of 44991\n",
            "batch: 44160 out of 44991\n",
            "batch: 44161 out of 44991\n",
            "batch: 44162 out of 44991\n",
            "batch: 44163 out of 44991\n",
            "batch: 44164 out of 44991\n",
            "batch: 44165 out of 44991\n",
            "batch: 44166 out of 44991\n",
            "batch: 44167 out of 44991\n",
            "batch: 44168 out of 44991\n",
            "batch: 44169 out of 44991\n",
            "batch: 44170 out of 44991\n",
            "batch: 44171 out of 44991\n",
            "batch: 44172 out of 44991\n",
            "batch: 44173 out of 44991\n",
            "batch: 44174 out of 44991\n",
            "batch: 44175 out of 44991\n",
            "batch: 44176 out of 44991\n",
            "batch: 44177 out of 44991\n",
            "batch: 44178 out of 44991\n",
            "batch: 44179 out of 44991\n",
            "batch: 44180 out of 44991\n",
            "batch: 44181 out of 44991\n",
            "batch: 44182 out of 44991\n",
            "batch: 44183 out of 44991\n",
            "batch: 44184 out of 44991\n",
            "batch: 44185 out of 44991\n",
            "batch: 44186 out of 44991\n",
            "batch: 44187 out of 44991\n",
            "batch: 44188 out of 44991\n",
            "batch: 44189 out of 44991\n",
            "batch: 44190 out of 44991\n",
            "batch: 44191 out of 44991\n",
            "batch: 44192 out of 44991\n",
            "batch: 44193 out of 44991\n",
            "batch: 44194 out of 44991\n",
            "batch: 44195 out of 44991\n",
            "batch: 44196 out of 44991\n",
            "batch: 44197 out of 44991\n",
            "batch: 44198 out of 44991\n",
            "batch: 44199 out of 44991\n",
            "batch: 44200 out of 44991\n",
            "batch: 44201 out of 44991\n",
            "batch: 44202 out of 44991\n",
            "batch: 44203 out of 44991\n",
            "batch: 44204 out of 44991\n",
            "batch: 44205 out of 44991\n",
            "batch: 44206 out of 44991\n",
            "batch: 44207 out of 44991\n",
            "batch: 44208 out of 44991\n",
            "batch: 44209 out of 44991\n",
            "batch: 44210 out of 44991\n",
            "batch: 44211 out of 44991\n",
            "batch: 44212 out of 44991\n",
            "batch: 44213 out of 44991\n",
            "batch: 44214 out of 44991\n",
            "batch: 44215 out of 44991\n",
            "batch: 44216 out of 44991\n",
            "batch: 44217 out of 44991\n",
            "batch: 44218 out of 44991\n",
            "batch: 44219 out of 44991\n",
            "batch: 44220 out of 44991\n",
            "batch: 44221 out of 44991\n",
            "batch: 44222 out of 44991\n",
            "batch: 44223 out of 44991\n",
            "batch: 44224 out of 44991\n",
            "batch: 44225 out of 44991\n",
            "batch: 44226 out of 44991\n",
            "batch: 44227 out of 44991\n",
            "batch: 44228 out of 44991\n",
            "batch: 44229 out of 44991\n",
            "batch: 44230 out of 44991\n",
            "batch: 44231 out of 44991\n",
            "batch: 44232 out of 44991\n",
            "batch: 44233 out of 44991\n",
            "batch: 44234 out of 44991\n",
            "batch: 44235 out of 44991\n",
            "batch: 44236 out of 44991\n",
            "batch: 44237 out of 44991\n",
            "batch: 44238 out of 44991\n",
            "batch: 44239 out of 44991\n",
            "batch: 44240 out of 44991\n",
            "batch: 44241 out of 44991\n",
            "batch: 44242 out of 44991\n",
            "batch: 44243 out of 44991\n",
            "batch: 44244 out of 44991\n",
            "batch: 44245 out of 44991\n",
            "batch: 44246 out of 44991\n",
            "batch: 44247 out of 44991\n",
            "batch: 44248 out of 44991\n",
            "batch: 44249 out of 44991\n",
            "batch: 44250 out of 44991\n",
            "batch: 44251 out of 44991\n",
            "batch: 44252 out of 44991\n",
            "batch: 44253 out of 44991\n",
            "batch: 44254 out of 44991\n",
            "batch: 44255 out of 44991\n",
            "batch: 44256 out of 44991\n",
            "batch: 44257 out of 44991\n",
            "batch: 44258 out of 44991\n",
            "batch: 44259 out of 44991\n",
            "batch: 44260 out of 44991\n",
            "batch: 44261 out of 44991\n",
            "batch: 44262 out of 44991\n",
            "batch: 44263 out of 44991\n",
            "batch: 44264 out of 44991\n",
            "batch: 44265 out of 44991\n",
            "batch: 44266 out of 44991\n",
            "batch: 44267 out of 44991\n",
            "batch: 44268 out of 44991\n",
            "batch: 44269 out of 44991\n",
            "batch: 44270 out of 44991\n",
            "batch: 44271 out of 44991\n",
            "batch: 44272 out of 44991\n",
            "batch: 44273 out of 44991\n",
            "batch: 44274 out of 44991\n",
            "batch: 44275 out of 44991\n",
            "batch: 44276 out of 44991\n",
            "batch: 44277 out of 44991\n",
            "batch: 44278 out of 44991\n",
            "batch: 44279 out of 44991\n",
            "batch: 44280 out of 44991\n",
            "batch: 44281 out of 44991\n",
            "batch: 44282 out of 44991\n",
            "batch: 44283 out of 44991\n",
            "batch: 44284 out of 44991\n",
            "batch: 44285 out of 44991\n",
            "batch: 44286 out of 44991\n",
            "batch: 44287 out of 44991\n",
            "batch: 44288 out of 44991\n",
            "batch: 44289 out of 44991\n",
            "batch: 44290 out of 44991\n",
            "batch: 44291 out of 44991\n",
            "batch: 44292 out of 44991\n",
            "batch: 44293 out of 44991\n",
            "batch: 44294 out of 44991\n",
            "batch: 44295 out of 44991\n",
            "batch: 44296 out of 44991\n",
            "batch: 44297 out of 44991\n",
            "batch: 44298 out of 44991\n",
            "batch: 44299 out of 44991\n",
            "batch: 44300 out of 44991\n",
            "batch: 44301 out of 44991\n",
            "batch: 44302 out of 44991\n",
            "batch: 44303 out of 44991\n",
            "batch: 44304 out of 44991\n",
            "batch: 44305 out of 44991\n",
            "batch: 44306 out of 44991\n",
            "batch: 44307 out of 44991\n",
            "batch: 44308 out of 44991\n",
            "batch: 44309 out of 44991\n",
            "batch: 44310 out of 44991\n",
            "batch: 44311 out of 44991\n",
            "batch: 44312 out of 44991\n",
            "batch: 44313 out of 44991\n",
            "batch: 44314 out of 44991\n",
            "batch: 44315 out of 44991\n",
            "batch: 44316 out of 44991\n",
            "batch: 44317 out of 44991\n",
            "batch: 44318 out of 44991\n",
            "batch: 44319 out of 44991\n",
            "batch: 44320 out of 44991\n",
            "batch: 44321 out of 44991\n",
            "batch: 44322 out of 44991\n",
            "batch: 44323 out of 44991\n",
            "batch: 44324 out of 44991\n",
            "batch: 44325 out of 44991\n",
            "batch: 44326 out of 44991\n",
            "batch: 44327 out of 44991\n",
            "batch: 44328 out of 44991\n",
            "batch: 44329 out of 44991\n",
            "batch: 44330 out of 44991\n",
            "batch: 44331 out of 44991\n",
            "batch: 44332 out of 44991\n",
            "batch: 44333 out of 44991\n",
            "batch: 44334 out of 44991\n",
            "batch: 44335 out of 44991\n",
            "batch: 44336 out of 44991\n",
            "batch: 44337 out of 44991\n",
            "batch: 44338 out of 44991\n",
            "batch: 44339 out of 44991\n",
            "batch: 44340 out of 44991\n",
            "batch: 44341 out of 44991\n",
            "batch: 44342 out of 44991\n",
            "batch: 44343 out of 44991\n",
            "batch: 44344 out of 44991\n",
            "batch: 44345 out of 44991\n",
            "batch: 44346 out of 44991\n",
            "batch: 44347 out of 44991\n",
            "batch: 44348 out of 44991\n",
            "batch: 44349 out of 44991\n",
            "batch: 44350 out of 44991\n",
            "batch: 44351 out of 44991\n",
            "batch: 44352 out of 44991\n",
            "batch: 44353 out of 44991\n",
            "batch: 44354 out of 44991\n",
            "batch: 44355 out of 44991\n",
            "batch: 44356 out of 44991\n",
            "batch: 44357 out of 44991\n",
            "batch: 44358 out of 44991\n",
            "batch: 44359 out of 44991\n",
            "batch: 44360 out of 44991\n",
            "batch: 44361 out of 44991\n",
            "batch: 44362 out of 44991\n",
            "batch: 44363 out of 44991\n",
            "batch: 44364 out of 44991\n",
            "batch: 44365 out of 44991\n",
            "batch: 44366 out of 44991\n",
            "batch: 44367 out of 44991\n",
            "batch: 44368 out of 44991\n",
            "batch: 44369 out of 44991\n",
            "batch: 44370 out of 44991\n",
            "batch: 44371 out of 44991\n",
            "batch: 44372 out of 44991\n",
            "batch: 44373 out of 44991\n",
            "batch: 44374 out of 44991\n",
            "batch: 44375 out of 44991\n",
            "batch: 44376 out of 44991\n",
            "batch: 44377 out of 44991\n",
            "batch: 44378 out of 44991\n",
            "batch: 44379 out of 44991\n",
            "batch: 44380 out of 44991\n",
            "batch: 44381 out of 44991\n",
            "batch: 44382 out of 44991\n",
            "batch: 44383 out of 44991\n",
            "batch: 44384 out of 44991\n",
            "batch: 44385 out of 44991\n",
            "batch: 44386 out of 44991\n",
            "batch: 44387 out of 44991\n",
            "batch: 44388 out of 44991\n",
            "batch: 44389 out of 44991\n",
            "batch: 44390 out of 44991\n",
            "batch: 44391 out of 44991\n",
            "batch: 44392 out of 44991\n",
            "batch: 44393 out of 44991\n",
            "batch: 44394 out of 44991\n",
            "batch: 44395 out of 44991\n",
            "batch: 44396 out of 44991\n",
            "batch: 44397 out of 44991\n",
            "batch: 44398 out of 44991\n",
            "batch: 44399 out of 44991\n",
            "batch: 44400 out of 44991\n",
            "batch: 44401 out of 44991\n",
            "batch: 44402 out of 44991\n",
            "batch: 44403 out of 44991\n",
            "batch: 44404 out of 44991\n",
            "batch: 44405 out of 44991\n",
            "batch: 44406 out of 44991\n",
            "batch: 44407 out of 44991\n",
            "batch: 44408 out of 44991\n",
            "batch: 44409 out of 44991\n",
            "batch: 44410 out of 44991\n",
            "batch: 44411 out of 44991\n",
            "batch: 44412 out of 44991\n",
            "batch: 44413 out of 44991\n",
            "batch: 44414 out of 44991\n",
            "batch: 44415 out of 44991\n",
            "batch: 44416 out of 44991\n",
            "batch: 44417 out of 44991\n",
            "batch: 44418 out of 44991\n",
            "batch: 44419 out of 44991\n",
            "batch: 44420 out of 44991\n",
            "batch: 44421 out of 44991\n",
            "batch: 44422 out of 44991\n",
            "batch: 44423 out of 44991\n",
            "batch: 44424 out of 44991\n",
            "batch: 44425 out of 44991\n",
            "batch: 44426 out of 44991\n",
            "batch: 44427 out of 44991\n",
            "batch: 44428 out of 44991\n",
            "batch: 44429 out of 44991\n",
            "batch: 44430 out of 44991\n",
            "batch: 44431 out of 44991\n",
            "batch: 44432 out of 44991\n",
            "batch: 44433 out of 44991\n",
            "batch: 44434 out of 44991\n",
            "batch: 44435 out of 44991\n",
            "batch: 44436 out of 44991\n",
            "batch: 44437 out of 44991\n",
            "batch: 44438 out of 44991\n",
            "batch: 44439 out of 44991\n",
            "batch: 44440 out of 44991\n",
            "batch: 44441 out of 44991\n",
            "batch: 44442 out of 44991\n",
            "batch: 44443 out of 44991\n",
            "batch: 44444 out of 44991\n",
            "batch: 44445 out of 44991\n",
            "batch: 44446 out of 44991\n",
            "batch: 44447 out of 44991\n",
            "batch: 44448 out of 44991\n",
            "batch: 44449 out of 44991\n",
            "batch: 44450 out of 44991\n",
            "batch: 44451 out of 44991\n",
            "batch: 44452 out of 44991\n",
            "batch: 44453 out of 44991\n",
            "batch: 44454 out of 44991\n",
            "batch: 44455 out of 44991\n",
            "batch: 44456 out of 44991\n",
            "batch: 44457 out of 44991\n",
            "batch: 44458 out of 44991\n",
            "batch: 44459 out of 44991\n",
            "batch: 44460 out of 44991\n",
            "batch: 44461 out of 44991\n",
            "batch: 44462 out of 44991\n",
            "batch: 44463 out of 44991\n",
            "batch: 44464 out of 44991\n",
            "batch: 44465 out of 44991\n",
            "batch: 44466 out of 44991\n",
            "batch: 44467 out of 44991\n",
            "batch: 44468 out of 44991\n",
            "batch: 44469 out of 44991\n",
            "batch: 44470 out of 44991\n",
            "batch: 44471 out of 44991\n",
            "batch: 44472 out of 44991\n",
            "batch: 44473 out of 44991\n",
            "batch: 44474 out of 44991\n",
            "batch: 44475 out of 44991\n",
            "batch: 44476 out of 44991\n",
            "batch: 44477 out of 44991\n",
            "batch: 44478 out of 44991\n",
            "batch: 44479 out of 44991\n",
            "batch: 44480 out of 44991\n",
            "batch: 44481 out of 44991\n",
            "batch: 44482 out of 44991\n",
            "batch: 44483 out of 44991\n",
            "batch: 44484 out of 44991\n",
            "batch: 44485 out of 44991\n",
            "batch: 44486 out of 44991\n",
            "batch: 44487 out of 44991\n",
            "batch: 44488 out of 44991\n",
            "batch: 44489 out of 44991\n",
            "batch: 44490 out of 44991\n",
            "batch: 44491 out of 44991\n",
            "batch: 44492 out of 44991\n",
            "batch: 44493 out of 44991\n",
            "batch: 44494 out of 44991\n",
            "batch: 44495 out of 44991\n",
            "batch: 44496 out of 44991\n",
            "batch: 44497 out of 44991\n",
            "batch: 44498 out of 44991\n",
            "batch: 44499 out of 44991\n",
            "batch: 44500 out of 44991\n",
            "batch: 44501 out of 44991\n",
            "batch: 44502 out of 44991\n",
            "batch: 44503 out of 44991\n",
            "batch: 44504 out of 44991\n",
            "batch: 44505 out of 44991\n",
            "batch: 44506 out of 44991\n",
            "batch: 44507 out of 44991\n",
            "batch: 44508 out of 44991\n",
            "batch: 44509 out of 44991\n",
            "batch: 44510 out of 44991\n",
            "batch: 44511 out of 44991\n",
            "batch: 44512 out of 44991\n",
            "batch: 44513 out of 44991\n",
            "batch: 44514 out of 44991\n",
            "batch: 44515 out of 44991\n",
            "batch: 44516 out of 44991\n",
            "batch: 44517 out of 44991\n",
            "batch: 44518 out of 44991\n",
            "batch: 44519 out of 44991\n",
            "batch: 44520 out of 44991\n",
            "batch: 44521 out of 44991\n",
            "batch: 44522 out of 44991\n",
            "batch: 44523 out of 44991\n",
            "batch: 44524 out of 44991\n",
            "batch: 44525 out of 44991\n",
            "batch: 44526 out of 44991\n",
            "batch: 44527 out of 44991\n",
            "batch: 44528 out of 44991\n",
            "batch: 44529 out of 44991\n",
            "batch: 44530 out of 44991\n",
            "batch: 44531 out of 44991\n",
            "batch: 44532 out of 44991\n",
            "batch: 44533 out of 44991\n",
            "batch: 44534 out of 44991\n",
            "batch: 44535 out of 44991\n",
            "batch: 44536 out of 44991\n",
            "batch: 44537 out of 44991\n",
            "batch: 44538 out of 44991\n",
            "batch: 44539 out of 44991\n",
            "batch: 44540 out of 44991\n",
            "batch: 44541 out of 44991\n",
            "batch: 44542 out of 44991\n",
            "batch: 44543 out of 44991\n",
            "batch: 44544 out of 44991\n",
            "batch: 44545 out of 44991\n",
            "batch: 44546 out of 44991\n",
            "batch: 44547 out of 44991\n",
            "batch: 44548 out of 44991\n",
            "batch: 44549 out of 44991\n",
            "batch: 44550 out of 44991\n",
            "batch: 44551 out of 44991\n",
            "batch: 44552 out of 44991\n",
            "batch: 44553 out of 44991\n",
            "batch: 44554 out of 44991\n",
            "batch: 44555 out of 44991\n",
            "batch: 44556 out of 44991\n",
            "batch: 44557 out of 44991\n",
            "batch: 44558 out of 44991\n",
            "batch: 44559 out of 44991\n",
            "batch: 44560 out of 44991\n",
            "batch: 44561 out of 44991\n",
            "batch: 44562 out of 44991\n",
            "batch: 44563 out of 44991\n",
            "batch: 44564 out of 44991\n",
            "batch: 44565 out of 44991\n",
            "batch: 44566 out of 44991\n",
            "batch: 44567 out of 44991\n",
            "batch: 44568 out of 44991\n",
            "batch: 44569 out of 44991\n",
            "batch: 44570 out of 44991\n",
            "batch: 44571 out of 44991\n",
            "batch: 44572 out of 44991\n",
            "batch: 44573 out of 44991\n",
            "batch: 44574 out of 44991\n",
            "batch: 44575 out of 44991\n",
            "batch: 44576 out of 44991\n",
            "batch: 44577 out of 44991\n",
            "batch: 44578 out of 44991\n",
            "batch: 44579 out of 44991\n",
            "batch: 44580 out of 44991\n",
            "batch: 44581 out of 44991\n",
            "batch: 44582 out of 44991\n",
            "batch: 44583 out of 44991\n",
            "batch: 44584 out of 44991\n",
            "batch: 44585 out of 44991\n",
            "batch: 44586 out of 44991\n",
            "batch: 44587 out of 44991\n",
            "batch: 44588 out of 44991\n",
            "batch: 44589 out of 44991\n",
            "batch: 44590 out of 44991\n",
            "batch: 44591 out of 44991\n",
            "batch: 44592 out of 44991\n",
            "batch: 44593 out of 44991\n",
            "batch: 44594 out of 44991\n",
            "batch: 44595 out of 44991\n",
            "batch: 44596 out of 44991\n",
            "batch: 44597 out of 44991\n",
            "batch: 44598 out of 44991\n",
            "batch: 44599 out of 44991\n",
            "batch: 44600 out of 44991\n",
            "batch: 44601 out of 44991\n",
            "batch: 44602 out of 44991\n",
            "batch: 44603 out of 44991\n",
            "batch: 44604 out of 44991\n",
            "batch: 44605 out of 44991\n",
            "batch: 44606 out of 44991\n",
            "batch: 44607 out of 44991\n",
            "batch: 44608 out of 44991\n",
            "batch: 44609 out of 44991\n",
            "batch: 44610 out of 44991\n",
            "batch: 44611 out of 44991\n",
            "batch: 44612 out of 44991\n",
            "batch: 44613 out of 44991\n",
            "batch: 44614 out of 44991\n",
            "batch: 44615 out of 44991\n",
            "batch: 44616 out of 44991\n",
            "batch: 44617 out of 44991\n",
            "batch: 44618 out of 44991\n",
            "batch: 44619 out of 44991\n",
            "batch: 44620 out of 44991\n",
            "batch: 44621 out of 44991\n",
            "batch: 44622 out of 44991\n",
            "batch: 44623 out of 44991\n",
            "batch: 44624 out of 44991\n",
            "batch: 44625 out of 44991\n",
            "batch: 44626 out of 44991\n",
            "batch: 44627 out of 44991\n",
            "batch: 44628 out of 44991\n",
            "batch: 44629 out of 44991\n",
            "batch: 44630 out of 44991\n",
            "batch: 44631 out of 44991\n",
            "batch: 44632 out of 44991\n",
            "batch: 44633 out of 44991\n",
            "batch: 44634 out of 44991\n",
            "batch: 44635 out of 44991\n",
            "batch: 44636 out of 44991\n",
            "batch: 44637 out of 44991\n",
            "batch: 44638 out of 44991\n",
            "batch: 44639 out of 44991\n",
            "batch: 44640 out of 44991\n",
            "batch: 44641 out of 44991\n",
            "batch: 44642 out of 44991\n",
            "batch: 44643 out of 44991\n",
            "batch: 44644 out of 44991\n",
            "batch: 44645 out of 44991\n",
            "batch: 44646 out of 44991\n",
            "batch: 44647 out of 44991\n",
            "batch: 44648 out of 44991\n",
            "batch: 44649 out of 44991\n",
            "batch: 44650 out of 44991\n",
            "batch: 44651 out of 44991\n",
            "batch: 44652 out of 44991\n",
            "batch: 44653 out of 44991\n",
            "batch: 44654 out of 44991\n",
            "batch: 44655 out of 44991\n",
            "batch: 44656 out of 44991\n",
            "batch: 44657 out of 44991\n",
            "batch: 44658 out of 44991\n",
            "batch: 44659 out of 44991\n",
            "batch: 44660 out of 44991\n",
            "batch: 44661 out of 44991\n",
            "batch: 44662 out of 44991\n",
            "batch: 44663 out of 44991\n",
            "batch: 44664 out of 44991\n",
            "batch: 44665 out of 44991\n",
            "batch: 44666 out of 44991\n",
            "batch: 44667 out of 44991\n",
            "batch: 44668 out of 44991\n",
            "batch: 44669 out of 44991\n",
            "batch: 44670 out of 44991\n",
            "batch: 44671 out of 44991\n",
            "batch: 44672 out of 44991\n",
            "batch: 44673 out of 44991\n",
            "batch: 44674 out of 44991\n",
            "batch: 44675 out of 44991\n",
            "batch: 44676 out of 44991\n",
            "batch: 44677 out of 44991\n",
            "batch: 44678 out of 44991\n",
            "batch: 44679 out of 44991\n",
            "batch: 44680 out of 44991\n",
            "batch: 44681 out of 44991\n",
            "batch: 44682 out of 44991\n",
            "batch: 44683 out of 44991\n",
            "batch: 44684 out of 44991\n",
            "batch: 44685 out of 44991\n",
            "batch: 44686 out of 44991\n",
            "batch: 44687 out of 44991\n",
            "batch: 44688 out of 44991\n",
            "batch: 44689 out of 44991\n",
            "batch: 44690 out of 44991\n",
            "batch: 44691 out of 44991\n",
            "batch: 44692 out of 44991\n",
            "batch: 44693 out of 44991\n",
            "batch: 44694 out of 44991\n",
            "batch: 44695 out of 44991\n",
            "batch: 44696 out of 44991\n",
            "batch: 44697 out of 44991\n",
            "batch: 44698 out of 44991\n",
            "batch: 44699 out of 44991\n",
            "batch: 44700 out of 44991\n",
            "batch: 44701 out of 44991\n",
            "batch: 44702 out of 44991\n",
            "batch: 44703 out of 44991\n",
            "batch: 44704 out of 44991\n",
            "batch: 44705 out of 44991\n",
            "batch: 44706 out of 44991\n",
            "batch: 44707 out of 44991\n",
            "batch: 44708 out of 44991\n",
            "batch: 44709 out of 44991\n",
            "batch: 44710 out of 44991\n",
            "batch: 44711 out of 44991\n",
            "batch: 44712 out of 44991\n",
            "batch: 44713 out of 44991\n",
            "batch: 44714 out of 44991\n",
            "batch: 44715 out of 44991\n",
            "batch: 44716 out of 44991\n",
            "batch: 44717 out of 44991\n",
            "batch: 44718 out of 44991\n",
            "batch: 44719 out of 44991\n",
            "batch: 44720 out of 44991\n",
            "batch: 44721 out of 44991\n",
            "batch: 44722 out of 44991\n",
            "batch: 44723 out of 44991\n",
            "batch: 44724 out of 44991\n",
            "batch: 44725 out of 44991\n",
            "batch: 44726 out of 44991\n",
            "batch: 44727 out of 44991\n",
            "batch: 44728 out of 44991\n",
            "batch: 44729 out of 44991\n",
            "batch: 44730 out of 44991\n",
            "batch: 44731 out of 44991\n",
            "batch: 44732 out of 44991\n",
            "batch: 44733 out of 44991\n",
            "batch: 44734 out of 44991\n",
            "batch: 44735 out of 44991\n",
            "batch: 44736 out of 44991\n",
            "batch: 44737 out of 44991\n",
            "batch: 44738 out of 44991\n",
            "batch: 44739 out of 44991\n",
            "batch: 44740 out of 44991\n",
            "batch: 44741 out of 44991\n",
            "batch: 44742 out of 44991\n",
            "batch: 44743 out of 44991\n",
            "batch: 44744 out of 44991\n",
            "batch: 44745 out of 44991\n",
            "batch: 44746 out of 44991\n",
            "batch: 44747 out of 44991\n",
            "batch: 44748 out of 44991\n",
            "batch: 44749 out of 44991\n",
            "batch: 44750 out of 44991\n",
            "batch: 44751 out of 44991\n",
            "batch: 44752 out of 44991\n",
            "batch: 44753 out of 44991\n",
            "batch: 44754 out of 44991\n",
            "batch: 44755 out of 44991\n",
            "batch: 44756 out of 44991\n",
            "batch: 44757 out of 44991\n",
            "batch: 44758 out of 44991\n",
            "batch: 44759 out of 44991\n",
            "batch: 44760 out of 44991\n",
            "batch: 44761 out of 44991\n",
            "batch: 44762 out of 44991\n",
            "batch: 44763 out of 44991\n",
            "batch: 44764 out of 44991\n",
            "batch: 44765 out of 44991\n",
            "batch: 44766 out of 44991\n",
            "batch: 44767 out of 44991\n",
            "batch: 44768 out of 44991\n",
            "batch: 44769 out of 44991\n",
            "batch: 44770 out of 44991\n",
            "batch: 44771 out of 44991\n",
            "batch: 44772 out of 44991\n",
            "batch: 44773 out of 44991\n",
            "batch: 44774 out of 44991\n",
            "batch: 44775 out of 44991\n",
            "batch: 44776 out of 44991\n",
            "batch: 44777 out of 44991\n",
            "batch: 44778 out of 44991\n",
            "batch: 44779 out of 44991\n",
            "batch: 44780 out of 44991\n",
            "batch: 44781 out of 44991\n",
            "batch: 44782 out of 44991\n",
            "batch: 44783 out of 44991\n",
            "batch: 44784 out of 44991\n",
            "batch: 44785 out of 44991\n",
            "batch: 44786 out of 44991\n",
            "batch: 44787 out of 44991\n",
            "batch: 44788 out of 44991\n",
            "batch: 44789 out of 44991\n",
            "batch: 44790 out of 44991\n",
            "batch: 44791 out of 44991\n",
            "batch: 44792 out of 44991\n",
            "batch: 44793 out of 44991\n",
            "batch: 44794 out of 44991\n",
            "batch: 44795 out of 44991\n",
            "batch: 44796 out of 44991\n",
            "batch: 44797 out of 44991\n",
            "batch: 44798 out of 44991\n",
            "batch: 44799 out of 44991\n",
            "batch: 44800 out of 44991\n",
            "batch: 44801 out of 44991\n",
            "batch: 44802 out of 44991\n",
            "batch: 44803 out of 44991\n",
            "batch: 44804 out of 44991\n",
            "batch: 44805 out of 44991\n",
            "batch: 44806 out of 44991\n",
            "batch: 44807 out of 44991\n",
            "batch: 44808 out of 44991\n",
            "batch: 44809 out of 44991\n",
            "batch: 44810 out of 44991\n",
            "batch: 44811 out of 44991\n",
            "batch: 44812 out of 44991\n",
            "batch: 44813 out of 44991\n",
            "batch: 44814 out of 44991\n",
            "batch: 44815 out of 44991\n",
            "batch: 44816 out of 44991\n",
            "batch: 44817 out of 44991\n",
            "batch: 44818 out of 44991\n",
            "batch: 44819 out of 44991\n",
            "batch: 44820 out of 44991\n",
            "batch: 44821 out of 44991\n",
            "batch: 44822 out of 44991\n",
            "batch: 44823 out of 44991\n",
            "batch: 44824 out of 44991\n",
            "batch: 44825 out of 44991\n",
            "batch: 44826 out of 44991\n",
            "batch: 44827 out of 44991\n",
            "batch: 44828 out of 44991\n",
            "batch: 44829 out of 44991\n",
            "batch: 44830 out of 44991\n",
            "batch: 44831 out of 44991\n",
            "batch: 44832 out of 44991\n",
            "batch: 44833 out of 44991\n",
            "batch: 44834 out of 44991\n",
            "batch: 44835 out of 44991\n",
            "batch: 44836 out of 44991\n",
            "batch: 44837 out of 44991\n",
            "batch: 44838 out of 44991\n",
            "batch: 44839 out of 44991\n",
            "batch: 44840 out of 44991\n",
            "batch: 44841 out of 44991\n",
            "batch: 44842 out of 44991\n",
            "batch: 44843 out of 44991\n",
            "batch: 44844 out of 44991\n",
            "batch: 44845 out of 44991\n",
            "batch: 44846 out of 44991\n",
            "batch: 44847 out of 44991\n",
            "batch: 44848 out of 44991\n",
            "batch: 44849 out of 44991\n",
            "batch: 44850 out of 44991\n",
            "batch: 44851 out of 44991\n",
            "batch: 44852 out of 44991\n",
            "batch: 44853 out of 44991\n",
            "batch: 44854 out of 44991\n",
            "batch: 44855 out of 44991\n",
            "batch: 44856 out of 44991\n",
            "batch: 44857 out of 44991\n",
            "batch: 44858 out of 44991\n",
            "batch: 44859 out of 44991\n",
            "batch: 44860 out of 44991\n",
            "batch: 44861 out of 44991\n",
            "batch: 44862 out of 44991\n",
            "batch: 44863 out of 44991\n",
            "batch: 44864 out of 44991\n",
            "batch: 44865 out of 44991\n",
            "batch: 44866 out of 44991\n",
            "batch: 44867 out of 44991\n",
            "batch: 44868 out of 44991\n",
            "batch: 44869 out of 44991\n",
            "batch: 44870 out of 44991\n",
            "batch: 44871 out of 44991\n",
            "batch: 44872 out of 44991\n",
            "batch: 44873 out of 44991\n",
            "batch: 44874 out of 44991\n",
            "batch: 44875 out of 44991\n",
            "batch: 44876 out of 44991\n",
            "batch: 44877 out of 44991\n",
            "batch: 44878 out of 44991\n",
            "batch: 44879 out of 44991\n",
            "batch: 44880 out of 44991\n",
            "batch: 44881 out of 44991\n",
            "batch: 44882 out of 44991\n",
            "batch: 44883 out of 44991\n",
            "batch: 44884 out of 44991\n",
            "batch: 44885 out of 44991\n",
            "batch: 44886 out of 44991\n",
            "batch: 44887 out of 44991\n",
            "batch: 44888 out of 44991\n",
            "batch: 44889 out of 44991\n",
            "batch: 44890 out of 44991\n",
            "batch: 44891 out of 44991\n",
            "batch: 44892 out of 44991\n",
            "batch: 44893 out of 44991\n",
            "batch: 44894 out of 44991\n",
            "batch: 44895 out of 44991\n",
            "batch: 44896 out of 44991\n",
            "batch: 44897 out of 44991\n",
            "batch: 44898 out of 44991\n",
            "batch: 44899 out of 44991\n",
            "batch: 44900 out of 44991\n",
            "batch: 44901 out of 44991\n",
            "batch: 44902 out of 44991\n",
            "batch: 44903 out of 44991\n",
            "batch: 44904 out of 44991\n",
            "batch: 44905 out of 44991\n",
            "batch: 44906 out of 44991\n",
            "batch: 44907 out of 44991\n",
            "batch: 44908 out of 44991\n",
            "batch: 44909 out of 44991\n",
            "batch: 44910 out of 44991\n",
            "batch: 44911 out of 44991\n",
            "batch: 44912 out of 44991\n",
            "batch: 44913 out of 44991\n",
            "batch: 44914 out of 44991\n",
            "batch: 44915 out of 44991\n",
            "batch: 44916 out of 44991\n",
            "batch: 44917 out of 44991\n",
            "batch: 44918 out of 44991\n",
            "batch: 44919 out of 44991\n",
            "batch: 44920 out of 44991\n",
            "batch: 44921 out of 44991\n",
            "batch: 44922 out of 44991\n",
            "batch: 44923 out of 44991\n",
            "batch: 44924 out of 44991\n",
            "batch: 44925 out of 44991\n",
            "batch: 44926 out of 44991\n",
            "batch: 44927 out of 44991\n",
            "batch: 44928 out of 44991\n",
            "batch: 44929 out of 44991\n",
            "batch: 44930 out of 44991\n",
            "batch: 44931 out of 44991\n",
            "batch: 44932 out of 44991\n",
            "batch: 44933 out of 44991\n",
            "batch: 44934 out of 44991\n",
            "batch: 44935 out of 44991\n",
            "batch: 44936 out of 44991\n",
            "batch: 44937 out of 44991\n",
            "batch: 44938 out of 44991\n",
            "batch: 44939 out of 44991\n",
            "batch: 44940 out of 44991\n",
            "batch: 44941 out of 44991\n",
            "batch: 44942 out of 44991\n",
            "batch: 44943 out of 44991\n",
            "batch: 44944 out of 44991\n",
            "batch: 44945 out of 44991\n",
            "batch: 44946 out of 44991\n",
            "batch: 44947 out of 44991\n",
            "batch: 44948 out of 44991\n",
            "batch: 44949 out of 44991\n",
            "batch: 44950 out of 44991\n",
            "batch: 44951 out of 44991\n",
            "batch: 44952 out of 44991\n",
            "batch: 44953 out of 44991\n",
            "batch: 44954 out of 44991\n",
            "batch: 44955 out of 44991\n",
            "batch: 44956 out of 44991\n",
            "batch: 44957 out of 44991\n",
            "batch: 44958 out of 44991\n",
            "batch: 44959 out of 44991\n",
            "batch: 44960 out of 44991\n",
            "batch: 44961 out of 44991\n",
            "batch: 44962 out of 44991\n",
            "batch: 44963 out of 44991\n",
            "batch: 44964 out of 44991\n",
            "batch: 44965 out of 44991\n",
            "batch: 44966 out of 44991\n",
            "batch: 44967 out of 44991\n",
            "batch: 44968 out of 44991\n",
            "batch: 44969 out of 44991\n",
            "batch: 44970 out of 44991\n",
            "batch: 44971 out of 44991\n",
            "batch: 44972 out of 44991\n",
            "batch: 44973 out of 44991\n",
            "batch: 44974 out of 44991\n",
            "batch: 44975 out of 44991\n",
            "batch: 44976 out of 44991\n",
            "batch: 44977 out of 44991\n",
            "batch: 44978 out of 44991\n",
            "batch: 44979 out of 44991\n",
            "batch: 44980 out of 44991\n",
            "batch: 44981 out of 44991\n",
            "batch: 44982 out of 44991\n",
            "batch: 44983 out of 44991\n",
            "batch: 44984 out of 44991\n",
            "batch: 44985 out of 44991\n",
            "batch: 44986 out of 44991\n",
            "batch: 44987 out of 44991\n",
            "batch: 44988 out of 44991\n",
            "batch: 44989 out of 44991\n",
            "batch: 44990 out of 44991\n"
          ]
        }
      ],
      "source": [
        "test_loss_list = anomaly_score(test_dataset, generator, discriminator, encoder, LAMBDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5Ja_buX8rCU",
        "outputId": "850de339-d2e1-49e7-876e-6c430f4254d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch: 0 out of 281\n",
            "batch: 1 out of 281\n",
            "batch: 2 out of 281\n",
            "batch: 3 out of 281\n",
            "batch: 4 out of 281\n",
            "batch: 5 out of 281\n",
            "batch: 6 out of 281\n",
            "batch: 7 out of 281\n",
            "batch: 8 out of 281\n",
            "batch: 9 out of 281\n",
            "batch: 10 out of 281\n",
            "batch: 11 out of 281\n",
            "batch: 12 out of 281\n",
            "batch: 13 out of 281\n",
            "batch: 14 out of 281\n",
            "batch: 15 out of 281\n",
            "batch: 16 out of 281\n",
            "batch: 17 out of 281\n",
            "batch: 18 out of 281\n",
            "batch: 19 out of 281\n",
            "batch: 20 out of 281\n",
            "batch: 21 out of 281\n",
            "batch: 22 out of 281\n",
            "batch: 23 out of 281\n",
            "batch: 24 out of 281\n",
            "batch: 25 out of 281\n",
            "batch: 26 out of 281\n",
            "batch: 27 out of 281\n",
            "batch: 28 out of 281\n",
            "batch: 29 out of 281\n",
            "batch: 30 out of 281\n",
            "batch: 31 out of 281\n",
            "batch: 32 out of 281\n",
            "batch: 33 out of 281\n",
            "batch: 34 out of 281\n",
            "batch: 35 out of 281\n",
            "batch: 36 out of 281\n",
            "batch: 37 out of 281\n",
            "batch: 38 out of 281\n",
            "batch: 39 out of 281\n",
            "batch: 40 out of 281\n",
            "batch: 41 out of 281\n",
            "batch: 42 out of 281\n",
            "batch: 43 out of 281\n",
            "batch: 44 out of 281\n",
            "batch: 45 out of 281\n",
            "batch: 46 out of 281\n",
            "batch: 47 out of 281\n",
            "batch: 48 out of 281\n",
            "batch: 49 out of 281\n",
            "batch: 50 out of 281\n",
            "batch: 51 out of 281\n",
            "batch: 52 out of 281\n",
            "batch: 53 out of 281\n",
            "batch: 54 out of 281\n",
            "batch: 55 out of 281\n",
            "batch: 56 out of 281\n",
            "batch: 57 out of 281\n",
            "batch: 58 out of 281\n",
            "batch: 59 out of 281\n",
            "batch: 60 out of 281\n",
            "batch: 61 out of 281\n",
            "batch: 62 out of 281\n",
            "batch: 63 out of 281\n",
            "batch: 64 out of 281\n",
            "batch: 65 out of 281\n",
            "batch: 66 out of 281\n",
            "batch: 67 out of 281\n",
            "batch: 68 out of 281\n",
            "batch: 69 out of 281\n",
            "batch: 70 out of 281\n",
            "batch: 71 out of 281\n",
            "batch: 72 out of 281\n",
            "batch: 73 out of 281\n",
            "batch: 74 out of 281\n",
            "batch: 75 out of 281\n",
            "batch: 76 out of 281\n",
            "batch: 77 out of 281\n",
            "batch: 78 out of 281\n",
            "batch: 79 out of 281\n",
            "batch: 80 out of 281\n",
            "batch: 81 out of 281\n",
            "batch: 82 out of 281\n",
            "batch: 83 out of 281\n",
            "batch: 84 out of 281\n",
            "batch: 85 out of 281\n",
            "batch: 86 out of 281\n",
            "batch: 87 out of 281\n",
            "batch: 88 out of 281\n",
            "batch: 89 out of 281\n",
            "batch: 90 out of 281\n",
            "batch: 91 out of 281\n",
            "batch: 92 out of 281\n",
            "batch: 93 out of 281\n",
            "batch: 94 out of 281\n",
            "batch: 95 out of 281\n",
            "batch: 96 out of 281\n",
            "batch: 97 out of 281\n",
            "batch: 98 out of 281\n",
            "batch: 99 out of 281\n",
            "batch: 100 out of 281\n",
            "batch: 101 out of 281\n",
            "batch: 102 out of 281\n",
            "batch: 103 out of 281\n",
            "batch: 104 out of 281\n",
            "batch: 105 out of 281\n",
            "batch: 106 out of 281\n",
            "batch: 107 out of 281\n",
            "batch: 108 out of 281\n",
            "batch: 109 out of 281\n",
            "batch: 110 out of 281\n",
            "batch: 111 out of 281\n",
            "batch: 112 out of 281\n",
            "batch: 113 out of 281\n",
            "batch: 114 out of 281\n",
            "batch: 115 out of 281\n",
            "batch: 116 out of 281\n",
            "batch: 117 out of 281\n",
            "batch: 118 out of 281\n",
            "batch: 119 out of 281\n",
            "batch: 120 out of 281\n",
            "batch: 121 out of 281\n",
            "batch: 122 out of 281\n",
            "batch: 123 out of 281\n",
            "batch: 124 out of 281\n",
            "batch: 125 out of 281\n",
            "batch: 126 out of 281\n",
            "batch: 127 out of 281\n",
            "batch: 128 out of 281\n",
            "batch: 129 out of 281\n",
            "batch: 130 out of 281\n",
            "batch: 131 out of 281\n",
            "batch: 132 out of 281\n",
            "batch: 133 out of 281\n",
            "batch: 134 out of 281\n",
            "batch: 135 out of 281\n",
            "batch: 136 out of 281\n",
            "batch: 137 out of 281\n",
            "batch: 138 out of 281\n",
            "batch: 139 out of 281\n",
            "batch: 140 out of 281\n",
            "batch: 141 out of 281\n",
            "batch: 142 out of 281\n",
            "batch: 143 out of 281\n",
            "batch: 144 out of 281\n",
            "batch: 145 out of 281\n",
            "batch: 146 out of 281\n",
            "batch: 147 out of 281\n",
            "batch: 148 out of 281\n",
            "batch: 149 out of 281\n",
            "batch: 150 out of 281\n",
            "batch: 151 out of 281\n",
            "batch: 152 out of 281\n",
            "batch: 153 out of 281\n",
            "batch: 154 out of 281\n",
            "batch: 155 out of 281\n",
            "batch: 156 out of 281\n",
            "batch: 157 out of 281\n",
            "batch: 158 out of 281\n",
            "batch: 159 out of 281\n",
            "batch: 160 out of 281\n",
            "batch: 161 out of 281\n",
            "batch: 162 out of 281\n",
            "batch: 163 out of 281\n",
            "batch: 164 out of 281\n",
            "batch: 165 out of 281\n",
            "batch: 166 out of 281\n",
            "batch: 167 out of 281\n",
            "batch: 168 out of 281\n",
            "batch: 169 out of 281\n",
            "batch: 170 out of 281\n",
            "batch: 171 out of 281\n",
            "batch: 172 out of 281\n",
            "batch: 173 out of 281\n",
            "batch: 174 out of 281\n",
            "batch: 175 out of 281\n",
            "batch: 176 out of 281\n",
            "batch: 177 out of 281\n",
            "batch: 178 out of 281\n",
            "batch: 179 out of 281\n",
            "batch: 180 out of 281\n",
            "batch: 181 out of 281\n",
            "batch: 182 out of 281\n",
            "batch: 183 out of 281\n",
            "batch: 184 out of 281\n",
            "batch: 185 out of 281\n",
            "batch: 186 out of 281\n",
            "batch: 187 out of 281\n",
            "batch: 188 out of 281\n",
            "batch: 189 out of 281\n",
            "batch: 190 out of 281\n",
            "batch: 191 out of 281\n",
            "batch: 192 out of 281\n",
            "batch: 193 out of 281\n",
            "batch: 194 out of 281\n",
            "batch: 195 out of 281\n",
            "batch: 196 out of 281\n",
            "batch: 197 out of 281\n",
            "batch: 198 out of 281\n",
            "batch: 199 out of 281\n",
            "batch: 200 out of 281\n",
            "batch: 201 out of 281\n",
            "batch: 202 out of 281\n",
            "batch: 203 out of 281\n",
            "batch: 204 out of 281\n",
            "batch: 205 out of 281\n",
            "batch: 206 out of 281\n",
            "batch: 207 out of 281\n",
            "batch: 208 out of 281\n",
            "batch: 209 out of 281\n",
            "batch: 210 out of 281\n",
            "batch: 211 out of 281\n",
            "batch: 212 out of 281\n",
            "batch: 213 out of 281\n",
            "batch: 214 out of 281\n",
            "batch: 215 out of 281\n",
            "batch: 216 out of 281\n",
            "batch: 217 out of 281\n",
            "batch: 218 out of 281\n",
            "batch: 219 out of 281\n",
            "batch: 220 out of 281\n",
            "batch: 221 out of 281\n",
            "batch: 222 out of 281\n",
            "batch: 223 out of 281\n",
            "batch: 224 out of 281\n",
            "batch: 225 out of 281\n",
            "batch: 226 out of 281\n",
            "batch: 227 out of 281\n",
            "batch: 228 out of 281\n",
            "batch: 229 out of 281\n",
            "batch: 230 out of 281\n",
            "batch: 231 out of 281\n",
            "batch: 232 out of 281\n",
            "batch: 233 out of 281\n",
            "batch: 234 out of 281\n",
            "batch: 235 out of 281\n",
            "batch: 236 out of 281\n",
            "batch: 237 out of 281\n",
            "batch: 238 out of 281\n",
            "batch: 239 out of 281\n",
            "batch: 240 out of 281\n",
            "batch: 241 out of 281\n",
            "batch: 242 out of 281\n",
            "batch: 243 out of 281\n",
            "batch: 244 out of 281\n",
            "batch: 245 out of 281\n",
            "batch: 246 out of 281\n",
            "batch: 247 out of 281\n",
            "batch: 248 out of 281\n",
            "batch: 249 out of 281\n",
            "batch: 250 out of 281\n",
            "batch: 251 out of 281\n",
            "batch: 252 out of 281\n",
            "batch: 253 out of 281\n",
            "batch: 254 out of 281\n",
            "batch: 255 out of 281\n",
            "batch: 256 out of 281\n",
            "batch: 257 out of 281\n",
            "batch: 258 out of 281\n",
            "batch: 259 out of 281\n",
            "batch: 260 out of 281\n",
            "batch: 261 out of 281\n",
            "batch: 262 out of 281\n",
            "batch: 263 out of 281\n",
            "batch: 264 out of 281\n",
            "batch: 265 out of 281\n",
            "batch: 266 out of 281\n",
            "batch: 267 out of 281\n",
            "batch: 268 out of 281\n",
            "batch: 269 out of 281\n",
            "batch: 270 out of 281\n",
            "batch: 271 out of 281\n",
            "batch: 272 out of 281\n",
            "batch: 273 out of 281\n",
            "batch: 274 out of 281\n",
            "batch: 275 out of 281\n",
            "batch: 276 out of 281\n",
            "batch: 277 out of 281\n",
            "batch: 278 out of 281\n",
            "batch: 279 out of 281\n",
            "batch: 280 out of 281\n"
          ]
        }
      ],
      "source": [
        "validation_loss_list = anomaly_score(validation_dataset, generator, discriminator, encoder, LAMBDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PUSxrU_zR8N"
      },
      "source": [
        "## Save in Drive the losses obtained on test or validation datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y15kwIVpl6Yp",
        "outputId": "7adf5e95-7434-4037-e124-c45201ddb083"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 379.85256958,  380.17330933,  380.24407959, ..., 1433.60131836,\n",
              "       1433.5826416 , 1434.12695312])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_loss_array = np.array([i.numpy().item() for i in test_loss_list])\n",
        "\n",
        "with open('test_loss.npy', 'wb') as f:\n",
        "  np.save(f, test_loss_array)\n",
        "\n",
        "!cp ./test_loss.npy ./drive/MyDrive/ml-applications/TimeSeriesAnomalyDetection_project/saved_losses_modifiedModel_swat/\n",
        "\n",
        "#with open('test_loss.npy', 'rb') as f:\n",
        "#  reloaded_array = np.load(f)\n",
        "#\n",
        "#reloaded_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "JaXQLD9_QDCy"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "#!cp ./drive/MyDrive/ml-applications/TimeSeriesAnomalyDetection_project/saved_losses/test_loss.npy .\n",
        "\n",
        "def compute_f1_score(anomaly_scores, ground_truth, thresholds):\n",
        "    f1_dict = {}\n",
        "\n",
        "    for t in thresholds:\n",
        "        assigned_labels = [0 if i < t else 1 for i in anomaly_scores]\n",
        "        f1_dict[t] = f1_score(ground_truth, assigned_labels)\n",
        "\n",
        "    return f1_dict\n",
        "\n",
        "def compute_precision_score(anomaly_scores, ground_truth, thresholds):\n",
        "    precision_dict = {}\n",
        "\n",
        "    for t in thresholds:\n",
        "        assigned_labels = [0 if i < t else 1 for i in anomaly_scores]\n",
        "        precision_dict[t] = precision_score(ground_truth, assigned_labels)\n",
        "\n",
        "    return precision_dict\n",
        "\n",
        "def compute_recall_score(anomaly_scores, ground_truth, thresholds):\n",
        "    recall_dict = {}\n",
        "\n",
        "    for t in thresholds:\n",
        "        assigned_labels = [0 if i < t else 1 for i in anomaly_scores]\n",
        "        recall_dict[t] = recall_score(ground_truth, assigned_labels)\n",
        "\n",
        "    return recall_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QhbLUX287KEA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "\n",
        "def plot_metric(metric_results_dict, chosen_th, model_name, x_axis_descr, y_axis_descr):\n",
        "\n",
        "  plt.figure().set_facecolor(\"w\")\n",
        "  plt.style.use('default')\n",
        "\n",
        "  plt.title(model_name)\n",
        "  plt.xlabel(x_axis_descr)\n",
        "  plt.ylabel(y_axis_descr)\n",
        "\n",
        "  plt.plot(metric_results_dict.keys(), metric_results_dict.values(), '-s', linewidth=1, markersize=2)\n",
        "\n",
        "  plt.plot(chosen_th, metric_results_dict[chosen_th], '-o', markersize=6, color='orange',\n",
        "           label='Score: {:.2f} @ chosen threshold={:.2f}'.format(metric_results_dict[chosen_th], chosen_th))\n",
        "\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "def compute_metrics(anomaly_scores, ground_truth, model_name, num_thresholds=10):\n",
        "\n",
        "  min_threshold = anomaly_scores.min()\n",
        "  max_threshold = anomaly_scores.max()\n",
        "\n",
        "  thresholds = np.linspace(start=min_threshold, stop=max_threshold, num=num_thresholds)\n",
        "\n",
        "  print(\"\\nF1 score\")\n",
        "  f1_res = compute_f1_score(anomaly_scores, ground_truth, thresholds)\n",
        "  for i in f1_res.keys():\n",
        "      print(f\"{i}: {f1_res[i]}\")\n",
        "\n",
        "  maxval = 0\n",
        "  chosen_th = 0\n",
        "  for k in f1_res.keys():\n",
        "    if f1_res[k] > maxval:\n",
        "      maxval = f1_res[k]\n",
        "      chosen_th = k\n",
        "\n",
        "  plot_metric(f1_res, chosen_th, model_name, \"threshold\", \"F1 score\")\n",
        "\n",
        "  print(\"\\nPrecision score:\")\n",
        "  precision_res = compute_precision_score(anomaly_scores, ground_truth, thresholds)\n",
        "  for i in precision_res.keys():\n",
        "      print(f\"{i}: {precision_res[i]}\")\n",
        "\n",
        "  plot_metric(precision_res, chosen_th, model_name, \"threshold\", \"Precision score\")\n",
        "\n",
        "  print(\"\\nRecall score:\")\n",
        "  recall_res = compute_recall_score(anomaly_scores, ground_truth, thresholds)\n",
        "  for i in recall_res.keys():\n",
        "    print(f\"{i}: {recall_res[i]}\")\n",
        "\n",
        "  plot_metric(recall_res, chosen_th, model_name, \"threshold\", \"Recall score\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp ./drive/MyDrive/ml-applications/TimeSeriesAnomalyDetection_project/saved_losses_modifiedModel/test_loss_swat.npy .\n",
        "\n",
        "with open('test_loss_swat.npy', 'rb') as f:\n",
        "  test_loss_array = np.load(f)\n",
        "\n",
        "test_loss_array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPx-NQir9QRR",
        "outputId": "ca9e4613-cf96-48db-b1fe-c4afaedac4ed"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 360.06552124,  360.66635132,  360.48953247, ..., 1415.41967773,\n",
              "       1415.62512207, 1416.81433105])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3XBIdzzN8ewr",
        "outputId": "549d1831-2a83-401f-8dc5-9a4f0b0d5248"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set metrics:\n",
            "\n",
            "F1 score\n",
            "112.2962417602539: 0.3664288437448941\n",
            "281.32319477626254: 0.44822995759787\n",
            "450.35014779227123: 0.7117937784669056\n",
            "619.3771008082799: 0.7843391992441242\n",
            "788.4040538242886: 0.7822168819465305\n",
            "957.4310068402972: 0.7810729355033152\n",
            "1126.457959856306: 0.7795484727755643\n",
            "1295.4849128723145: 0.7775560795695025\n",
            "1464.5118658883232: 0.21121644369628778\n",
            "1633.538818904332: 0.004153070305547316\n",
            "1802.5657719203405: 0.0019798059790140563\n",
            "1971.5927249363492: 0.0017820017820017822\n",
            "2140.619677952358: 0.0013862758688979108\n",
            "2309.6466309683665: 0.0013862758688979108\n",
            "2478.673583984375: 0.00019815713861091847\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Precision score:\n",
            "112.2962417602539: 0.22431152897246115\n",
            "281.32319477626254: 0.29833945917563665\n",
            "450.35014779227123: 0.6964725962450218\n",
            "619.3771008082799: 0.970622624963461\n",
            "788.4040538242886: 0.9934381199450634\n",
            "957.4310068402972: 0.9970760233918129\n",
            "1126.457959856306: 0.9973741118319431\n",
            "1295.4849128723145: 0.9973631146269583\n",
            "1464.5118658883232: 0.9974895397489539\n",
            "1633.538818904332: 1.0\n",
            "1802.5657719203405: 1.0\n",
            "1971.5927249363492: 1.0\n",
            "2140.619677952358: 1.0\n",
            "2309.6466309683665: 1.0\n",
            "2478.673583984375: 1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Recall score:\n",
            "112.2962417602539: 1.0\n",
            "281.32319477626254: 0.9008125247720967\n",
            "450.35014779227123: 0.727804201347602\n",
            "619.3771008082799: 0.6580459770114943\n",
            "788.4040538242886: 0.6450653983353151\n",
            "957.4310068402972: 0.6419936583432422\n",
            "1126.457959856306: 0.6398137138327388\n",
            "1295.4849128723145: 0.6371383273880301\n",
            "1464.5118658883232: 0.11811335711454618\n",
            "1633.538818904332: 0.002080856123662307\n",
            "1802.5657719203405: 0.0009908838684106222\n",
            "1971.5927249363492: 0.0008917954815695601\n",
            "2140.619677952358: 0.0006936187078874356\n",
            "2309.6466309683665: 0.0006936187078874356\n",
            "2478.673583984375: 9.908838684106223e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "test_ground_truth = labels_attack\n",
        "\n",
        "print(\"Test set metrics:\")\n",
        "compute_metrics(test_loss_array, test_ground_truth, \"ModifiedTAnoGAN on SWaT\", num_thresholds=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROC curve"
      ],
      "metadata": {
        "id": "cIEqGHenKdO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def roc_plot(losses, labels, plot_title, num_thresholds=10):\n",
        "  min_threshold = losses.min()\n",
        "  max_threshold = losses.max()\n",
        "\n",
        "  thresholds = np.linspace(start=min_threshold, stop=max_threshold, num=15)\n",
        "\n",
        "  sens = list()\n",
        "  fpr = list()\n",
        "  for threshold in thresholds:\n",
        "    predicted_labels = (losses >= threshold).astype(int)  # losses above threshold are predicted as anomaly\n",
        "\n",
        "    matching = (predicted_labels == labels).astype(int)\n",
        "    tp = matching[predicted_labels==1].astype(int).sum()\n",
        "\n",
        "    cm_anomaly = np.zeros((2, 2))\n",
        "    n_samples = len(losses)\n",
        "    n_not_collisions = n_samples - sum(labels)\n",
        "    n_detected = sum(predicted_labels)\n",
        "\n",
        "    fp = n_detected - tp\n",
        "    fn = sum(labels) - tp\n",
        "    tn = n_not_collisions - fp\n",
        "\n",
        "    sens.append(tp / (tp + fn))\n",
        "    fpr.append(1-tn /(fp + tn))\n",
        "\n",
        "  fig, ax = plt.subplots(1, 1)\n",
        "  ax.plot(fpr, sens)\n",
        "  plt.title(plot_title)"
      ],
      "metadata": {
        "id": "UHuFxJolRgdE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roc_plot(test_loss_array, test_ground_truth, \"ROC curve ModifiedTAnoGAN on SWaT\", 15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "X3SYwWmdKkbD",
        "outputId": "f0177dab-c8cd-4d3c-946e-878f305a753b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hAG6OVM0LqSA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}